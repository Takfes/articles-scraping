url,title,text,keywords,summary,language,authors,publish_date,tags,images,parsed_date,parse_duration
https://medium.com/bigdatarepublic/machine-learning-for-predictive-maintenance-where-to-start-5f3b7586acfb,Machine learning for predictive maintenance: where to start?,"Think about all the machines you use during a year, all of them, from a toaster every morning to an airplane every summer holiday. Now imagine that, from now on, one of them would fail every day. What impact would that have? The truth is that we are surrounded by machines that make our life easier, but we also get more and more dependent on them. Therefore, the quality of a machine is not only based on how useful and efficient it is, but also on how reliable it is. And together with reliability comes maintenance.

When the impact of a failure cannot be afforded, such as a malfunctioning airplane engine for example, the machine is subjected to preventive maintenance, which involves periodic inspection and repair, often scheduled based on time-in-service. The challenge of proper scheduling grows with the complexity of machines: in a system with many components working together and influencing each other’s lifetime, how can we find the right moment when maintenance should be performed so that components are not prematurely replaced but the whole system still stays functioning reliably? Providing an answer to this question is the aim of predictive maintenance, where we seek to build models that quantify the risk of failure for a machine in any moment in time and use this information to improve scheduling of maintenance.

The success of predictive maintenance models depend on three main components: having the right data available, framing the problem appropriately and evaluating the predictions properly.

In this post, we will elaborate the first two points and give insights on how to choose the modelling technique that best fits the question you are trying to answer and the data you have at hand.

DATA COLLECTION

To build a failure model, we require enough historical data that allows us to capture information about events leading to failure. In addition to that, general “static” features of the system can also provide valuable information, such as mechanical properties, average usage and operating conditions. However, more data is not always better. When collecting data to support a failure model, it is important to make an inventory the following:

What are the types of failure that can occur? Which ones will we try to predict?

How does the “failure process” look like? Is it a slow degradation process or an acute one?

Which parts of the machine/system could be related to each type of failure? What can be measured about each of them that reflect their state? How often and with which accuracy do these measurements need to be performed?

The life span of machines is usually in the order of years, which means that data has to be collected for an extended period of time in order to observe the system throughout its degradation process.

In an ideal scenario both data scientists and domain experts would be involved in the data collection plan to ensure that the data gathered is suitable for the model to be built. However, what mostly happens in real life is that the data has already been collected before the data scientist arrives and he/she must try to make the best of what is available.

Depending on the characteristics of the system and on the data available, a proper framing of the model to be built is essential: which question do we want the model to answer and is it possible with the data we have at hand?

PROBLEM FRAMING

When thinking about how to frame a predictive maintenance model, it is important to keep a couple of questions in mind:

What kind of output should the model give?

Is enough historical data available or just static data?

Is every recorded event labelled, i.e. which measurements correspond to good functioning and which ones correspond to failure? Or at least, is it known when each machine failed (if at all)?

When labelled events are available, what is the proportion of the number of events of each type of failure and events of well functioning?

How long in advance should the model be able to indicate that a failure will occur?

What are the performance targets that the model should be optimized for? High precision, high sensitivity/recall, high accuracy? What is the consequence of not predicting a failure or predicting a failure that will not happen?

With all this information at hand, we can now decide which modelling strategy fits best to the available data and the desired output, or at least which one is the best candidate to start with. There are multiple modelling strategies for predictive maintenance and we will describe four of them in relation to the question they aim to answer and which kind of data they require:

Regression models to predict remaining useful lifetime (RUL) Classification models to predict failure within a given time window Flagging anomalous behaviour Survival models for the prediction of failure probability over time

STRATEGY 1: Regression models to predict remaining useful lifetime (RUL)

OUTPUT: How many days/cycles are left before the system fails?

DATA CHARACTERISTICS: Static and historical data are available, and every event is labelled. Several events of each type of failure are present in the dataset.

BASIC ASSUMPTIONS/REQUIREMENTS:

Based on static characteristics of the system and on how it behaves now, the remaining useful time can be predicted, which implies that both static and historical data are required and that the degradation process is smooth.

Just one type of “path to failure” is being modelled: if many types of failure are possible and the system’s behaviour preceding each one of them differs, one dedicated model should be made for each of them.

Labelled data is available and measurements were taken at different moments during the system’s lifetime.

STRATEGY 2: Classification models to predict failure within a given time window

Creating a model which can predict lifetimes very accurate can be very challenging. In practice however, one usually does not need to predict the lifetime very accurate far in the future. Often the maintenance team only needs to know if the machine will fail ‘soon’. This results in the next strategy:

QUESTION: Will a machine fail in the next N days/cycles?

DATA CHARACTERISTICS: Same as for strategy 1

BASIC ASSUMPTIONS/REQUIREMENTS: The assumptions of a classification model are very similar to those of regression models. They mostly differ on:

Since we are defining a failure in a time window instead of an exact time, the requirement of smoothness of the degradation process is relaxed.

Classification models can deal with multiple types of failure, as long as they are framed as a multi-class problem, e.g.: class = 0 corresponding to no failure in the next n days, class = 1 for failure type 1 in the next n days, class = 2 for failure type 2 in the next n days and so forth.

Labelled data is available and there are “enough” cases of each type of failure to train and evaluate the model.

In general, what regression and classification models are doing is modelling the relationship between features and the degradation path of the system. That means that if the model is applied to a system that will exhibit a different type of failure not present in the training data, the model will fail to predict it.

STRATEGY 3: Flagging anomalous behaviour

Both previous strategies require a lot of examples of both normal behaviour (of which we often have a lot of) and examples of failures. However, how many planes will you let crash to collect data? If you have mission critical systems, in which acute repairs are difficult, there are often only limited, or no examples of failures at all. In this case, a different strategy is necessary:

QUESTION: Is the behaviour shown normal?

DATA CHARACTERISTICS: Static and historical data are available, but either labels are unknown or too few failure events were observed or there are too many types of failure

BASIC ASSUMPTIONS/REQUIREMENTS: It is possible to define what normal behaviour is and the difference between current and “normal” behaviour is related to degradation leading to failure.

The generality of an anomaly detection model is both its biggest advantage and pitfall: the model should be able to flag every type of failure, despite of not having any previous knowledge about them. Anomalous behaviour, however, does not necessarily lead to failure. And if it does, the model does not give information about the time span it should occur.

The evaluation of an anomaly detection model is also challenging due to the lack of labelled data. If at least some labelled data of failure events is available, it can and should be used for evaluating the algorithm. When no labelled data is available, the model is usually made available and domain experts provide feedback on the quality of its anomaly flagging ability.

STRATEGY 4: Survival models for the prediction of failure probability over time

The previous three approaches focus on prediction, giving you enough information to apply maintenance before failure. If you however are interested in the degradation process itself and the resulting failure probability, this last strategy suits you best.

QUESTION: Given a set of characteristics, how does the risk of failure change in time?

DATA CHARACTERISTICS: Static data available, information on the reported failure time of each machine or recorded date of when a given machine became unobservable for failure.

A survival model estimates the probability of failure for a given type of machine given static features and is also useful to analyse the impact of certain features on lifetime. It provides, therefore, estimates for a group of machines of similar characteristics. Therefore, for a specific machine under investigation it does not take its specific current status into account.

Bottom line:

What is the most suitable approach for a predictive maintenance model? As for all other data science problems, there is no free lunch! The advice here is to start by understanding which types of failure you are trying to model, which type of output you would like the model to give and which kind of data is available. Having put all this put together with the advice given above, I hope you now know from where to start!

Some useful links:

Survival analysis in scikit-learn: https://github.com/sebp/scikit-survival

Imbalanced classes: https://svds.com/learning-imbalanced-classes/

Novelty and outlier detection on scikit-learn: http://scikit-learn.org/stable/modules/outlier_detection.html

BigData Republic provides these type of Big Data Solutions. We are experienced in advanced predictive modeling and deploying large-scale big data pipelines. Interested in what we can do for you? Feel free to contact us.","['machine', 'models', 'failure', 'type', 'static', 'learning', 'maintenance', 'data', 'model', 'available', 'system', 'start', 'predictive']","The success of predictive maintenance models depend on three main components: having the right data available, framing the problem appropriately and evaluating the predictions properly.
PROBLEM FRAMINGWhen thinking about how to frame a predictive maintenance model, it is important to keep a couple of questions in mind:What kind of output should the model give?
: class = 0 corresponding to no failure in the next n days, class = 1 for failure type 1 in the next n days, class = 2 for failure type 2 in the next n days and so forth.
DATA CHARACTERISTICS: Static data available, information on the reported failure time of each machine or recorded date of when a given machine became unobservable for failure.
Bottom line:What is the most suitable approach for a predictive maintenance model?",en,['Bigdata Republic'],2018-03-06 08:10:08.893000+00:00,"{'Big Data', 'Data Science'}","{'https://miro.medium.com/fit/c/80/80/1*uLt7KmAcmllD88BlXmYfnQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*DRFsgdI0ByEck1Qx2zaKGQ.jpeg', 'https://miro.medium.com/fit/c/160/160/2*C8a3XXK0p1XwyNB99ut-uw.png', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/96/96/2*C8a3XXK0p1XwyNB99ut-uw.png', 'https://miro.medium.com/fit/c/160/160/1*15z1Vbg_K1RLZLSlIzM5Qg.png', 'https://miro.medium.com/fit/c/80/80/0*TLLAvqqo9cEusYNY', 'https://miro.medium.com/max/146/1*XQIZETPN2FQWGo90KbDbHA.png'}",2020-03-05 00:05:35.722459,0.9081218242645264
https://medium.com/@andykashyap/top-5-tricks-to-make-plots-look-better-9f6e687c1e08,Top 5 tricks to make plots look better.,"Top 5 tricks to make plots look better.

A big part of being a good data scientist is having the ability to convey your point using relevant and beautiful graphs and visualizations. Visualizations take months of practice to nail down. But there are some low hanging fruits that can help improve the look & feel of your graphs immediately. We will dive below into some of the easy customizations that can be done to improve the way your graphs look.

Setting the theme of your plots

This is one of the easiest ways of changing the look and feel of your graphs. There are several themes available. An easy way to look at what is available is by this code

import matplotlib.style as style style.available

style.available

The output of which looks like this:

['seaborn-dark',

'seaborn-darkgrid',

'seaborn-ticks',

'fivethirtyeight',

'seaborn-whitegrid',

'classic',

'_classic_test',

'seaborn-talk',

'seaborn-dark-palette',

'seaborn-bright',

'seaborn-pastel',

'grayscale',

'seaborn-notebook',

'ggplot',

'seaborn-colorblind',

'seaborn-muted',

'seaborn',

'seaborn-paper',

'bmh',

'seaborn-white',

'dark_background',

'seaborn-poster',

'seaborn-deep']

You can set the style of your plot (this is a global setting — will apply to all cells in your jupyter notebook) by the following syntax:

style.use('seaborn-poster') #sets the size of the charts

style.use('ggplot')

Here is how the transformation looks:

The default setting

With ggplot styling

2. Changing the color of your bars/lines

Changing the color of your bars makes a big difference in visualization. Often times, if you want one bar to stand out, changing its color to a sharper contrast helps make it stand out. Below we will learn simple tricks to change the color of your bars.

First let us have a look @ what colors can be called by name:

import matplotlib

for name, hex in matplotlib.colors.cnames.items():

colorname.append(name)

colorid.append(hex)

and to see them in a list

zippedcolors = list(zip(colorname, colorid))

zippedcolors = sorted(zippedcolors, key=lambda x: x[1])

The list of 954 colors looks like this:

[('black', '#000000'),

('navy', '#000080'),

('darkblue', '#00008B'),

('mediumblue', '#0000CD'),

('blue', '#0000FF'),

('darkgreen', '#006400'),

('green', '#008000'),

('teal', '#008080'),

('darkcyan', '#008B8B'),

('deepskyblue', '#00BFFF'),

('darkturquoise', '#00CED1'),

('mediumspringgreen', '#00FA9A'),

('lime', '#00FF00'),

('springgreen', '#00FF7F'),

('aqua', '#00FFFF'),

('cyan', '#00FFFF'),

('midnightblue', '#191970'),

('dodgerblue', '#1E90FF'),

('lightseagreen', '#20B2AA'),

('forestgreen', '#228B22'),

('seagreen', '#2E8B57'),

('darkslategray', '#2F4F4F'),

('darkslategrey', '#2F4F4F'),

('limegreen', '#32CD32'),

('mediumseagreen', '#3CB371'),

('turquoise', '#40E0D0'),

('royalblue', '#4169E1'),

('steelblue', '#4682B4'),

('darkslateblue', '#483D8B'),

('mediumturquoise', '#48D1CC'),

('indigo', '#4B0082'),

...]

An example where the color can be changed is as follows:

barplot = plt.bar(y_pos, odds_number, color = 'darkgreen', alpha = 0.85)

barplot[0].set_color('darkred')

plt.xlabel('Factors', fontsize = 15, weight = 'bold')

Changing the color of one bar makes it stand out

What do the other colors look like?

Example of the various colors available.

3. Changing the font family

Matplotlib offers various font families. It is worth exploring the various font family to make sure that the graphs blend in with the presentation font. So, if you are making your presentation in ‘Georgia’, then use font-family as Serif

matplotlib.rcParams['font.family'] = ""serif""

To get a list of font families available for matplotlib, you can use the following code:

[f.name for f in matplotlib.font_manager.fontManager.afmlist]

4. Setting the style in seaborn

Seaborn has an excellent setting called style that formats the size of the text, plot and titles according to the style that is set. This helps scale all visible aspects of a graph instead of individually changing each of them.

sns.set_context('poster') #Everything is larger sns.set_context('paper') #Everything is smaller sns.set_context('talk') #Everything is sized for a presentation

The fonts look small don’t they? Set_context — Paper

Now they look just fine! Set_context — Poster

5. Choosing a Palette

Choosing a color palette helps in setting the overal tone of your plots. Do you want your bar plots to be in complete harmony with the theme of your presentation? Do red bar plots not look good with a blue theme and does blue color look too gaudy?

Have no fear, color palettes are here. Color palettes change the entire theme of a plot’s colors. Here is an example:

This doesn’t look too good. The colors are all over the place. Giving it a theme using palettes is a good idea!

ax = sns.barplot(y= ""Deaths"", x = ""Causes"", data = deaths_pd, palette=(""Blues_d""))

sns.set_context(""poster"")

Voila! This looks so much better than the rainbow graph above.

Here is a list of the available palettes & how they look:

Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Vega10, Vega10_r, Vega20, Vega20_r, Vega20b, Vega20b_r, Vega20c, Vega20c_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spectral, spectral_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r

Hope this helps you improving the quality of your graph. We will dive deeper into making visualizations better with some more examples in the next blog post.","['graphs', 'theme', 'changing', 'setting', 'plots', 'colors', 'better', 'style', 'color', 'font', 'tricks', 'look']","Top 5 tricks to make plots look better.
We will dive below into some of the easy customizations that can be done to improve the way your graphs look.
Setting the theme of your plotsThis is one of the easiest ways of changing the look and feel of your graphs.
Do red bar plots not look good with a blue theme and does blue color look too gaudy?
Color palettes change the entire theme of a plot’s colors.",en,['Anirudh Kashyap'],2017-12-22 11:43:38.497000+00:00,"{'Data Science', 'Design', 'Data Visualization', 'Data', 'Visualization'}","{'https://miro.medium.com/max/60/1*5DV7SUzdqaPVJqVwt0pMdg.png?q=20', 'https://miro.medium.com/max/60/1*SI0qDBSkTPpuozwa6BGhbA.png?q=20', 'https://miro.medium.com/max/1840/1*wayQvu2lEwxse0mPQi3xdw.png', 'https://miro.medium.com/max/792/1*o2yNiv79Sdo3nA7HUZBmHA.png', 'https://miro.medium.com/max/60/1*l0u_ifhYnMjh4KSnUBW-7A.png?q=20', 'https://miro.medium.com/max/1434/1*VGpgAIR50A8rJPfBaT0iPQ.png', 'https://miro.medium.com/fit/c/80/80/1*JySEHXhHzsvI1pOljFeyfw.jpeg', 'https://miro.medium.com/max/60/1*wayQvu2lEwxse0mPQi3xdw.png?q=20', 'https://miro.medium.com/max/1638/1*SI0qDBSkTPpuozwa6BGhbA.png', 'https://miro.medium.com/max/60/1*o2yNiv79Sdo3nA7HUZBmHA.png?q=20', 'https://miro.medium.com/max/2326/1*l0u_ifhYnMjh4KSnUBW-7A.png', 'https://miro.medium.com/max/396/1*o2yNiv79Sdo3nA7HUZBmHA.png', 'https://miro.medium.com/fit/c/96/96/0*68caKZPKFWMIs3O0.', 'https://miro.medium.com/fit/c/80/80/1*UtThEsUH-4zPoIp_9T-hug@2x.jpeg', 'https://miro.medium.com/max/60/1*vpwdusq7MfgDyIn1L9LRRA.png?q=20', 'https://miro.medium.com/max/60/1*2EMfGGHBnwW6TTe2K4KJcw.png?q=20', 'https://miro.medium.com/max/1840/1*2EMfGGHBnwW6TTe2K4KJcw.png', 'https://miro.medium.com/max/1402/1*5DV7SUzdqaPVJqVwt0pMdg.png', 'https://miro.medium.com/fit/c/160/160/0*68caKZPKFWMIs3O0.', 'https://miro.medium.com/max/1154/1*vpwdusq7MfgDyIn1L9LRRA.png', 'https://miro.medium.com/fit/c/80/80/2*GljF8L_ld119qC4OywbOHg.png', 'https://miro.medium.com/max/60/1*VGpgAIR50A8rJPfBaT0iPQ.png?q=20'}",2020-03-05 00:05:37.840357,2.1178975105285645
https://medium.com/@vaibhavshukla182/want-to-know-the-diff-among-pd-factorize-a8591eb3347d,"Want to know the diff among pd.factorize, pd.get_dummies, sklearn.preprocessing.LableEncoder and OneHotEncoder","These four encoders can be split in two categories:

Encode labels into categorical variables : Pandas factorize and scikit-learn LabelEncoder . The result will have 1 dimension (It is important as it could be used while implementing any model)

: Pandas and scikit-learn . The result will have (It is important as it could be used while implementing any model) Encode categorical variable into dummy/indicator (binary) variables: Pandas get_dummies and scikit-learn OneHotEncoder . The result will have n dimensions, one by distinct value of the encoded categorical variable.

The main difference between pandas and scikit-learn encoders is that scikit-learn encoders are made to be used in scikit-learn pipelines with fit and transform methods.

Encode labels into categorical variables

Pandas factorize and scikit-learn LabelEncoder belong to the first category. They can be used to create categorical variables for example to transform characters into numbers.

from sklearn import preprocessing

# Test data

df = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])

df['Fact'] = pd.factorize(df['Col'])[0]

le = preprocessing.LabelEncoder()

df['Lab'] = le.fit_transform(df['Col'])



print(df)

# Col Fact Lab

# 0 A 0 0

# 1 B 1 1

# 2 B 1 1

# 3 C 2 2

Encode categorical variable into dummy/indicator (binary) variables

Pandas get_dummies and scikit-learn OneHotEncoder belong to the second category. They can be used to create binary variables. OneHotEncoder can only be used with categorical integers while get_dummies can be used with other type of variables.","['onehotencoder', 'pdfactorize', 'know', 'encoders', 'pandas', 'variables', 'used', 'pdget_dummies', 'sklearnpreprocessinglableencoder', 'scikitlearn', 'get_dummies', 'diff', 'categorical', 'b', 'result']","These four encoders can be split in two categories:Encode labels into categorical variables : Pandas factorize and scikit-learn LabelEncoder .
The result will have (It is important as it could be used while implementing any model) Encode categorical variable into dummy/indicator (binary) variables: Pandas get_dummies and scikit-learn OneHotEncoder .
The main difference between pandas and scikit-learn encoders is that scikit-learn encoders are made to be used in scikit-learn pipelines with fit and transform methods.
They can be used to create categorical variables for example to transform characters into numbers.
OneHotEncoder can only be used with categorical integers while get_dummies can be used with other type of variables.",en,['Vaibhav Shukla'],2018-06-07 01:19:30.825000+00:00,"{'Vaibhav', 'Pandas', 'Data Science', 'Artificial Intelligence', 'Machine Learning'}","{'https://miro.medium.com/fit/c/80/80/1*z2adtYxryCIdzWpgPRSmQA.jpeg', 'https://miro.medium.com/fit/c/96/96/1*z2adtYxryCIdzWpgPRSmQA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*VmdbajrpX9nwOc9UtkV3Yg.png', 'https://miro.medium.com/fit/c/160/160/1*z2adtYxryCIdzWpgPRSmQA.jpeg', 'https://miro.medium.com/max/1644/1*7ltTcHfoXg9_YAfbCumXHw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*irc94hqKmKE1RETkapA97Q.png', 'https://miro.medium.com/max/60/1*7ltTcHfoXg9_YAfbCumXHw.jpeg?q=20', 'https://miro.medium.com/max/822/1*7ltTcHfoXg9_YAfbCumXHw.jpeg'}",2020-03-05 00:05:38.969412,1.1280627250671387
https://medium.com/jbennetcodes/how-to-rewrite-your-sql-queries-in-pandas-and-more-149d341fc53e,"How to rewrite your SQL queries in Pandas, and more","Fifteen years ago, there were only a few skills a software developer would need to know well, and he or she would have a decent shot at 95% of the listed job positions. Those skills were:

Object-oriented programming.

Scripting languages.

JavaScript, and…

SQL.

SQL was a go-to tool when you needed to get a quick-and-dirty look at some data, and draw preliminary conclusions that might, eventually, lead to a report or an application being written. This is called exploratory analysis.

These days, data comes in many shapes and forms, and it’s not synonymous with “relational database” anymore. You may end up with CSV files, plain text, Parquet, HDF5, and who knows what else. This is where Pandas library shines.

What is Pandas?

Python Data Analysis Library, called Pandas, is a Python library built for data analysis and manipulation. It’s open-source and supported by Anaconda. It is particularly well suited for structured (tabular) data. For more information, see http://pandas.pydata.org/pandas-docs/stable/index.html.

What can I do with it?

All the queries that you were putting to the data before in SQL, and so many more things!

Great! Where do I start?

This is the part that can be intimidating for someone used to expressing data questions in SQL terms.

SQL is a declarative programming language: https://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Declarative_languages.

With SQL, you declare what you want in a sentence that almost reads like English.

Pandas’ syntax is quite different from SQL. In Pandas, you apply operations on the dataset, and chain them, in order to transform and reshape the data the way you want it.

We’re going to need a phrasebook!

The anatomy of a SQL query

A SQL query consists of a few important keywords. Between those keywords, you add the specifics of what data, exactly, you want to see. Here is a skeleton query without the specifics:

SELECT… FROM… WHERE…

GROUP BY… HAVING…

ORDER BY…

LIMIT… OFFSET…

There are other terms, but these are the most important ones. So how do we translate these terms into Pandas?

First we need to load some data into Pandas, since it’s not already in database. Here is how:

I got this data at http://ourairports.com/data/.

SELECT, WHERE, DISTINCT, LIMIT

Here are some SELECT statements. We truncate results with LIMIT, and filter them with WHERE. We use DISTINCT to remove duplicated results.

SELECT with multiple conditions

We join multiple conditions with an &. If we only want a subset of columns from the table, that subset is applied in another pair of square brackets.

ORDER BY

By default, Pandas will sort things in ascending order. To reverse that, provide ascending=False.

IN… NOT IN

We know how to filter on a value, but what about a list of values — IN condition? In pandas, .isin() operator works the same way. To negate any condition, use ~.

GROUP BY, COUNT, ORDER BY

Grouping is straightforward: use the .groupby() operator. There’s a subtle difference between semantics of a COUNT in SQL and Pandas. In Pandas, .count() will return the number of non-null/NaN values. To get the same result as the SQL COUNT, use .size().

Below, we group on more than one field. Pandas will sort things on the same list of fields by default, so there’s no need for a .sort_values() in the first example. If we want to use different fields for sorting, or DESC instead of ASC, like in the second example, we have to be explicit:

What is this trickery with .to_frame() and .reset_index()? Because we want to sort by our calculated field (size), this field needs to become part of the DataFrame. After grouping in Pandas, we get back a different type, called a GroupByObject. So we need to convert it back to a DataFrame. With .reset_index(), we restart row numbering for our data frame.

HAVING

In SQL, you can additionally filter grouped data using a HAVING condition. In Pandas, you can use .filter() and provide a Python function (or a lambda) that will return True if the group should be included into the result.

Top N records

Let’s say we did some preliminary querying, and now have a dataframe called by_country, that contains the number of airports per country:

In the next example, we order things by airport_count and only select the top 10 countries with the largest count. Second example is the more complicated case, in which we want “the next 10 after the top 10”:

Aggregate functions (MIN, MAX, MEAN)

Now, given this dataframe or runway data:

Calculate min, max, mean, and median length of a runway:

A reader pointed out that SQL does not have median function. Let’s pretend you wrote a user-defined function to calculate this statistic (since the important part here is syntactic differences between SQL and Pandas).

You will notice that with this SQL query, every statistic is a column. But with this Pandas aggregation, every statistic is a row:

Nothing to worry about —simply transpose the dataframe with .T to get columns:

JOIN

Use .merge() to join Pandas dataframes. You need to provide which columns to join on (left_on and right_on), and join type: inner (default), left (corresponds to LEFT OUTER in SQL), right (RIGHT OUTER), or outer (FULL OUTER).

UNION ALL and UNION

Use pd.concat() to UNION ALL two dataframes:

To deduplicate things (equivalent of UNION), you’d also have to add .drop_duplicates().

INSERT

So far, we’ve been selecting things, but you may need to modify things as well, in the process of your exploratory analysis. What if you wanted to add some missing records?

There’s no such thing as an INSERT in Pandas. Instead, you would create a new dataframe containing new records, and then concat the two:

UPDATE

Now we need to fix some bad data in the original dataframe:

DELETE

The easiest (and the most readable) way to “delete” things from a Pandas dataframe is to subset the dataframe to rows you want to keep. Alternatively, you can get the indices of rows to delete, and .drop() rows using those indices:

Immutability

I need to mention one important thing — immutability. By default, most operators applied to a Pandas dataframe return a new object. Some operators accept a parameter inplace=True, so you can work with the original dataframe instead. For example, here is how you would reset an index in-place:

However, the .loc operator in the UPDATE example above simply locates indices of records to updates, and the values are changed in-place. Also, if you updated all values in a column:

or added a new calculated column:

these things would happen in-place.

And more!

The nice thing about Pandas is that it’s more than just a query engine. You can do other things with your data, such as:

Export to a multitude of formats:

Plot it:

to see some really nice charts!

Share it.

The best medium to share Pandas query results, plots and things like this is Jupyter notebooks (http://jupyter.org/). In facts, some people (like Jake Vanderplas, who is amazing), publish the whole books in Jupyter notebooks: https://github.com/jakevdp/PythonDataScienceHandbook.

It’s that easy to create a new notebook:

After that:

- navigate to localhost:8888

- click “New” and give your notebook a name

- query and display the data

- create a GitHub repository and add your notebook (the file with .ipynb extension).

GitHub has a great built-in viewer to display Jupyter notebooks with Markdown formatting.

And now, your Pandas journey begins!

I hope you are now convinced that Pandas library can serve you as well as your old friend SQL for the purposes of exploratory data analysis — and in some cases, even better. It’s time to get your hands on some data to query!","['things', 'rewrite', 'queries', 'pandas', 'need', 'sql', 'example', 'data', 'count', 'values', 'query', 'dataframe']","All the queries that you were putting to the data before in SQL, and so many more things!
The anatomy of a SQL queryA SQL query consists of a few important keywords.
You will notice that with this SQL query, every statistic is a column.
By default, most operators applied to a Pandas dataframe return a new object.
The best medium to share Pandas query results, plots and things like this is Jupyter notebooks (http://jupyter.org/).",en,['Irina Truong'],2019-10-30 16:08:53.862000+00:00,"{'Coding', 'Data Science', 'Software Development', 'Python', 'Sql'}","{'https://miro.medium.com/fit/c/80/80/1*mt7aid3ff3k1tLXbPL__dQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*s1bvWz3OeTK9xTxRpohTeg.png', 'https://miro.medium.com/max/60/1*gKYCyrcudAeE5e5KAbRhBQ.jpeg?q=20', 'https://miro.medium.com/max/60/0*wiV3vIJWP7_c3sT7.?q=20', 'https://miro.medium.com/max/60/0*dl1ZaGt2fYUDlfIL.?q=20', 'https://miro.medium.com/max/1092/0*7BtzYznnc0Eu5Ghv.', 'https://miro.medium.com/max/824/0*dl1ZaGt2fYUDlfIL.', 'https://miro.medium.com/max/600/1*VSUZqrAzg3FxexZxzaTzOA.png', 'https://miro.medium.com/max/616/0*hONoWL47JSn4LdwW.', 'https://miro.medium.com/fit/c/160/160/1*s1bvWz3OeTK9xTxRpohTeg.png', 'https://miro.medium.com/max/60/0*7BtzYznnc0Eu5Ghv.?q=20', 'https://miro.medium.com/max/2190/1*gKYCyrcudAeE5e5KAbRhBQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*6sNsnaRpF-6jQahOaFju1g.png', 'https://miro.medium.com/max/60/0*5uJqmyB2KdwpsoY5.?q=20', 'https://miro.medium.com/max/1206/0*wiV3vIJWP7_c3sT7.', 'https://miro.medium.com/max/480/1*VSUZqrAzg3FxexZxzaTzOA.png', 'https://miro.medium.com/max/60/0*hONoWL47JSn4LdwW.?q=20', 'https://miro.medium.com/max/1095/1*gKYCyrcudAeE5e5KAbRhBQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*s1bvWz3OeTK9xTxRpohTeg.png', 'https://miro.medium.com/max/310/0*5uJqmyB2KdwpsoY5.'}",2020-03-05 00:05:40.897163,1.927750825881958
https://towardsdatascience.com/dplyr-style-data-manipulation-with-pipes-in-python-380dcb137000,dplyr-style Data Manipulation with Pipes in Python,"Piping

Let’s say you want to perform n discrete transformation operations on your dataset before outputting the final result. The most common way is to perform the operations step by step and store the result of each step in a variable. The variable holding the intermediate result is then used in the next step of the transformation pipeline. Let’s take a look at an abstract example.

# 'original_data' could be a pandas DataFrame.

result_1 = transformation_1(original_data, *args, **kwargs)

result_2 = transformation_2(result_1, *args, **kwargs)

result_3 = transformation_3(result_2, *args, **kwargs)

.

.

.

final_result = transformation_n(result_n-1, *args, **kwargs)

This isn’t very elegant code and it can get confusing and messy to write. This is where piping comes to the rescue. Piping allows us to rewrite the above code without needing those intermediate variables.

final_result = original_data -->

transformation_1(*args, **kwargs) -->

transformation_2(*args, **kwargs) -->

transformation_3(*args, **kwargs) -->

.

.

.

transformation_n(*args, **kwargs)

Magic?! No, it isn’t. Piping works by implicitly making the output of one stage the input of the following stage. In other words, each transformation step works on the transformed result of its previous step.

Piping with dfply

dfply allows chaining multiple operations on a pandas DataFrame with the >> operator. One can chain operations and assign the final output (a pandas DataFrame, since dfply works directly on DataFrames) to a variable. In dfply, the DataFrame result of each step of a chain of operations is represented by X .

For example, if you want to select three columns from a DataFrame in a step, drop the third column in the next step, and then show the first three rows of the final dataframe, you could do something like this:

# 'data' is the original pandas DataFrame

(data >>

select(X.first_col, X.second_col, X.third_col) >>

drop(X.third_col) >>

head(3))

select and drop are both dfply transformation functions, while X represents the result of each transformation step.

Exploring some of dfply’s transformation methods

dfply provides a set of functions for selecting and dropping columns, subsetting and filtering rows, grouping data, and reshaping data, to name a few.

Select and drop columns with select() and drop()

Occassionally, you will work on datasets with a lot of columns, but only a subset of the columns will be of interest; select() allows you to select these columns.

For example, to select the origin , dest , and hour columns in the flight_data DataFrame we loaded earlier, we do:

(flight_data >>

select(X.origin, X.dest, X.hour))

drop() is the inverse of select() . It returns all the columns except those passed in as arguments.

For example, to get all the columns except the year , month , and day columns:

(flight_data >>

drop(X.year, X.month, X.day))

You can also drop columns inside the select() method by putting a tilde ~ in front of the column(s) you wish to drop.

For example, to select all but the hour and minute columns in the flight_data DataFrame:

(flight_data >>

select(~X.hour, ~X.minute))

Filter rows with mask()

mask() allows you to select a subset of rows in a pandas DataFrame based on logical criteria. mask() selects all the rows where the criteria is/are true.

For example, to select all flights longer than 10 hours that originated from JFK airport on January 1:","['rows', 'pipes', 'operations', 'transformation', 'pandas', 'manipulation', 'dplyrstyle', 'python', 'columns', 'example', 'data', 'step', 'select', 'dataframe', 'result']","PipingLet’s say you want to perform n discrete transformation operations on your dataset before outputting the final result.
The most common way is to perform the operations step by step and store the result of each step in a variable.
# 'original_data' could be a pandas DataFrame.
In other words, each transformation step works on the transformed result of its previous step.
One can chain operations and assign the final output (a pandas DataFrame, since dfply works directly on DataFrames) to a variable.",en,['Akinkunle Allen'],2018-01-05 17:11:35.566000+00:00,"{'Python Pandas', 'Data Science', 'Data Wrangling', 'Dplyr', 'Dfply'}","{'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/380/1*G9KGiF3_bO_LvIK8dcKkbA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*RRrtP7Ifly015iF28uWEWg.png?q=20', 'https://miro.medium.com/max/56/1*MgnKWLa1pr7oyUTumOiYQA.png?q=20', 'https://miro.medium.com/max/60/1*GHlf98bwbcTuwpcbeMBY8Q.png?q=20', 'https://miro.medium.com/max/60/1*0zbLTkbj8AIJvFVskG85QA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/0*hTN1m3vJoRzQVdjJ.jpg', 'https://miro.medium.com/max/60/1*G9KGiF3_bO_LvIK8dcKkbA.png?q=20', 'https://miro.medium.com/max/360/1*GHlf98bwbcTuwpcbeMBY8Q.png', 'https://miro.medium.com/max/322/1*MgnKWLa1pr7oyUTumOiYQA.png', 'https://miro.medium.com/max/2604/1*RRrtP7Ifly015iF28uWEWg.png', 'https://miro.medium.com/max/2338/1*WZZHuTJajDuxuHK3Cdsdrg.png', 'https://miro.medium.com/max/60/1*BwmfPK5UNf78IfaE6DZddw.png?q=20', 'https://miro.medium.com/max/60/1*WZZHuTJajDuxuHK3Cdsdrg.png?q=20', 'https://miro.medium.com/max/2242/1*BwmfPK5UNf78IfaE6DZddw.png', 'https://miro.medium.com/max/60/1*ElH1TzAM8c0S2OegFB2XoQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2560/1*ElH1TzAM8c0S2OegFB2XoQ.jpeg', 'https://miro.medium.com/max/2358/1*0zbLTkbj8AIJvFVskG85QA.png', 'https://miro.medium.com/max/1200/1*ElH1TzAM8c0S2OegFB2XoQ.jpeg', 'https://miro.medium.com/fit/c/160/160/0*hTN1m3vJoRzQVdjJ.jpg'}",2020-03-05 00:05:43.607378,2.7092156410217285
https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe,How Does Spotify Know You So Well?,"Recommendation Model #1: Collaborative Filtering

First, some background: When people hear the words “collaborative filtering,” they generally think of Netflix, as it was one of the first companies to use this method to power a recommendation model, taking users’ star-based movie ratings to inform its understanding of which movies to recommend to other similar users.

After Netflix was successful, the use of collaborative filtering spread quickly, and is now often the starting point for anyone trying to make a recommendation model.

Unlike Netflix, Spotify doesn’t have a star-based system with which users rate their music. Instead, Spotify’s data is implicit feedback — specifically, the stream counts of the tracks and additional streaming data, such as whether a user saved the track to their own playlist, or visited the artist’s page after listening to a song.

But what is collaborative filtering, truly, and how does it work? Here’s a high-level rundown, explained in a quick conversation:

Image source: Collaborative Filtering at Spotify, by Erik Bernhardsson, ex-Spotify.

What’s going on here? Each of these individuals has track preferences: the one on the left likes tracks P, Q, R, and S, while the one on the right likes tracks Q, R, S, and T.

Collaborative filtering then uses that data to say:

“Hmmm… You both like three of the same tracks — Q, R, and S — so you are probably similar users. Therefore, you’re each likely to enjoy other tracks that the other person has listened to, that you haven’t heard yet.”

Therefore, it suggests that the one on the right check out track P — the only track not mentioned, but that his “similar” counterpart enjoyed — and the one on the left check out track T, for the same reasoning. Simple, right?

But how does Spotify actually use that concept in practice to calculate millions of users’ suggested tracks based on millions of other users’ preferences?

With matrix math, done with Python libraries!

In actuality, this matrix you see here is gigantic. Each row represents one of Spotify’s 140 million users — if you use Spotify, you yourself are a row in this matrix — and each column represents one of the 30 million songs in Spotify’s database.

Then, the Python library runs this long, complicated matrix factorization formula:

Some complicated math…

When it finishes, we end up with two types of vectors, represented here by X and Y. X is a user vector, representing one single user’s taste, and Y is a song vector, representing one single song’s profile.

The User/Song matrix produces two types of vectors: user vectors and song vectors. Image source: From Idea to Execution: Spotify’s Discover Weekly, by Chris Johnson, ex-Spotify.

Now we have 140 million user vectors and 30 million song vectors. The actual content of these vectors is just a bunch of numbers that are essentially meaningless on their own, but are hugely useful when compared.

To find out which users’ musical tastes are most similar to mine, collaborative filtering compares my vector with all of the other users’ vectors, ultimately spitting out which users are the closest matches. The same goes for the Y vector, songs: you can compare a single song’s vector with all the others, and find out which songs are most similar to the one in question.

Collaborative filtering does a pretty good job, but Spotify knew they could do even better by adding another engine. Enter NLP.","['know', 'songs', 'vectors', 'vector', 'users', 'tracks', 'spotify', 'similar', 'does', 'track', 'collaborative', 'filtering']","But what is collaborative filtering, truly, and how does it work?
Here’s a high-level rundown, explained in a quick conversation:Image source: Collaborative Filtering at Spotify, by Erik Bernhardsson, ex-Spotify.
But how does Spotify actually use that concept in practice to calculate millions of users’ suggested tracks based on millions of other users’ preferences?
The same goes for the Y vector, songs: you can compare a single song’s vector with all the others, and find out which songs are most similar to the one in question.
Collaborative filtering does a pretty good job, but Spotify knew they could do even better by adding another engine.",en,['Sophia Ciocca'],2018-04-05 17:44:52.721000+00:00,"{'Music', 'Tech', 'Artificial Intelligence', 'Spotify', 'Machine Learning'}","{'https://miro.medium.com/fit/c/80/80/1*sIRoUgfyKcPbfwZ8GPIp8A.jpeg', 'https://miro.medium.com/max/788/1*oGub3-TXJSNvKz1GQtbJxQ.png', 'https://miro.medium.com/max/60/1*a1a_pG-shrVnvMZefrC-hg.png?q=20', 'https://miro.medium.com/max/60/1*srOKaVeDN8i5uqEQepjPPw.png?q=20', 'https://miro.medium.com/max/60/0*KS_nvbVyvOdQzjyI.?q=20', 'https://miro.medium.com/max/60/0*NXVODvFr8yVL4_fv.?q=20', 'https://miro.medium.com/max/60/1*lys6vccczPSJiyOTiMEp8g.png?q=20', 'https://miro.medium.com/max/60/1*cs6FT4dt3sujiauIKF_HYg.png?q=20', 'https://miro.medium.com/max/1630/1*cs6FT4dt3sujiauIKF_HYg.png', 'https://miro.medium.com/fit/c/160/160/1*y8L8lmSpagtZ-aGZqhCIGA.jpeg', 'https://miro.medium.com/max/1646/1*cp07MRMUjndZsvV7QElSXg.png', 'https://miro.medium.com/fit/c/80/80/2*mBntNutyE9pKaTGsF2hk2Q.jpeg', 'https://miro.medium.com/max/1856/0*zl0-pZtZzslGC-R8.', 'https://miro.medium.com/max/1846/1*F0YJ1c2tBbCIjP13llMqTg.png', 'https://miro.medium.com/max/1132/1*_EU2Q9hPaxtKyzt_KS85FA.png', 'https://miro.medium.com/max/1412/1*srOKaVeDN8i5uqEQepjPPw.png', 'https://miro.medium.com/max/1886/1*kJTtf1i3W2VrWG782_gCFw.png', 'https://miro.medium.com/max/1200/1*shZ8Pwo8_OqDw2Udjb12XA.png', 'https://miro.medium.com/focal/1200/632/69/51/1*FXgfbYapq_y3RPXA2ymRCw.jpeg', 'https://miro.medium.com/max/990/0*NXVODvFr8yVL4_fv.', 'https://miro.medium.com/max/4000/1*Lfl5nMKUwGjhZvC_3vPCKQ.png', 'https://miro.medium.com/max/60/1*cp07MRMUjndZsvV7QElSXg.png?q=20', 'https://miro.medium.com/max/2048/0*KS_nvbVyvOdQzjyI.', 'https://miro.medium.com/fit/c/96/96/1*y8L8lmSpagtZ-aGZqhCIGA.jpeg', 'https://miro.medium.com/max/60/1*Lfl5nMKUwGjhZvC_3vPCKQ.png?q=20', 'https://miro.medium.com/max/60/0*zl0-pZtZzslGC-R8.?q=20', 'https://miro.medium.com/fit/c/80/80/0*vCLJTtpu_ArvSPnZ.', 'https://miro.medium.com/max/60/1*shZ8Pwo8_OqDw2Udjb12XA.png?q=20', 'https://miro.medium.com/max/54/1*_EU2Q9hPaxtKyzt_KS85FA.png?q=20', 'https://miro.medium.com/max/1356/1*lys6vccczPSJiyOTiMEp8g.png', 'https://miro.medium.com/max/60/1*F0YJ1c2tBbCIjP13llMqTg.png?q=20', 'https://miro.medium.com/max/60/1*oGub3-TXJSNvKz1GQtbJxQ.png?q=20', 'https://miro.medium.com/max/60/1*kJTtf1i3W2VrWG782_gCFw.png?q=20', 'https://miro.medium.com/max/1796/1*a1a_pG-shrVnvMZefrC-hg.png'}",2020-03-05 00:05:44.592975,0.9846405982971191
https://medium.com/ml-everything/using-python-and-linear-programming-to-optimize-fantasy-football-picks-dc9d1229db81,Using Python and Linear Programming to Optimize Fantasy Football Picks,"I’m not a big sports fan but I always liked the numbers. That’s why I was interested in Fantasy Football. It struck me as a relatively simple optimization problem. And with the rise of DraftKings and FanDuel, I figured there would be a lot of historical information available.

The data was very easy to get and I was able to generalize the problem into an optimization problem. I then used PuLP to solve the problem.

To skip to the code, skip to Fantasy Football Using Linear Programming or check out my jupyter notebook.

Linear Programming

If you’re familiar with linear programming, feel free to skip this section.

Linear programming is a method to achieve the best outcome of a given function given a series of constraints. The goal and constraints require linear relationships to have the math work in your favor.

For instance, suppose you have flour and eggs from which you can make pasta or unleavened bread to sell. Assume that pasta and bread units are in pounds and we can make partial pounds.

flour = 30

eggs = 40 pasta = flour * 2.5 + egg * 5.0

bread = flour * 3.5 + egg * 2.5 pasta_sale_price = 3

bread_sale_price = 2.5

Now consider the constraints and objectives.

Constraints:

pasta * 2.5 + bread * 3.5 <= 30 # flour

pasta * 5.0 + bread * 2.5 <= 40 # eggs

bread >= 0

pasta >= 0 Objective Function:

maximize pasta * 3 + bread * 2.5 Solve for pasta and bread such that revenue is maximized.

If we swap out the pasta and bread with x and y, respectively, we can graph all the possible solutions with the following equation:

2.5x + 3.5y <= 30 and 5.0x + 2.5y <= 40 and x >= 0 and y >= 0

WolframAlpha tells me the chart looks something like this:

The shaded area is the feasible region. Everything inside the region is sub-optimal. We only care about the edge. Since everything is linear, we can actually get away with only looking at the vertices.

Consider the extremes of putting all resources into making pasta or bread alone:

100% pasta:

flour_constraint: 2.5x = 30 => x = 12

egg_constraint: 5.0x = 40 => x = 8

x = min(flour_constraint, egg_constraint) => 8

profit = 8 * 3 + 0 * 2.5 => 24 100% bread:

flour_constraint: 3.5x = 30 => x = 8.57

egg_constraint: 2.5x = 40 => x = 16

y = min(flour_constraint, egg_constraint) => 8.57

profit = 0 * 3 + 8.57 * 2.5 => 21.42

If you notice, producing solely pasta or bread, we are leaving some ingredients unused. In fact, there is only one point at which we use 100% of all our ingredients: 5.78 units of pasta and 4.44 units of bread (i.e. the final vertex).

x = 5.78

y = 4.44 2.5 * 5.78 + 3.5 * 4.44 = 30 # 100% of flour

5.0 * 5.78 + 2.5 * 4.44 = 40 # 100% of eggs profit = 5.78 * 3 + 4.44 * 2.5 => 28.44

Making 5.78 pounds of pasta and 4.44 pounds of bread is the best solution, resulting in revenue of $28.44 given our constraints.

Just because we use 100% of the ingredients doesn’t mean that’s necessarily the best choice. In this case it is, but it depends on the objective function. If the sales price of pasta were 10 times that of the bread, then it would make sense to make more pasta even if it means leaving unused ingredients.

This just scratches the surface of linear programming. If we had more variables, we could add more dimensions. In this example we only have 2, but everything generalizes to any number of dimensions. More constraints would just mean a different shape. For instance, we could have made a constraint that said no more than 6 pounds of pasta and that would just be a vertical line at x = 6. Our feasible region would only included the shaded are to the left of that. More examples can be found here.

We can use these same principles when deciding on our Fantasy Football pool.

Fantasy Football in a Nutshell

The fantasy football I experiment with (DraftKings) consists of picking players to make up your team. Your score is a function of the players that you picked. The function isn’t so important but it’s based on their real world performance. For instance, 1 point per 25 passing yards, 4 points for a passing touchdown, and so on. You’re limited by a salary cap and positions (1 quarterback, 2 running backs, 3 wide-receivers, 1 tight end, 1 flex and 1 defensive special teams).

DraftKings makes it easy for people. They give you the average points per game for each player for the season. A general strategy involves going through each position, looking at the best historical performers (most points in the last game), and doing some kind of cost analysis (e.g.absolute points or points / salary). Here some sports fans may let their own biases influence their choices. Also, someone may consider the athlete’s opponent this week. Finally, players try to get as close as possible to their salary cap.

Fantasy Football Using Linear Programming

I’ll be using python, pandas and PuLP to make my decision. We’ll be working off the naive assumption that whatever the person scored last time, he will score this time. We’ll optimize for the highest possible score given our salary and position constraints.

Note that API results may have changed since I ran this. The value they provide is season average so as games played, this value updates.

First we have to download and clean up the data a bit.

import urllib, json

import pandas as pd

import re

from itertools import permutations

from pulp import * LATEST_URL = "" https://api.draftkings.com/draftgroups/v1/draftgroups/21434/draftables?format=json response = urllib.request.urlopen(LATEST_URL)

data = json.loads(response.read())

current = pd.DataFrame.from_dict(data[""draftables""]) # Remove players that are out or questionable

current = current[current.status == ""None""]

DraftKings has a Flex position that can be filled by any running back, wide receiver or tight end. Generally a player can only fill one role, so we need to add those eligible to the flex position back to our data frame and label them as position “FLEX”.

# Add flex position

flex = current[current.position.isin([""RB"",""WR"",""TE""])].copy()

flex.position = ""FLEX""

current = pd.concat([current, flex])

The previous points the player scored is nested inside a “draftStatAttributes” field. For instance:

""draftStatAttributes"":[{""id"":90,""value"":""46.1"",""sortValue"":""46.1""},{""id"":-2,""value"":""29th"",""sortValue"":""29"",""quality"":""High""}]

For some reason its in a list. What we want is the “value” float in the list. It’s not always the first element so we need to find a float and extract that.

def get_float(l, key):

"""""" Returns first float value from a list of dictionaries based on key. Defaults to 0.0 """"""

for d in l:

try:

return float(d.get(key))

except:

pass

return 0.0 points = [get_float(x, ""value"") for x in

current.draftStatAttributes]

current[""points""] = points

We now have everything we need. A few of the records are duplicated, so we can trim everything down and group by the fields we need: position, displayName, salary and points.

availables = current[[""position"", ""displayName"", ""salary"",

""points""]].groupby([""position"", ""displayName"", ""salary"",

""points""]).agg(""count"")

availables = availables.reset_index()

availables[availables.position==""QB""].head(15)

Available quarter backs

Since we have a constraint on position (i.e. only one QB, two RB, etc), we need to pivot our salaries and points on position. We also need to define the number of each position we will be constrained to.

salaries = {}

points = {}

for pos in availables.position.unique():

available_pos = availables[availables.position == pos]

salary = list(available_pos[[""displayName"",""salary""]].set_index(""displayName"").to_dict().values())[0]

point = list(available_pos[[""displayName"",""points""]].set_index(""displayName"").to_dict().values())[0]

salaries[pos] = salary

points[pos] = point pos_num_available = {

""QB"": 1,

""RB"": 2,

""WR"": 3,

""TE"": 1,

""FLEX"": 1,

""DST"": 1

}

If we look at the salaries variable, it’s just a dictionary of player names and salaries pivoted on position. points is the same.

salaries

{'QB': {'AJ McCarron': 4600,

'Aaron Rodgers': 6800,

'Alex Smith': 6000,

'Andrew Luck': 6200,

'Baker Mayfield': 4600,

'Ben Roethlisberger': 6900,

'Blaine Gabbert': 4700,

...}

Our salary cap is 50k.

SALARY_CAP = 50000

Now we have to define our variables. We want a variable for each position (e.g. QB). There will be an index for each player and the variable will be binary (0 or 1) meant to represent whether the player is included or excluded.

_vars = {k: LpVariable.dict(k, v, cat=""Binary"") for k, v in points.items()}

The _vars is a dictionary of position and an LpVariable.

_vars

{'QB': {'AJ McCarron': QB_AJ_McCarron,

'Aaron Rodgers': QB_Aaron_Rodgers,

'Alex Smith': QB_Alex_Smith,

'Andrew Luck': QB_Andrew_Luck,

'Baker Mayfield': QB_Baker_Mayfield,

'Ben Roethlisberger': QB_Ben_Roethlisberger,

'Blaine Gabbert': QB_Blaine_Gabbert,

...}

Now we can setup our problem. Our cost will just be our salaries indexed for the player times either 0 (not included) or 1 (included). Same is true for our reward. And finally we have a constraint on the positions available that we had defined earlier.

prob = LpProblem(""Fantasy"", LpMaximize)

rewards = []

costs = []

position_constraints = [] # Setting up the reward

for k, v in _vars.items():

costs += lpSum([salaries[k][i] * _vars[k][i] for i in v])

rewards += lpSum([points[k][i] * _vars[k][i] for i in v])

prob += lpSum([_vars[k][i] for i in v]) <= pos_num_available[k]



prob += lpSum(rewards)

prob += lpSum(costs) <= SALARY_CAP

Now that everything is setup, we can solve:

prob.solve()

The prob object is now solved. It has a variables function that has all our variables and each variable has a varValue which will be either 0 or 1. Below is a helper function to display the results.

def summary(prob):

div = '---------------------------------------

'

print(""Variables:

"")

score = str(prob.objective)

constraints = [str(const) for const in prob.constraints.values()]

for v in prob.variables():

score = score.replace(v.name, str(v.varValue))

constraints = [const.replace(v.name, str(v.varValue)) for const in constraints]

if v.varValue != 0:

print(v.name, ""="", v.varValue)

print(div)

print(""Constraints:"")

for constraint in constraints:

constraint_pretty = "" + "".join(re.findall(""[0-9\.]*\*1.0"", constraint))

if constraint_pretty != """":

print(""{} = {}"".format(constraint_pretty, eval(constraint_pretty)))

print(div)

print(""Score:"")

score_pretty = "" + "".join(re.findall(""[0-9\.]+\*1.0"", score))

print(""{} = {}"".format(score_pretty, eval(score)))

And now for the optimal Fantasy Football picks for the week of 9/10/2018:

summary(prob) Variables:



DST_Jets_ = 1.0

FLEX_DeSean_Jackson = 1.0

QB_Ryan_Fitzpatrick = 1.0

RB_Alvin_Kamara = 1.0

RB_James_Conner = 1.0

TE_Jared_Cook = 1.0

WR_DeSean_Jackson = 1.0

WR_Randall_Cobb = 1.0

WR_Tyreek_Hill = 1.0

---------------------------------------



Constraints:

2500*1.0 + 4900*1.0 + 5500*1.0 + 9500*1.0 + 6700*1.0 + 3600*1.0 + 4900*1.0 + 4600*1.0 + 7600*1.0 = 49800.0

---------------------------------------



Score:

26.0*1.0 + 34.6*1.0 + 45.3*1.0 + 46.1*1.0 + 38.2*1.0 + 30.0*1.0 + 34.6*1.0 + 32.2*1.0 + 45.3*1.0 = 332.3

Note that since these scores are selected based on results from last week, this is a very optimistic score. We essentially picked the best performers and their scores from last week, which likely contain a lot of outliers. In fact, if you check actual results, you’ll see that this lineup performed terribly this week, but I’ll leave that as an exercise to the reader.

Full source is available on Github.

Alternative Methods

How much better is our search versus what most people do? Earlier I said that I would look at the numbers and do a relative kind of comparison, or just pick the top QB in points, then the top RB in points, and so on. That’s called a greedy search. To make it fair, we’ll consider all possible order permutations and pick the best one.

def eval_players(players):

return sum([current[current.displayName == player].iloc[0].points for player in players]) def greedy(val):

remaining = SALARY_CAP

positions = current.position.unique()

best_players = []

best_so_far = -float(""inf"")

for comb_position in permutations(positions):

players = []

for pos in comb_position:

for _ in range(pos_num_available[pos]):

available = current[(~current.displayName.isin(players)) &

(current.position == pos) &

(current.salary <= remaining)]

if available.size > 0:

best = available.sort_values(val,ascending=False).iloc[0]

players.append(best.displayName)

remaining -= best.salary

cur_eval = eval_players(players)

if cur_eval > best_so_far:

best_players = players

best_so_far = cur_eval

return best_players

How does it do?

greedy_points = greedy(""points"")

print(greedy_points)

eval_players(greedy_points) ['Alvin Kamara', 'James Conner', 'Tyreek Hill', 'Michael Thomas', 'DeSean Jackson', 'Ryan Fitzpatrick', 'Jared Cook', 'Jets '] 307.5

About 25 points behind our optimal choice, which isn’t too bad. If you notice there is only 8 choices since we ran out of money by the time we got to the end. Let’s try a points per salary dollar.

points_per_dollar = current.points / current.salary

current[""points_per_dollar""] = points_per_dollar

greedy_points = greedy(""points_per_dollar"")

print(greedy_points)

eval_players(greedy_points) ['James Conner', 'Isaiah Crowell', 'DeSean Jackson', 'Randall Cobb', 'Tyreek Hill', 'Ryan Fitzpatrick', 'Jared Cook', 'Jets ', 'Austin Ekeler']

300.40000000000003

Here we got all 9 players, but a lower score. It’s likely due to leaving money on the table and although the bargains are good values, if you have some extra salary, you would do better picking up that more expensive marginal point.

Final Thoughts

The linear programming method is not likely to bring you into elite status of fantasy football, but it greatly simplifies the problem. The basic assumption we were working on was that the points generated last week will be generated this week. Now all there is left is to solve for how many points we can expect a player to generate. This is a simpler problem that the original problem. We can run a regression, enrich the data with other sources, look at trailing averages or a combination of any number of techniques. The goal is now to predict expected points scored this week. And when we have our numbers, we can run it through this selection method.","['football', 'fantasy', 'position', 'python', 'player', 'points', 'linear', 'programming', 'pasta', 'bread', '25', 'salary', 'x', 'week', 'picks', 'optimize', 'using']","To skip to the code, skip to Fantasy Football Using Linear Programming or check out my jupyter notebook.
Constraints:pasta * 2.5 + bread * 3.5 <= 30 # flourpasta * 5.0 + bread * 2.5 <= 40 # eggsbread >= 0pasta >= 0 Objective Function:maximize pasta * 3 + bread * 2.5 Solve for pasta and bread such that revenue is maximized.
Fantasy Football in a NutshellThe fantasy football I experiment with (DraftKings) consists of picking players to make up your team.
Fantasy Football Using Linear ProgrammingI’ll be using python, pandas and PuLP to make my decision.
Final ThoughtsThe linear programming method is not likely to bring you into elite status of fantasy football, but it greatly simplifies the problem.",en,['Branko Blagojevic'],2018-10-08 09:30:25.936000+00:00,"{'NFL', 'Fantasy Sports', 'Python', 'Fantasy Football', 'Machine Learning'}","{'https://miro.medium.com/fit/c/80/80/0*WD29wq3pp45p5JBV.', 'https://miro.medium.com/max/596/1*4vhRxYRwFw6vS5iWcxeVZQ.png', 'https://miro.medium.com/fit/c/160/160/1*MjvBaktVTSAEhMxpeOpWLw.jpeg', 'https://miro.medium.com/max/60/0*MlUyWKc4d-b57btP?q=20', 'https://miro.medium.com/max/48/1*4vhRxYRwFw6vS5iWcxeVZQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*WD29wq3pp45p5JBV.', 'https://miro.medium.com/max/298/1*4vhRxYRwFw6vS5iWcxeVZQ.png', 'https://miro.medium.com/max/400/0*MlUyWKc4d-b57btP', 'https://miro.medium.com/fit/c/96/96/0*WD29wq3pp45p5JBV.'}",2020-03-05 00:05:45.508946,0.9149718284606934
https://medium.com/@hanishsidhu/whats-so-special-about-catboost-335d64d754ae,What’s so special about CatBoost?,"CatBoost is based on gradient boosting. A new machine learning technique developed by Yandex that outperforms many existing boosting algorithms like XGBoost, Light GBM.

While deep learning algorithms require lots of data and computational power, boosting algorithms are still needed for most business problems. However, boosting algorithms like XGBoost takes hours to train and sometimes you’ll get frustrated while tuning hyper-parameters.

On the other hand, CatBoost is easy to implement and very powerful. It provides excellent results in it’s very first to run. So, let’s find out what so special about CatBoost.

Base tree structure :

One main difference between CatBoost and other boosting algorithms is that the CatBoost implements symmetric trees. This may sound crazy but helps in decreasing prediction time, which is extremely important for low latency environments.

Base Trees are symmetric in CatBoost

Default max_depth = 6

Procedure for other gradient boosting algorithms (XG boost, Light GBM)

Step 1: Consider all (or a sample ) the data points to train a highly biased model.

Step 2: Calculate residuals (errors) for each data point.

Step 3: Train another model with the same data points and corresponding residuals (errors) as class labels.

Step 4: Repeat Step 2 & Step 3 ( for n iterations).

This procedure is prone to overfitting, because we are calculating residuals of each data point by using the model that has already been trained on same set of data points.

CatBoost Procedure

CatBoost does gradient boosting in a very elegant manner. Below is an explanation of CatBoost using a toy example.

Let’s say, we have 10 data points in our dataset and are ordered in time as shown below.

toy dataset

If data doesn’t have time, CatBoost randomly creates an artificial time for each datapoint.

Step 1 : Calculate residuals for each data point using a model that has been trained on all the other data points at that time (For Example, to calculate residual for x5 datapoint, we train one model using x1, x2, x3, and x4 ). Hence we train different models to calculate residuals for different data points. In the end, we are calculating residuals for each data point that the corresponding model has never seen that datapoint before.

: Calculate residuals for each data point using a model that has been trained on all the (For Example, to calculate residual for x5 datapoint, we train one model using x1, x2, x3, and x4 ). Hence we train different models to calculate residuals for different data points. In the end, we are calculating residuals for each data point that the corresponding model has never seen that datapoint before. Step 2 : train the model by using the residuals of each data point as class labels

: train the model by using the residuals of each data point as class labels Step 3: Repeat Step 1 & Step 2 (for n iterations)

For the above toy dataset, we should train 9 different models to get residuals for 9 data points. This is computationally expensive when we have many data points.

Hence by default, instead of training different models for each data point, it trains only log(num_of_datapoints) models. Now if a model has been trained on n data points then that model is used to calculate residuals for the next n data points.

A model that has been trained on the first data point is used for calculating residuals of the second data point.

Another model that has been trained on the first two data points is used for calculating residuals of third and fourth data points

and so on…

In the above toy dataset, now we calculate residuals of x5,x6,x7 and x8 using a model that has been trained on x1, x2,x3, and x4.

All this procedure that I have explained until now is known as ordered boosting.

Random Permutations

CatBoost divides a given dataset into random permutations and applies ordered boosting on those random permutations. By default, CatBoost creates four random permutations. With this randomness, we can further stop overfitting our model. We can further control this randomness by tuning parameter bagging_temperature. This is something that you have already seen in other boosting algorithms.

Handling Categorical Features.

CatBoost has a very good vector representation of categorical data. It takes concepts of ordered boosting and applies the same to response coding .

. In response coding, we represent each categorical feature using the mean to the target values of all the data points with the same categorical feature. We are representing a feature value of the data point with its class label. This leads to target leakage.

the data points with the same categorical feature. We are representing a feature value of the data point with class label. This leads to CatBoost considers only the previous data points to that time and calculates the mean to the target values of those data points having the same categorical feature. Below is a detailed explanation with examples.

let’s take a toy dataset. (all the data points are ordered in time/day)

predicting max_temperature using fearture_1

If data doesn’t have time, CatBoost randomly creates an artificial time for each datapoint.

We have feature 1, a categorical feature that has 3 different categories.

With response coding, we represent cloudy = (15 +14 +20+25)/4 = 18.5

This actually leads to target leakage. Because we are vectorising a data point using target value of the same datapoint.

CatBoost vectorize all the categorical features without any target leakage. Instead of considering all the data points, it will consider only data points that are past in time to a data point. For Example,

On Friday, it represents cloudy = (15+14) /2 = 15.5

On Saturday, it represents cloudy = (15+14+20)/3 = 16.3

But on Tuesday, it represents cloudy = 0/0?

To overcome this, we all knew what Laplace smoothing does in Naive Bayes. CatBoost implements the same.

Below is one another neat example,

In the above dataset, we have a feature with two categories(SDE, PR) and let’s assume that all the data points are ordered in time. For ith data point, we represent SDE as (with some constant added to the numerator and denominator to overcome 0/0 error).

Categorical Feature Combinations

CatBoost combines multiple categorical features. For the most number of times combining two categorical features makes sense. CatBoost does this for you automatically.

In this dataset, there are two features (country and hair length). We can easily observe that whenever a person is from India, his/her hair color black. We can represent those two features into a single feature. In the real world, many categorical features can represent a single feature.

CatBoost does feature combinations by building a base tree with the root node consisting only a single feature and for the child nodes, it randomly selects the other best feature and represents it along with the feature in the root node.

Below is the neat diagram of CatBoost representing two features as a single feature at level 2 of the tree.

the symmetric tree structure in cat boost

At the very first level of the tree, we have a single feature. When the level of tree increases, the number of categorical feature combinations increases proportionally.

One-hot Encoding in CatBoost

By default, CatBoost internally represents all the categorical features with One-hot encoding if and only if a categorical feature has two different categories.

If you would like to implement One-hot encoding on a categorical feature that has N different categories then you can change parameter one_hot_max_size = N.

Handling Numerical Features

CatBoost handles the numerical features in the same way that other tree algorithms do. We select the best possible split based on the Information Gain.

Limitations

At this point, CatBoost doesn’t support sparse matrices .

support sparse matrices When the dataset has many numerical features, CatBoost takes more time to train than Light GBM.

CatBoost in various situations:

While Hyper-parameter tuning is not an important aspect for CatBoost. The most important thing is to set the right parameters based on the problem we are solving. Below are a few important situations.

1. When data is changing over time

We are living in the 21st century where the distribution of data changes recklessly over time. Especially in most of the internet companies, user preferences change over time to time. There are many situations in the real world where data changes over time. CatBoost can perform very well in these situations by setting parameter has_time = True.

2. Low latency requirements

Customer satisfaction is the most important aspect of every business. A user usually expects very fast service from the website/model. CatBoost is the only boosting algorithm with very less prediction time. Thanks to its symmetric tree structure. It is comparatively 8x faster then XGBoost while predicting.

3. Weighting data points

There are some situations where we need to give more importance to certain data points. Especially when you do temporal train-test split, you need the model to train mostly on the earlier data points. When you give more weightage to a data point, It has a higher chance of getting selected in the random permutations.

We can give more weightage to certain data points by setting parameter

For example, you can give linear weightage all the datapoints

sample_weight = [ x for x in range(train.shape[0])]

4. Working with small datasets

There are some instances when you have less number of data points and you need minimal Log-loss. In those situations you can set parameters fold_len_multiplier as close as to 1 (must be >1) and approx_on_full_history =True . With these parameters, CatBoost calculates residuals for each data point using a different model.

5. Working with large datasets

For large datasets, you can train CatBoost on GPUs by setting parameter task_type = GPU. It also supports multi-server distributed GPUs. CatBoost also supports older GPUs that you can train it in Google Colabs.

6. Monitoring Errors / Loss function

It’s a very good practice to monitor your model for every iteration. You can monitor any metrics of your choice along with your optimizing loss function by setting parameter custom_metric=[‘AUC’,‘Logloss’].

You can visualize all the metrics that you did choose to monitor. Make sure that you have installed ipywidgets using pip to visualize plots in Jupyter Notebook and set parameter plot = True.

7. Staged prediction & Shrinking Models

This is again one powerful method provided by CatBoost library. You have trained a model and you want to know how your model predicts at a particular iteration. You can call the staged_predict( ) method to check how your model performs at that stage. If you did notice that in a particular stage that the model is performing better than your final trained model, then you can use a shrink( ) method to shrink the model to that particular stage. Check documentation for more info.

8. Handling different situations

Whether it’s a festival season or week-end or a normal day, the model should predict the best results in every given situation.

For this kind of problem, you can train different models on different cross-validation datasets and blend all the models with some weights assigning to each model using the sum_models( ) method. Later based on the situation you could change the weights of each model.

Many More…

By default, CatBoost has an overfitting detector that it stops training when CV error starts increasing. You can set parameter od_type = Iter to stop training your model after few iterations.

= to stop training your model few iterations. Like other algorithms, we can also balance an imbalanced dataset with the class_weight parameter.

balance imbalanced dataset with the parameter. CatBoost gives not only important features. But it also tells us that for a given data point what are the important features.

Code for training CatBoost is simply straight forwarded and it is almost similar to the sklearn module. I have explained only some important aspects of CatBoost. You can further read the full documentation of CatBoost here for a better understanding.

Goodbye to Hyper-parameter tuning?

CatBoost which is implemented by powerful theories like ordered Boosting, Random permutations. It makes sure that we are not overfitting our model. It also implements symmetric trees which eliminate parameters like (min_child_leafs ). We can further tune with parameters like learning_rate, random_strength, L2_regulariser, but the results don’t vary much.

EndNote:

CatBoost is freaking fast and it outperforms all the gradient boosting algorithms. It’s a good choice to train if most of the features in your dataset are categorical. You can further practice CatBoost on the assignments that are provided by the CatBoost team here. A model that is robust to over-fitting and with very powerful tools, what else you are waiting for? Start working on CatBoost !!!

References:","['catboost', 'feature', 'whats', 'train', 'point', 'points', 'model', 'special', 'data', 'categorical', 'residuals', 'using']","Base tree structure :One main difference between CatBoost and other boosting algorithms is that the CatBoost implements symmetric trees.
Below is an explanation of CatBoost using a toy example.
Instead of considering all the data points, it will consider only data points that are past in time to a data point.
With these parameters, CatBoost calculates residuals for each data point using a different model.
Working with large datasetsFor large datasets, you can train CatBoost on GPUs by setting parameter task_type = GPU.",en,['Hanish Sai Rohit Pallapothu'],2020-01-24 06:18:57.148000+00:00,"{'Xgboost', 'Boosting', 'Machine Learning', 'Catboost', 'Gradient Boosting'}","{'https://miro.medium.com/max/60/1*iEO6b3roCesXxI0vkx9IPg.png?q=20', 'https://miro.medium.com/max/60/0*_6ZgnG_8yoD2tMVa?q=20', 'https://miro.medium.com/max/60/0*cUlpwPDrq2-uTDEu?q=20', 'https://miro.medium.com/fit/c/80/80/1*RUtnaV9XF1xtfYj9Pjit-w.jpeg', 'https://miro.medium.com/max/2560/0*0SmuD3DHT013X2vd', 'https://miro.medium.com/max/60/1*zDejbm6ver9mVmHUSWY4Eg.png?q=20', 'https://miro.medium.com/max/606/0*_6ZgnG_8yoD2tMVa', 'https://miro.medium.com/max/798/0*cUlpwPDrq2-uTDEu', 'https://miro.medium.com/max/120/0*z_BUQVZOmCIMEUVw', 'https://miro.medium.com/max/60/0*3Q7nCJ43EYbBvLHl?q=20', 'https://miro.medium.com/max/1200/1*iEO6b3roCesXxI0vkx9IPg.png', 'https://miro.medium.com/fit/c/80/80/1*AsyevvFnxt21FHQJsy5d5A@2x.jpeg', 'https://miro.medium.com/max/900/0*3Q7nCJ43EYbBvLHl', 'https://miro.medium.com/max/440/1*rgTe4ycx_4wnKgWK83JBdw.gif', 'https://miro.medium.com/max/606/1*K-2XayuU9Y4OklIlDWg1AQ.png', 'https://miro.medium.com/max/60/0*z_BUQVZOmCIMEUVw?q=20', 'https://miro.medium.com/max/1004/1*HbFjRMOJbTSPCSvOL8A3Fg.png', 'https://miro.medium.com/max/60/1*K-2XayuU9Y4OklIlDWg1AQ.png?q=20', 'https://miro.medium.com/max/60/0*0SmuD3DHT013X2vd?q=20', 'https://miro.medium.com/max/912/1*zDejbm6ver9mVmHUSWY4Eg.png', 'https://miro.medium.com/fit/c/160/160/1*mck4jx8rPBryLSIsUTQsvQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*mck4jx8rPBryLSIsUTQsvQ.jpeg', 'https://miro.medium.com/max/2686/1*iEO6b3roCesXxI0vkx9IPg.png', 'https://miro.medium.com/freeze/max/60/1*rgTe4ycx_4wnKgWK83JBdw.gif?q=20', 'https://miro.medium.com/max/60/1*HbFjRMOJbTSPCSvOL8A3Fg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*eVvYVSkqyFwiuwJGfpCduw.png'}",2020-03-05 00:05:47.174561,1.6656150817871094
https://towardsdatascience.com/an-end-to-end-introduction-to-gans-bf253f1fa52f,An End to End Introduction to GANs,"Generator architecture

One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.

The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.

Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:

How do we get such an architecture?

In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:

We don’t have to worry about any weights right now as the network itself will learn those while training.

Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don’t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.

Now, What are Transpose convolutions?

In most simple terms, transpose convolutions provide us with a way to upsample images. While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:

Upsampling a 2x2 image to 4x4 image

Q: We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don’t we use Un-pooling?

It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.

Discriminator architecture

Now, as we have understood the generator architecture, here is the discriminator as a black box.

In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:

Takes an image as input and predicts if it is real/fake. Every image conv net ever.

Data preprocessing and visualization

The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:

The resultant output is as follows:

We get to see the sizes of the images and the images themselves.

We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.

We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.

As you will see, we will be using the preceding defined functions in the training part of our code.","['introduction', 'dense', 'maps', 'vector', 'generator', 'image', 'end', 'architecture', 'images', 'gans', '4x4', 'following', 'convolutions']","Generator architectureOne of the main problems we face with GANs is that the training is not very stable.
Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.
The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture.
In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector.
Discriminator architectureNow, as we have understood the generator architecture, here is the discriminator as a black box.",en,['Rahul Agarwal'],2019-12-25 11:39:06.748000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/15904/0*KgvJNS2eBH1HUa9f', 'https://miro.medium.com/max/24/1*oSzLQpc1VgOg2YE_sebVgA.png?q=20', 'https://miro.medium.com/max/1888/0*gB21j4tJpkIzMxnc.png', 'https://miro.medium.com/max/60/0*gB21j4tJpkIzMxnc.png?q=20', 'https://miro.medium.com/max/10368/0*2SepujHOsQHutYj8', 'https://miro.medium.com/max/771/0*DGcskWESjWM59OEs.png', 'https://miro.medium.com/max/688/0*r94-kX2tOj-W73Bi.png', 'https://miro.medium.com/max/5200/0*G-tHnbG4LqP7fj1_.png', 'https://miro.medium.com/max/60/0*knKOWYKYjfesr_Xj.png?q=20', 'https://miro.medium.com/max/816/0*Qn0oyAYAK67oawZl.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/0*DYURqDN7siVz3uNs.png?q=20', 'https://miro.medium.com/max/60/0*btO2vBsXLOT-VY9z?q=20', 'https://miro.medium.com/proxy/1*g_x1-5iYRn-SmdVucceiWw.png', 'https://miro.medium.com/max/60/0*G-tHnbG4LqP7fj1_.png?q=20', 'https://miro.medium.com/freeze/max/60/1*rLPMvOP6EDjn-9zRNgnbJA.gif?q=20', 'https://miro.medium.com/fit/c/96/96/1*4rmhtiqOfW3SNTdnTS1W5g.png', 'https://miro.medium.com/max/20/1*zW9vQ-2SKZg1c-GpF4cIxg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1542/0*DGcskWESjWM59OEs.png', 'https://miro.medium.com/max/1250/1*zW9vQ-2SKZg1c-GpF4cIxg.png', 'https://miro.medium.com/max/60/0*u25dV4ccF-xDljhR.png?q=20', 'https://miro.medium.com/max/2196/0*5vJpiPPC366PPsir.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*SCwcaj-Clra42SOd.png?q=20', 'https://miro.medium.com/max/1962/0*u25dV4ccF-xDljhR.png', 'https://miro.medium.com/max/1424/0*SCwcaj-Clra42SOd.png', 'https://miro.medium.com/max/10368/0*OjIw7GFIkonGjfcc', 'https://miro.medium.com/max/1404/0*DYURqDN7siVz3uNs.png', 'https://miro.medium.com/max/2268/1*rLPMvOP6EDjn-9zRNgnbJA.gif', 'https://miro.medium.com/max/1250/1*oSzLQpc1VgOg2YE_sebVgA.png', 'https://miro.medium.com/fit/c/160/160/1*4rmhtiqOfW3SNTdnTS1W5g.png', 'https://miro.medium.com/max/2270/0*rfKbSQzG8IRliFSM.png', 'https://miro.medium.com/max/60/0*OjIw7GFIkonGjfcc?q=20', 'https://miro.medium.com/max/60/0*KgvJNS2eBH1HUa9f?q=20', 'https://miro.medium.com/max/3720/0*MQspqgJbMj2BnO22.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*5vJpiPPC366PPsir.png?q=20', 'https://miro.medium.com/max/60/0*MQspqgJbMj2BnO22.png?q=20', 'https://miro.medium.com/max/60/0*2SepujHOsQHutYj8?q=20', 'https://miro.medium.com/max/60/0*Qn0oyAYAK67oawZl.png?q=20', 'https://miro.medium.com/max/2298/0*knKOWYKYjfesr_Xj.png', 'https://miro.medium.com/max/54/0*r94-kX2tOj-W73Bi.png?q=20', 'https://miro.medium.com/max/11006/0*btO2vBsXLOT-VY9z', 'https://miro.medium.com/max/48/0*rfKbSQzG8IRliFSM.png?q=20', 'https://miro.medium.com/max/60/0*DGcskWESjWM59OEs.png?q=20'}",2020-03-05 00:05:58.054846,10.879291296005249
https://towardsdatascience.com/how-to-automate-hyperparameter-optimization-73950c9fdb5,How to Automate Hyperparameter Optimization,"In the machine learning and deep learning paradigm, model “parameters” and “hyperparameters” are two frequently used terms where “parameters” define configuration variables that are internal to the model and whose values can be estimated from the training data and “hyperparameters” define configuration variables that are external to the model and whose values cannot be estimated from the training data ( What is the Difference Between a Parameter and a Hyperparameter? ). Thus, the hyperparameter values need to be manually assigned by the practitioner.

Every machine learning and deep learning model that we make has a different set of hyperparameter values that need to be fine-tuned to be able to obtain a satisfactory result. Compared to machine learning models, deep learning models tend to have a larger number of hyperparameters that need optimizing in order to get the desired predictions due to its architectural complexity over typical machine learning models.

Repeatedly experimenting with different value combinations manually to derive the optimal hyperparameter values for each of these hyperparameters can be a very time consuming and tedious task that requires good intuition, a lot of experience, and a deep understanding of the model. Moreover, some hyperparameter values may require continuous values, which will have an undefined number of possibilities, and even if the hyperparameters require a discrete value, the number of possibilities is enormous, thus manually performing this task is rather difficult. Having said all that, hyperparameter optimization might seem like a daunting task but thanks to several libraries that are readily available in the cyberspace, this task has become more straightforward. These libraries aid in implementing different hyperparameter optimization algorithms with less effort. A few such libraries are Scikit-Optimize, Scikit-Learn, and Hyperopt.

There are several hyperparameter optimization algorithms that have been employed frequently throughout the years, they are Grid Search, Random Search, and automated hyperparameter optimization methods. Grid Search and Random Search both set up a grid of hyperparameters but in Grid Search every single value combination will be exhaustively explored to find the hyperparameter value combination that gives the best accuracy values making this method very inefficient. On the other hand, Random Search will repeatedly select random combinations from the grid until the specified number of iterations is met and is proven to yield better results compared to the Grid Search. However, even though it manages to give a good hyperparameter combination we cannot be certain that it is, in fact, the best combination. Automated hyperparameter optimization uses different techniques like Bayesian Optimization that carries out a guided search for the best hyperparameters ( Hyperparameter Tuning using Grid and Random Search). Research has shown that Bayesian optimization can yield better hyperparameter combinations than Random Search ( Bayesian Optimization for Hyperparameter Tuning).

In this article, we will be providing a step-by-step guide into performing a hyperparameter optimization task on a deep learning model by employing Bayesian Optimization that uses the Gaussian Process. We used the gp_minimize package provided by the Scikit-Optimize (skopt) library to perform this task. We will be performing the hyperparameter optimization on a simple stock closing price forecasting model developed using TensorFlow.

Scikit-Optimize (skopt)

Scikit-Optimize is a library that is relatively easy to use than other hyperparameter optimization libraries and also has better community support and documentation. This library implements several methods for sequential model-based optimization by reducing expensive and noisy black-box functions.

Bayesian Optimization using the Gaussian Process

Bayesian optimization is one of the many functions that skopt offers. Bayesian optimization finds a posterior distribution as the function to be optimized during the parameter optimization, then uses an acquisition function (eg. Expected Improvement-EI, another function etc) to sample from that posterior to find the next set of parameters to be explored. Since Bayesian optimization decides the next point based on more systematic approach considering the available data it is expected to yield achieve better configurations faster compared to the exhaustive parameter optimization techniques such as Grid Search and Random Search. You can read more about the Bayesian Optimizer in skopt from here.

Code Alert!

So, enough with the theory, let’s get down to business!

This example code is done using python and TensorFlow. Furthermore, the goal of this hyperparameter optimization task is to obtain the set of hyperparameter values that can give the lowest possible Root Mean Square Error (RMSE) for our deep learning model. We hope this will be very straight forward for any first-timer.

First, let us install Scikit-Optimize. You can install it using pip by executing this command.

Please note that you will have to make some adjustments to your existing deep learning model code in order to make it work with the optimization.

First, let’s do some necessary imports.

We will now set the TensorFlow and Numpy seed as we want to get reproducible results.

Shown below are some essential python global variables that we have declared. Among the variables, we have also declared the hyperparameters that we are hoping to optimize (the 2nd set of variables).

The “input_size” depicts a part of the shape of the prediction. The “features” depict the number of features in the data set and the “columns” list has the header names of the two features. The “column_min_max” variable contains the upper and lower scaling bounds of both the features (this was taken by examining validation and training splits).

After declaring all these variables it’s finally time to declare the search space for each of the hyperparameters we are hoping to optimize.

If you look closely you will be able to see that we have declared the ‘lstm_init_learning_rate’ prior to log-uniform without just putting uniform. What this does is that, if you had put prior as uniform, the optimizer will have to search from 1e-4 (0.0001 ) to 1e-1 (0.1) in a uniform distribution. But when declared as log-uniform, the optimizer will search between -4 and -1, thus making the process much more efficient. This has been advised when assigning the search space for learning rate by the skopt library.

There are several data types using which you can define the search space. Those are Categorical, Real and Integer. When defining a search space that involves floating point values you should go for “Real” and if it involves integers, go for “Integer”. If your search space involves categorical values like different activation functions, then you should go for the “Categorical” type.

We are now going to put down the parameters that we are going to optimize in the ‘dimensions’ list. This list will be passed to the ‘gp_minimize’ function later on. You can see that we have also declared the ‘default_parameters’. These are the default parameter values we have given to each hyperparameter. Remember to type in the default values in the same order as you listed the hyperparameters in the ‘dimensions’ list.

The most important thing to remember is that the hyperparameters in the “default_parameters” list will be the starting point of your optimization task. The Bayesian Optimizer will use the default parameters that you have declared in the first iteration and depending on the result, the acquisition function will determine which point it wants to explore next.

It can be said that if you have run the model several times previously and found a decent set of hyperparameter values, you can put them as the default hyperparameter values and start your exploration from there. What this may do is that it will help the algorithm find the lowest RMSE value faster (fewer iterations). However, do keep in mind that this might not always be true. Also, remember to assign a value that is within the search space that you have defined when assigning the default values.

What we have done up to now is setting up all the initial work for the hyperparameter optimization task. We will now focus on the implementation of our deep learning model. We will not be discussing the data pre-processing of the model development process as this article only focuses on the hyperparameter optimization task. We will include the GitHub link of the complete implementation at the end of this article.

However, to give you a little bit more context, we divided our data set into three splits for training, validation, and testing. The training set was used to train the model and the validation set was used to do the hyperparameter optimization task. As mentioned before, we are using the Root Mean Square Error (RMSE) to evaluate the model and perform the optimization (minimize RMSE).

The accuracy assessed using the validation split cannot be used to evaluate the model since the selected hyperparameters minimizing the RMSE with validation split can be overfitted to the validation set during the hyperparameter optimization process. Therefore, it is standard procedure to use a test split that has not used at any point in the pipeline to measure the accuracy of the final model.

Shown below is the implementation of our deep learning model:

The “setupRNN” function contains our deep learning model. Still, you may not want to understand those details, as Bayesian optimization considers that function as a black-box that takes certain hyperparameters as the inputs and then outputs the prediction. So if you are not interested in understanding what we have inside that function, you may skip the next paragraph.

Our deep learning model contains an LSTM layer, a dropout layer and an output layer. The necessary information required for the model to work needs to be sent to this function (in our case, it was the input and the dropout rate). You can then proceed with implementing your deep learning model inside this function. In our case, we used an LSTM layer to identify the temporal dependencies of our stock data-set.

We then fed the last output of the LSTM to the dropout layer for regularization purposes and obtained the prediction through the output layer. Finally, remember to return this prediction (in a classification task this can be your logit) to the function that will be passed to the Bayesian Optimization ( “setupRNN” will be called by this function).

If you are performing a hyperparameter optimization for a machine learning algorithm (using a library like Scikit-Learn) you will not need a separate function to implement your model as the model itself is already given by the library and you will only be writing code to train and obtain predictions. Therefore, this code can go inside the function that will be returned to the Bayesian Optimization.

We have now come to the most important section of the hyperparameter optimization task, the ‘fitness’ function.

As shown above, we are passing the hyperparameter values to a function named “fitness.” The “fitness” function will be passed to the Bayesian hyperparameter optimization process ( gp_minimize). Note that in the first iteration, the values passed to this function will be the default values that you defined and from there onward Bayesian Optimization will choose the hyperparameter values on its own. We then assign the chosen values to the python global variables we declared at the beginning so that we will be able to use the latest chosen hyperparameter values outside the fitness function.

We then come to a rather critical point in our optimization task. If you have used TensorFlow prior to this article, you would know that TensorFlow operates by creating a computational graph for any kind of deep learning model that you make.

During the hyperparameter optimization process, in each iteration, we will be resetting the existing graph and constructing a new one. This process is done to minimize the memory taken for the graph and prevent the graphs from stacking on top of each other. Immediately after resetting the graph you will have to set the TensorFlow random seed in order to obtain reproducible results. After the above process, we can finally declare the TensorFlow session.

After this point, you can start adding code responsible for training and validating your deep learning model as you normally would. This section is not really related to the optimization process but the code after this point will start utilizing the hyperparameter values chosen by the Bayesian Optimization.

The main point to remember here is to return the final metric value (in this case the RMSE value) obtained for the validation split. This value will be returned to the Bayesian Optimization process and will be used when deciding the next set of hyperparameters that it wants to explore.

Note: if you are dealing with a classification problem you would want to put your accuracy as a negative value (eg. -96) because, even though the higher the accuracy the better the model, the Bayesian function will keep trying to reduce the value as it is designed to find the hyperparameter values for the lowest value that is returned to it.

Let us now put down the execution point for this whole process, the “main” function. Inside the main function, we have declared the “gp_minimize” function. We are then passing several essential parameters to this function.

The “func” parameter is the function you would want to finally model using the Bayesian Optimizer. The “dimensions” parameter is the set of hyperparameters that you are hoping to optimize and the “acq_func” stands for the acquisition function and is the function that helps to decide the next set of hyperparameter values that should be used. There are 4 types of acquisition functions supported by gp_minimize. They are:

LCB: lower confidence bound

EI: expected improvement

PI: probability of improvement

gp_hedge: probabilistically choose one of the above three acquisition functions at every iteration

The above information was extracted from the documentation. Each of these has its own advantages but if you are a beginner to Bayesian Optimization, try using “EI” or “gp_hedge” as “EI” is the most widely used acquisition function and “gp_hedge” will choose one of the above-stated acquisition functions probabilistically thus, you wouldn’t have to worry too much about that.

Keep in mind that when using different acquisition functions there might be other parameters that you might want to change that affects your chosen acquisition function. Please refer the parameter list in the documentation for this.

Back to explaining the rest of the parameters, the “n_calls” parameter is the number of times you would want to run the fitness function. The optimization task will start by using the hyperparameter values defined by “x0”, the default hyperparameter values. Finally, we are setting the random state of the hyperparameter optimizer as we need reproducible results.

Now when you run the gp_optimize function the flow of events will be:

The fitness function will use with the parameters passed to x0. The LSTM will be trained with the specified epochs and the validation input will be run to get the RMSE value for its predictions. Then depending on that value, the Bayesian optimizer will decide what the next set of hyperparameter values it wants to explore with the help of the acquisition function.

In the 2nd iteration, the fitness function will run with the hyperparameter values that the Bayesian optimization has derived and the same process will repeat until it has iterated “n_call” times. When the complete process comes to an end, the Scikit-Optimize object will get assigned to the “search _result” variable.

We can use this object to retrieve useful information as stated in the documentation.

x [list]: location of the minimum.

fun [float]: function value at the minimum.

models: surrogate models used for each iteration.

x_iters [list of lists]: location of function evaluation for each iteration.

func_vals [array]: function value for each iteration.

space [Space]: the optimization space.

specs [dict]`: the call specifications.

rng [RandomState instance]: State of the random state at the end of minimization.

The “search_result.x” gives us optimal hyperparameter values and using “search_result.fun” we can obtain the RMSE value of the validation set corresponding to the obtained hyperparameter values (The lowest RMSE value obtained for the validation set).

Shown below are the optimal hyperparameter values that we obtained for our model and the lowest RMSE value of the validation set. If you are finding it hard to figure out the order in which the hyperparameter values are being listed when using “search_result.x”, it is in the same order as you specified your hyperparameters in the “dimensions” list.

Hyperparameter Values:

lstm_num_steps: 6

lstm_size: 171

lstm_init_epoch: 3

lstm_max_epoch: 58

lstm_learning_rate_decay: 0.7518394019565194

lstm_batch_size: 24

lstm_dropout_rate: 0.21830825193089087

lstm_init_learning_rate: 0.0006401363567813549

Lowest RMSE: 2.73755355221523

Convergence Graph

The hyperparameters that produced the lowest point of the Bayesian Optimization in this graph is what we get as the optimal set of hyperparameter values.

The graph shows a comparison of the lowest RMSE values recorded for each iteration (50 iterations) in Bayesian Optimization and Random Search. We can see that the Bayesian Optimization has been able to converge rather better than the Random Search. However, in the beginning, we can see that Random search has found a better minimum faster than the Bayesian Optimizer. This can be due to random sampling being the nature of Random Search.

We have finally come to the end of this article, so to conclude, we hope this article made your deep learning model building task easier by showing you a better way of finding the optimal set of hyperparameters. Here’s to no more stressing over hyperparameter optimization. Happy coding, fellow geeks!

Useful Materials:","['set', 'function', 'bayesian', 'learning', 'hyperparameter', 'model', 'search', 'values', 'optimization', 'automate', 'value']","There are several hyperparameter optimization algorithms that have been employed frequently throughout the years, they are Grid Search, Random Search, and automated hyperparameter optimization methods.
Automated hyperparameter optimization uses different techniques like Bayesian Optimization that carries out a guided search for the best hyperparameters ( Hyperparameter Tuning using Grid and Random Search).
Research has shown that Bayesian optimization can yield better hyperparameter combinations than Random Search ( Bayesian Optimization for Hyperparameter Tuning).
As shown above, we are passing the hyperparameter values to a function named “fitness.” The “fitness” function will be passed to the Bayesian hyperparameter optimization process ( gp_minimize).
The optimization task will start by using the hyperparameter values defined by “x0”, the default hyperparameter values.",en,['Suleka Helmini'],2019-06-02 23:39:27.846000+00:00,"{'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Bayesian Optimization', 'Hyperparameter Tuning'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*zAh9pLwrioujKHXE.jpg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2560/0*zAh9pLwrioujKHXE.jpg', 'https://miro.medium.com/fit/c/96/96/0*orixWf7jAmLV61wo', 'https://miro.medium.com/max/9194/0*PGXAzu24ax4nA38r', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/0*orixWf7jAmLV61wo', 'https://miro.medium.com/max/60/0*PGXAzu24ax4nA38r?q=20', 'https://miro.medium.com/max/1200/0*PGXAzu24ax4nA38r', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:06:08.422791,2.032367467880249
https://towardsdatascience.com/artificial-intelligence-in-supply-chain-management-predictive-analytics-for-demand-forecasting-80d2d512f155,Artificial Intelligence in Supply Chain Management,"Artificial Intelligence in Supply Chain Management

Utilizing data to drive operational performance

Supply chain management (SCM) is critical in almost every industry today. Still, despite the importance, it hasn`t received the same amount of focus from AI startups and vendors as many other domains. However, given the vast amounts of data collected by industrial logistics, transportation and warehousing, this is an area with a lot of potential.

Digital transformation, digitalization, Industry 4.0, etc….

These are all terms you have probably heard or read about before. However, behind all of these buzz words, the main goal is the use of technology and data to increase productivity and efficiency. The connectivity and flow of information and data between devices and sensors allows for an abundance of available data. The key enabler is then being able to use these vast amounts of available data and actually extract useful information, making it possible to reduce costs, optimize capacity, and keep downtime to a minimum. This is where the recent buzz around machine learning and data analytics comes into play.

AI for the supply chain

Like in any other industry, the current focus on digitalization is transforming also supply chain management. Improving the efficiency of the suppy chain is of great importance for many companies. Operating within tough profit margins, even slight improvements can have substantial impact on the bottom line profit.

Examples where data analytics and machine learning can be beneficial for supply chain management is e.g. within demand forecasting and warehouse optimization. Given the vast amounts of data collected by industrial logistics, transportation and warehousing, being able to harness these data to drive operational performance can be a gamechanger for those that do it correctly.

Case study: Predictive analytics for demand forecasting

To illustrate the use of machine learning in the supply chain, I will go through an example case study focused on demand forecasting. The following example covers a hypothetical retailer in Norway, and includes individual stores on various locations as well as a main central warehouse, as illustrated in the below figure.

One of the challenges for such a retailer, is to optimize localized vs. centralized warehouse storage of goods: On the one hand, substantial local storage is expensive, on the other hand, relying mostly on centralized storage and running the risk of sold out items in the stores is another factor. Warehouse optimization is thus of great importance, and having access to accurate sales forecasts would be extremely useful information.

To limit the amount of data in the case study, the anonymized dataset includes the number of sold items for a subset of 50 items for 10 different shops during the time period from 2013–2017. In total this adds up to a dataset of approximately 1 million rows, on the data format illustrated below:

The historical sales records thus represents the data we are trying to extract useful information from in order to predict future sales. Of course, this could ideally be complemented with other available data sources such as e.g. weather data, as it is not unlikely that sales might be affected by weather conditions. For example, if you want to predict the amount of ice cream and BBQ food sold during the following week in a grocery shop, having information on the weather forecast might be extremely useful information (especially for the example retailer in Norway, where summer weather can be unstable to say the least!). Fortunately, the Norwegian Metrological Institute has a “WeatherAPI”, where where you can download weather data and experiment with for free if you want to have a go!

Training data and target variables

In this case, we are trying to predict the number of sold items in the 10 shops for each of the 50 (anonymized) items included in the example dataset. The basic idea is that the historical sales records might contain some hidden patterns that our machine learning model can pick up. And, if this is the case, the model can then utilize these patterns to make accurate predictions of future sales.

We use the historical sales records from January 2013 to September 2017 as training data for our model, and we then try to predict the number of sold items during the last quarter of 2017 (October-December).

A subset of the training data for “item 15” from store 10 is illustrated in the below figure to the left. This exhibits a clear yearly periodicity (with highest sales during the summer months), and also a linearly increasing trend where sales increase year by year. Our goal then, is to predict the sales during October-December 2017, as illustrated in the figure to the right.","['sales', 'weather', 'information', 'intelligence', 'management', 'predict', 'supply', 'artificial', 'example', 'sold', 'chain', 'data', 'items']","Artificial Intelligence in Supply Chain ManagementUtilizing data to drive operational performanceSupply chain management (SCM) is critical in almost every industry today.
AI for the supply chainLike in any other industry, the current focus on digitalization is transforming also supply chain management.
Examples where data analytics and machine learning can be beneficial for supply chain management is e.g.
Case study: Predictive analytics for demand forecastingTo illustrate the use of machine learning in the supply chain, I will go through an example case study focused on demand forecasting.
Fortunately, the Norwegian Metrological Institute has a “WeatherAPI”, where where you can download weather data and experiment with for free if you want to have a go!",en,['Vegard Flovik'],2020-03-01 12:47:24.669000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Supply Chain'}","{'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*ewT7wjxRN-BFnnH-?q=20', 'https://miro.medium.com/max/960/1*vj9WUCMF0QoAdzneQRAqLg.jpeg', 'https://miro.medium.com/max/2928/0*lkTvC_RZ0m82EkbW', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*z2m9CfEoI7aUhbEsiNK7nA.jpeg', 'https://miro.medium.com/max/2044/0*baoZZdGfnZvtX9W6', 'https://miro.medium.com/max/60/1*vj9WUCMF0QoAdzneQRAqLg.jpeg?q=20', 'https://miro.medium.com/max/60/0*q5choxKTGRylNIie?q=20', 'https://miro.medium.com/fit/c/160/160/1*z2m9CfEoI7aUhbEsiNK7nA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/4464/0*ewT7wjxRN-BFnnH-', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/40/0*baoZZdGfnZvtX9W6?q=20', 'https://miro.medium.com/max/4464/0*q5choxKTGRylNIie', 'https://miro.medium.com/max/60/0*lkTvC_RZ0m82EkbW?q=20', 'https://miro.medium.com/max/1920/1*vj9WUCMF0QoAdzneQRAqLg.jpeg'}",2020-03-05 00:06:17.306097,8.882297277450562
https://towardsdatascience.com/colab-synergy-with-mlflow-how-to-monitor-progress-and-store-models-fbfdf7bb0d7d,Colab synergy with MLflow: how to monitor progress and store models.,"An increasing number of models in deep learning rely on the usage of GPU. As a deep learning practitioner, I even thought of buying one to accelerate development of my side projects. But new models of GPU released every year make an old GPU somewhat ancient, slow, and not cool. Cool things are the new cloud technologies that allow to rent a virtual machine (VM) with GPU of the last generation and train your model (high) in the cloud. However, it might be still pricey, both in terms of money and time to set-up your cloud VM.

What if I tell (or remind?) you about the cloud solution that solves all these issues? Colab Notebook from Google provides a free GPU for up to 12 hours. However, after 12 hours everything turns into pumpkin: all stored data is gone.

Photo by Dan Gold on Unsplash

Convergence to pumpkin is not the only option. To save trained models one needs to connect Google Storage or Google Drive with Colab Notebook. To monitor a training progress, an additional tool such as Colab Tensorboard has to be used. Alternatively, MLflow provides a solution both to store models and to monitor the training progress. In this blog-post, I present a guide on how to setup MLflow on Google Cloud.

MLflow stores two types of data:

structured data: metrics of training progress and model parameters (float numbers and integers)

unstructured data: training artifacts (images, models, etc.)

We can either store these types of data in databases or locally on VM. Let’s start with a databases option. For the metrics of training, the SQL or Databricks databases can be used. For training artifacts, the databases are S3, Google Storage or Azure Storage. To observe the training progress one needs to deploy MLflow server (GUI to manage the stored data) and to connect it to the databases. The other option would be to deploy MLflow server on a VM and to store everything locally on it.

I decided to proceed with the first option and to connect MLflow server to databases. This setup looks more robust to me as we do not need to rely on MLflow server, but only on cloud databases. Also, the databases option allows to deploy MLflow server locally on your laptop which is more secure. For a cloud provider, I chose Google Cloud, as I still had some free credits left:). Here is a sketch of an architecture:

Let us now follow 5 hands-on steps and deploy the server locally that will be connected to the SQL database for metrics and to the Google storage for artifacts.

1. Setup service account in IAM control. My current setup is not the most secure one (see comments on security in the end). However, one thing that is definitely worth doing is to create a service app in IAM control of Google Cloud. To create service IAM follow:

IAM→Service Account → Select Project → Create service account →…

The following permission should be granted to the app Cloud SQL Editor, Storage Object Admin. The reason to create IAM service account is that if somebody occasionally figures out your app credentials, the maximum damage is limited to SQL and Storage. For connection to SQL and Storage, we will use service IAM password. To create JSON with passwords:

Service account details → Grant this service account access to project (choose the roles) → Grant users access to this service account → create a key in JSON format and save this file.

Now you have a service app with a JSON key to it.

2. Create and configure Google SQL server. Existing solutions in Google cloud which MLflow supports are MySQL and PostgreSQL. Setting-up PostgreSQL is so far easier. There are several issues with MySQL in version 1.0.0 of MLflow which will certainly be fixed in the next release. In this post, I describe a setup with PostgreSQL.

2a. After starting PostgreSQL server, set public IP for your SQL service with rule 0.0.0.0/0 (here you expose your SQL to the Internet and everybody who knows the password of your SQL database can have access to your SQL). Write down your public IP number as you will need it later.

2b. Create a table in your SQL where you will store data from MLflow.

2c. Setup username and password for SQL database.

3. Configure Google Storage account.

3a. Create a bucket in Google Storage.

3b. Add roles: “Legacy Bucket Owner” and “Legacy Bucker Reader” to IAM app-user which you created in step 1. To do so follow:

Choose the bucket → Permissions → Choose service account → Add corresponding roles

4. Start local MLflow Server.

MLflow provides a nice GUI, called MLflow Server. You can manage your models stored in databases via this GUI. One option is to start the MLflow server locally on your laptop.

4a. First, install MLflow and the following necessary packages:

pip install google-cloud-storage mlflow psycopg2

4b. Remember JSON file from step 1? Now we need it! Type:

export GOOGLE_APPLICATION_CREDENTIALS=”path/to/jsonfile”

4c. This is it! Start your MLflow server with a command:

mlflow server \

--backend-store-uri 'postgresql://<username>:<password>@<sql ip>/<name of table>'\

--default-artifact-root gs://<name of bucket in gs>/

Here ‘postgresql://<username>:<password>@<sql ip>/<name of table>’ is an SQL Alchemy string to connect to your PostgreSQL database. Insert here corresponding credentials from step 2.

After that, your MLflow server should work. Check it by typing in the browser address line: http://127.0.0.1:5000. You expect to see a start page of your MLflow server:

MLflow Server first start.

5. Test MLflow setup with Colab.

To test the connection you should first set your credentials. For example, like that:

And then install MLflow with other packages from Colab and save some metrics and artifacts to databases:

The full notebook is available in GitHub .

Note that I used the presented setup to train models on a publicly accessible dataset and security was not the main concern. There are several vulnerabilities, such as exposing SQL server to the Internet as well as writing password from service account directly to Colab notebook. Limitation of service-account rights is of high importance in this setup. A more secure way to setup MLflow would be to establish the connection to Google cloud via gcloud tools and to use Google Storage SQL proxy without exposing SQL service to the Internet. You know, there is always a trade-off between simplicity and security:)

Happy model managing and training with Colab!","['server', 'models', 'monitor', 'create', 'cloud', 'databases', 'sql', 'setup', 'service', 'colab', 'store', 'storage', 'synergy', 'mlflow', 'progress', 'google']","To save trained models one needs to connect Google Storage or Google Drive with Colab Notebook.
Alternatively, MLflow provides a solution both to store models and to monitor the training progress.
In this blog-post, I present a guide on how to setup MLflow on Google Cloud.
The other option would be to deploy MLflow server on a VM and to store everything locally on it.
MLflow provides a nice GUI, called MLflow Server.",en,['Dmitry Azarnyh'],2019-07-01 22:34:04.775000+00:00,"{'Google Cloud Platform', 'Deep Learning', 'Python', 'Machine Learning', 'Jupyter Notebook'}","{'https://miro.medium.com/max/1200/0*CW1qo5bm-wN-kuuH', 'https://miro.medium.com/max/2092/1*vfd_tnz3XIw8uNZOPowq5g.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*FjD4Zm-aPLRCYYPBrqce-A.jpeg?q=20', 'https://miro.medium.com/max/60/0*CW1qo5bm-wN-kuuH?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1910/1*FjD4Zm-aPLRCYYPBrqce-A.jpeg', 'https://miro.medium.com/fit/c/96/96/2*2fGtUW0ARmU0u41jL53-zw.jpeg', 'https://miro.medium.com/fit/c/160/160/2*2fGtUW0ARmU0u41jL53-zw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/11232/0*CW1qo5bm-wN-kuuH', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*vfd_tnz3XIw8uNZOPowq5g.png?q=20'}",2020-03-05 00:06:18.961262,1.6541616916656494
https://towardsdatascience.com/jupyterlab-a-next-gen-python-data-science-ide-562d216b023d,JupyterLab — A Next Gen Python Data Science IDE,"Why JupyterLab?

Python notebooks got a lot of attention in the recent years as a tool showing code and results in an interactive and nicely layouted manner. It certainly helps to lower the barrier to start with programming and helps in education, because an input is presented together with its processed output instantly in a browser, which many users are very familiar with. Despite Python notebooks’ popularity, a classic Python IDE or text editor becomes more convenient the more coding needs to be done. Wouldn’t it be nice if there would be a tool taking the best of each and consequently combines both worlds? JupyterLab is working towards that goal by enabling users to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner [1].

Jupyter notebooks’ “evolution of the Jupyter web interface” to JupyterLab is based on a user experience survey conducted in 2015 highlighting following three success factors [2]:

Users like the notebook experience. Users want to combine and remix different Jupyter building blocks. Users need the ability to easily collaborate.

These factors are not surprising looking into some Pros and Cons based on my experience with Jupyter notebooks.

Pros:

+ Jupyter notebooks are particularly strong when it comes to visualization functionalities. E.g. tools like Google Facets have been developed for being used in Jupyter notebooks [3].

+ The interaction with plots is very convenient e.g. by just simply using %matplotlib notebook or ipywidgets [4].

+ One can add a pretty and concise documentation of a piece of code by changing a cell from Code to Markdown.

+ Jupyter notebooks are a pretty neat tool for data storytelling and presentation as its possible to show a documentation along with a code’s output.

Cons:

- The absence of a built-in variable inspector is one of the first things experienced standard IDE users are missing in Jupyter notebooks. I want to highlight though that there is a extremely helpful community-contributed unofficial extension using the notebook’s metadata [8].

- Jupyter notebooks do not provide a convenient file explorer view when developing code [5]. Thus, reading and writing files becomes clumsy.

- One needs to prefix terminal commands with a exclamation mark ! within a cell in order to interact with the operating system’s terminal or use the terminal view added as addon [5].

- Opening and exploring files is clunky as one needs to load the file first and choose an appropriate way to display it programmatically. This requires more effort than opening e.g. a jpg file with a double click within an IDE.

- Testing and modularity are difficult to handle within Jupyter notebooks.

- Seemingless integration with a version control system is missing, although there is interesting progress with add ons like nbdime making diffing and merging of notebooks easier [7].

- Absence of a convenient visual debugging plus profiling functionality, despite really promising developments like the PixieDebugger [10].

I want to highlight that this is not an exhaustive list of Pros and Cons. A statement listed under in the Cons section does indicate that the mentioned functionality is not achievable at all. It is also listed under Cons in case its not intuitively available in Jupyter notebook.

Let’s look into the details with the currently available version of JupyterLab (0.35.6) and see what is gonna be covered when moving from Jupyter notebook to JupyterLab.

Python and Jupyter notebook files sharing a single kernel

JupyterLab lets you develop complex python code as well as Jupyter notebooks and making it easy to connect them to the same kernel. I see this as a key feature for tackling the Cons.

In the following animation you see how to connect multiple Python files and notebooks in JupyterLab.

Creation of two Python files and one Jupyter notebook in JupyterLab. Consecutively, you see the selection of one common kernel for each of the files. At the end you can observe that all three files have access to the same kernel as they are using the the variables a and b interactively.

Now look at the bellow animation as it shows the simplicity of loading data into a dataframe, developing models separately while testing and visualizing them with the power of Jupyter notebooks in a seamless manner. All of this is possible in addition to having one common variable inspector and file explorer. You can see here a simple manual function approximation task.

Exploration of the csv file and loading it into a dataframe in a kernel which is shared among the open files. The dataframe is visible in the variable inspector. First the given x and y vectors are plotted in blue. Afterwards, the function approximator plotted in orange is iteratively improved by manually adjusting the function fun in the file model.py. The approximator covers fully the given data input at the end. Therefore, only an orange line is visible anymore.

Effectively this decouples extraction, modeling and visualization without having to write and read files to share the data frames. This is a massive time saver for your daily work, as it reduces the risk of mistakes in the file loads, and because it's much faster to setup your EDA along with trials in the early stages of projects. Furthermore, it helps to reduce the number of code lines in case you add as many asserts into your data pipeline as me.

In case you need a terminal really quick within the same context of your project, then you can just simply open the launchpad and create a new Terminal view. This is particular useful if want to check the resources needed by your model or algorithm, as shown in the following animation.

JupyterLab- Ian Rose (UC Berkeley), Chris Colbert (Project Jupyter) at 14:30 shows how to open a terminal within JupyterLab [9].

Opening a data file is also pretty neat with JupyterLab. It is rendered in a nicely e.g. in tabular form for csv files and utilizes lazy loading, hence making it fast plus it supports enormous file sizes. The next animation shows opening the IRIS data set from a csv file.

JupyterLab- Ian Rose (UC Berkeley), Chris Colbert (Project Jupyter) at 19:15 shows the IRIS data set in a csv file being opened with a simple click [9].

You can also open image files with just a click, which comes pretty handy when working on computer visions tasks. In the following animation you see how Jupyterlab renders an image of the hubble telescope in a separate of the last used panel.

JupyterLab- Ian Rose (UC Berkeley), Chris Colbert (Project Jupyter) at 17:58, shows an image being rendered in by clicking on it in the built in file explorer [9].

Furthermore, you can navigate and utilize Git with JupyterLab’s Git extension as shown below.

Parul Pandey’s gif showing the navigation in the Git extension provided in [6].

There is no visual debugging and profiling functionality available in JupyterLab at the time writing this article. It is currently planned for a future release [11]. Hence, development will start earliest after version 1.0 has been released. Despite this plans, there is work being done to enable PixieDebugger for notebooks in Jupyterlab [12].

Conclusion

JupyterLab adds a complete IDE around Jupyter notebooks making it surely a strong evolution of the Jupyter notebooks. It integrates so well into the data scientists’ daily work that it can be also seen as the Next Gen tool. The ease of decoupling data extraction, transformation, modeling visualization and testing is already really powerful.

With this in mind I hope seeing the 1.0 release popping soon. In case you got excited about the JupyterLab project and want to try it yourself, just follow the instructions in Parul Pandey ‘s article:","['file', 'code', 'files', 'gen', 'shows', 'users', 'python', 'jupyterlab', 'data', 'notebooks', 'terminal', 'jupyter', 'ide', 'science']","These factors are not surprising looking into some Pros and Cons based on my experience with Jupyter notebooks.
tools like Google Facets have been developed for being used in Jupyter notebooks [3].
Cons:- The absence of a built-in variable inspector is one of the first things experienced standard IDE users are missing in Jupyter notebooks.
JupyterLab- Ian Rose (UC Berkeley), Chris Colbert (Project Jupyter) at 14:30 shows how to open a terminal within JupyterLab [9].
ConclusionJupyterLab adds a complete IDE around Jupyter notebooks making it surely a strong evolution of the Jupyter notebooks.",en,['Rene Draschwandtner'],2019-07-05 19:51:30.571000+00:00,"{'Jupyterlab', 'Data Science', 'Python', 'Towards Data Science', 'Jupyter Notebook'}","{'https://miro.medium.com/fit/c/160/160/2*Xpbi24LrTR5v0BhAgwCLNw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/freeze/max/60/1*qy4d9m51owrsWSQzNunyng.gif?q=20', 'https://miro.medium.com/max/3806/1*qy4d9m51owrsWSQzNunyng.gif', 'https://miro.medium.com/max/2554/1*dT0NaAhB7fiOkS3r1DR62w.gif', 'https://miro.medium.com/proxy/1*13Vk_9wT3mUZsG9rVBJvpw.gif', 'https://miro.medium.com/fit/c/96/96/2*Xpbi24LrTR5v0BhAgwCLNw.jpeg', 'https://miro.medium.com/max/2800/0*RfuQURQDtz9W4bf9.jpg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2554/1*7dwt_zsQbIIlVakwSPHRPA.gif', 'https://miro.medium.com/freeze/max/60/1*Et1jDEwKsKeoc8bnO5nfMA.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/freeze/max/60/1*7dwt_zsQbIIlVakwSPHRPA.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*dT0NaAhB7fiOkS3r1DR62w.gif?q=20', 'https://miro.medium.com/max/60/0*RfuQURQDtz9W4bf9.jpg?q=20', 'https://miro.medium.com/max/1200/0*RfuQURQDtz9W4bf9.jpg', 'https://miro.medium.com/freeze/max/60/1*hRoumnvMN7GMeTfkNxX_Pg.gif?q=20', 'https://miro.medium.com/max/2554/1*hRoumnvMN7GMeTfkNxX_Pg.gif', 'https://miro.medium.com/max/3806/1*Et1jDEwKsKeoc8bnO5nfMA.gif', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png'}",2020-03-05 00:06:28.584738,9.623475313186646
https://towardsdatascience.com/computer-vision-for-beginners-part-1-7cca775f58ef,Computer Vision for Beginners: Part 1,"Computer Vision is one of the hottest topics in artificial intelligence. It is making tremendous advances in self-driving cars, robotics as well as in various photo correction apps. Steady progress in object detection is being made every day. GANs is also a thing researchers are putting their eyes on these days. Vision is showing us the future of technology and we can’t even imagine what will be the end of its possibilities.

So do you want to take your first step in Computer Vision and participate in this latest movement? Welcome you are at the right place. From this article, we’re going to have a series of tutorials on the basics of image processing and object detection. This is the first part of OpenCV tutorial for beginners and the complete set of the series is as follows:

The first story of this series will be about installing OpenCV, explaining color models and drawing figures on images. The complete code for this tutorial is also available on Github. Now let’s get it started.

Introduction to OpenCV

Image processing is performing some operations on images to get an intended manipulation. Think about what we do when we start a new data analysis. We do some data preprocessing and feature engineering. It’s the same with image processing. We do image processing to manipulate the pictures for extracting some useful information from them. We can reduce noises, control the brightness and color contrast. To learn detailed image processing fundamentals, visit this video.

OpenCV stands for Open Source Computer Vision library and it’s invented by Intel in 1999. It’s first written in C/C++ so you may see tutorials more in C languages than Python. But now it’s also getting commonly used in Python for computer vision as well. First things first, let’s set up a proper environment for using OpenCV. The installation can be processed as follows but you can also find the detailed description here.

pip install opencv-python==3.4.2

pip install opencv-contrib-python==3.3.1

After you finish the installation, try importing the package to see if it works well. If you get the return without any errors, then you’re now ready to go!

import cv2

cv2.__version__

The first step we’re going to do with OpenCV is importing an image and it can be done as follows.

import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline # Import the image

img = cv2.imread('burano.jpg')

plt.imshow(img)

Have you ever been to Burano? It’s one of the most beautiful islands in Italy. If you haven’t been there, you should definitely check this place for your next holidays. But if you already know this island, you’d probably notice there’s something different in this picture. It’s a little bit different from the pictures we usually see from Burano. It should be more delightful than this!

This is because the default setting of the color mode in OpenCV comes in the order of BGR, which is different from that of Matplotlib. Therefore to see the image in RGB mode, we need to convert it from BGR to RGB as follows.

# Convert the image into RGB

img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

plt.imshow(img_rgb)

Now, this is Burano! Such a lovely island in Italy!

More than just RGB

Let’s talk about color modes a little bit more. A color model is a system for creating a full range of colors using the primary colors. There are two different color models here: additive color models and subtractive color models. Additive models use light to represent colors in computer screens while subtractive models use inks to print those digital images on papers. The primary colors are red, green and blue (RGB) for the first one and cyan, magenta, yellow and black (CMYK) for the latter one. All the other colors we see on images are made by combining or mixing these primary colors. So the pictures can be depicted a little bit differently when they are represented in RGB and CMYK.","['models', 'beginners', 'opencv', 'image', 'vision', 'colors', 'rgb', 'color', 'images', 'processing', 'computer']","Computer Vision is one of the hottest topics in artificial intelligence.
So do you want to take your first step in Computer Vision and participate in this latest movement?
OpenCV stands for Open Source Computer Vision library and it’s invented by Intel in 1999.
But now it’s also getting commonly used in Python for computer vision as well.
There are two different color models here: additive color models and subtractive color models.",en,['Jiwon Jeong'],2019-08-08 00:46:45.224000+00:00,"{'Data Science', 'Python', 'Computer Vision', 'Opencv', 'Image Processing'}","{'https://miro.medium.com/fit/c/96/96/1*Yqh4zWBcAo7nEUw9dwNSmg.jpeg', 'https://miro.medium.com/max/60/1*fMZjDpFbVwTncrQAvOazdg.png?q=20', 'https://miro.medium.com/max/1000/1*CwSvaseYli4Jst1ibNXTgQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*ZnWzOppbxJoHp6Gb2R3OZA.png?q=20', 'https://miro.medium.com/max/60/1*Oe3JzhYDb3I6wovNZ-488w.png?q=20', 'https://miro.medium.com/max/1142/1*Oe3JzhYDb3I6wovNZ-488w.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2334/1*Gs_f7aWJQktSkIWLy2m70g.png', 'https://miro.medium.com/max/1808/1*Q3GfBHxWFS_ziXb1CGQkMg.gif', 'https://miro.medium.com/max/3078/1*CSmlQDizc03csCaSgMgMIw.png', 'https://miro.medium.com/max/60/1*CwSvaseYli4Jst1ibNXTgQ.jpeg?q=20', 'https://miro.medium.com/max/2434/1*f3pRIVbutpa9KBwuqsl43w.png', 'https://miro.medium.com/fit/c/160/160/1*Yqh4zWBcAo7nEUw9dwNSmg.jpeg', 'https://miro.medium.com/max/1136/1*ZnWzOppbxJoHp6Gb2R3OZA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*dIUzwVFYunVzBtRdOb7BSA.png?q=20', 'https://miro.medium.com/max/60/1*t6gJMcMAu8EUXcgbhhGy5Q.png?q=20', 'https://miro.medium.com/max/60/1*Gs_f7aWJQktSkIWLy2m70g.png?q=20', 'https://miro.medium.com/max/60/1*E1cRhyj4ByJ_qrTev3hTlA.png?q=20', 'https://miro.medium.com/max/60/1*6elxrI90FEiDhWplc74mYg.png?q=20', 'https://miro.medium.com/max/60/1*CSmlQDizc03csCaSgMgMIw.png?q=20', 'https://miro.medium.com/max/60/1*f3pRIVbutpa9KBwuqsl43w.png?q=20', 'https://miro.medium.com/max/2000/1*CwSvaseYli4Jst1ibNXTgQ.jpeg', 'https://miro.medium.com/max/2526/1*E1cRhyj4ByJ_qrTev3hTlA.png', 'https://miro.medium.com/max/2514/1*zeVGeXCBFE4FLSbv_bUaLw.png', 'https://miro.medium.com/max/60/1*zeVGeXCBFE4FLSbv_bUaLw.png?q=20', 'https://miro.medium.com/max/1142/1*fMZjDpFbVwTncrQAvOazdg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/60/1*Q3GfBHxWFS_ziXb1CGQkMg.gif?q=20', 'https://miro.medium.com/max/1136/1*dIUzwVFYunVzBtRdOb7BSA.png', 'https://miro.medium.com/max/1136/1*6elxrI90FEiDhWplc74mYg.png', 'https://miro.medium.com/max/1984/1*t6gJMcMAu8EUXcgbhhGy5Q.png'}",2020-03-05 00:06:30.718919,2.134181499481201
https://medium.com/@MailSlurp/easy-analytics-with-grafana-postgres-and-kubernetes-a9451f41d0be,"Easy analytics with Grafana, Postgres, and Kubernetes. The fast and free path to application insights.","Easy analytics with Grafana, Postgres, and Kubernetes. The fast and free path to application insights.

You don’t need to pay for commercial data metrics platforms — you can build powerful dashboards with Grafana for free on any Kubernetes cluster.

Why we need data insights

If you have a business or app you probably want to know who’s using it and what they’re doing the most. You might also want to know whether your user count is increasing and what what kind of load your servers are under.

At MailSlurp we use Grafana, Postgres, and Kubernetes to graph growth and user acquisition in a free and flexible way. In this article we will show you how we do it and explain why open source data platforms are a good BI solution. First, some background.

Background

Our company, MailSlurp, is a API for testing transactional emails. It lets you send and receive email from randomly generated email addresses via REST. We use metrics to analyse product usage and costs. Data dashboards also help us to price our product and plan product features.

Existing SAAS offerings

Google Analytics is a popular free data platform. But do you trust Google with your data?

To answer data related questions most people usually turn to commercial analytics platforms. These services include Google Analytics, MixPanel, DataDog, Tableau and many more.

While these services offer a range of great features they do have a number of issues:

pricing (hidden costs)

privacy (data retention and ownership)

scale (will feature-set scale with requirements?)

MixPanel for instance costs $779 per year while hosted Tableau prices will make a small company’s head spin.

Free services like Google Analytics are very popular but raise serious privacy concerns: is your sensitive company data safe? Will Google sell your customer activity? Is your company under GDPR requirements?

Lastly many of these SAAS products work well at small scale. When your company’s need grow will they still provide the insights you require?

Open source solutions

Luckily, there are a number of open source data analytics options:

the ELK stack is a great one (Elasticsearch, Logstash, Kibana)

Prometheus is a excellent metrics platform

and Grafana is an extremely flexible and fun to use data dashboard package.

In this article we’ll concentrate on Grafana and how you can use it to gain powerful insights into your users — free, securely, and at scale.

What is Grafana?

No matter where your data is, or what kind of database it lives in, you can bring it together with Grafana. Beautifully.

Grafana is an self-hosted service that let’s you load in data and display it on interactive graphs, tables, and text areas. More specifically it is a backend server written in Go (that handles data connections and persisting graphs) and an interactive frontend written in Typescript (that displays information and handles graph creation).

The Grafana website has a great demo dashboard you can play around to get a feel for its features. Here’s what a typical dashboard looks like:

A screenshot of a Grafana dashboard

Deploying Grafana with Kubernetes

If your application runs on a Kubernetes cluster it’s easy to deploy Grafana. Grafana has a number of official docker images — all we need to do is write a Kubernetes service and deployment for the image and we can access it via port-forwarding. MailSlurp relies heavily on Kubernetes so integrating Grafana was easy.

Here’s our full Kubernetes setup for Grafana. Notice that we create a PersistentVolumeClaim. This lets the Grafana image persist dashboards and config to disk. We also specify grafana/grafana:5.0.0 as our image as later versions made Docker deployments a little bit more difficult.

# deployment of grafana 5 docker image

apiVersion: apps/v1

kind: Deployment

metadata:

labels:

app: grafana

name: grafana-deployment

namespace: default

spec:

replicas: 1

selector:

matchLabels:

component: grafana

template:

metadata:

labels:

component: grafana

spec:

volumes:

- name: grafana-claim

persistentVolumeClaim:

claimName: grafana-claim

containers:

- name: grafana

image: grafana/grafana:5.0.0

imagePullPolicy: IfNotPresent

ports:

- containerPort: 3000

resources:

limits:

cpu: 500m

memory: 2500Mi

requests:

cpu: 100m

memory: 100Mi

volumeMounts:

- mountPath: /var/lib/grafana

name: grafana-claim

---

# grafana persistent volume claim for storing dashboards

apiVersion: v1

kind: PersistentVolumeClaim

metadata:

creationTimestamp: null

labels:

component: grafana

name: grafana-claim

namespace: default

spec:

accessModes:

- ReadWriteOnce

resources:

requests:

storage: 10Gi

---

# grafana service for accessing via port 3000

apiVersion: v1

kind: Service

metadata:

name: grafana-ip-service

namespace: default

spec:

type: ClusterIP

selector:

component: grafana

ports:

- port: 3000

targetPort: 3000

If we save this to a YAML file and apply it to our cluster we should see the Grafana service and pods with Kubectl.

kubectl apply -f grafana.yml

Connecting to Grafana inside a cluster

Now that we have deployed Grafana we should see its pods, deployments and services with Kubectl.

As we did not add an ingress rule for this service we can only access the Grafana frontend via port-forwarding. This suits us nicely as we don’t want to expose our data to the outside world. (If you did want to expose the service, just add an ingress route for this service.)

We can connect to the Grafana service using the Kubectl port-forward command to make the remote port 3000 available on our localhost.

kubectl port-forward svc/grafana-ip-service 3000:3000

A screenshot of the initial login page accessed via port-forwarding

Now if you type http://localhost:3000 into a browser you’ll see the default Grafana login page. The initial username and password are admin and admin . I recommend changing these immediately.

Configuring a datasource (connecting Postgres)

Once we have Grafana deployed and accessible via port-forwarding we need to enable a datasource. This will allow us to query a database and build graphical dashboards from the results.

Many applications these days run on Postgres but Grafana also supports MySQL, Graphite and a variety of other data sources.

Dashboards are collections of graphs, text, and tables that display the results of queries against your datasource.

If you navigate to the configuration page and click on datasources you will see a configuration form. Use this to add your application database. You may want to create a special READ_ONLY database user for Grafana so that you don’t accidentally destroy data while building your graphs.

Screenshot of how to add Postgres datasource to Grafana

Building dashboards with Grafana

Now that you have a datasource enabled you can build your first dashboard. Dashboards are collections of graphs, text, and tables that display the results of queries against your datasource. So what you want to display dictates the type of graph you should use and the type of query you should write.

A simple Text example

For a simple example let’s display the total number of rows in a table.

Say we have an event table that our application writes to each time it does something interesting. Let’s query the total event count and display it in a text field. The steps are as follows:","['kubernetes', 'analytics', 'fast', 'easy', 'lets', 'postgres', 'dashboards', 'insights', 'need', 'path', 'free', 'data', 'display', 'application', 'service', 'grafana', 'text']","Easy analytics with Grafana, Postgres, and Kubernetes.
At MailSlurp we use Grafana, Postgres, and Kubernetes to graph growth and user acquisition in a free and flexible way.
Data dashboards also help us to price our product and plan product features.
Here’s what a typical dashboard looks like:A screenshot of a Grafana dashboardDeploying Grafana with KubernetesIf your application runs on a Kubernetes cluster it’s easy to deploy Grafana.
We can connect to the Grafana service using the Kubectl port-forward command to make the remote port 3000 available on our localhost.",en,"['Mailslurp', 'Email Apis For Developers']",2019-07-08 15:35:56.573000+00:00,"{'Data Science', 'Startup', 'Kubernetes', 'Programming', 'DevOps'}","{'https://miro.medium.com/fit/c/96/96/2*hGcPGDTOtpgRyNeRcZEnnw.png', 'https://miro.medium.com/max/60/1*MVcbybEAcD8rMWGUptObuQ.png?q=20', 'https://miro.medium.com/max/1598/1*2qmEGuFL9BI7wKy77xUdRg.png', 'https://miro.medium.com/fit/c/80/80/1*E2glGxVhP7xeDTJ-L7Fpnw.gif', 'https://miro.medium.com/max/1700/1*earINPFowa9ywksgHPMv_Q.png', 'https://miro.medium.com/max/60/1*4_ChROEmcIBYDdM5Xku7Nw.png?q=20', 'https://miro.medium.com/max/1598/1*hXWEfqx6UWY212e5-i1PYg.png', 'https://miro.medium.com/max/2268/1*qqxWe9J-4TMJC7S-9aqJyA.png', 'https://miro.medium.com/max/1598/1*jqkm3Qo_ug4H6Yq1OzbsdQ.png', 'https://miro.medium.com/fit/c/80/80/0*yC6o_snKKpDkqRFL.jpg', 'https://miro.medium.com/fit/c/160/160/2*hGcPGDTOtpgRyNeRcZEnnw.png', 'https://miro.medium.com/max/1200/1*C2tUE2V2d0hYQ-ehN_NG_A.png', 'https://miro.medium.com/max/60/1*JZVh1g6v9pVzr7BKSicUoA.png?q=20', 'https://miro.medium.com/max/8344/1*C2tUE2V2d0hYQ-ehN_NG_A.png', 'https://miro.medium.com/fit/c/80/80/1*HOhPvuxUiOm-CgOwx9SLZg.jpeg', 'https://miro.medium.com/max/2268/1*4_ChROEmcIBYDdM5Xku7Nw.png', 'https://miro.medium.com/max/60/1*2qmEGuFL9BI7wKy77xUdRg.png?q=20', 'https://miro.medium.com/max/60/1*qqxWe9J-4TMJC7S-9aqJyA.png?q=20', 'https://miro.medium.com/max/2276/1*MVcbybEAcD8rMWGUptObuQ.png', 'https://miro.medium.com/max/60/1*jqkm3Qo_ug4H6Yq1OzbsdQ.png?q=20', 'https://miro.medium.com/max/2268/1*sgW-LjyDIaDsoRVkeGsSYA.png', 'https://miro.medium.com/max/3414/1*JZVh1g6v9pVzr7BKSicUoA.png', 'https://miro.medium.com/max/60/1*hXWEfqx6UWY212e5-i1PYg.png?q=20', 'https://miro.medium.com/max/60/1*sgW-LjyDIaDsoRVkeGsSYA.png?q=20', 'https://miro.medium.com/max/60/1*C2tUE2V2d0hYQ-ehN_NG_A.png?q=20', 'https://miro.medium.com/max/60/1*earINPFowa9ywksgHPMv_Q.png?q=20'}",2020-03-05 00:06:32.313369,1.5934464931488037
https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64,Object-oriented programming for data scientists: Build your ML estimator,"What is the problem?

Data scientists often come from a background which is quite far removed from traditional computer science/software engineering — physics, biology, statistics, economics, electrical engineering, etc.

But ultimately, they are expected to pick up a sufficient amount of programming/software engineering to be truly impactful for their organization and business.

And, what is at the heart of most modern programming languages and software engineering paradigms?

Object-oriented programming (OOP).

But the principles of OOP can feel little alien or even intimidating to the uninitiated at first. Consequently, data scientists, whose background did not include formal training in computer programming, may find the concepts of OOP somewhat difficult to embrace in their day-to-day work.

The popular MOOCs and boot camps for data science/AI/ML do not help either.

They try to give the budding data scientists the flavor of a mixed soup of statistics, numerical analysis, scientific programming, machine learning (ML) algorithms, visualization, and perhaps even a bit of web framework to deploy those ML models.

Almost all of these can be learned and practiced even without rigorously adhering to the principles of OOP. In fact, young data scientists, who are hungry to learn the latest neural network architecture or the coolest data visualization techniques, may even feel suffocated if bombarded with all the nitty-gritty of the OOP programming paradigm. So, the MOOCs don’t generally mix or emphasize it in their data science curriculum.

A simple example (and some more…)

Let me give an example of this problem using Python, as it is the fastest growing language for data science and machine learning tasks.

The arithmetic example

If you are asked to write a program to implement addition, subtraction, multiplication, and division involving a couple of numbers a and b , what will you most likely do?

You will most likely open up a Jupyter notebook and type the following in a cell, hit shift-enter and get the result.

a+b

a-b

a*b

a/b

If you like to tidy things up by working with functions, then you may do,

def add(a,b):

return a+b

...

But will you go as far as defining (complete with an initializer method) a Calc class and putting these functions inside that class as methods? These are all operations of a similar nature and they work on similar data. Why not encapsulate them within a single higher-order object then? Why not the following code?

class Calc:

def __init__(self,a,b):

self.a = a

self.b = b

def add(self):

return self.a+self.b

def sub(self):

return self.a-self.b

def mult(self):

return self.a*self.b

def div(self):

return self.a/self.b

No, you won’t do this. It probably does not make sense to do it for this particular problem either. But the idea is valid — if you have data and functions (methods as they are called in the parlance of OOP), which can be combined logically, then they should be encapsulated in a class.

But it looks too much work just for getting quick answers to some simple numerical computations. So, what’s the point? Data scientists are often valued on whether they can get the right answer to the data problem, not on what elaborate objects they are using in the code.

Data scientist’s example

If data scientists are not coding this way, is it not the case, that they really don’t need to use these elaborate programming constructs?

Wrong.

Without consciously being aware, data scientists make heavy use of the benefits of the OOP paradigm. All the time.

Remember plt.plot after import matplotlib.pyplot as plt ?

Those . symbols. You have a dash of object-oriented programming. Right there.

Or, do you remember being happy to learn the cool trick in the Jupyter notebook — hitting Tab after putting a DOT (.), thereby showing all the functions that can be associated with an object? Like this,

What does this example show?

This example shows adherence to logical consistency.

Without the OOP paradigm, we would have to name those functions as linear_model_linear_regression_fit , linear_model_linear_regression_predict , and so on. They won’t be grouped under a common logical unit.

Why? Because they are different functions and work on a different set of data. While the fit function expects both training features and targets, predict needs only a test data set. The fit function is not expected to return anything, while predict is expected to return a set of predictions.

So, why are they visible under the same drop-down? In spite of being different, they have the commonality that they can both be imagined to be essential parts of the overall linear regression process — we expect a linear regression to fit some training data, and then be able to predict for future unseen data. We also expect the linear regression model to provide us some indication about how good the fit was — generally in the form of a single numeric quantity or score called coefficient of regression or R². As expected, we see a function score , which returns exactly that R² number, also hanging around fit and predict .

Neat and clean, isn’t it?

Data, functions, and parameters are cohabitating inside a single logical unit.

How was it made possible?

It was possible because we rose above the individual differences and thought about the linear regression as a high-level process and decided what essential actions it should serve and what critical parameters it should inform its users about.

We made a high-level class called LinearRegression under which all those apparently disparate functions can be grouped together for easy book-keeping and enhanced usability.

Once we imported this class from the library, we just had to create an instance of the class — we called it lm . That’s it. All the functions, grouped under the class, became accessible to us through that newly defined instance lm .

If we are not satisfied with some of the internal implementation of the functions, we can work on them and re-attach them to the main class after modification. Only the code of the internal function changes, nothing else.

See, how logical and scalable it sounds?

Create your own ML estimator

Traditional introduction to OOP will have plenty of examples using classes such as — animals, sports, geometric shapes.

But for data scientists, why not illustrate the concepts using the example of an object they use every day in their code — a machine learning estimator. Just like the lm object from the Scikit-learn library, shown in the picture above.

A good, old Linear Regression estimator — with a twist

In this Github repo, I have shown, step-by-step, how to build a simple linear regression (single or multivariate) estimator class following the OOP paradigm.

Yes, it is the good old linear regression class. It has the usual fit and predict methods as in the LinearRegression class from Scikit-learn. But it has more functionalities. Here is a sneak peek…

Yes, this estimator is richer than the Scikit-learn estimator in the sense that it has, in addition to standard fit , predict , and R² score functions, a host of other utilities which are essential for a linear regression modeling task.

Especially, for data scientists and statistical modeling folks — who not only want to predict but also would like to

How do you start building the class?

We start with a simple code snippet to define the class. We name it — MyLinearRegression .

Here, self denotes the object itself and __init__ is a special function which is invoked when an instance of the class is created somewhere in the code. As the name suggests, __init__ can be used to initialize the class with necessary parameters (if any).

We can add a simple description string to keep it honest :-)

We add the core fit method next. Note the docstring describing the purpose of the method, what it does and what type of data it expects. All of these are part of good OOP principles.

We can generate some random data to test our code so far. We create a linear function of two variables. Here are the scatter plots of the data.

Now, we can create an instance of the class MyLinearRegression called mlr . What happens if we try to print the regression parameters?

Because the self.coef_ was set to None , we get the same while trying to print mlr.coef_ . Note, how the self became synonymous to the instance of the class — mlr once it is created.

But the definition of fit includes setting the attributes once the fitting is done. Therefore, we can just call mlr.fit() and print out the fitted regression parameters.

The quintessential Predict method

After fitting, comes prediction. We can add that method easily to our regression class.

What if we want to add a (or a few) plotting utility function?

At this point, we start expanding our regression class and add stuff which is not even present in the standard scikit-learn class! For example, we always want to see how the fitted values compare to the ground truth. It is easy to create a function for that. We will call it plot_fitted .

Note that a method is like a normal function. It can take additional arguments. Here, we have an argument reference_line (default set to False ) which draws a 45-degree reference line on the fitted vs true plot. Also, note the docstring description.

We can test the method plot_fitted by simply doing the following,

m = MyLinearRegression()

m.fit(X,y)

m.plot_fitted()

Or, we can opt to draw the reference line,

m.plot_fitted(reference_line=True)

We get the following plots!

Once we understood that we can add any useful methods to work on the same data (a training set), related to the same purpose (linear regression), there is no bound to our imagination! How about we add the following plots to our class?

Pairplots (plots pairwise relation between all features and outputs, much like the pairs function in R)

(plots pairwise relation between all features and outputs, much like the function in R) Fitted vs. residual plot (this falls under diagnostic plots for the linear regression i.e. to check the validity of the fundamental assumptions)

plot (this falls under diagnostic plots for the linear regression i.e. to check the validity of the fundamental assumptions) Histogram and the quantile-quantile (Q-Q) plot of the residuals (this checks for the assumption of Normality of the error distribution)

Inheritance — don’t overburden your main class

As we enthusiastically plan utility methods to add to the class, we recognize that this approach may make the code of the main class very long and difficult to debug. To solve the conundrum, we can make use of another beautiful principle of OOP — inheritance.

We further recognize that all plots are not of the same type. Pairplots and fitted vs. true data plots are of similar nature as they can be derived from the data only. Other plots are related to the goodness-of-fit and residuals.

Therefore, we can create two separate classes with those plotting functions — Data_plots and Diagnostic_plots .

And guess what! We can define our main MyLinearRegression class in terms of these utility classes. That is an instance of inheritance.

Note: This may seem a little different from standard parent class-child class inheritance practice but for the same feature of the language is used here for keeping the main class clean and compact while inheriting useful methods from other similarly constructed classes.

Note the following code snippets are only for illustration. Please use the Github link above to see the actual code.

Data_plots class

Diagnostics_plots class

And the definition of MyLinearregression is changed only slightly,

class MyLinearRegression(Diagnostics_plots,Data_plots):



def __init__(self, fit_intercept=True):

self.coef_ = None

self.intercept_ = None

self._fit_intercept = fit_intercept

...

By simply passing on the reference of Data_plots and Diagnostics_plots to the definition of MyLinearRgression class, we inherit all the methods and properties of those classes.

Now, to check the Normality assumptions of the error terms, we can simply fit the model and run those methods.

m = MyLinearRegression() # A brand new model instance

m.fit(X,y) # Fit the model with some data m.histogram_resid() # Plot histogram of the residuals

m.qqplot_resid() # Q-Q plot of the residuals

We get,

Again, the separation of code is at work here. You can modify and improve the core plotting utilities without touching the main class. Highly flexible and less error-prone approach!

Do more with the power of OOP

We will not elaborate further on the various utility classes and methods we can add to MyLinearRegression . You can check the Github repo.

Additional classes added

Just for completeness, we added,

A class Metrics for computing various regression metrics — SSE, SST, MSE, R², and Adjusted R².

for computing various regression metrics — SSE, SST, MSE, R², and Adjusted R². A class Outliers to plot Cook’s distance, leverage, and influence plots

to plot Cook’s distance, leverage, and influence plots A class Multicollinearity to compute variance inflation factors (VIF)

All in all, the grand scheme looks like following,

Is this class richer than the Scikit-learn’s LinearRegression class? You decide.

Add syntactic sugar by creating grouped utilities

Once you have inherited other classes, they behave just like the usual Python module you are familiar with. So, you can add utility methods to the main class to execute multiple methods from a sub-class together.

For example, the following method runs all the usual diagnostics checks at once. Note how we are accessing the plot methods by putting a simple .DOT i.e. Diagnostics_plot.histogram_resid . Just like accessing a function from Pandas or NumPy library!

run_diagnostics method in the main class

With this, we can run all the diagnostics with a single line of code after fitting data.

m = MyLinearRegression() # A brand new model instance

m.fit(X,y) # Fit the model with some data m.run_diagnostics()

Similarly, you can add all the outlier plots in a single utility method.

Modularization — import the class as a module

Although not a canonical OOP principle, the essential advantage of following the OOP paradigm is to be able to modularize your code.

You can experiment and develop all this code in a standard Jupyter notebook. But for maximum modularity, consider converting the Notebook into a standalone executable Python script (with a .py extension). As a good practice, remove all the unnecessary comments and test code from this file and keep only the classes together.

Here is the link to the script I put together for this article.

Once you do that, you can import the MyLinearRgression class from a completely different Notebook. This is often the preferred way of testing your code as this does not touch the core model but only tests it with various data samples and functional parameters.

At this point, you can consider putting this Python script on a Github, creating a Setup.py file, creating the proper directory structure, and releasing it as a standalone linear regression package which does fitting, prediction, plotting, diagnostics, and more.

Of course, you have to add a lot of docstring description, examples of usage of a function, assertion checks, and unit tests to make it a good package.

But as a data scientist, now you have added a significant skill to your repertoire - software development following OOP principles.

It was not so difficult, was it?

Epilogue

Motivation and related articles

To write this post, I was inspired by this fantastic article, which drills down to the concept of OOP in Python in more detail with a context of machine learning.

I wrote a similar article, touching even more basic approaches, in the context of deep learning. Check it out here,

Courses?

I tried to look for relevant courses and found little if you are using Python. Most software engineering courses out there are taught using Java. Here are two which may be of help,","['scientists', 'fit', 'regression', 'function', 'add', 'ml', 'linear', 'programming', 'data', 'build', 'class', 'functions', 'oop', 'estimator', 'objectoriented', 'code']","Data scientist’s exampleIf data scientists are not coding this way, is it not the case, that they really don’t need to use these elaborate programming constructs?
While the fit function expects both training features and targets, predict needs only a test data set.
The fit function is not expected to return anything, while predict is expected to return a set of predictions.
A good, old Linear Regression estimator — with a twistIn this Github repo, I have shown, step-by-step, how to build a simple linear regression (single or multivariate) estimator class following the OOP paradigm.
to check the validity of the fundamental assumptions)plot (this falls under diagnostic plots for the linear regression i.e.",en,['Tirthajyoti Sarkar'],2019-07-31 21:02:12.325000+00:00,"{'Data Science', 'Software Development', 'Machine Learning', 'Programming', 'Technology'}","{'https://miro.medium.com/max/2798/1*3A2Gm2FTriEQfdQGfkcxbQ.png', 'https://miro.medium.com/max/996/1*dC-48wpBj22qgDo9vuJz0g.png', 'https://miro.medium.com/max/2262/1*iZ3hVlZXIB2X2qBc104l5A.png', 'https://miro.medium.com/max/3248/1*nEX7iiMS218eDmtmo2borA.png', 'https://miro.medium.com/max/1658/1*P5GqUKT1PgqcYP2L1EJ47A.png', 'https://miro.medium.com/max/1748/1*e0zKGYFqS3Mx4odyRPwC2Q.png', 'https://miro.medium.com/fit/c/160/160/1*dROuRoTytntKE6LLBKKzKA.jpeg', 'https://miro.medium.com/max/60/1*M_XOnlAT7CEkkhedW7kZEg.jpeg?q=20', 'https://miro.medium.com/max/60/1*hYxPOKNAefv_2B3NkFNsSw.png?q=20', 'https://miro.medium.com/max/60/1*sZDzG7sadyRntVCNm7PKfg.png?q=20', 'https://miro.medium.com/max/60/1*Z3VS7Uu_VHxqFSuDwnKcMg.png?q=20', 'https://miro.medium.com/max/9692/1*M_XOnlAT7CEkkhedW7kZEg.jpeg', 'https://miro.medium.com/max/60/1*UdREqLUvf8_gG2hT6gRqiQ.png?q=20', 'https://miro.medium.com/max/60/1*nEX7iiMS218eDmtmo2borA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1806/1*A0C8A1kJdM6pU4h9qOXSkA.png', 'https://miro.medium.com/max/42/1*oNK55U3LdmXK5-6aIFVGvg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*dROuRoTytntKE6LLBKKzKA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*_TrlIleH6DqR_eZumY1PBQ.png?q=20', 'https://miro.medium.com/max/60/1*UtGDL8Ol7WZMoaBssJVkog.png?q=20', 'https://miro.medium.com/max/60/1*QHBGiPDfwkOqF-MDL_a1bQ.png?q=20', 'https://miro.medium.com/max/2574/1*QHBGiPDfwkOqF-MDL_a1bQ.png', 'https://miro.medium.com/max/1062/1*oNK55U3LdmXK5-6aIFVGvg.png', 'https://miro.medium.com/max/1310/1*IvHmTCLzsIwY3tk0wnxmNA.png', 'https://miro.medium.com/max/60/1*dC-48wpBj22qgDo9vuJz0g.png?q=20', 'https://miro.medium.com/max/60/1*A0C8A1kJdM6pU4h9qOXSkA.png?q=20', 'https://miro.medium.com/max/60/1*P5GqUKT1PgqcYP2L1EJ47A.png?q=20', 'https://miro.medium.com/max/1716/1*2WGuijSPcK5V0OVkR5CBrA.png', 'https://miro.medium.com/max/60/1*3uiWtj3zk-3Afa3rgMU--Q.png?q=20', 'https://miro.medium.com/max/1200/1*M_XOnlAT7CEkkhedW7kZEg.jpeg', 'https://miro.medium.com/max/2326/1*_TrlIleH6DqR_eZumY1PBQ.png', 'https://miro.medium.com/max/1830/1*ojeY7TwWZR_jmi9RDYNraQ.png', 'https://miro.medium.com/max/60/1*ojeY7TwWZR_jmi9RDYNraQ.png?q=20', 'https://miro.medium.com/max/1762/1*Z3VS7Uu_VHxqFSuDwnKcMg.png', 'https://miro.medium.com/max/4180/1*Nzq0H9VzkC-L4ap3hLxR1g.png', 'https://miro.medium.com/max/60/1*iZ3hVlZXIB2X2qBc104l5A.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*TYGDjhKRO6b0JT-fRrQX1w.png?q=20', 'https://miro.medium.com/max/2598/1*Breja-YYTpyKdAUupZPxEg.png', 'https://miro.medium.com/max/2198/1*TYGDjhKRO6b0JT-fRrQX1w.png', 'https://miro.medium.com/max/3080/1*sZDzG7sadyRntVCNm7PKfg.png', 'https://miro.medium.com/max/60/1*Nzq0H9VzkC-L4ap3hLxR1g.png?q=20', 'https://miro.medium.com/max/1878/1*UtGDL8Ol7WZMoaBssJVkog.png', 'https://miro.medium.com/max/60/1*e0zKGYFqS3Mx4odyRPwC2Q.png?q=20', 'https://miro.medium.com/max/60/1*8sW_El43vAm7FCtNnU9fQw.png?q=20', 'https://miro.medium.com/max/1858/1*UdREqLUvf8_gG2hT6gRqiQ.png', 'https://miro.medium.com/max/2578/1*hYxPOKNAefv_2B3NkFNsSw.png', 'https://miro.medium.com/max/3324/1*3uiWtj3zk-3Afa3rgMU--Q.png', 'https://miro.medium.com/max/60/1*3A2Gm2FTriEQfdQGfkcxbQ.png?q=20', 'https://miro.medium.com/max/60/1*IvHmTCLzsIwY3tk0wnxmNA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1712/1*8sW_El43vAm7FCtNnU9fQw.png', 'https://miro.medium.com/max/60/1*Breja-YYTpyKdAUupZPxEg.png?q=20', 'https://miro.medium.com/max/60/1*2WGuijSPcK5V0OVkR5CBrA.png?q=20'}",2020-03-05 00:06:42.130555,9.816185474395752
https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12,Case study: explaining credit modeling predictions with SHAP,"Introduction

At Fiddler labs, we are all about explaining machine learning models. One recent interesting explanation technology is SHAP (SHapely Additive exPlanations). To learn more about how SHAP works in practice, we applied it to predicting loan defaults in data from Lending Club.

We built three models (random, logistic regression, and boosted trees), and looked at several feature explanation diagrams from each.

We are not financial experts, so this blog post focuses on comparing algorithms, not insights into loan data.

Hopefully, we can bring SHAP into sharper focus.

What is SHAP?

SHAP (SHapely Additive exPlanations) is a way to build a linear explanation model of feature importance for a given prediction. It works for any model, even non-linear ones like boosted trees or deep learning neural networks. Some model types (like logistic regression or trees) have a fast exact version, while for others there are tradeoffs between approximations, speed, and sampling variability.

SHAP is appealing because it is based on the Shapley values from game theory, which are the only explanations within a broad class of possibilities that satisfy three useful properties, termed by Lundberg et al.:

1. local accuracy (roughly: the explanation model matches the real model at the point being explained)

2. missingness (Lundberg: “missingness requires features missing in the original input to have no impact”)

3. consistency (Lundberg: “Whenever we change a model such that it relies more on a feature, then the attributed importance for that feature should not decrease.”)

Lundberg says consistency allows us to meaningfully compare the feature importance between two models.

We won’t further re-explain SHAP here, since that would be a post by itself. For that, see the shap library, a blog post by the author, the 2017 NIPS paper, and a nice blog post by Cody Marie Wild. Note the SHAP python library (link) generated many of the graphs in this blog post.

Also, we’ve integrated SHAP into the Fiddler Explainable AI Engine.

The data

We took a set of loans at least 3 years old that were either paid (“Fully Paid”) or in default (“Charged Off”) from Lending Club data. You can download our cleaned dataset from github (link). If you wish to look at the meaning of variable names, we used mostly a subset of the variables from this Kaggle dataset (link) with this data dictionary (link).

We also cleaned the data in a variety of ways detailed in this Jupyter notebook (for example, removing variables with missing values). Also, we removed variables not available at loan application time (e.g., total_pymnt, the total amount paid), variables from Lending Club’s own model (e.g., int_rate, the interest rate, or sub_grade, the loan grade), and issue date (which might encode general economic conditions).

We chose the first 294,520 records (in date order) as a model training set, the next 56,568 as a validation set, and the final 119,015 rows as a test set. We train on the training set, and do our analysis on the validation set. (We never use the test set in this post.)

The default rate (of “Charging Off”) is 13% in training, and 15% in the validation set.

The models

We built models to predict whether a loan was “Charged Off”, i.e. not paid in full.

We used scikit-learn 0.20.2 to run a random predictor and a logistic regression (the old linear workhorse), lightGBM 2.2.3 for boosted decision trees, and SHAP library 0.28.5.

For modeling, we took a “kitchen sink” approach (i.e., toss in most of the variables), because people are often tempted to “let the machine sort it out.” (In practice, this may not be best. Choosing variables may depend on how much data you have, what it means, what your purpose for the model is, how well your model technology can handle highly related variables, and so on.) Our models have 130 features after pre-processing.

AUC (area under an ROC curve, a common measure of prediction accuracy) was 0.5 for random (as we’d expect), and 0.68 for both logistic and lightGBM. These are not fantastically accurate models, but better than random, which is what we need for learning purposes. Also, if we could predict loan default perfectly, we’d design faster-than-light travel next!

Speeding into the future after perfectly predicting loan default.

Measuring SHAP values on the models

We got SHAP explanatory values on a subsample of the validation data for each model. Let’s walk through what that means.

We chose a 1,000-row subsample from our validation set, to speed up the time-consuming SHAP calculation.

We used shap.KernelExplainer, which uses sampling with a kernel to estimate the SHAP values, as described in the SHAP paper. Roughly: for each feature in a single row of validation data, it changes the other features to randomly chosen values from other data points (that SHAP calls the background data), weighted by a kernel function to preserve the SHAP properties. That produces a large set of data to which it fits a linear model. It is a “local” model because the linear explanation model is fit around this single row of original data.

There are other, more specific explainers for our model types: we could use shap.LinearExplainer for our logistic model, and shap.TreeExplainer for our boosted trees model. They would be faster, and produce exact values. However, for simplicity we’ll use KernelExplainer for everything. (We actually checked exact versions with slightly different parameters, and they look similar, though not the same. We might post again on this topic.)

For background data, it is slow to use the whole training dataset, so (as recommended by the library) we used shap.kmeans to create 30 representative rows from another subsample of 20,000 points from the training set.

We run that SHAP local model for each of the 1,000 rows, so we end up with 1,000 SHAP values for each feature.

Why 20,000? Why 30? Why 1,000? This is all trading off amount of data for speed, to try to estimate SHAP values in minutes instead of hours or days.

Moreover, if you look at the shap code carefully, there are a number of other choices buried in the code. For example, the l1_reg parameter of the shap_values function says auto (the default) ‘uses “aic” [to regularize feature selection] when less that 20% of the possible sample space is enumerated, otherwise it uses no regularization.’ We ended up changing the l1_reg parameter to a different version of regularization to choose 20 features, because that is what we would use for this blog post.

There are many nuances to understand, or choices to make, or perhaps defaults to accept. Such is the life of a data scientist or machine learning engineer: it’s messy putting theory into practice.

Random classifier

First, as a sanity check, we tried a random classifier (DummyClassifier from scikit-learn, set to the default settings including the ‘stratified’ strategy, meaning it predicts randomly with the distribution of the training data).

Below is a histogram of the predicted default probabilities, showing mostly zeros (in line with the rate of default in the training set):

Below is an ROC curve, showing that the result are completely random (right on the diagonal). We don’t really need a graph, we could just report AUC of 0.5. However, the graph really brings it home.

Below are the mean absolute SHAP values for the 1,000 rows of our validation subsample from KernelExplainer. These are indicators of how important each feature is to prediction. The default graph shows the top 20 features.

There is some variation, but as we’d expect for a random predictor, most features are of similar importance. Also, we’ll see later that these mean values are small compared to the other models.

This is what the “missingness” property of SHAP looks like in random data. Really, none of these features are important. We could hope they would show up with zero importance, but apparently that would be too much to hope for. Since this is an average of absolute values of a bunch of random things, what we get is “small and undifferentiated importance”, as long as we have enough samples.

Below is a graph of every SHAP value for every feature in each of the 1,000 rows of the validation subsample.

Here’s how you interpret this graph:

as the feature value goes higher, the color moves from blue to red

the X axis shows the impact on a model prediction

the Y axis shows the feature, sorted by mean absolute SHAP value

These are the same 20 features as the previous graph, because this graph also sorts features by mean absolute SHAP value.

The graph looks to have fairly similar shapes for all the features. The tail is longer and thinner to the right than to the left, which is not an obvious outcome. It might have to do with the overall rate of default. (Note there is a visual gap in feature importances between 0 and 0.01. That is because of the regularization l1_reg parameter in shap_values that explicitly chooses the top 20 features. If we change that parameter, those gaps vanish.)

Red and blue intermingle freely, because feature values are not related to model predictions. So for any particular feature value (color), there will be some data rows with high SHAP values, and some with low SHAP values.

Logistic regression

Next we tried a logistic regression, a call to LogisticRegression that used all default values in scikit-learn 0.20.2. This is a logistic regression with some L2 regularization.

Below is a histogram of the predicted default probabilities.

They are roughly centered around the background rate of default in the training set (13%), but occasionally the model is slightly more certain of default.

Below is an ROC curve with an AUC of 0.68, showing that the results are not completely random (since they are above the diagonal).

For logistic regression, the SHAP overall (mean absolute) feature impact should be similar to the logistic regression coefficients.

Another way to think of this is that “local” explanations (SHAP linear approximations of model predictions around each data point) and “global” explanations (model coefficients) should be similar, although some factors will prevent an exact match, e.g., l1 regularization of the features (see the notebook).

Let’s look at regression coefficients versus SHAP values. Below are the regression coefficients.

Below are the SHAP importance values. We use the link=’logit’ parameter of KernelExplainer to make the feature values log-odds units, hence more comparable to the logistic regression. (An identity link function might be more human-interpretable because it is in probability units, but would likely match the logistic coefficients more poorly.)

First, the explanations agree a lot: 15 of the top 20 variables are in common between the top logistic regression coefficients and the SHAP features with highest mean absolute SHAP values. Below is a graph comparing these values for the top 20 logistic coefficients:

Second, the values are often similar in magnitude, though not the same. For example, the absolute value of the total_bc_limit coefficient is about 0.25 in the logistic fit, and the SHAP mean absolute importance value looks to be about 0.18.

In fact, the SHAP importance values all seem a bit lower than the absolute values of the logistic coefficients. Perhaps the regularized logistic regression and SHAP treat collinear (highly related) features slightly differently. Or perhaps the multiple levels of sampling in SHAP (both in the subsample we chose, and in the feature values chosen per data row) shift things around a bit.

We looked at a subsample of size 2,000 instead of 1,000, to see if the SHAP values would shift a lot. Below are a graph of those values.

They don’t shift much. Between a subsample of 1,000 and 2,000, all 20 variables are the same, in almost the same order, with almost the same magnitude. That gives us confidence that these values are stable.

Now let’s look at a more detailed display that shows SHAP values for the 1,000 points in our subsample of the validation set.

Let’s look at the top feature: total_acc. If the feature value is low (blue), the model prediction (probability of default) tends to be higher (X value is positive). I don’t know this data well enough to understand why having more accounts would make you less likely to default. Perhaps people who are trying to consolidate their debt have a lot of accounts at application time, and those people are more likely to pay off their consolidation loan? Or, perhaps this feature is highly correlated to another feature that predicts risk and the model has decided to use this feature to cancel out the effects of that other feature. The open_acc feature is similar.

The feature ‘fico_range_midpoint’ makes more sense to me. It represents the applicant’s credit score at time of application. If credit score is higher (red), predicted probability of default goes down (X value is negative).

Several other observations:

1. Looking at data instead of averages shows that some features (e.g., total_bc_limit with its long tail) have more outliers in SHAP values than other features (e.g., dti, with its squat shape).

2. SHAP effects are monotonic for a linear model: the color always goes smoothly from one color to the other across the graph. (Logistic regression is linear enough.)

3. Integer-valued features (like acc_open_past_24_mths or delinq_2yrs) map to evenly spaced SHAP values for a linear model

4. We can see indicator variables (e.g., “emp_length_10+ years”). Features with a smaller SHAP value impact (e.g., “emp_length_10+ years”) may have a higher overall (mean absolute) impact if they occur more often, compared to a larger impact but more rare (like purpose_credit_card).

5. Many features look to have small impact, but with a long tail.

lightGBM (boosted trees)

Now let’s look at boosted trees. We make a plain vanilla default call to lightgbm.train in lightGBM 2.2.3. Now there is no easily summarized global explanation like “coefficients.”

Below is a graph of the predicted default probabilities.

As with the logistic model, these values are roughly centered around the probability of default in the training data.

Below is an ROC curve with an AUC of 0.68, showing that the results are not completely random (above the diagonal). We didn’t gain much over logistic in this case.

Below is a list of lightGBM feature importances, as reported by the feature_importance function, with importance_type=’gain’.

And below is a graph of SHAP feature importances (mean absolute SHAP value).

Lundberg (the inventor of SHAP) explains that SHAP values disagree with information gain in his blog post about XGBoost, and why he trusts SHAP. In short, information gain attributes importance inconsistently when Lundberg makes a small change to a toy model (making “cough” +10 in the red model), because information gain gives more weight to nodes lower in the tree, when it should do the opposite (root nodes are more important).

After all that discussion of why they might be different, these SHAP mean absolute values agree surprisingly well with the lightGBM information gain. Below is a chart where we scale SHAP values to be visually comparable to the information gain values. The relative values of the features look quite similar. (Note: we have seen other data where this is not true.)

Below is a graph of all the SHAP values of a 1,000-row subsample of the validation set.

First, not all the features are monotonic. That is, the color doesn’t slide uniformly from red to blue. For example, if you look closely, the top feature annual_inc is a mixed cloud of blue and red for negative values. If I were working on this problem for real, I’d want to dig into that. It could be that there are different types of loan applications, for example dividing into quadrants: feature low/SHAP low, feature low/SHAP high, feature high/SHAP low, feature high/SHAP high.

However, many features are still monotonic. For example, high fico_range_midpoint (red) means a lower probability of default (X axis negative). Or dti (debt-to-income ratio), a feature people expect to be monotonically correlated to default rate, and it is (mostly).

(Side note: the average absolute SHAP values for boosted trees are an order of magnitude lower than those for logistic regression. This is because we ran logistic SHAP with a logit link function in order to make the SHAP values comparable to the logistic regression, but we ran the boosted trees SHAP without a logit link function.)

Below is the same graph, but on a 2,000-row subsample.

It is very similar-looking to the 1,000-row subsample. All top 20 variables are the same (with a couple of swaps in order). All the graphs look substantially graphically similar. This again gives us confidence that these measurements are stable.

Comparing logistic regression and boosted trees

Logistic regression and boosted trees had similar accuracy, an AUC of 0.68. Their predicted values also agree (correlation coefficient of 0.86).

However, their SHAP values don’t agree, likely because we used a logit link function for logistic but not boosted trees.

Summary

We looked at explanations for three models to predict defaults in loan data.

In the two models that are easier to globally understand, SHAP mostly matched our expectations.

Explanations for a random model show that no features stand out. All features contribute similar small amounts to the predictions.

Explanations for a logistic regression model show features that mostly correspond to the global regression coefficients.

In the case of boosted trees (a non-linear model), we had fewer expectations about what SHAP would say about the features. As it turned out, the logistic and boosted trees model do agree on feature importance, but not SHAP values, because we used different link functions for logistic regression and boosted trees.

Also, the detailed dataset SHAP values diagram added more to our understanding of each feature: we could see which features were monotonic and which were not; we could see long tails and the relative importance and strength of indicator variables; and so on.

Finally, we looked at one way to investigate the stability of SHAP by eye: compute a larger sample, and see if SHAP feature summary values look stable.

In conclusion: SHAP helps explain the features most important to model predictions, but there is still a lot of nuance to understanding and using it correctly.

Thanks to Peter Skomoroch, Luke Merrick and Scott Lundberg for their feedback.","['graph', 'feature', 'credit', 'regression', 'modeling', 'study', 'logistic', 'explaining', 'default', 'model', 'case', 'data', 'shap', 'features', 'values', 'predictions']","Measuring SHAP values on the modelsWe got SHAP explanatory values on a subsample of the validation data for each model.
We used shap.KernelExplainer, which uses sampling with a kernel to estimate the SHAP values, as described in the SHAP paper.
So for any particular feature value (color), there will be some data rows with high SHAP values, and some with low SHAP values.
For logistic regression, the SHAP overall (mean absolute) feature impact should be similar to the logistic regression coefficients.
Lundberg (the inventor of SHAP) explains that SHAP values disagree with information gain in his blog post about XGBoost, and why he trusts SHAP.",en,['Dan Frankowski'],2019-03-21 23:23:25.959000+00:00,"{'Shap', 'Peer To Peer Lending', 'Explainable Ai', 'Machine Learning', 'Fintech'}","{'https://miro.medium.com/max/60/0*7BBHlk6zSvijrOLj?q=20', 'https://miro.medium.com/max/3840/0*LL3r5gYP9f8cUx89.jpg', 'https://miro.medium.com/max/1642/0*1529Ty9SYtERWPKM', 'https://miro.medium.com/fit/c/80/80/1*4WaBuCSmIQie9tJuXJ2p8g.jpeg', 'https://miro.medium.com/max/1686/0*4s27uUPGmp_bh8nR', 'https://miro.medium.com/max/2172/0*bvorWhTDtIPGjtKq', 'https://miro.medium.com/max/1246/0*kJ1TmRGy5x5wgOKJ', 'https://miro.medium.com/max/60/0*mF5D4T0jdrmeKUJl?q=20', 'https://miro.medium.com/max/60/0*mYoc2oMIQXRQvu-I?q=20', 'https://miro.medium.com/max/60/0*CojOH31yX9Qx_rAt?q=20', 'https://miro.medium.com/fit/c/96/96/2*R3sTj0t0EaWvFkJa3WOITw.jpeg', 'https://miro.medium.com/max/60/0*8eP_r_Og1reDEm9F?q=20', 'https://miro.medium.com/max/2090/0*8eP_r_Og1reDEm9F', 'https://miro.medium.com/fit/c/80/80/1*_XZD2GQ3S0RLLLsEv8c2Vg.jpeg', 'https://miro.medium.com/fit/c/160/160/2*R3sTj0t0EaWvFkJa3WOITw.jpeg', 'https://miro.medium.com/max/60/0*LL3r5gYP9f8cUx89.jpg?q=20', 'https://miro.medium.com/max/1468/0*IPhb2-CEjQZXOXGL', 'https://miro.medium.com/max/2090/0*L85JQHrDINgXNzQe', 'https://miro.medium.com/max/60/0*TUJ82Tg2RJCaG-Hv?q=20', 'https://miro.medium.com/max/60/0*6mdGiSUmhy89Jsfs?q=20', 'https://miro.medium.com/max/2090/0*7BBHlk6zSvijrOLj', 'https://miro.medium.com/fit/c/80/80/1*SD3hMVq2yKA5BoxDIOp-AA.jpeg', 'https://miro.medium.com/max/60/0*kJ1TmRGy5x5wgOKJ?q=20', 'https://miro.medium.com/max/1272/0*6mdGiSUmhy89Jsfs', 'https://miro.medium.com/max/60/0*WhGBe_ZDbVq0-9k-?q=20', 'https://miro.medium.com/max/60/0*SdHcTD1eJrw98vXk?q=20', 'https://miro.medium.com/max/1532/0*mYoc2oMIQXRQvu-I', 'https://miro.medium.com/max/60/0*bvorWhTDtIPGjtKq?q=20', 'https://miro.medium.com/max/972/0*sw72zpOUMOmwo7MW', 'https://miro.medium.com/max/60/0*7viGJaYdJ_gBCgeG?q=20', 'https://miro.medium.com/max/970/0*SdHcTD1eJrw98vXk', 'https://miro.medium.com/max/2062/0*WhGBe_ZDbVq0-9k-', 'https://miro.medium.com/max/166/1*24LYj3uk7LjrPTS-rtKybA.png', 'https://miro.medium.com/max/1200/0*TUJ82Tg2RJCaG-Hv', 'https://miro.medium.com/max/2062/0*mF5D4T0jdrmeKUJl', 'https://miro.medium.com/max/60/0*4s27uUPGmp_bh8nR?q=20', 'https://miro.medium.com/fit/c/160/160/1*fG53-Efa8GfddCEGeqEiEA.png', 'https://miro.medium.com/max/60/0*RLAyfmZKQ30H65Ht?q=20', 'https://miro.medium.com/max/3200/0*TUJ82Tg2RJCaG-Hv', 'https://miro.medium.com/max/2144/0*7viGJaYdJ_gBCgeG', 'https://miro.medium.com/max/1232/0*nRzQk_nAHxqsRzCW', 'https://miro.medium.com/max/2062/0*LiLmagULDO9kyh5a', 'https://miro.medium.com/max/60/0*IPhb2-CEjQZXOXGL?q=20', 'https://miro.medium.com/max/970/0*RLAyfmZKQ30H65Ht', 'https://miro.medium.com/max/60/0*L85JQHrDINgXNzQe?q=20', 'https://miro.medium.com/max/60/0*LiLmagULDO9kyh5a?q=20', 'https://miro.medium.com/max/60/0*sw72zpOUMOmwo7MW?q=20', 'https://miro.medium.com/max/60/0*1529Ty9SYtERWPKM?q=20', 'https://miro.medium.com/max/60/0*nRzQk_nAHxqsRzCW?q=20', 'https://miro.medium.com/max/1600/0*CojOH31yX9Qx_rAt'}",2020-03-05 00:06:43.289269,1.1577558517456055
https://medium.com/bigdatarepublic/pachyderm-for-data-scientists-d1d1dff3a2fa,Pachyderm for data scientists,"Pachyderm in action

Let’s setup a local Pachyderm cluster. The example here is on Max OS, using homebrew, for other operating systems one can refer to the Pachyderm documentation.

We will now put Pachyderm in action by:

Installing the prerequisites

Installing Pachyderm

Putting data in Pachyderm

Creating a pipeline in Pachyderm

Processing new data with the Pipeline

Updating the pipeline

Prerequisites

We start with MiniKube, a local Kubernetes cluster:

$ brew cask install minikube

==> Satisfying dependencies

All Formula dependencies satisfied.

==> Downloading https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-darwin-amd64

######################################################################## 100.0%

==> Verifying SHA-256 checksum for Cask 'minikube'.

==> Installing Cask minikube

==> Linking Binary 'minikube-darwin-amd64' to '/usr/local/bin/minikube'.

🍺 minikube was successfully installed!

Then Kubernetes’ command line interface:

$ brew install kubectl

After which we can start the MiniKube cluster:

$ minikube start

Starting local Kubernetes v1.10.0 cluster...

Starting VM...

Downloading Minikube ISO

160.27 MB / 160.27 MB [============================================] 100.00% 0s

Getting VM IP address...

Moving files into cluster...

Downloading kubeadm v1.10.0

Downloading kubelet v1.10.0

Finished Downloading kubelet v1.10.0

Finished Downloading kubeadm v1.10.0

Setting up certs...

Connecting to cluster...

Setting up kubeconfig...

Starting cluster components...

Kubectl is now configured to use the cluster.

Loading cached images from config file.

Is our cluster running?

$ kubectl api-versions

admissionregistration.k8s.io/v1beta1

apiextensions.k8s.io/v1beta1

apiregistration.k8s.io/v1

apiregistration.k8s.io/v1beta1

apps/v1

apps/v1beta1

apps/v1beta2

authentication.k8s.io/v1

authentication.k8s.io/v1beta1

authorization.k8s.io/v1

authorization.k8s.io/v1beta1

autoscaling/v1

autoscaling/v2beta1

batch/v1

batch/v1beta1

certificates.k8s.io/v1beta1

events.k8s.io/v1beta1

extensions/v1beta1

networking.k8s.io/v1

policy/v1beta1

rbac.authorization.k8s.io/v1

rbac.authorization.k8s.io/v1beta1

storage.k8s.io/v1

storage.k8s.io/v1beta1

v1

Yes it is, so let’s continue.

Installing Pachyderm

With a running Kubernetes cluster we’re ready to deploy the Pachyderm services in our cluster.

First add the HomeBrew tap:

$ brew tap pachyderm/tap

==> Tapping pachyderm/tap

Cloning into '/usr/local/Homebrew/Library/Taps/pachyderm/homebrew-tap'...

remote: Counting objects: 11, done.

remote: Compressing objects: 100% (10/10), done.

remote: Total 11 (delta 5), reused 3 (delta 0), pack-reused 0

Unpacking objects: 100% (11/11), done.

Tapped 5 formulae (42 files, 29.2KB).

Then we can install the components locally:

$ brew install pachyderm/tap/pachctl@1.7

==> Installing pachctl@1.7 from pachyderm/tap

==> Downloading https://github.com/pachyderm/pachyderm/releases/download/v1.7.7/pachctl_1.7.7_darwin_amd64.zip

==> Downloading from https://github-production-release-asset-2e65be.s3.amazonaws.com/23653453/615d2200-b1c4-11e8-812f-222bad0b9676?X-Amz

######################################################################## 100.0%

🍺 /usr/local/Cellar/pachctl@1.7/v1.7.7: 3 files, 50.0MB, built in 10 seconds

And subsequently deploy it to our Kubernetes cluster:

$ pachctl deploy local

No config detected. Generating new config...

No UserID present in config. Generating new UserID and updating config at /Users/gerben/.pachyderm/config.json

serviceaccount/pachyderm created

clusterrole.rbac.authorization.k8s.io/pachyderm created

clusterrolebinding.rbac.authorization.k8s.io/pachyderm created

deployment.apps/etcd created

service/etcd created

service/pachd created

deployment.apps/pachd created

service/dash created

deployment.apps/dash created

secret/pachyderm-storage-secret created



Pachyderm is launching. Check its status with ""kubectl get all""

Once launched, access the dashboard by running ""pachctl port-forward""

Is it running?

$ kubectl get all

NAME READY STATUS RESTARTS AGE

pod/dash-5d974d8668-f9nc8 2/2 Running 0 1m

pod/etcd-66858555cd-hcr4w 1/1 Running 0 1m

pod/pachd-5d5c8759b8-8fg2k 1/1 Running 0 1m



NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE

service/dash NodePort 10.111.248.26 <none> 8080:30080/TCP,8081:30081/TCP 1m

service/etcd NodePort 10.97.89.252 <none> 2379:32379/TCP 1m

service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 9m

service/pachd NodePort 10.111.26.206 <none> 650:30650/TCP,651:30651/TCP,652:30652/TCP,999:30999/TCP 1m



NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE

deployment.apps/dash 1 1 1 1 1m

deployment.apps/etcd 1 1 1 1 1m

deployment.apps/pachd 1 1 1 1 1m



NAME DESIRED CURRENT READY AGE

replicaset.apps/dash-5d974d8668 1 1 1 1m

replicaset.apps/etcd-66858555cd 1 1 1 1m

replicaset.apps/pachd-5d5c8759b8 1 1 1 1m

Yes it is (if AVAILABLE and READY are not yet 1 , you have to wait a little longer).

Now we can start the Pachyderm daemon and dashboard:

$ pachctl port-forward

CTRL-C to exit

NOTE: kubernetes port-forward often outputs benign error messages, these should be ignored unless they seem to be impacting your ability to connect over the forwarded port.

Forwarding the dash (Pachyderm dashboard) UI port to http://localhost:30080 ...

Forwarding the pachd (Pachyderm daemon) port...

If you then go to the mentioned url, you will see a nice dashboard showing the status of all your (currently none) processes:

Pachyderm dashboard

Using Pachyderm — Create a repository

As we now have a working Pachyderm cluster, we can start using it.

As test case we’ll use the Iris dataset, divided in 3 parts. Just as code in Git, a file in Pachyderm lives in a repo. So we first create a repository:

pachctl create-repo iris

Next we add our file (first part of iris dataset), to the iris dataset, on the master branch:

pachctl put-file iris master /raw/iris_1.csv -f data/raw/iris_1.csv

Now let’s see what we’ve created. First get overview of all repo’s:

$ pachctl list-repo

NAME CREATED SIZE

iris 2 minutes ago 4.492KiB

And we can retrieve the commits made to it:

$ pachctl list-commit iris

REPO ID PARENT STARTED DURATION SIZE

iris d804137d942a4916864b803fe3f0be0f <none> 26 seconds ago Less than a second 4.492KiB

and we can list the contents, of a certain branch:

$ pachctl list-file iris master

NAME TYPE SIZE

raw dir 4.492KiB

The Pachdash dashboard also shows statistics about the repo. The cloud button allows you to manually upload data (it supports gcs://, as://, s3://, gs://, wasb:// or http://).

Using Pachyderm — create a pipeline

In Pachyderm there are repositories (holding the data) and pipelines (holding the transformations). Pipelines read from any repository, and have their own associated repository to hold their output.

A pipeline can be any arbitrary program, as long as it reads from a file, and writes to a file. It does have to be implemented as a Docker image. Pachyderm will then mount the source & destination repositories into the running image, allowing you to interact with the files. Pachyderm will ensure the files are retrieved from the repository and are moved into the final repository.

As pipeline we’ll use a python based command line script which uses a pickle file of a sklearn pipeline to create predictions. Pachyderm will make new or updates files available under the pfs mount. In our case, we’ll configure that to be /pfs/iris . Data is expected to be written to /pfs/out . Knowing that, we can create a simple python application, which we will store in dsprod/models/train_model.py :

This will run in a Docker image, containing also the conda environment (created from a environment.yml ) and the persisted model models/model.p . It’s too verbose to paste the full source code here, it can be found on Github. The Docker image is defined by the following Dockerfile :

We want to run our python file from within the conda environment. To have a simple entry point, the following runit.sh is used:

We now will create the Docker image and push it to Docker Hub, such that Pachyderm will be able to download it:

$ docker build . --tag gerbeno/iris:`date +%s`

...

Successfully tagged gerbeno/iris:1538732911

$ docker login

...

$ docker push gerbeno/iris:1538732911

Using Pachyderm — deploy pipeline

To be able to deploy the pipeline to Pachyderm, we need to tell Pachyderm which docker image to use, and which repository should be monitored for new source data. Output is always sent to the repository associated with the pipeline, having the name of the pipeline. Note that the pipeline spec is really flexible and allows a lot of configuration. In our case the following simple definition suffices:

The docker tag is added such that we can exactly specify which image is to be used. Using latest is possible, but then we won’t be able to force retrieval of a new docker. We would also lose reproducibility as the actual image depends on the moment of deployment. Therefore we use an explicit tag. Now we can finally deploy our transformation:

$ pachctl create-pipeline -f iris.json

It doesn’t give any feedback, so let’s ask Kubernetes if our Docker is running. The output shows that we now have one pod running our pipeline:

$ kubectl get all

NAME READY STATUS RESTARTS AGE

pod/dash-5d974d8668-f9nc8 2/2 Running 6 26d

pod/etcd-66858555cd-hcr4w 1/1 Running 3 26d

pod/pachd-5d5c8759b8-8fg2k 1/1 Running 3 26d

pod/pipeline-iris-classifier-v2-rjm6b 2/2 Running 0 6m



NAME DESIRED CURRENT READY AGE

replicationcontroller/pipeline-iris-classifier-v2 1 1 1 6m

The Pachdash also shows the changed pipeline. It changed in 3 aspects:

The iris repo shows that it’s input to the iris_classifier .

repo shows that it’s input to the . A new arrow is created, representing the iris_classifier

There is a companion repository named iris_classifier holding the output of our pipeline.

The pipeline runs jobs when data is either added or modified. As we already have some input data in place, which hasn’t been processed by this pipeline yet, deploying a dependent pipeline immediately results in a job:

$ pachctl list-job

ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE

a1c...388 iris_classifier/849...f22 34 seconds ago 6 seconds 0 1 + 0 / 1 4.492KiB 1.268KiB success

To get the logs from the job itself, we can simply ask Pachyderm, and it will give the stdout :

$ pachctl get-logs --job a1c352e12fb749e1ba3e6d9ab7c8b388

... - __main__ - INFO - Loading the model from models/model.p

... - __main__ - INFO - Predicting on /pfs/iris/raw/iris_1.csv

It’s also possible to get logging information from the pod using Kubernetes:

$ kubectl logs pod/pipeline-iris-classifier-v2-rjm6b user

{""pipelineName"":""iris_classifier"",""workerId"":""pipeline-iris-classifier-v2-rjm6b"",""master"":true,""ts"":""2018-10-08T19:51:52.807797579Z"",""message"":""Launching worker master process""}

{""pipelineName"":""iris_classifier"",""workerId"":""pipeline-iris-classifier-v2-rjm6b"",""master"":true,""ts"":""2018-10-08T19:51:52.868557066Z"",""message"":""waitJob: a1c352e12fb749e1ba3e6d9ab7c8b388""}

Using Pachyderm — new data

With the pipeline in place, it will immediately process any data that is added or modified in the repository. Note that the granularity of processing is the complete file, Pachyderm won’t diff the files. A new file can then be added, and it will then show 2 runs.

$ pachctl put-file iris master /raw/iris_2.csv -f data/raw/iris_2.csv

$ pachctl list-job

Using Pachyderm — updating a pipeline

As the pipeline holds your latest greatest transformations, it’s destined to be changed. It’s also possible that a previous pipeline has some errors (like forgetting to chmod +x *.sh ).

In that case one updates the docker image, and can push that as a replacement to Pachyderm. An issue is the explicit docker tag reference in iris.json . With a new docker, one should also need to update the reference. Therefore the documentation of Pachyderm shows that using --push-images one can have that done automatically. So let’s update our pipeline accordingly:

$ pachctl update-pipeline -f iris.json --push-images

error parsing auth: Failed to read authentication from dockercfg, try running `docker login`

$ docker login

Authenticating with existing credentials...

Login Succeeded

$ pachctl update-pipeline -f iris.json --push-images

error parsing auth: Failed to read authentication from dockercfg, try running `docker login`

Unfortunately that doesn’t work as expected, at least not on my machine. Let’s do it ourselves:

$ docker build . --tag gerbeno/iris:`date +%s`

Successfully tagged gerbeno/iris:[tag]

Now we have to update our pipeline configuration iris.json to refer to our new docker tag. Note that you can’t use a tag like latest , as Pachyderm will only download it once. Pachyderm will download the docker from DockerHub, so let’s push it there, and subsequently update the pipeline:

$ docker login

...

$ docker push gerbeno/iris:[tag]

$ pachctl update-pipeline -f iris.json

The new pipeline is then in place, and will run for any updates or additions in the source repository. If the previous version was incorrect (or just failed), the files are already processed. In that case you can reprocess all existing data as follows:","['scientists', 'kubernetes', 'minikube', 'pachyderm', 'docker', 'image', 'data', 'pachctl', 'repository', 'running', 'pipeline']","Installing PachydermWith a running Kubernetes cluster we’re ready to deploy the Pachyderm services in our cluster.
Using Pachyderm — create a pipelineIn Pachyderm there are repositories (holding the data) and pipelines (holding the transformations).
Pachyderm will then mount the source & destination repositories into the running image, allowing you to interact with the files.
Successfully tagged gerbeno/iris:1538732911$ docker login...$ docker push gerbeno/iris:1538732911Using Pachyderm — deploy pipelineTo be able to deploy the pipeline to Pachyderm, we need to tell Pachyderm which docker image to use, and which repository should be monitored for new source data.
In that case one updates the docker image, and can push that as a replacement to Pachyderm.",en,['Gerben Oostra'],2018-11-24 20:13:26.142000+00:00,"{'Pachyderm', 'Data Engineering', 'Data Science', 'Data', 'Docker'}","{'https://miro.medium.com/max/2494/1*pqBzTPPz0KAjz_77vzgtsQ.png', 'https://miro.medium.com/fit/c/80/80/1*uLt7KmAcmllD88BlXmYfnQ.jpeg', 'https://miro.medium.com/max/1200/1*Fku-hdxkIc3bOfAtfRhoNQ.png', 'https://miro.medium.com/fit/c/80/80/1*DRFsgdI0ByEck1Qx2zaKGQ.jpeg', 'https://miro.medium.com/max/60/1*Fku-hdxkIc3bOfAtfRhoNQ.png?q=20', 'https://miro.medium.com/max/5608/1*xGxWP2Qur3syWj8IOpovuQ.png', 'https://miro.medium.com/max/600/1*Fku-hdxkIc3bOfAtfRhoNQ.png', 'https://miro.medium.com/fit/c/160/160/1*DRFsgdI0ByEck1Qx2zaKGQ.jpeg', 'https://miro.medium.com/max/60/1*xGxWP2Qur3syWj8IOpovuQ.png?q=20', 'https://miro.medium.com/max/60/1*12a5KL9ZqsS1Zzq9uizxAQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*15z1Vbg_K1RLZLSlIzM5Qg.png', 'https://miro.medium.com/fit/c/80/80/0*TLLAvqqo9cEusYNY', 'https://miro.medium.com/max/60/1*gxsz0mcLZZVDS5tKBGNYEA.png?q=20', 'https://miro.medium.com/max/3864/1*12a5KL9ZqsS1Zzq9uizxAQ.png', 'https://miro.medium.com/fit/c/96/96/1*DRFsgdI0ByEck1Qx2zaKGQ.jpeg', 'https://miro.medium.com/max/60/1*pqBzTPPz0KAjz_77vzgtsQ.png?q=20', 'https://miro.medium.com/max/146/1*XQIZETPN2FQWGo90KbDbHA.png', 'https://miro.medium.com/max/2768/1*gxsz0mcLZZVDS5tKBGNYEA.png'}",2020-03-05 00:06:46.231287,2.9410524368286133
https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a,The Hitchhikers guide to handle Big Data using Spark,"Transformation Basics

So let us say you have got your data in the form of an RDD.

To requote your data is now accessible to the worker machines. You want to do some transformations on the data now.

You may want to filter, apply some function, etc.

In Spark, this is done using Transformation functions.

Spark provides many transformation functions. You can see a comprehensive list here. Some of the main ones that I use frequently are:

1. Map:

Applies a given function to an RDD.

Note that the syntax is a little bit different from Python, but it necessarily does the same thing. Don’t worry about collect yet. For now, just think of it as a function that collects the data in squared_rdd back to a list.

data = [1,2,3,4,5,6,7,8,9,10]

rdd = sc.parallelize(data,4)

squared_rdd = rdd.map(lambda x:x**2)

squared_rdd.collect()

------------------------------------------------------

[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

2. Filter:

Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.

data = [1,2,3,4,5,6,7,8,9,10]

rdd = sc.parallelize(data,4)

filtered_rdd = rdd.filter(lambda x:x%2==0)

filtered_rdd.collect()

------------------------------------------------------

[2, 4, 6, 8, 10]

3. distinct:

Returns only distinct elements in an RDD.

data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]

rdd = sc.parallelize(data,4)

distinct_rdd = rdd.distinct()

distinct_rdd.collect()

------------------------------------------------------

[8, 4, 1, 5, 9, 2, 10, 6, 3, 7]

4. flatmap:

Similar to map , but each input item can be mapped to 0 or more output items.

data = [1,2,3,4]

rdd = sc.parallelize(data,4)

flat_rdd = rdd.flatMap(lambda x:[x,x**3])

flat_rdd.collect()

------------------------------------------------------

[1, 1, 2, 8, 3, 27, 4, 64]

5. Reduce By Key:

The parallel to the reduce in Hadoop MapReduce.

Now Spark cannot provide the value if it just worked with Lists.

In Spark, there is a concept of pair RDDs that makes it a lot more flexible. Let's assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data.

data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]

rdd = sc.parallelize(data,4)

Right now our RDD rdd holds tuples.

Now we want to find out the total sum of revenue that we got from each category.

To do that we have to transform our rdd to a pair rdd so that it only contains key-value pairs/tuples.

category_price_rdd = rdd.map(lambda x: (x[1],x[2]))

category_price_rdd.collect()

-----------------------------------------------------------------

[(‘Fruit’, 200), (‘Fruit’, 24), (‘Fruit’, 56), (‘Vegetable’, 103), (‘Vegetable’, 34)]

Here we used the map function to get it in the format we wanted. When working with textfile, the RDD that gets formed has got a lot of strings. We use map to convert it into a format that we want.

So now our category_price_rdd contains the product category and the price at which the product sold.

Now we want to reduce on the key category and sum the prices. We can do this by:

category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y)

category_total_price_rdd.collect()

--------------------------------------------------------- [(‘Vegetable’, 137), (‘Fruit’, 280)]

6. Group By Key:

Similar to reduceByKey but does not reduces just puts all the elements in an iterator. For example, if we wanted to keep as key the category and as the value all the products we would use this function.

Let us again use map to get data in the required form.

data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]

rdd = sc.parallelize(data,4)

category_product_rdd = rdd.map(lambda x: (x[1],x[0]))

category_product_rdd.collect()

------------------------------------------------------------

[('Fruit', 'Apple'), ('Fruit', 'Banana'), ('Fruit', 'Tomato'), ('Vegetable', 'Potato'), ('Vegetable', 'Carrot')]

We then use groupByKey as:

grouped_products_by_category_rdd = category_product_rdd.groupByKey()

findata = grouped_products_by_category_rdd.collect()

for data in findata:

print(data[0],list(data[1]))

------------------------------------------------------------

Vegetable ['Potato', 'Carrot']

Fruit ['Apple', 'Banana', 'Tomato']","['hitchhikers', 'function', 'category', 'transformation', 'vegetable', 'big', 'rdd', 'fruit', 'data', 'reduce', 'handle', 'spark', 'map', 'using', 'guide']","Transformation BasicsSo let us say you have got your data in the form of an RDD.
For now, just think of it as a function that collects the data in squared_rdd back to a list.
data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]rdd = sc.parallelize(data,4)Right now our RDD rdd holds tuples.
category_price_rdd = rdd.map(lambda x: (x[1],x[2]))category_price_rdd.collect()-----------------------------------------------------------------[(‘Fruit’, 200), (‘Fruit’, 24), (‘Fruit’, 56), (‘Vegetable’, 103), (‘Vegetable’, 34)]Here we used the map function to get it in the format we wanted.
data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]rdd = sc.parallelize(data,4)category_product_rdd = rdd.map(lambda x: (x[1],x[0]))category_product_rdd.collect()------------------------------------------------------------[('Fruit', 'Apple'), ('Fruit', 'Banana'), ('Fruit', 'Tomato'), ('Vegetable', 'Potato'), ('Vegetable', 'Carrot')]We then use groupByKey as:grouped_products_by_category_rdd = category_product_rdd.groupByKey()findata = grouped_products_by_category_rdd.collect()for data in findata:print(data[0],list(data[1]))------------------------------------------------------------Vegetable ['Potato', 'Carrot']Fruit ['Apple', 'Banana', 'Tomato']",en,['Rahul Agarwal'],2020-02-21 06:28:05.536000+00:00,"{'Data Science', 'Machine Learning', 'Towards Data Science', 'Programming', 'Big Data'}","{'https://miro.medium.com/max/58/1*1u6-deQqFV24NpV8wQxrYQ.png?q=20', 'https://miro.medium.com/max/2876/1*xbFnY8R2GMU4VizLXSoPYw.png', 'https://miro.medium.com/max/60/0*p3GQMEIPLX3TpTk0?q=20', 'https://miro.medium.com/max/2992/1*mqbJfUss0Bn1amBWjD9klA.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/5060/1*FlnBNKEiXOsvhmyuFouLeQ.png', 'https://miro.medium.com/max/60/0*_Xne4_sz6lroaINt.png?q=20', 'https://miro.medium.com/max/60/0*R3J6DVBDinxLU3h-.png?q=20', 'https://miro.medium.com/max/2860/1*U2ucRU02MofHF8G5_qdxTw.png', 'https://miro.medium.com/max/2884/1*kIY2mHEBo0SGfxi6tP_Jlg.png', 'https://miro.medium.com/max/2876/1*0dt_8cE4i-_KDrTUdM5tPg.png', 'https://miro.medium.com/max/2848/1*ZKetrC8Q9EHgJBHcVWm-Zg.png', 'https://miro.medium.com/max/60/1*vpFeIR-W-vm2vwFZWDW1qQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*4rmhtiqOfW3SNTdnTS1W5g.png', 'https://miro.medium.com/max/2876/1*tdmL9lWCrvYYVDtqjLHa2g.png', 'https://miro.medium.com/max/60/1*dbAOlFRRRRJPByxq-bClzw.png?q=20', 'https://miro.medium.com/max/1908/0*R3J6DVBDinxLU3h-.png', 'https://miro.medium.com/max/10796/0*e94sY_GitJyJz02J', 'https://miro.medium.com/max/2892/1*1tFAPP9sHFsybQLZDdwjAg.png', 'https://miro.medium.com/max/60/1*tdmL9lWCrvYYVDtqjLHa2g.png?q=20', 'https://miro.medium.com/max/5460/1*3xVyJoqUqMwLL5sx6vQSgA.png', 'https://miro.medium.com/max/1648/1*SgPCahKbLaAobCm--WJEAg.png', 'https://miro.medium.com/max/2888/1*vpFeIR-W-vm2vwFZWDW1qQ.png', 'https://miro.medium.com/max/60/1*SgPCahKbLaAobCm--WJEAg.png?q=20', 'https://miro.medium.com/max/60/1*nln4rJ01sulsOk1ZhbThgA.png?q=20', 'https://miro.medium.com/proxy/1*sQGVLk43kXJTEw1mtJRoDw.png', 'https://miro.medium.com/max/2876/1*h74EduHHrSsG5wgOMPo7NA.png', 'https://miro.medium.com/max/60/1*-T8LTnsXH2AhzhbmajacXw.png?q=20', 'https://miro.medium.com/max/60/1*mqbJfUss0Bn1amBWjD9klA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1200/0*Xb5ACP76W6d9jysr', 'https://miro.medium.com/max/60/1*FlnBNKEiXOsvhmyuFouLeQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*4rmhtiqOfW3SNTdnTS1W5g.png', 'https://miro.medium.com/max/60/0*e94sY_GitJyJz02J?q=20', 'https://miro.medium.com/max/60/1*1tFAPP9sHFsybQLZDdwjAg.png?q=20', 'https://miro.medium.com/max/60/1*kIY2mHEBo0SGfxi6tP_Jlg.png?q=20', 'https://miro.medium.com/max/2868/1*Kq_p7zOvdfBlI1xnSxKQlA.png', 'https://miro.medium.com/max/60/1*4yMESTh-H1vGXlCIobeDYg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*6PGa9_7ISy0CPt5hp1G-Eg.png?q=20', 'https://miro.medium.com/max/2876/1*4yMESTh-H1vGXlCIobeDYg.png', 'https://miro.medium.com/max/11232/0*Xb5ACP76W6d9jysr', 'https://miro.medium.com/max/60/1*3xVyJoqUqMwLL5sx6vQSgA.png?q=20', 'https://miro.medium.com/max/1234/0*_Xne4_sz6lroaINt.png', 'https://miro.medium.com/max/2876/1*6PGa9_7ISy0CPt5hp1G-Eg.png', 'https://miro.medium.com/max/60/1*4LZXgPaneDmXZcl9zy3haw.png?q=20', 'https://miro.medium.com/max/2860/1*rTy2s0F8DKVDfR4a5aw39Q.png', 'https://miro.medium.com/max/60/1*U2ucRU02MofHF8G5_qdxTw.png?q=20', 'https://miro.medium.com/max/60/1*DjRGPII7iHtjuBotlxfRgw.png?q=20', 'https://miro.medium.com/max/1688/1*nln4rJ01sulsOk1ZhbThgA.png', 'https://miro.medium.com/max/60/1*FGsdH8GOwMCE7RF7II_x4g.png?q=20', 'https://miro.medium.com/max/10336/0*TK-uI698Vdxjh5kL', 'https://miro.medium.com/max/60/1*h74EduHHrSsG5wgOMPo7NA.png?q=20', 'https://miro.medium.com/max/10368/0*wcxaKwNMIiEsGmW2', 'https://miro.medium.com/max/7920/0*p3GQMEIPLX3TpTk0', 'https://miro.medium.com/max/60/0*Xb5ACP76W6d9jysr?q=20', 'https://miro.medium.com/max/1756/1*1u6-deQqFV24NpV8wQxrYQ.png', 'https://miro.medium.com/max/2836/1*DjRGPII7iHtjuBotlxfRgw.png', 'https://miro.medium.com/max/60/1*Kq_p7zOvdfBlI1xnSxKQlA.png?q=20', 'https://miro.medium.com/max/60/1*nKoPRzASoH0rQ8Qof4b10g.png?q=20', 'https://miro.medium.com/max/2868/1*4LZXgPaneDmXZcl9zy3haw.png', 'https://miro.medium.com/max/2560/1*-T8LTnsXH2AhzhbmajacXw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2848/1*FGsdH8GOwMCE7RF7II_x4g.png', 'https://miro.medium.com/max/60/0*wcxaKwNMIiEsGmW2?q=20', 'https://miro.medium.com/max/60/1*Q4rljxTeBMZJzVsYBcL2CA.png?q=20', 'https://miro.medium.com/max/60/1*xbFnY8R2GMU4VizLXSoPYw.png?q=20', 'https://miro.medium.com/max/60/1*ZKetrC8Q9EHgJBHcVWm-Zg.png?q=20', 'https://miro.medium.com/max/60/1*0dt_8cE4i-_KDrTUdM5tPg.png?q=20', 'https://miro.medium.com/max/60/1*rTy2s0F8DKVDfR4a5aw39Q.png?q=20', 'https://miro.medium.com/max/1660/1*dbAOlFRRRRJPByxq-bClzw.png', 'https://miro.medium.com/max/60/1*LP9yglc4UeUxDFBoTlfS9w.png?q=20', 'https://miro.medium.com/proxy/1*nCX6bsSNUF_v2hFKgnaQIA.png', 'https://miro.medium.com/max/2960/1*Q4rljxTeBMZJzVsYBcL2CA.png', 'https://miro.medium.com/max/3264/1*nKoPRzASoH0rQ8Qof4b10g.png', 'https://miro.medium.com/max/60/0*TK-uI698Vdxjh5kL?q=20', 'https://miro.medium.com/max/2560/1*LP9yglc4UeUxDFBoTlfS9w.png', 'https://miro.medium.com/max/2880/1*VN4eADfelg6njlAWlZ5mgw.png', 'https://miro.medium.com/max/60/1*VN4eADfelg6njlAWlZ5mgw.png?q=20'}",2020-03-05 00:06:50.653432,4.421145677566528
https://medium.com/high-alpha/data-stream-processing-for-newbies-with-kafka-ksql-and-postgres-c30309cfaaf8,"Data Stream Processing for Newbies with Kafka, KSQL, and Postgres","With today’s terabyte and petabyte scale datasets and an uptick in demand for real-time analytics, traditional batch-oriented data processing doesn’t suffice. To keep up with the flow of incoming data, organizations are increasingly moving towards stream processing.

In batch processing, data are collected over a given period of time. These data are processed non-sequentially as a bounded unit, or batch, and pushed into an analytics system that periodically executes.

In stream processing, data are continuously processed as new data become available for analyzing. These data are processed sequentially as an unbounded stream and may be pulled in by a “listening” analytics system.

Though stream processing does not need to be real-time, it can enable data processing and analysis used by many real-time systems that may require fast reactions to incoming data — log or clickstream monitoring, financial analysis on a trading floor, or sensor data from an Internet of Things (IOT) device, for example.

Stream Processing Tools

Stream processing requires different tools from those used in traditional batch processing architecture. With large datasets, the canonical example of batch processing architecture is Hadoop’s MapReduce over data in HDFS. A number of new tools have popped up for use with data streams — e.g., a bunch of Apache tools like Storm / Twitter’s Heron, Flink, Samza, Kafka, Amazon’s Kinesis Streams, and Google DataFlow. And some tools are available for both batch and stream processing — e.g., Apache Beam and Spark. (Spark only sort of / kinda but I guess good enough. It is dropping support for its original streaming in favor of “structured streaming” for microbatch processing, which is non-ideal in some cases so I am pretend mad at it right now).

A Simple Recipe for Data Processing with Kafka and KSQL

My favorite new stream processing tool is Apache Kafka, originally a pub/sub messaging queue thought up by folks at LinkedIn and rebranded as a more general distributed data stream processing platform. Kafka takes data published by ‘producers’ (which may be, e.g., apps, files / file systems, or databases) and makes it available for ‘consumers’ subscribed to streams of different ‘topics.’ In my previous life as an astronomer, I did a lot of playing with Kafka for real-time distribution of alert data on new and changing astronomical object detection from some cool new telescopes [link for the curious].

Image pulled from: https://kafka.apache.org/intro.html

The Kafka ecosystem is growing in support and has been supplemented with the Kafka Streams system, for building streaming apps, and KSQL, a SQL-like stream interface. I like Kafka especially because of the availability of an API for user-friendly Python and its easy integration with many other tools via Kafka Connect.

Here I’ll outline a fully reproducible step-by-step tutorial on how to stream tables from Postgres to Kafka, perform calculations with KSQL, and sync results back to Postgres using Connect.

All materials are available on my GitHub at https://github.com/mtpatter/postgres-kafka-demo. To follow along, clone the repo to your local environment. You should need only Docker and docker-compose on your system.

Ingredients

We will be using the following technologies through Docker containers:

Kafka, the main data streaming platform

Zookeeper, Kafka’s sidekick used for managing consumers

KSQL server, which we will use to create live updating tables

Kafka’s schema registry, needed to use the Avro data format, a json-based binary format that enforces schemas on our data

Kafka Connect (pulled from Debezium), which will source and sink data back and forth to/from Postgres through Kafka

PostgreSQL (also pulled from Debezium and tailored for use with Connect)

Directions

The data used here were originally taken from the Graduate Admissions open dataset available on Kaggle. The admit csv files are records of students and test scores with their chances of college admission. The research csv files contain a flag per student for whether or not they have research experience.

1. Bring up the compute environment

docker-compose up -d

2. Load data into Postgres

We will bring up a container with a psql command line, mount our local data files inside, create a database called students , and load the data on students’ chance of admission into the admission table.

docker run -it --rm --network=postgres-kafka-demo_default \

-v $PWD:/home/data/ \

postgres:11.0 psql -h postgres -U postgres

Password = postgres

At the psql command line, create a database and connect:

CREATE DATABASE students;

\connect students;

Load the admission data table with:

CREATE TABLE admission

(student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE PRECISION, admit_chance DOUBLE PRECISION,

CONSTRAINT student_id_pk PRIMARY KEY (student_id)); \copy admission FROM ‘/home/data/admit_1.csv’ DELIMITER ‘,’ CSV HEADER

Load the research data table with:

CREATE TABLE research

(student_id INTEGER, rating INTEGER, research INTEGER,

PRIMARY KEY (student_id)); \copy research FROM ‘/home/data/research_1.csv’ DELIMITER ‘,’ CSV HEADER

3. Connect the Postgres database as a source to Kafka

The postgres-source.json file contains the configuration settings needed to sink all of the students database to Kafka:

Submit the source file to Connect via curl:

The connector postgres-source should show up when curling for the list of existing connectors:

curl -H “Accept:application/json” localhost:8083/connectors/

The two tables in the students database will now show up as topics in Kafka. You can check this by entering the Kafka container with the container ID (which you can get via docker ps );

docker exec -it <kafka-container-id> /bin/bash

and listing available topics:

/usr/bin/kafka-topics — list — zookeeper zookeeper:2181

4. Start KSQL

Bring up a KSQL server command line client as a container:



--interactive --tty --rm \

confluentinc/cp-ksql-cli:latest \

http://ksql-server:8088 docker run --network postgres-kafka-demo_default \--interactive --tty --rm \confluentinc/cp-ksql-cli:latest \

To see your updates, a few settings need to be configured by first running:

set ‘commit.interval.ms’=’2000';

set ‘cache.max.bytes.buffering’=’10000000';

set ‘auto.offset.reset’=’earliest’;

5. Mirror Postgres tables in KSQL

The Postgres table topics will be visible as dbserver1.public.admission and dbserver1.public.research in KSQL:

SHOW TOPICS;

We will create KSQL streams (a source stream subscribed to the corresponding Kafka topic and a rekeyed stream we need to populate a table) to auto update KSQL tables mirroring the Postgres tables. Since the data are sourced from Postgres via Connect, the data format will be set to Avro.

CREATE STREAM admission_src (student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE) \

WITH (KAFKA_TOPIC=’dbserver1.public.admission’, VALUE_FORMAT=’AVRO’); CREATE STREAM admission_src_rekey WITH (PARTITIONS=1) AS \

SELECT * FROM admission_src PARTITION BY student_id; SHOW STREAMS; CREATE TABLE admission (student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE)\

WITH (KAFKA_TOPIC=’ADMISSION_SRC_REKEY’, VALUE_FORMAT=’AVRO’, KEY=’student_id’); SHOW TABLES; CREATE STREAM research_src (student_id INTEGER, rating INTEGER, research INTEGER)\

WITH (KAFKA_TOPIC=’dbserver1.public.research’, VALUE_FORMAT=’AVRO’); CREATE STREAM research_src_rekey WITH (PARTITIONS=1) AS \

SELECT * FROM research_src PARTITION BY student_id; CREATE TABLE research (student_id INTEGER, rating INTEGER, research INTEGER)\

WITH (KAFKA_TOPIC=’RESEARCH_SRC_REKEY’, VALUE_FORMAT=’AVRO’, KEY=’student_id’);

The table and stream names I’ve used above are lowercase, but currently KSQL will enforce uppercase casing convention for stream, table, and field names no matter what.

6. Create downstream tables

We will create a new KSQL streaming table to join students’ chance of admission with research experience.

CREATE TABLE research_boost AS \

SELECT a.student_id as student_id, \

a.admit_chance as admit_chance, \

r.research as research \

FROM admission a \

LEFT JOIN research r on a.student_id = r.student_id;

and another table calculating the average chance of admission for students with and without research experience:

CREATE TABLE research_ave_boost AS \

SELECT research, SUM(admit_chance)/COUNT(admit_chance) as ave_chance \

FROM research_boost \

WITH (KAFKA_TOPIC=’research_ave_boost’, VALUE_FORMAT=’delimited’, KEY=’research’) \

GROUP BY research;

7. Add a connector to sink a KSQL table back to Postgres

The postgres-sink.json configuration file will create a RESEARCH_AVE_BOOST table and send the data back to Postgres:

Submit the sink file to Connect:

8. Update the source Postgres tables and watch the Postgres sink table update

The RESEARCH_AVE_BOOST table should now be available in Postgres to query:

SELECT “AVE_CHANCE” FROM “RESEARCH_AVE_BOOST”

WHERE cast(“RESEARCH” as INT)=0;

With these data the average admission chance will be 65.19%.

Add some new data to the admission and research tables in Postgres:

\copy admission FROM ‘/home/data/admit_2.csv’ DELIMITER ‘,’ CSV HEADER \copy research FROM ‘/home/data/research_2.csv’ DELIMITER ‘,’ CSV HEADER

With the same query above on the RESEARCH_AVE_BOOST table, the average chance of admission for students without research experience has been updated to 63.49%.

A few things to note:

The tables are forced by KSQL to uppercase and are case sensitive, which is annoying and also buggy. You can watch this KSQL GitHub issue for updates. The research field needs to be cast because it has been typed as text instead of integer (though it is integer type in KSQL), which may also be a bug in KSQL or Connect.

field needs to be cast because it has been typed as text instead of integer (though it is integer type in KSQL), which may also be a bug in KSQL or Connect. I use a Debezium PostgresConnector for my connector.class in my Connect source file and a Confluent JdbcSinkConnector for my connector.class in my Connect sink file. I tried both for source and sink, but this was the only configuration I could get to work correctly.

Wrapping Up

Now you should be in good shape to be able to start trying out KSQL with continuously running queries on your own database tables. Here we’ve walked through new data arriving via database table updates. Because we are using Kafka already, we could easily substitute Kafka producers publishing data directly to a Kafka topic or a continuously updating file (like a log, for example) to replace a Postgres table source. These data would appear as a stream available to KSQL just as above. There are a number of different Kafka connectors available for sourcing/sinking to/from databases, file systems, and even Twitter on the Confluent Hub.

Stream processing allows data analysis pipeline results to be continuously updated with the arrival of new data, which enables automation and scalability. This architecture is one of the many cool things we work with to build and scale analysis pipelines on the Data Science team at High Alpha.

If you have any comments/questions about data stream pipelines, feel free to drop me a line here or hit me up via Twitter @OpenSciPinay.","['ksql', 'stream', 'admission', 'newbies', 'postgres', 'kafka', 'data', 'research', 'integer', 'table', 'processing']","With today’s terabyte and petabyte scale datasets and an uptick in demand for real-time analytics, traditional batch-oriented data processing doesn’t suffice.
In stream processing, data are continuously processed as new data become available for analyzing.
Stream Processing ToolsStream processing requires different tools from those used in traditional batch processing architecture.
A Simple Recipe for Data Processing with Kafka and KSQLMy favorite new stream processing tool is Apache Kafka, originally a pub/sub messaging queue thought up by folks at LinkedIn and rebranded as a more general distributed data stream processing platform.
Stream processing allows data analysis pipeline results to be continuously updated with the arrival of new data, which enables automation and scalability.",en,['Maria Patterson'],2020-01-22 16:12:57.992000+00:00,"{'Data Pipelines', 'Apache Kafka', 'Data Science', 'Software Development', 'Big Data'}","{'https://miro.medium.com/fit/c/160/160/1*EWPlpjxW5BExG-G4vhf6GQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*cApOhpyngpYFOpdAtjC5Ig.jpeg', 'https://miro.medium.com/max/8056/1*235uqbLFqIJKifPdp34qTQ.jpeg', 'https://miro.medium.com/max/1200/1*235uqbLFqIJKifPdp34qTQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*1cUY2icrqUF70nw6tx4_Sw.jpeg', 'https://miro.medium.com/max/332/1*P8UrTWAQzhYZwiELkEFvMw.png', 'https://miro.medium.com/max/60/1*235uqbLFqIJKifPdp34qTQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*kQXkMQTrMrG4VJ3KZehaqA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*HIQTmjD-xF5ecT7w.jpeg', 'https://miro.medium.com/fit/c/80/80/1*1cUY2icrqUF70nw6tx4_Sw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*1cUY2icrqUF70nw6tx4_Sw.jpeg', 'https://miro.medium.com/max/2138/1*kQXkMQTrMrG4VJ3KZehaqA.png'}",2020-03-05 00:06:51.550176,0.8954792022705078
https://medium.com/unit8-machine-learning-publication/from-pandas-wan-to-pandas-master-4860cf0ce442,From Pandas-wan to Pandas-master,"Here is some description of the dataset.

>>> df.columns

Index(['country', 'year', 'sex', 'age', 'suicides_no', 'population',

'suicides_per_100k', 'country_year', 'HDI for year', 'gdp_year',

'gdp_capita', 'generation'],

dtype='object')

There are 101 countries, years ranging from 1985 to 2016, two genders, six generations, and six age groups. To get that information, a couple of useful methods might be handy.

unique() and nunique() to get unique values (or the number of unique values) inside columns.

>>> df['generation'].unique()

array(['Generation X', 'Silent', 'G.I. Generation', 'Boomers',

'Millenials', 'Generation Z'], dtype=object)

>>> df['country'].nunique()

101

describe() to output different statistical numbers (e.g. min, max, mean, count) for every numerical column, and — if specified with include=’all' — the number of unique elements and the top element (i.e. most frequent) for every object columns.

head() and tail() to visualize small portions of the data frame.

With all of these methods, you can have insights pretty quickly about the tabular file you are analyzing.

Memory optimization

Before handling data, an important step is to understand the data and choosing the right type for the columns of our data frame.

Internally, Pandas stores data frames as numpy arrays, for every different type (e.g. one float64 matrix, one int32 matrix).

Here are two methods that can drastically lower your memory consumption.

Pandas comes with a method memory_usage() that analyzes the memory consumption of a data frame. In the code, deep=True is specified to make sure that the actual system usage is taken into account.

Understanding column types is important. It can save you up to 90% of the memory usage with two simple things:

Knowing which types your data frame is using Knowing which types exist that could be used by your data frame to reduce memory usage (e.g. float64 might create unnecessary overhead if it is used in a price column where values range between 0 and 59 with one decimal)

Apart from lowering the size of a numerical type, you are using ( int32 instead of int64 ), Pandas comes with a categorical type.

If you are a R developper, you will recognize it as being the same thing as factor type.

This categorical type allows to replace repetitive values with an index and to store the actual values someplace else. The schoolbook example would be countries. Instead of storing many times the same string “Switzerland” or “Poland”, why not simply replace them by 0 and 1, and just store a dictionary.

categorical_dict = {0: 'Switzerland', 1: 'Poland'}

Pandas does pretty much the same while adding all of the methods to actually be able to use this type and still having the country names displayed.

Back to our method convert_df() , it automatically converts the column’s type to category if less than 50% of the values in the column are unique. This number is arbitrary but since casting types in a data frame means moving data between numpy arrays, the gain needs to be worth it.

Let’s see what happens with our data.

>>> mem_usage(df)

10.28 MB

>>> mem_usage(df.set_index(['country', 'year', 'sex', 'age']))

5.00 MB

>>> mem_usage(convert_df(df))

1.40 MB

>>> mem_usage(convert_df(df.set_index(['country', 'year', 'sex', 'age'])))

1.40 MB

By using our “smart” converter, the data frame is using almost 10x (7.34x to be precise) less memory.

Indexing

Pandas is powerful but it costs something. When you load a DataFrame , it creates indices and stores the data inside numpy arrays. So what does it mean? Once the data frame is loaded, you can access the data incredibly fast if you manage your indices properly.



There are mainly two ways to access data, namely by indexing and by querying. Depending on the situation, you will rather go for one or another. However, Index (and MultiIndex) are, in most cases, the best choice. Let’s take the following example.

>>> %%time

>>> df.query('country == ""Albania"" and year == 1987 and sex == ""male"" and age == ""25-34 years""')

CPU times: user 7.27 ms, sys: 751 µs, total: 8.02 ms

# ==================

>>> %%time

>>> mi_df.loc['Albania', 1987, 'male', '25-34 years']

CPU times: user 459 µs, sys: 1 µs, total: 460 µs

What? 20x speedup?

You’re going to ask yourselves, how much time does it take to create this MultiIndex?

%%time

mi_df = df.set_index(['country', 'year', 'sex', 'age'])

CPU times: user 10.8 ms, sys: 2.2 ms, total: 13 ms

This is 1.5x time to execute the query. If you just want to retrieve data only once (which is rarely the case), query is the right method. Otherwise, stick with indices, your CPU will thank you for that.

.set_index(drop=False) allows to not drop the column(s) used as the new index

.loc[] / .iloc[] methods are performing really good when you want to read a data frame, but not to modify it. If you need to construct by hand (e.g. using loops), consider another data structure (e.g. dictionary, list) and then create your DataFrame once you have all the data ready. Otherwise, for every new row in your DataFrame , Pandas will update the index, which is not a simple hashmap.

>>> (pd.DataFrame({'a':range(2), 'b': range(2)}, index=['a', 'a'])

.loc['a']

)

a b

a 0 0

a 1 1

Due to this, an unsorted index can reduce performances. In order to check if an index is sorted and to sort it, there are mainly two methods.

%%time

>>> mi_df.sort_index()

CPU times: user 34.8 ms, sys: 1.63 ms, total: 36.5 ms

>>> mi_df.index.is_monotonic

True

To read for more details

Method chaining

Method chaining with DataFrame is an act of chaining multiple methods that return a DataFrame and therefore are methods from DataFrame class. In the current version of Pandas, the reason to use method chaining is to not store intermediate variables and to avoid the following situation:

And replace it with the following chain.

Let’s be honest, the second code snippet is way nicer and cleaner.","['sex', 'type', 'pandasmaster', 'frame', 'methods', 'data', 'times', 'pandaswan', 'values', 'dataframe', 'ms', 'using']","head() and tail() to visualize small portions of the data frame.
Memory optimizationBefore handling data, an important step is to understand the data and choosing the right type for the columns of our data frame.
Pandas comes with a method memory_usage() that analyzes the memory consumption of a data frame.
This number is arbitrary but since casting types in a data frame means moving data between numpy arrays, the gain needs to be worth it.
Once the data frame is loaded, you can access the data incredibly fast if you manage your indices properly.",en,['Rudolf Höhn'],2019-07-09 09:05:35.860000+00:00,"{'Pandas', 'Data Science', 'Python', 'Machine Learning', 'Best Practices'}","{'https://miro.medium.com/max/60/1*qr85NnTGp3EINGyZZ4frEA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/max/60/1*wayhpHDRJISZXrxtOniA_g.png?q=20', 'https://miro.medium.com/max/2602/1*wayhpHDRJISZXrxtOniA_g.png', 'https://miro.medium.com/max/1200/1*wayhpHDRJISZXrxtOniA_g.png', 'https://miro.medium.com/max/1018/1*_dWVFEu4unVkdTL8Is-ZoQ.jpeg', 'https://miro.medium.com/max/60/1*_dWVFEu4unVkdTL8Is-ZoQ.jpeg?q=20', 'https://miro.medium.com/max/1000/1*RKYXAT6UWwb4y0X2LBXPSA.jpeg', 'https://miro.medium.com/fit/c/160/160/2*bi7HmD0xBoZ9EuEzpQL8Dw.png', 'https://miro.medium.com/max/48/1*RKYXAT6UWwb4y0X2LBXPSA.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*n7ACRIZ3RsRo3GSSX1nRow.png', 'https://miro.medium.com/fit/c/80/80/1*ugQPlmpQUq7MvU5dpHk5Iw.jpeg', 'https://miro.medium.com/max/150/1*693QB5qkOqjUFA4cPPxDEw.png', 'https://miro.medium.com/fit/c/96/96/2*bi7HmD0xBoZ9EuEzpQL8Dw.png', 'https://miro.medium.com/max/1108/1*qr85NnTGp3EINGyZZ4frEA.jpeg'}",2020-03-05 00:06:52.783963,1.2337872982025146
https://medium.com/@8451marketing/linux-for-data-scientists-part-1-9fd8b0beb208,"Linux for Data Scientists, Part 1","Ethan Swan, Data Scientist

Everyone’s path to data science is different. Some of my colleagues have backgrounds in fields like anthropology, psychology, philosophy, physics, and finance. But to be reductionist, most people enter the data science world from one of two directions: mathematics/statistics and computer science.

I majored in computer science in my undergraduate career, and my strengths lie there. Because of my background, some of my mathematically-inclined colleagues have solicited my advice on how to become more adept with computers. Personally, I believe that acquiring a base level of computer knowledge can make a data scientist several times more productive (especially on routine projects). This efficiency is gained primarily in two ways: coming to understand the Linux environment in which you work, and becoming skilled at manipulating that environment. This distinction may seem arbitrary, but it’s important to draw the line between knowing and using; the latter without the former can lead to major mistakes.

You may notice I’ve assumed above that your environment is running Linux. While I am myself a fan of the Linux family of operating systems, this isn’t just a personal bias. The vast majority of serious computing platforms run on Linux, as do nearly all web servers. Linux has become the de facto standard for production-quality and high-performance software, especially in data science. For this reason, this post will assume that your regular environment is running Linux, though MacOS supports many of these features as well.

Getting down to business, this walkthrough assumes you already know the very basics of Linux, such as:

Knowing your environment

Looking under the hood is daunting, but you can’t unlock the full power of your tools (or your own skills) until you know your environment.

This is especially true when your code fails or runs slowly. Where is the bottleneck? It takes a bit of computer know-how to figure this out, but once you do, significant performance improvements become attainable. For example, if you determine that your program is CPU-bound, you can take steps to parallelize your algorithm. If memory is the problem, you may decide that you need to implement your algorithm in a distributed environment like Spark.

top

First, let me introduce top , a great one-stop shop for information about what’s happening on your machine.

top shows you the processes running on your machine, ordered by resource consumption (thus the name). For most users, the important columns are %CPU and %MEM – these indicate the percent of CPU time being used and the percent of memory being occupied by the program, respectively. By default, programs are ordered by %CPU.

Many users run top every day in its basic form, but aren’t familiar with some simple ways to get more detailed information. Pressing M (capital M, to be clear) will reorder the list by %MEM. Pressing 1 will break down resource consumption at the core level instead of the program level. This can be useful to determine whether further parallelization could make your scripts faster. u followed by a username and <enter> will limit the list to only processes run by that user. Last, q returns you to the command prompt.

As a general rule of thumb, top should be the first tool you turn to when diagnosing one of your own programs.

free

When you want a very high-level view of the memory situation on a Linux machine, use free . When deployed with the -h flag (for human-readable, meaning that raw counts of bytes should be converted to megabytes, gigabytes, etc.), free will display a simple readout of how much memory is used and available on your system.

What you should look at is total (how much memory your machine has in total), used (how much memory is in use), and available (how much memory is left for you to use). You may notice that the free column displays a different count than the available column — this is not unusual. Some memory is not fully freed by the operating system until necessary, but it is available if needed.

Note that free , unlike all of the other commands in this article, is not implemented in MacOS. See this old StackExchange question if you’re interested.

du

Short for disk usage, du is extremely useful for estimating the size of directories. You may have noticed that running ls -l on a directory shows the directory as only consuming a tiny amount of disk space – often 4K or less.

Of course, the items in this directory might be using more than 4096 bytes; but you asked Linux for the size of the directory itself, and it reported to you how much space the abstraction of a “directory” actually requires. Since Linux treats everything as a file, a directory is just a small file that points to other files (whatever that directory contains).

However, you’re a human and the size of a file that points to other files is probably not what you were after. It’s here that du is a real asset. du will do the hard work of totalling all the contents of a directory, recursively, so you can accurately assess how much space a directory is taking on disk. As with free , du is best used with the -h flag so that its readout is in megabytes or gigabytes as appropriate. And to tell du that we want just a single output per input (not the size of every single thing inside a directory, listed), we also want to pass the -s flag for summarize. Thus, the full command we will run is du -sh <directory_name> .

408 kilobytes. A lot more than 4.

df

df stands for disk free. Because of their similar names, df and du are easily confused. However, df is for monitoring disk space at the filesystem level. Most enterprise servers are actually comprised of several separate but linked filesystems. Without getting into the nitty gritty details of filesystems, it’s just important to know that each entry displayed by df is a separate area with a finite storage limit – and it’s possible that one area can fill up entirely, preventing you from creating new files or modifying current ones there.

Run df with our friend -h to get a nice summary of the filesystems on a server.

The Mounted on column shows where the relevant filesystem is “mounted” on our current system. While something like /dev may look like any other folder, in this case it is actually a separate filesystem. That means that we could run out of space in /dev but still have room on the rest of the server — or vice versa.

It’s common for enterprises to link several servers by mounting a single filesystem on all of them, and then making the actual per-server disk space very small. But because many configuration files live in the /home directory, the individual servers may still run out of space and produce strange errors when unable to update configurations. This is when you should take a look at df to assess what’s going on.

Working with Files and Text

While it’s tempting to stay within the seemingly-friendly realm of GUI-based applications, becoming skilled at operating directly in the terminal can make you massively faster at typical tasks. The best example of this truth can be seen in Vim, a famously powerful but esoteric text editor. I highly recommend becoming a Vim power user, but I won’t talk about it here; the subject merits an entire article to itself. Other (simpler) command line tools have less-daunting learning curves than Vim, but still offer significant gains in productivity.

wc

The simplest command in our toolbox is wc . Its name stands for word count, but actually wc counts characters and lines as well as words. Invoke it with wc <filename> .

This file contains 95 lines, 308 words, and 3467 characters. Because this output is a little bit noisy, and usually a line count is enough, wc is commonly used with the -l flag to just return a line count.

grep

grep might be the most famous command line tool. It’s a superpowered text-searching utility. The name comes from Globally search a Regular Expression and Print. In its simplest form, grep <string> <filename> , it will look through a file and find all instances of a certain string. Let’s search a dictionary of valid words for my name.

Who knew I was in the dictionary?

Adding the -i flag makes your search case-insensitive (like most things Linux, grep is case-sensitive by default). When dealing with prose, like readme files, this is very handy. There are many, many more flags available, and grep can handle more complicated match strings, called regular expressions. Consult the man page ( man grep ) for the details.

awk

awk bears the distinction of being one of the least-intuitively named command line tools. The name is just an abbreviation for its authors: Aho, Weinberger, and Kernighan.

awk is essentially a very simple programming language optimized for dealing with delimited files (e.g. CSVs, TSVs). It’s Turing Complete, and in theory you could do pretty much anything with it, including data science. But please don’t.

The time for awk is when you have a text file that you want to manipulate like a table. If you find yourself thinking I need to see the third field of each line, or What’s the sum of the first four fields of this file?, you’re entering awk territory.

Invoke it with awk <command> <filename> . awk can be a little intimidating at first, so let’s look at an example. Here’s a file of Steph Curry’s stats by season, from Basketball Reference. Columns are separated by spaces.

Fields in awk are referenced by a dollar sign and their position; $1 is the first field, $4 is the fourth field, etc. (Note that awk is not zero-indexed!) Always issue your command in quotes to tell the shell not to parse it and instead to pass the whole string to awk . Since right now we want our command to run line-by-line, we will use curly braces within the quotes. If you wanted to see just the first column of each row, the command would be '{print $1}' . The first and the fifth field would be '{print $1, $5}' .

Okay, so we can extract columns. This comes in handy when examining big files. But we can do more! What if we wanted to know how many free throws plus field goal attempts Curry has recorded each season?

Why is the first line zero? Because the first line of the file was headers, we tried to add “FGA” to “FTA”. awk knows that these aren’t numbers, and so it just treats them as zeros.

This file was space-delimited, which obviously plays well with awk . How would we handle a file delimited by a different character? Fortunately, this is simple: the -F flag, for field separator, takes an argument that awk will use to split the file. To be safe, pass the argument in quotes.

Let’s split the Curry data on a dash instead of spaces, and print the first field.

awk returned each line up to the first dash, as expected. Using this technique, you can use awk on a huge variety of flat files.

We’ve barely scratched the surface of what awk can do, but this is a good starting point.

sed

sed stands for Stream EDitor. Like awk , it’s Turing Complete and extremely powerful. For now, we’re just going to focus on its find-and-replace abilities.

The basic syntax for this is sed 's/<find_pattern>/<replace_pattern>/g' <filename> . The ‘s’ stands for substitute, a more formal term for find-and-replace. The ‘g’ is for global – without it, sed will just replace the first occurence of your find pattern on each line.

Let’s try to modify the Curry data, to make it comma-delimited instead of space-delimited.

This output is very useful as long as we’re willing to copy and paste it. What if we wanted to actually modify the input file itself? sed supports this feature, using the -i (for in-place) flag. So sed -i 's/ /,/g' curry.txt would update the file to be a CSV – although if we wanted the file extension to reflect this change, we’d have to handle that ourselves.

Again, as with awk , sed has many many useful options and modes. At the beginning, though, try to just get comfortable with this functionality and research more options as you need them.

| (pipe)

We’ve gone through a lot of information so far, and maybe it doesn’t even seem that useful. How often do I only need to view a column or search for a word?, you might think, I need to do all of these things at the same time, sometimes repeatedly, which I can only do in a more fully-featured tool like Python or SQL.

But no! In fact, the simplicity of these tools is perhaps their greatest asset. Unix, the family of operatings systems from which Linux descends, is built on modularity. Each tool is to do one thing, and only one thing, and do it well. Tools can be chained together using an operator called the pipe, which takes output from one command and sends it to another as input.

Before demonstrating the pipe, I need to introduce another tool: cat . Short for concatenate, cat simply takes a filename and outputs its contents. Sticking with the Curry data:

cat , on the surface, seems like a superfluous tool. However, with the pipe at your disposal, it becomes highly useful:

Take a look at what’s going on here. Unlike when we used awk above, we didn’t need to provide it with a filename. Instead, it took the output of the command before it ( cat curry.txt ) and used that as its input. What if we wanted to make this output comma-delimited?

Pipes allow you to chain together as many commands as you’d like, using each command to transform the output of the last. This unlocks an enormous amount of power without requiring you to import data into a specialized tool. Remember, data movement is often the slowest part of a process, so operating directly on the filesystem can be extremely efficient.

Piping isn’t limited strictly to operations on files. Remember df ? What if we just wanted to see how much space is available on the home mount?

We just used grep to search for lines that contain ‘home’ in the output of df -h . Linux is designed with these types of operations in mind, so this logical chaining works surprisingly smoothly. Wondering how many years Curry played for Golden State?

This is a very common technique. Use a tool like grep to find rows of your data that meet a condition, and then use wc -l to count how many rows were selected.

With practice, you’ll start thinking in terms of piping. Complex operations become easier to break down into their components and corresponding tools required.

> (chevron)

The last feature I’ll talk about is the chevron (sometimes called “waka” in computer-speak). Both chevron directions ( < and > ) are supported, but the right-pointing one is generally more useful. > is used for output redirection: sending what usually displays on screen into a file instead.

Let’s start with an example:

This operation produces no output. What happened?

Aha! So we just created a file with the output of free -h . This could be useful for regularly logging how much memory is in use on the server.

Output redirection is especially useful when doing file manipulation. Above, in the pipe section, we extracted Curry’s field goal percentage each year and made it comma-delimited. You might want to save this modified data in a new file.

Suddenly, we’ve created a CSV of this data on disk, in one line. This is the power of the pipe and output redirection used in tandem.

Wrapping Up

Because this is a lot to digest, I want to close with a cheat sheet. Below is a table explaining, very briefly, what each tool we’ve discussed is used for.

With these tools in your belt, many operations typically reserved for high-level technologies are possible in Linux. You can save time that you once spent copying files locally, modifying them, and then copying them back to the server.

Whenever you are in need (or are simply interested) in more information about how a command works, look to to the man pages (short for manual). Though you might expect man pages to be terse and dry, in general they’re very readable and extremely comprehensive. A good place to begin would be man wc , which calls up a fairly short and digestible entry.","['scientists', 'file', 'files', 'awk', 'directory', 'used', 'linux', 'data', 'command', 'output', 'line']","Ethan Swan, Data ScientistEveryone’s path to data science is different.
Linux has become the de facto standard for production-quality and high-performance software, especially in data science.
Since Linux treats everything as a file, a directory is just a small file that points to other files (whatever that directory contains).
Other (simpler) command line tools have less-daunting learning curves than Vim, but still offer significant gains in productivity.
Unlike when we used awk above, we didn’t need to provide it with a filename.",en,[],2019-06-19 13:41:23.488000+00:00,"{'Linux', 'Computer Science', 'Linux Tutorial', 'Data Science'}","{'https://miro.medium.com/max/60/1*RqFJvFsWX5X2mddPK37OqQ.png?q=20', 'https://miro.medium.com/max/2092/1*Wm8N0gsr86Q1GpN2-WNPBQ.png', 'https://miro.medium.com/max/60/1*-uiJLbo1sugiMwP8q9ICdA.png?q=20', 'https://miro.medium.com/max/2592/1*77EDLmLrLs5YExWvwLHmWw.png', 'https://miro.medium.com/max/60/1*GvfO-GeYLMyckTR8NoVTPw.png?q=20', 'https://miro.medium.com/max/4264/1*-uiJLbo1sugiMwP8q9ICdA.png', 'https://miro.medium.com/max/60/1*77EDLmLrLs5YExWvwLHmWw.png?q=20', 'https://miro.medium.com/max/60/0*u06QoFCUyGxUdAWU.jpg?q=20', 'https://miro.medium.com/fit/c/160/160/0*luLUsMEf3_OkbX3e.jpg', 'https://miro.medium.com/max/2412/1*aylhpYjj0FFLjd9ZEgNVIg.png', 'https://miro.medium.com/max/2954/1*0UAI87vYyU0bIW7T-XV4nA.png', 'https://miro.medium.com/max/60/0*AbpKVQ8XStVIIo5U.jpg?q=20', 'https://miro.medium.com/max/2200/1*GvfO-GeYLMyckTR8NoVTPw.png', 'https://miro.medium.com/max/2344/1*s-1XX6YFAvzVzSV6_gujow.png', 'https://miro.medium.com/max/2512/1*RqFJvFsWX5X2mddPK37OqQ.png', 'https://miro.medium.com/max/2736/1*j9BDIqOoN_mqyAkeWeQhmw.png', 'https://miro.medium.com/max/2662/0*u06QoFCUyGxUdAWU.jpg', 'https://miro.medium.com/max/60/1*s-1XX6YFAvzVzSV6_gujow.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*Qwnj-D-pKrstIIjRUALTXA.jpeg', 'https://miro.medium.com/max/60/1*j9BDIqOoN_mqyAkeWeQhmw.png?q=20', 'https://miro.medium.com/max/60/1*0UAI87vYyU0bIW7T-XV4nA.png?q=20', 'https://miro.medium.com/max/60/1*aylhpYjj0FFLjd9ZEgNVIg.png?q=20', 'https://miro.medium.com/max/2760/1*Dpt_raZ_YmKvnMaXoo5fNA.png', 'https://miro.medium.com/max/4192/1*CzqaNoSBps_27VjVynND4Q.png', 'https://miro.medium.com/max/60/1*f_a2JfmWF2r7sA1cOU68tg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*luLUsMEf3_OkbX3e.jpg', 'https://miro.medium.com/max/60/1*kMN_fmiVjV6GBf9GV9eyTg.png?q=20', 'https://miro.medium.com/max/3644/1*UFhYIyNMv8QSg4MWFLwYCA.png', 'https://miro.medium.com/max/60/0*X3Ae9Msq3tq0T6vu.JPG?q=20', 'https://miro.medium.com/max/2964/1*kMN_fmiVjV6GBf9GV9eyTg.png', 'https://miro.medium.com/max/2322/1*WTudXLK3ZbYyCckc7FlHwA.png', 'https://miro.medium.com/max/60/1*UFhYIyNMv8QSg4MWFLwYCA.png?q=20', 'https://miro.medium.com/max/60/1*WTudXLK3ZbYyCckc7FlHwA.png?q=20', 'https://miro.medium.com/max/60/1*yguUAb2C30ZP3dlR7gV61w.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*luLUsMEf3_OkbX3e.jpg', 'https://miro.medium.com/max/1678/0*X3Ae9Msq3tq0T6vu.JPG', 'https://miro.medium.com/max/4212/1*yguUAb2C30ZP3dlR7gV61w.png', 'https://miro.medium.com/max/1922/1*INxpzXvLPAkwGdJL353dIg.png', 'https://miro.medium.com/max/1200/1*kMN_fmiVjV6GBf9GV9eyTg.png', 'https://miro.medium.com/max/60/1*Dpt_raZ_YmKvnMaXoo5fNA.png?q=20', 'https://miro.medium.com/max/1980/0*AbpKVQ8XStVIIo5U.jpg', 'https://miro.medium.com/max/60/1*INxpzXvLPAkwGdJL353dIg.png?q=20', 'https://miro.medium.com/max/2076/1*f_a2JfmWF2r7sA1cOU68tg.png', 'https://miro.medium.com/max/60/1*Wm8N0gsr86Q1GpN2-WNPBQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*GljF8L_ld119qC4OywbOHg.png', 'https://miro.medium.com/max/60/1*qCVgj06bQKFSdoGq4hplUg.png?q=20', 'https://miro.medium.com/max/60/1*6reUW2lA-9zjJQAUtcgdeQ.png?q=20', 'https://miro.medium.com/max/60/1*CzqaNoSBps_27VjVynND4Q.png?q=20', 'https://miro.medium.com/max/2262/1*6reUW2lA-9zjJQAUtcgdeQ.png', 'https://miro.medium.com/max/2868/1*qCVgj06bQKFSdoGq4hplUg.png'}",2020-03-05 00:06:53.746371,0.9624075889587402
https://towardsdatascience.com/build-your-own-data-dashboard-93e4848a0dcf,Build Your own Data Dashboard,"Definitive Guide for Data Professionals (Business Intelligence)

Build Your own Data Dashboard

Building dashboard web app quickly with Python Dash

Source: Unsplash

The Problem

The emergence of BI Dashboarding Tools. source: ezDataMunch

For data scientists, it is very important to communicate our data and results to the non technical users. Especially in the format which could be understood and reacted quickly. This is why data visualization is very important especially in BI products such as Power BI, Tableau and Qlikview.

Although they have an easy to use interface to produce stunning visualization, the technical licenses for these could be very costly. For Tableau, it could reach up to $50 per month. Furthermore for data professionals like me, I think that most BI Tools are not versatile enough to keep up with the dynamic growth of Python use cases. It still remains very clunky to embed this rather than seamlessly integrating with our web application.

Therefore, we need a better solution to this question.

Can we build an dashboard Web Application with Python for free?

The surprising answer is YES! I am going to exactly show you just that with the open source library — Dash Python.

Meet Dash!

Simply put, Dash is an open source Python Library to build web applications which are optimized for data visualization. The best thing about Dash is that it is built on top of Data visualization library such as Plotly and Matplotlib, Web Application Library (Flask), and finally data portable through Pandas! As the Javascript layer is handled via Plotly and Flask, you do not even need to touch other programming language to create a stunning web application.

The end result is the beautiful marriage of familiarity, conventions, and practicality. You could pick it up, develop, and deploy the application quickly. All of these are strictly writing only in Python and no other languages are necessary (although options are still available).

The power of Dash

One of the best features is that Dash supports declarative programming. This allows you to build Dash applications based on the input data and output properties. You will only state what you need and not the details on how to achieve your goal.

Source: Unsplash

Let us say you want to buy eggs.

Declarative Programming will say “ Find and buy eggs for me. Here are the cash”

However, Traditional Programming will say “ Go to the Courts Supermarket, go to the aisle 6 to find the eggs at your right hand corner, go to the cashier and pay with $5 cash”.

Obviously from this example declarative programming will offload the “how”. Similarly with money and eggs, you only need to hand over the input data and output properties then the visualization results will render automatically.

You do not even need to understand how Dash process your visualization. You will just instruct and receive the results. That is the beauty of Dash to support declarative programming.

In fact, this is not foreign at all. Many languages are also built with the same concepts. One example of the declarative language is SQL (Structured Query Language) and Kubernetes yaml. Both are important tools for Data Scientists to optimize their data retrieval and devops process.

Hope I make you excited!! Let’s get started

Dash in Action

Our Application in Big Picture, today we are going to learn how to build Dashboard with Dash

In this tutorial, you will learn how to build Dashboard Application with Dash Python.

We will visit our previous projects on task scheduler to web scrape data from Lazada (eCommerce) website and dump it into SQLite RDBMS Database. Then we will generate data visualizations to learn about price changes in Lazada Products over date and time.

Feel free to just enjoy this article or visit my Github Repo for the complete codes.

In this scenario let us visualize the change of price over the period of 3 days.

Price Optimization Dashboard Scraped from Lazada over 3 days

Let’s finish these 5 important steps to generate your first dashboard!

Importing and Activating Dash Preparing the data Visualizing Charts Dropdowns and Input Filter Selections Styling and Finishing

Importing and Activating Dash

As usual, let us import Dash Libraries on Python.

import dash

from dash.dependencies import Input, Output

import dash_core_components as dcc

import dash_html_components as html

you can download these libraries with a simple pip install command.

pip install <library name e.g: dash>

We will also activate the Dash Server with this code.

app = dash.Dash(__name__)

server = app.server

if __name__ == '__main__':

app.run_server(debug=True)

Once you run this Python script, it will run a server which you can open with http://127.0.0.1:8050/. This will open a flask web application which you could deploy anywhere such as Heroku. Notice that we put the run_server parameter debug =True. This will allow you to automatically update your local deployment once you save any changes in the scripts. Very neat time saving tricks to relaunch your application.

Info logs to run dash app on Command Prompt

Preparing the data

In this sector, we are going to read our product information from our database and dump them into a Pandas Dataframe. The dbm is a module that we created in SQLite RDBMS project before. This will dump a SQLite table into Pandas Dataframe which is called product_df . Feel free to extract the module from my Github.

global product_df

product_df = dbm.read()

The keyword global will globalize the product_df so that it is accessible to all call-back functions to generate data visualizations.

Visualizing Charts

Dash Graph with Component Details

We will create an app layout which will encapsulate html objects in the html module. This will be our main access to layout the graphs and and adjust the relative sizes to your view screen sizes.

app.layout = html.Div([

html.Div([

html.H1('Price Optimization Dashboard'),

html.H2('Choose a product name'),

dcc.Dropdown(

id='product-dropdown',

options=dict_products,

multi=True,

value = [""Ben & Jerry's Wake and No Bake Cookie Dough Core Ice Cream"",""Brewdog Punk IPA""]

),

dcc.Graph(

id='product-like-bar'

)

], style={'width': '40%', 'display': 'inline-block'}),

html.Div([

html.H2('All product info'),

html.Table(id='my-table'),

html.P(''),

], style={'width': '55%', 'float': 'right', 'display': 'inline-block'}),

html.Div([

html.H2('price graph'),

dcc.Graph(id='product-trend-graph'),

html.P('')

], style={'width': '100%', 'display': 'inline-block'})



])

Notice the dcc module. This is the dash core components which will store basic visualizations for web application such as barchart, dropdown, and line chart.

The rest is straightforward and specific to html module. You can create H1 or H2 headers, div (boxes to contain your web component), and even table. Think about it as the html codes abstracted so you do not need to even look at it.

Now let us talk about id . What does the id my-table in dcc.Graph(id=’my-table’) exactly mean? This shows the which function to call for a certain graph output. By inserting the code, we will call the function below.

@app.callback(Output('my-table', 'children'), [Input('product-dropdown', 'value')])

def generate_table(selected_dropdown_value, max_rows=20):



product_df_filter = product_df[(product_df['product_title'].isin(selected_dropdown_value))] product_df_filter = product_df_filter.sort_values(['index','datetime'], ascending=True)



return [html.Tr([html.Th(col) for col in product_df_filter .columns])] + [html.Tr([

html.Td(product_df_filter.iloc[i][col]) for col in product_df_filter .columns

]) for i in range(min(len(product_df_filter ), max_rows))]

On the top of the function code, you will see @app.callback which will run the magic. This means that you are exporting the return function exactly into the my-table component. You can also specify the input which is the selection from the drop down. This will be used to filter the product_df.

Noted the filtered product_df will be used to populate the return statement where we design the table using html module.

The beauty of this is that if you change the dropdown input in the dashboard, the function will render the filtered product_df.

Exactly how you use Tableau, but free and more versatile (plug and play)!!

Source: Meme Crunch

Dropdowns and Input Filter Selections

Notice the @app.callback input? This is the time where you can specify your own filter to render your visualization component.

dcc.Dropdown(

id='product-dropdown',

options=dict_products,

multi=True,

value = [""Ben & Jerry's Wake and No Bake Cookie Dough Core Ice Cream"",""Brewdog Punk IPA""]

),

The id is the same with the input annotation in the callback functions. This designates the function to call.

The options will insert the key value pairs of all the available options. This could be stocks ticker such as {‘GOOG’:google,’MSFT’: microsoft} or anything. For our case we will insert the same key value pairs which are the product name.

The multi attribute will allow you to select more than 1 option, which is perfect for this case to do a side by side price comparison in one chart.

Finally the value attribute will store your dropdown values at the start of server run.

Dropdown values to affect Python Dash Elements

Styling and Finishing

Styling in Dash is easy. By default, Dash already has a preconfigured setting to access assets folder. This is where you could overwrite the css for styling and js for web behavior. You can insert the stylesheet.css to beautify your Dash Web Application. Specific room for improvements would be the margin among components and table borders.

Inserting assets folder with stylesheet.css for styling

End Results

Congratulations!! You have created your first interactive dashboard. If you did it properly, you would be able to receive this result. If not, feel free to refer back to my Github Codes or post your questions here.

Your First Python Price Optimization Dashboard!!!

Now set free and create your own Dash Dashboard!

More References

If you need more examples and better insights of what Dash can do. Feel free to visit the following links. I assure you these will boost your Dashboard Design Skills to solve real life business problems.

Finally…

Source: Unsplash

I really hope this has been a great read and a source of inspiration for you to develop and innovate.

Please Comment out below to suggest and feedback. Just like you, I am still learning how to become a better Data Scientist and Engineer. Please help me improve so that I could help you better in my subsequent article releases.

Thank you and Happy coding :)

About the Author

Vincent Tatan is a Data and Technology enthusiast with relevant working experiences from Visa Inc. and Lazada to implement microservice architectures, business intelligence, and analytics pipeline projects.

Vincent is a native Indonesian with a record of accomplishments in problem solving with strengths in Full Stack Development, Data Analytics, and Strategic Planning.

He has been actively consulting SMU BI & Analytics Club, guiding aspiring data scientists and engineers from various backgrounds, and opening up his expertise for businesses to develop their products .

Please reach out to Vincent via LinkedIn , Medium or Youtube Channel","['web', 'function', 'dashboard', 'python', 'need', 'dash', 'data', 'build', 'visualization', 'application', 'input']","Definitive Guide for Data Professionals (Business Intelligence)Build Your own Data DashboardBuilding dashboard web app quickly with Python DashSource: UnsplashThe ProblemThe emergence of BI Dashboarding Tools.
Can we build an dashboard Web Application with Python for free?
Simply put, Dash is an open source Python Library to build web applications which are optimized for data visualization.
This allows you to build Dash applications based on the input data and output properties.
Let’s get startedDash in ActionOur Application in Big Picture, today we are going to learn how to build Dashboard with DashIn this tutorial, you will learn how to build Dashboard Application with Dash Python.",en,['Vincent Tatan'],2019-07-23 02:11:08.388000+00:00,"{'Web Development', 'Business', 'Data Science', 'Data Visualization', 'Data Analysis'}","{'https://miro.medium.com/max/60/0*5rMLWlPy5li2Sotk?q=20', 'https://miro.medium.com/max/2244/1*eS5yD0WHY7v9_OT1xE8Ddw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2000/0*5rMLWlPy5li2Sotk', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*hIwVz4L2c6DOC92EeyqWGg.png?q=20', 'https://miro.medium.com/max/60/0*GyMwNhwH-Wcr8z_E?q=20', 'https://miro.medium.com/max/1000/0*gD9i8cvINOXAuDQM', 'https://miro.medium.com/proxy/1*GHwxic3w73JFs2RB1gPESw.gif', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/56/1*ezWTGbgsIQeT6aQwVDdFng.png?q=20', 'https://miro.medium.com/max/60/0*gD9i8cvINOXAuDQM?q=20', 'https://miro.medium.com/max/60/0*ZJqb44o_vHwnm8aH.jpg?q=20', 'https://miro.medium.com/fit/c/96/96/2*5VLESjo09eI1gMxevfyehg.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2000/0*gD9i8cvINOXAuDQM', 'https://miro.medium.com/max/1386/1*hIwVz4L2c6DOC92EeyqWGg.png', 'https://miro.medium.com/max/524/0*yiTptWRIUXOv0mY4', 'https://miro.medium.com/freeze/max/60/1*UxDIipaiJ5IHPqclyQEAFg.gif?q=20', 'https://miro.medium.com/max/60/0*yiTptWRIUXOv0mY4?q=20', 'https://miro.medium.com/max/60/1*pk-QNpgvL8sDoOiN6cB4qw.png?q=20', 'https://miro.medium.com/max/384/1*ezWTGbgsIQeT6aQwVDdFng.png', 'https://miro.medium.com/max/3840/1*pk-QNpgvL8sDoOiN6cB4qw.png', 'https://miro.medium.com/max/1600/0*ZJqb44o_vHwnm8aH.jpg', 'https://miro.medium.com/max/2000/0*GyMwNhwH-Wcr8z_E', 'https://miro.medium.com/max/60/1*eS5yD0WHY7v9_OT1xE8Ddw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*UxDIipaiJ5IHPqclyQEAFg.gif', 'https://miro.medium.com/fit/c/160/160/2*5VLESjo09eI1gMxevfyehg.jpeg'}",2020-03-05 00:07:02.188232,8.440861463546753
https://medium.com/zenofai/machine-learning-operations-mlops-pipeline-using-google-cloud-composer-a8ebcbd6d766,Machine Learning Operations (MLOps) Pipeline using Google Cloud Composer,"In an earlier post, we had described the need for automating the Data Engineering pipeline for Machine Learning based systems. Today, we will expand the scope to setup a fully automated MLOps pipeline using Google Cloud Composer.

Cloud Composer

Cloud Composer is official defined as a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers. Built on the popular Apache Airflow open source project and operated using the Python programming language, Cloud Composer is free from lock-in and easy to use.

So let’s get on with the required steps to create this MLOps infrastructure on Google Cloud Platform

Creating a Cloud Composer Environment

Step1: Please enable the Cloud Composer API.

Step2: Go to create environment page in GCP console. Composer is available in Big Data section.

Step3: Click on create to start creating a Composer environment

Step4: Please select the Service account which has the required permissions to access GCS, Big Query, ML Engine and Composer environment. The required roles for accessing Composer environment is Composer Administrator and Composer Worker.

For more details about access control in Composer environment please see this.

Step5: Please use Python Version 3 and latest Image version.

Step6: Click on create. It will take about 15–20 minutes to create the environment. Once it completes, the environment page shall look like the following.

Click on Airflow to see Airflow WebUI. The Airflow WebUI looks as follows

DAGs folder is where our dag file is stored. DAG folder is nothing but a folder inside a GCS bucket which is created by the environment. To know more about the concept of DAG and general introduction to Airflow, please refer to this post.

You could see Composer related logs in Logging.

Step7: Please add the following PyPI packages in Composer environment.

Click on created environment and navigate to PYPI packages and click on edit to add packages

The required packages are:

# to read data from MongoDB

pymongo==3.8.0

oauth2client==4.1.3

# to read data from firestore

google-cloud-firestore==1.3.0

firebase-admin==2.17.0

google-api-core==1.13.0

Create a ML model

Step1: Please create a folder structure like the following on your instance.

ml_model

├── setup.py

└── trainer

├── __init__.py

└── train.py

Step2: Please place the following code in train.py file, which shall upload the model to GCS bucket as shown below. This model would be used to create model versions as explained a bit later. Please make sure you replace the placeholders below (in caps) with appropriate values for your use case.

from google.cloud import bigquery

import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

import numpy as np

from google.cloud import storage

import datetime

import json

import pickle

client = bigquery.Client()

sql = '''

SELECT *

FROM `<PROJECT_ID>.<DATASET>.<TABLENAME>`

''' df = client.query(sql).to_dataframe()

df = df[['is_stressed', 'is_engaged', 'status']] df['is_stressed'] = df['is_stressed'].fillna('n')

df['is_engaged'] = df['is_engaged'].fillna('n')

df['stressed'] = np.where(df['is_stressed']=='y', 1, 0)

df['engaged'] = np.where(df['is_engaged']=='y', 1, 0)

df['status'] = np.where(df['status']=='complete', 1, 0) feature_cols = ['stressed', 'engaged']

X = df[feature_cols]

y = df.status

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

logreg = LogisticRegression()

logreg.fit(X_train,y_train)

pkl_filename = ""model.pkl""

with open(pkl_filename, 'wb') as file:

pickle.dump(logreg, file)

BUCKET_NAME=BUCKET_NAME

# Upload the model to GCS

bucket = storage.Client().bucket(BUCKET_NAME)

file_path = datetime.datetime.now().strftime('machine_learning/models/%Y%m%d_%H%M%S')

blob = bucket.blob('{}/{}'.format(

file_path,

pkl_filename))

blob.upload_from_filename(pkl_filename) file_location = 'gs://{BUCKET_NAME}/{file_path}'.format(bucket_name=BUCKET_NAME, file_path=file_path)

file_config = json.dumps({'file_location': file_location}) COMPOSER_BUCKET = '<GCS-bucket-created-by-composer-environment>'

bucket = storage.Client().bucket(COMPOSER_BUCKET)

blob = bucket.blob('data/file_config.json')

blob.upload_from_string(file_config)

Step3: Create an empty init.py file inside the trainer directory.

Step4: Please place the following code in setup.py file. The setup.py file contains required packages to execute code.

import setuptools REQUIRED_PACKAGES = [

'pandas-gbq==0.3.0',

'cloudml-hypertune',

'google-cloud-bigquery==1.14.0',

'urllib3'

] setuptools.setup(

name='ml_model',

version='1.0',

install_requires=REQUIRED_PACKAGES,

packages=setuptools.find_packages(),

include_package_data=True,

description='',

)

Step5: Packaging the code using the following command. It creates a gz file inside ml_model directory.

python3 setup.py sdist

Step6: The package name is the name that is specified in setup.py file. The package name becomes ml_model-1.0.tar.gz

Copy the package to gs://{your-GCS-bucket}/machine_learning/. This becomes the base directory for your machine learning activities described in this post.

Creating a DAG

In this use case, we have created a DAG file which exports some table data from a MongoDB instance into a GCS bucket and then creates a BigQuery table off of that exported data. It trains a model and creates version for that model as shown in the code below. The DAG file supports full data extraction and daily data extraction explained in the code below using a variable tot_data. This variable is extracted from Airflow configurations set by the user. The process to set this configuration is described later in this post.

Please place the following code in the DAG file.

import airflow

from airflow import DAG

from airflow.models import Variable

from airflow.operators.bash_operator import BashOperator

from datetime import timedelta, datetime

from airflow.operators.python_operator import PythonOperator

import pprint

import json

import re from pymongo import MongoClient

from google.cloud import storage

from google.cloud.storage import blob

from google.cloud import storage

import os from airflow import models

from mlengine_operator import MLEngineTrainingOperator, MLEngineVersionOperator ts = datetime.now()

today = str(ts.date()) + 'T00:00:00.000Z'

yester_day = str(ts.date() - timedelta(days = 1)) + 'T00:00:00.000Z' str_ts = ts.strftime('%Y_%m_%d_%H_%m_%S') config = Variable.get(""mongo_conf"", deserialize_json=True)

host = config['host']

db_name = config['db_name']

table_name = config['table_name']

file_prefix = config['file_prefix']

bucket_name = config['bucket_name']

# file_path = file_prefix + '/' + table_name + '.json'

file_path = '{file_prefix}/{table_name}/{table_name}_{str_ts}.json'.format(file_prefix=file_prefix, str_ts=str_ts, table_name=table_name)

file_location = 'gs://' + bucket_name + '/' + file_prefix + '/' + table_name + '/' + table_name + '_*.json'

config['file_location'] = file_location

bq_dataset = config['bq_dataset']

tot_data = config['tot_data'].lower() BUCKET_NAME = config['ml_configuration']['BUCKET_NAME']

BASE_DIR = config['ml_configuration']['BASE_DIR']

PACKAGE_NAME = config['ml_configuration']['PACKAGE_NAME']

TRAINER_BIN = os.path.join(BASE_DIR, 'packages', PACKAGE_NAME)

TRAINER_MODULE = config['ml_configuration']['TRAINER_MODULE']

RUNTIME_VERSION = config['ml_configuration']['RUNTIME_VERSION']

PROJECT_ID = config['ml_configuration']['PROJECT_ID']

MODEL_NAME = config['ml_configuration']['MODEL_NAME'] MODEL_FILE_BUCKET = config['ml_configuration']['MODEL_FILE_BUCKET']

model_file_loc = config['ml_configuration']['MODEL_FILE_LOCATION'] bucket = storage.Client().bucket(MODEL_FILE_BUCKET)

blob = bucket.get_blob(model_file_loc)

file_config = json.loads(blob.download_as_string().decode(""utf-8""))

export_uri = file_config['file_location'] def flatten_json(y):

out = {} def flatten(x, name=''):

if type(x) is dict:

for a in x:

flatten(x[a], name + a + '_')

elif type(x) is list:

i = 0

for a in x:

flatten(a, name + str(i) + '_')

i += 1

else:

out[name[:-1]] = x flatten(y)

return out def mongoexport():

client = storage.Client()

bucket = client.get_bucket(bucket_name)

blob = bucket.blob(file_path) client = MongoClient(host)

db = client[db_name]

tasks = db[table_name]

pprint.pprint(tasks.count_documents({}))

# if tot_data is set to 'yes' in airflow configurations, full data

# is processed.

if tot_data == 'no':

query = {""edit_datetime"": { ""$gte"": yester_day, ""$lt"": today}}

print(query)

data = tasks.find(query)

else:

data = tasks.find()

emp_list = []

for record in data:

emp_list.append(json.dumps(record, default=str))

flat_list =[]

for data in emp_list:

flat_list.append((flatten_json(json.loads(data))))

data = '

'.join(json.dumps({re.sub('[^0-9a-zA-Z_ ]+', '', str(k)).lower().replace(' ', '_'): str(v) for k, v in record.items()}) for record in flat_list)

blob.upload_from_string(data) default_args = {

'start_date': airflow.utils.dates.days_ago(0),

'retries': 1,

'retry_delay': timedelta(minutes=5)

} with DAG('ml_pipeline', schedule_interval=None, default_args=default_args) as dag: # priority_weight has type int in Airflow DB, uses the maximum.

pymongo_export_op = PythonOperator(

task_id='pymongo_export',

python_callable=mongoexport,

) update_bq_table_op = BashOperator(

task_id='update_bq_table',

bash_command='''

bq rm -f {bq_dataset}.{table_name}

bq load --autodetect --source_format=NEWLINE_DELIMITED_JSON --ignore_unknown_values=True {bq_dataset}.{table_name} {file_location}

'''.format(bq_dataset=bq_dataset, table_name=table_name, file_location=file_location)

) date_nospecial = '{{ execution_date.strftime(""%Y%m%d"") }}'

date_min_nospecial = '{{ execution_date.strftime(""%Y%m%d_%H%m"") }}'

uuid = '{{ macros.uuid.uuid4().hex[:8] }}' training_op = MLEngineTrainingOperator(

task_id='submit_job_for_training',

project_id=PROJECT_ID,

job_id='{}_{}_{}'.format(table_name, date_nospecial, uuid),

package_uris=[os.path.join(TRAINER_BIN)],

training_python_module=TRAINER_MODULE,

training_args=[

'--base-dir={}'.format(BASE_DIR),

'--event-date={}'.format(date_nospecial),

],

region='us-central1',

runtime_version=RUNTIME_VERSION,

python_version='3.5') create_version_op = MLEngineVersionOperator(

task_id='create_version',

project_id=PROJECT_ID,

model_name=MODEL_NAME,

version={

'name': 'version_{}_{}'.format(date_min_nospecial, uuid),

'deploymentUri': export_uri,

'runtimeVersion': RUNTIME_VERSION,

'pythonVersion': '3.5',

'framework': 'SCIKIT_LEARN',

},

operation='create') pymongo_export_op >> update_bq_table_op >> training_op >> create_version_op

Once file is created, please upload the file to DAGs folder. And also please add the following plugin dependency file named mlengine_operator in DAGs folder.

Place the following code in mlengine_operator.py file.

import re from apiclient import errors from airflow.contrib.hooks.gcp_mlengine_hook import MLEngineHook

from airflow.exceptions import AirflowException

from airflow.operators import BaseOperator

from airflow.utils.decorators import apply_defaults

from airflow.utils.log.logging_mixin import LoggingMixin log = LoggingMixin().log def _normalize_mlengine_job_id(job_id): # Add a prefix when a job_id starts with a digit or a template

match = re.search(r'\d|\{{2}', job_id)

if match and match.start() is 0:

job = 'z_{}'.format(job_id)

else:

job = job_id # Clean up 'bad' characters except templates

tracker = 0

cleansed_job_id = ''

for m in re.finditer(r'\{{2}.+?\}{2}', job):

cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',

job[tracker:m.start()])

cleansed_job_id += job[m.start():m.end()]

tracker = m.end() # Clean up last substring or the full string if no templates

cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:]) return cleansed_job_id class MLEngineBatchPredictionOperator(BaseOperator):



template_fields = [

'_project_id',

'_job_id',

'_region',

'_input_paths',

'_output_path',

'_model_name',

'_version_name',

'_uri',

] @apply_defaults

def __init__(self,

project_id,

job_id,

region,

data_format,

input_paths,

output_path,

model_name=None,

version_name=None,

uri=None,

max_worker_count=None,

runtime_version=None,

gcp_conn_id='google_cloud_default',

delegate_to=None,

*args,

**kwargs):

super(MLEngineBatchPredictionOperator, self).__init__(*args, **kwargs) def __init__(self,project_id,job_id,region,data_format,input_paths,output_path,model_name=None,version_name=None,uri=None,max_worker_count=None,runtime_version=None,gcp_conn_id='google_cloud_default',delegate_to=None,*args,**kwargs):super(MLEngineBatchPredictionOperator, self).__init__(*args, **kwargs) self._project_id = project_id

self._job_id = job_id

self._region = region

self._data_format = data_format

self._input_paths = input_paths

self._output_path = output_path

self._model_name = model_name

self._version_name = version_name

self._uri = uri

self._max_worker_count = max_worker_count

self._runtime_version = runtime_version

self._gcp_conn_id = gcp_conn_id

self._delegate_to = delegate_to if not self._project_id:

raise AirflowException('Google Cloud project id is required.')

if not self._job_id:

raise AirflowException(

'An unique job id is required for Google MLEngine prediction '

'job.') if self._uri:

if self._model_name or self._version_name:

raise AirflowException('Ambiguous model origin: Both uri and '

'model/version name are provided.') if self._version_name and not self._model_name:

raise AirflowException(

'Missing model: Batch prediction expects '

'a model name when a version name is provided.') if not (self._uri or self._model_name):

raise AirflowException(

'Missing model origin: Batch prediction expects a model, '

'a model & version combination, or a URI to a savedModel.') def execute(self, context):

job_id = _normalize_mlengine_job_id(self._job_id)

prediction_request = {

'jobId': job_id,

'predictionInput': {

'dataFormat': self._data_format,

'inputPaths': self._input_paths,

'outputPath': self._output_path,

'region': self._region

}

} if self._uri:

prediction_request['predictionInput']['uri'] = self._uri

elif self._model_name:

origin_name = 'projects/{}/models/{}'.format(

self._project_id, self._model_name)

if not self._version_name:

prediction_request['predictionInput'][

'modelName'] = origin_name

else:

prediction_request['predictionInput']['versionName'] = \

origin_name + '/versions/{}'.format(self._version_name) if self._max_worker_count:

prediction_request['predictionInput'][

'maxWorkerCount'] = self._max_worker_count if self._runtime_version:

prediction_request['predictionInput'][

'runtimeVersion'] = self._runtime_version hook = MLEngineHook(self._gcp_conn_id, self._delegate_to) # Helper method to check if the existing job's prediction input is the

# same as the request we get here.

def check_existing_job(existing_job):

return existing_job.get('predictionInput', None) == \

prediction_request['predictionInput'] try:

finished_prediction_job = hook.create_job(

self._project_id, prediction_request, check_existing_job)

except errors.HttpError:

raise if finished_prediction_job['state'] != 'SUCCEEDED':

self.log.error('MLEngine batch prediction job failed: {}'.format(

str(finished_prediction_job)))

raise RuntimeError(finished_prediction_job['errorMessage']) return finished_prediction_job['predictionOutput'] class MLEngineModelOperator(BaseOperator):

template_fields = [

'_model',

] @apply_defaults

def __init__(self,

project_id,

model,

operation='create',

gcp_conn_id='google_cloud_default',

delegate_to=None,

*args,

**kwargs):

super(MLEngineModelOperator, self).__init__(*args, **kwargs)

self._project_id = project_id

self._model = model

self._operation = operation

self._gcp_conn_id = gcp_conn_id

self._delegate_to = delegate_to def __init__(self,project_id,model,operation='create',gcp_conn_id='google_cloud_default',delegate_to=None,*args,**kwargs):super(MLEngineModelOperator, self).__init__(*args, **kwargs)self._project_id = project_idself._model = modelself._operation = operationself._gcp_conn_id = gcp_conn_idself._delegate_to = delegate_to def execute(self, context):

hook = MLEngineHook(

gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to)

if self._operation == 'create':

return hook.create_model(self._project_id, self._model)

elif self._operation == 'get':

return hook.get_model(self._project_id, self._model['name'])

else:

raise ValueError('Unknown operation: {}'.format(self._operation)) class MLEngineVersionOperator(BaseOperator):



template_fields = [

'_model_name',

'_version_name',

'_version',

] @apply_defaults

def __init__(self,

project_id,

model_name,

version_name=None,

version=None,

operation='create',

gcp_conn_id='google_cloud_default',

delegate_to=None,

*args,

**kwargs): def __init__(self,project_id,model_name,version_name=None,version=None,operation='create',gcp_conn_id='google_cloud_default',delegate_to=None,*args,**kwargs): super(MLEngineVersionOperator, self).__init__(*args, **kwargs)

self._project_id = project_id

self._model_name = model_name

self._version_name = version_name

self._version = version or {}

self._operation = operation

self._gcp_conn_id = gcp_conn_id

self._delegate_to = delegate_to def execute(self, context):

if 'name' not in self._version:

self._version['name'] = self._version_name hook = MLEngineHook(

gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to) if self._operation == 'create':

assert self._version is not None

return hook.create_version(self._project_id, self._model_name,

self._version)

elif self._operation == 'set_default':

return hook.set_default_version(self._project_id, self._model_name,

self._version['name'])

elif self._operation == 'list':

return hook.list_versions(self._project_id, self._model_name)

elif self._operation == 'delete':

return hook.delete_version(self._project_id, self._model_name,

self._version['name'])

else:

raise ValueError('Unknown operation: {}'.format(self._operation)) class MLEngineTrainingOperator(BaseOperator):



template_fields = [

'_project_id',

'_job_id',

'_package_uris',

'_training_python_module',

'_training_args',

'_region',

'_scale_tier',

'_runtime_version',

'_python_version',

'_job_dir'

] @apply_defaults

def __init__(self,

project_id,

job_id,

package_uris,

training_python_module,

training_args,

region,

scale_tier=None,

runtime_version=None,

python_version=None,

job_dir=None,

gcp_conn_id='google_cloud_default',

delegate_to=None,

mode='PRODUCTION',

*args,

**kwargs):

super(MLEngineTrainingOperator, self).__init__(*args, **kwargs)

self._project_id = project_id

self._job_id = job_id

self._package_uris = package_uris

self._training_python_module = training_python_module

self._training_args = training_args

self._region = region

self._scale_tier = scale_tier

self._runtime_version = runtime_version

self._python_version = python_version

self._job_dir = job_dir

self._gcp_conn_id = gcp_conn_id

self._delegate_to = delegate_to

self._mode = mode def __init__(self,project_id,job_id,package_uris,training_python_module,training_args,region,scale_tier=None,runtime_version=None,python_version=None,job_dir=None,gcp_conn_id='google_cloud_default',delegate_to=None,mode='PRODUCTION',*args,**kwargs):super(MLEngineTrainingOperator, self).__init__(*args, **kwargs)self._project_id = project_idself._job_id = job_idself._package_uris = package_urisself._training_python_module = training_python_moduleself._training_args = training_argsself._region = regionself._scale_tier = scale_tierself._runtime_version = runtime_versionself._python_version = python_versionself._job_dir = job_dirself._gcp_conn_id = gcp_conn_idself._delegate_to = delegate_toself._mode = mode if not self._project_id:

raise AirflowException('Google Cloud project id is required.')

if not self._job_id:

raise AirflowException(

'An unique job id is required for Google MLEngine training '

'job.')

if not package_uris:

raise AirflowException(

'At least one python package is required for MLEngine '

'Training job.')

if not training_python_module:

raise AirflowException(

'Python module name to run after installing required '

'packages is required.')

if not self._region:

raise AirflowException('Google Compute Engine region is required.') def execute(self, context):

job_id = _normalize_mlengine_job_id(self._job_id)

training_request = {

'jobId': job_id,

'trainingInput': {

'scaleTier': self._scale_tier,

'packageUris': self._package_uris,

'pythonModule': self._training_python_module,

'region': self._region,

'args': self._training_args,

}

} if self._runtime_version:

training_request['trainingInput']['runtimeVersion'] = self._runtime_version if self._python_version:

training_request['trainingInput']['pythonVersion'] = self._python_version if self._job_dir:

training_request['trainingInput']['jobDir'] = self._job_dir if self._mode == 'DRY_RUN':

self.log.info('In dry_run mode.')

self.log.info('MLEngine Training job request is: {}'.format(

training_request))

return hook = MLEngineHook(

gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to) # Helper method to check if the existing job's training input is the

# same as the request we get here.

def check_existing_job(existing_job):

return existing_job.get('trainingInput', None) == \

training_request['trainingInput'] try:

finished_training_job = hook.create_job(

self._project_id, training_request, check_existing_job)

except errors.HttpError:

raise if finished_training_job['state'] != 'SUCCEEDED':

self.log.error('MLEngine training job failed: {}'.format(

str(finished_training_job)))

raise RuntimeError(finished_training_job['errorMessage'])

Import variables from composer_conf.json file into Airflow Variables.

Go to Airflow WebUI → Admin → Variables → Browse to file path or configure variables manually.

Please place the following in composer_conf

{

""mongo_conf"": {

""host"": ""mongodb://<instance-internal-ip>:27017"",

""db_name"": ""DBNAME"",

""table_name"": ""TABLENAME"",

""file_prefix"": ""Folder In GCS Bucket"",

""bq_dataset"": ""BigQuery Dataset"",

""bucket_name"": ""GCS Bucket"",

""tot_data"": ""yes"",

""ml_configuration"": {

""BUCKET_NAME"": ""GCS Bucket"",

""BASE_DIR"": ""gs://GCS Bucket/machine_learning/"",

""PACKAGE_NAME"": ""PACKAGE NAME FROM setup.py FILE in ML"",

""TRAINER_MODULE"": ""trainer.train"",

""RUNTIME_VERSION"": ""1.13"",

""PROJECT_ID"": ""GCP Project"",

""MODEL_FILE_BUCKET"": ""BUCKET CREATED BY Composer Environment"",

""MODEL_FILE_LOCATION"": ""data/MODEL LOCATION FILE"",

""MODEL_NAME"": ""MODEL_NAME""

}

}

Please store any configuration files or credentials file that are used by Composer in the data folder in the bucket created by Composer environment.

After configuring variables accordingly, you can see the DAG named ml_pipeline in the Airflow WebUI.

Please trigger the DAG file from Airflow WebUI. Once the DAG ran successfully. It looks like the following:

Thanks for the read and look forward to your comments.

This story is authored by PV Subbareddy. Subbareddy is a Big Data Engineer specializing on Cloud Big Data Services and Apache Spark Ecosystem.","['machine', 'composer', 'file', 'import', 'operations', 'mlops', 'def', 'cloud', 'required', 'learning', 'dag', 'data', 'airflow', 'model', 'following', 'google', 'using', 'pipeline']","In an earlier post, we had described the need for automating the Data Engineering pipeline for Machine Learning based systems.
Today, we will expand the scope to setup a fully automated MLOps pipeline using Google Cloud Composer.
So let’s get on with the required steps to create this MLOps infrastructure on Google Cloud PlatformCreating a Cloud Composer EnvironmentStep1: Please enable the Cloud Composer API.
The required roles for accessing Composer environment is Composer Administrator and Composer Worker.
The DAG file supports full data extraction and daily data extraction explained in the code below using a variable tot_data.",en,['Engineering Zenofai'],2019-08-02 10:52:06.328000+00:00,"{'Google Cloud Platform', 'AI', 'Python', 'Machine Learning', 'Cloud Computing'}","{'https://miro.medium.com/max/60/0*uOZJSI2lT6mNUOYc?q=20', 'https://miro.medium.com/fit/c/80/80/1*reUhERID6Vqsf1iavFUQUA.png', 'https://miro.medium.com/max/60/0*Xq5DXY1zLWrjczUB?q=20', 'https://miro.medium.com/max/1940/0*kPSm26BgcidGYcQD', 'https://miro.medium.com/fit/c/96/96/1*reUhERID6Vqsf1iavFUQUA.png', 'https://miro.medium.com/max/220/1*A_PNwDwmi2aQpYsHyDwEeg.png', 'https://miro.medium.com/max/1240/0*Xq5DXY1zLWrjczUB', 'https://miro.medium.com/fit/c/160/160/1*reUhERID6Vqsf1iavFUQUA.png', 'https://miro.medium.com/max/4000/1*TLI2Bj-iskzT0AsHRt0Qsg.png', 'https://miro.medium.com/max/1196/0*aPK7iL5vt2cv-Q0o', 'https://miro.medium.com/max/1792/0*jazYiI6-ElaQrabh', 'https://miro.medium.com/max/60/0*j-G9ycbGMZYW27f6?q=20', 'https://miro.medium.com/fit/c/160/160/1*xgyas5FP93agYnRGBmugtg.png', 'https://miro.medium.com/max/2048/0*uhYCU1QBnmcGbxW9', 'https://miro.medium.com/max/60/1*TLI2Bj-iskzT0AsHRt0Qsg.png?q=20', 'https://miro.medium.com/max/60/0*uhYCU1QBnmcGbxW9?q=20', 'https://miro.medium.com/max/1790/0*j-G9ycbGMZYW27f6', 'https://miro.medium.com/max/1924/0*uOZJSI2lT6mNUOYc', 'https://miro.medium.com/max/2712/0*cRzqhUK7D4M2J3Qp', 'https://miro.medium.com/max/60/0*cRzqhUK7D4M2J3Qp?q=20', 'https://miro.medium.com/max/60/0*aPK7iL5vt2cv-Q0o?q=20', 'https://miro.medium.com/max/1200/1*TLI2Bj-iskzT0AsHRt0Qsg.png', 'https://miro.medium.com/max/60/0*jazYiI6-ElaQrabh?q=20', 'https://miro.medium.com/max/60/0*kPSm26BgcidGYcQD?q=20'}",2020-03-05 00:07:04.053345,1.8651130199432373
https://towardsdatascience.com/automatic-version-control-for-data-scientists-fc4968bef8f,Automatic Version Control for Data Scientists,"One of the best parts about Data Science is the adventure of going through a full project, end-to-end. Starting from data cleaning, to visualisation, to feature building, and finally, training a machine learning model. The skills of a great Data Scientist are in high demand, commanding excellent salaries.

A major challenge in all of this is organisation.

These days, Data Scientists are working in small teams of a least a few members each. Every one of them is playing around with the same data, building on the work of each other.

The many different components of great Data Science

Unfortunately, Data Science team members often work in their own mini silos. They create their own codebase, run their own Jupyter Notebooks, and display their own visualisations. Some how, they hastily mash it all together to create the final report. It’s a less than stellar coordination, to say the least.

But there must be a better way. Some sort of system or tooling that allows someone who works extensively with data and graphing, a data scientist, to efficiently work with the rest of their team, building on each others work. That system needs to be flexible, easy to use, and something that doesn’t interrupt the Data Science workflow.

Let’s see what we can do……

How git can work for Data Science

In a regular old software engineering team, Git is a commonly used tool for organizing code, projects, and team collaboration. Team members work on their own local code, but always sync up their progress and new developments with a central repository.

This ensures that everyone is up to date and in sync. It’s a great way of having the work of the team compound on each other, rather than conflicting, which leads to higher output.

Git flow. Team members work on their own “Feature” branches which are then all synced up in the “Develop” branch. Once the Develop branch is nice and polished, the production version of the code is synced up with Master.

Data science is a bit trickier, since a lot of it is a combination of research and software engineering:

The data itself is large, so shared storage of it can be challenging. Git-LFS helps, but is pretty slow to use.

Much of the time, the code changes are minor, but the visualisations look completely different. Git doesn’t allow the ability to show changes in graphs from commit to commit

Data Scientists commonly work with Jupyter Notebooks. Visualising the changes in the notebooks on GitHub is terrible. It shows the difference in the .ipynb file, but not the actual code on graphs that we find useful in the notebook.

With all of that said, we can start to get an idea of what a good version control system for Data Science should look like.

First off, we’re looking for ease of use. DevOps and Backend engineers may be very skilled with git, but Data Science tends to be more on the research side than engineering. Dealing with code conflicts should be especially smooth.

We’re also going to want to see what the actual changes are in the data science project. Seeing code changes is fine, but it doesn’t tell the full story. That’s especially true if the team is using Jupyter Notebooks where visually, the code is far too complicated to tell us much about how some of the graphs or data manipulation has changed.

Lastly, a central data storage or working platform can be extremely helpful. Without a central storage, data scientists would always have to be downloading and preparing their own copies of the data — highly inefficient for large datasets.

With these in mind, we can now investigate a solution

A platform for using Git in Data Science

One great option for getting a version control system in place for Data Science is the Saturn Cloud platform. With it, multiple Data Scientists can work on the same Jupyter Notebook, with the same dataset, yet running on their own machines.

Data Science Collaboration

To get started, we’ll create our own hosted Jupyter Notebook, which anyone with a link can access. After creating an account, the video below shows you how it’s done.

Creating a hosted Jupyter Notebook

Once we have that up and running, we can go ahead and start up our Jupyter Lab. Once opened, you should see your project folder already setup.

For this experiment, we’re going to be using the 2018 Kaggle ML and DS Survey dataset. The dataset consists of over 23,000 multiple choice answers to a set of questions posed to Data Scientists and Machine Learning practitioners regarding the compensation.

A few interesting facts from the dataset:

“Data Scientist” had the highest compensation, higher than “Data Engineer” and “Research Scientist”

Technology companies pay the most, followed by Marketing and Finance

More experience and higher education increases pay

Compensation is highest in the US, followed by Switzerland and Australia

You can check out the full Jupyter Notebook for the Data Science compensation here.

Data Scientist compensation

To get started with the team collaboration, go to your Dashboard and click on the Collaborators link. In it, you’ll find the ability to add collaborators to your project by Saturn Cloud username.

Adding a user to your project sends them a personal email inviting them to collaborate on the project. Once they accept the invitation, their own cloud instance will be created with the exact same settings, libraries, and software.

The user (Data Scientist) instantly gets access to their own version of the project’s Jupyter Notebooks and datasets. This allows for multiple Data Scientists to be working in identical environments, with identical data, and an identical starting point for their notebooks!

Collaboration for Data Scientists

Data Science Version Control

Now we can get into Saturn Cloud’s version control system.

Once you open up the Jupyter Lab on your instance, you’ll see your working interface. On the left-hand side of your screen there’s a project folder. Any Jupyter Notebooks which are created and saved in that project folder will automatically have version control enabled. I’ve placed my ds_jobs.ipynb Notebook into the project folder to enable version control on it.

To showcase how the version control works, we’re going to modify some of the data visualisations. We’ll pretend we’re only interested in data from European countries and filter out everything else. Once the visualisation is changed, we’ll be able to push our changes to a remote git repository. The 45 second video down below shows exactly how that’s done!

Data Science Version Control

Great!

We were able to make some quick and easy modifications to our Jupyter Notebook. That Notebook is accessible to all project collaborators (Data Scientists). So whenever they log on, they’ll be able to see that you’ve made a change and optionally pull those changes so they can stay up to date!

Data Science on Demand — Rewinding

Another awesome feature here is the rewind function. It works similarly to a git reset which basically just reverts our changes so we can go back to an earlier state. This is super useful if you feel like you’ve made a mistake and want to roll back or just want to see what things looked like before.

Let’s say we want to rollback to our visualisation with all the regions shown for data science compensation. We’ll again click on the git button in the bottom right. This time, click on the you can rewind this repository link. A slider pops up where you can select the time you wish to rollback to. Once set, you’ll be able to see your old code and graphs!

Rewinding Jupyter Notebooks

Now that’s control!","['scientists', 'version', 'code', 'automatic', 'control', 'project', 'changes', 'team', 'data', 'work', 'git', 'jupyter', 'science']","The many different components of great Data ScienceUnfortunately, Data Science team members often work in their own mini silos.
That system needs to be flexible, easy to use, and something that doesn’t interrupt the Data Science workflow.
With all of that said, we can start to get an idea of what a good version control system for Data Science should look like.
Collaboration for Data ScientistsData Science Version ControlNow we can get into Saturn Cloud’s version control system.
Data Science Version ControlGreat!",en,['George Seif'],2019-09-14 00:43:17.292000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Programming', 'Technology'}","{'https://miro.medium.com/max/1200/1*K7f6k5DhvXgJZZJBvc_pwg.png', 'https://miro.medium.com/freeze/max/60/1*hDfRCnr3E4yf6ANvquv_hw.gif?q=20', 'https://miro.medium.com/max/1600/1*74veN8-comyNgz_cM0iofg.gif', 'https://miro.medium.com/max/1600/1*hDfRCnr3E4yf6ANvquv_hw.gif', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/6666/1*k0kFbUGVhO5Uwxxte_xQZA.png', 'https://miro.medium.com/fit/c/96/96/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/max/942/1*lIGiPQQ6562KSNuSt8KWlg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*k0kFbUGVhO5Uwxxte_xQZA.png?q=20', 'https://miro.medium.com/max/3234/1*K7f6k5DhvXgJZZJBvc_pwg.png', 'https://miro.medium.com/freeze/max/60/1*gzwMb5MopT3cSzPEg4rL2Q.gif?q=20', 'https://miro.medium.com/max/60/1*lIGiPQQ6562KSNuSt8KWlg.png?q=20', 'https://miro.medium.com/max/60/1*K7f6k5DhvXgJZZJBvc_pwg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*74veN8-comyNgz_cM0iofg.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/60/1*-1OlwKKCRl5mOAz_jB6xcQ.gif?q=20', 'https://miro.medium.com/max/1600/1*-1OlwKKCRl5mOAz_jB6xcQ.gif', 'https://miro.medium.com/max/1600/1*gzwMb5MopT3cSzPEg4rL2Q.gif'}",2020-03-05 00:07:15.898269,11.843923568725586
https://towardsdatascience.com/integrating-business-optimization-with-a-machine-learning-model-ad0471420693,Integrating business optimization with a machine learning model,"Integrating business optimization with a machine learning model

In this article, we illustrate the concept of integrating the goal of optimizing a business metric with a machine learning pipeline. Specifically, we show a case illustration of how a simple optimization loop can be wrapped around a core ML algorithm for guiding it toward achieving a specific business objective.

Introduction

Machine learning (ML) is in serious demand. This branch of AI is upending traditional and modern business practices and bringing once-in-a-lifetime transformative changes.

However, scholarly discussions and online articles about ML algorithms, tools, and techniques, often focus exclusively on their implementation, performance, deployment, and scalability.

The business side of ML is also discussed at length, but often those discussions are somewhat distant from the core algorithm or the ML pipeline. It is not easy to find a simple illustration of an ML pipeline which integrates the dual goal of achieving decent ML performance and satisfying a fairly common and intuitive business objective.

However, large corporations are constantly doing this very integration in their operations — applying ML and tuning the pipeline to propel it toward satisfying the overall business objective (or a subset thereof).

For young data science and ML practioners, hwoever, it is not very intuitive, how to demonstrate this idea with a simple enough example — perhaps a few lines of code.

In this article, we show one such example.

The machine learning problem: High accuracy

For the case illustration, we will imagine a very simple scenario — a brand new startup company generates revenue by offering ML-based predictive analytics.

In particular, they receive a data stream and predict a binary class as output — ‘Yes’ or ‘No’. At the core of their service, they use a decision tree-based classification model with an ensemble technique — using AdaBoost classifier.

Higher the accuracy of their prediction, higher the revenue.

Of course, one can make a random prediction (without any model underneath) and still get about 50% prediction right. So, they only get paid for higher accuracy above a certain threshold, which can be as low as 50%. For example, if their prediction accuracy is 75% for some a job, then they get paid a revenue proportional to 75% — 50% = 25%.

But how do they achieve higher accuracy?

An easy and intuitive answer is by tuning the hyperparameters of the algorithm. There are quite a few hyperparameters in an algorithm like AdaBoost — mostly related to the underlying base estimator. For a decision tree, these can be the minimum number of samples per leaf, maximum tree depth, splitting criterion like Gini index, etc. However, to keep this case illustration simple, we choose the most intuitive hyperparameter — the number of tree estimators applied to the boosting.

At their core, ensemble techniques like Boosting work by keeping the base estimator relatively simple and low accuracy (slightly above 50% is fine). They achieve a robust generalization power by employing a large number of such simple base estimators in parallel and averaging their predictions and dynamically updating focus on the examples that the estimators got wrong in the previous iteration.

Source: Realizing Low-Energy Classification Systems by Implementing Matrix Multiplication Directly Within an ADC ( IEEE Transactions on Biomedical Circuits and Systems)

It should be, therefore, no surprise that a higher number of base estimators may lead to greater generalization power — higher accuracy on the customer data (the true unknown test set).

Previously, we established the simple scenario that revenue is directly proportional to the level of accuracy in the predictions for the customer-supplied data.

So, it seems that the strategy to maximize ML model accuracy, and thereby, maximize the revenue of the company, is to keep the individual base estimators real simple — choosing the maximum depth of trees something like 2 or 3 — and employing a large number of them.

It seems a straightforward strategy. But, it may not be an optimum one.

Revenue is not profit. In all likelihood, the young startup company would like to maximize profit and not just focus on revenue because that shows their long-term viability and help get them more investment and more customers.

Let’s drill down to the profit aspect a bit.

The business objective: Maximize profit

Profit is the king (in most business situations anyway) and a very good indicator of the economic value added for most types of business. Nobody likes it better when they can earn a decent revenue with a low operating cost.

We understand the relationship between the ML model and the revenue. But, how is the operating cost related to the model?

In a real-life scenario, it can be pretty complicated. But for the sake of case illustration, we can simply assume that the cost is proportional to the total compute time for model fitting and prediction.

This is not very hard to imagine, as the case would be similar if the young startup firm had rented some kind of cloud service to host and run their ML algorithms e.g. AWS EC2, which is billed based on the total compute time.

Now, do you remember the hyperparameter of interest — number of base estimators for the Boosting algorithm? The bad news is that bigger this number, larger is the computational load for the algorithm, and higher the compute time for model fitting and prediction.

Thereby, we identify the key relationships — between a single hyperparameter of the ML algorithm and two business metrics — revenue and cost.

And what is profit? It is the good old definition,

Profit = Revenue — cost

The optimization problem: How to choose the ML algorithm to maximize profit?

This is somewhat different than traditional discussions on ML algorithmic choice, isn’t it? You may have done the following many times,

Bias-variance trade-off analysis

Grid-search on hyperparameters to determine the best accuracy

Debating the correct metric for the ML performance measure — accuracy/precision/recall? F1 score? ROC curve and AUC?

Brainstorming the data acquisition and annotation strategy — does it make sense to do a particular feature engineering? does it make sense to pay for a few more annotations to increase the training set size?

And, all of these are still critically important.

But, from a business perspective, it could well be the case that the only thing you will be judged on, is how much profit your ML algorithm could generate. If it is a large positive number, higher management, most likely, won’t grill you on the algorithmic details. If it is negative, all hell may break loose!

So, we must do the following balancing act,

We constructed an extremely simplistic scenario, but at least, it shows that there is a fair chance that an algorithmic choice can be strongly coupled to a key business metric.

And what do good engineers do when they have a model parameter, which, impacts two outputs (accuracy and cost in our case) simultaneously?

They optimize.

They try to find the optimum setting of that model parameter which will maximize the business metric — profit in this case.

Let’s see how this can be done through a simple code demo.

Demo: Business-centered ML optimization

Code is, in fact, tedious to follow and can be distracting. Ideas and pictures are much better :-)

Therefore, I’ll let you fork and copy the code for this demo from the Github repo here. But the main idea is as follows,

The hyperparameter of interest — the number of decision tree estimators — has been coded as an argument to an objective function, which an optimizer algorithm can minimize. The objective function value is computed taking into account both the Boosting algorithm accuracy on the validation set and a simple cost term proportional to the time it takes for model fitting and prediction.

Here is how the training and validation set accuracy varies with the number of decision tree estimators.

And here is the computation time (model fit and prediction),

Clearly, the accuracy starts at a low value for a small number of estimators but saturates after that number reaches a certain level. On the other hand, the computational load keeps on increasing.

Therefore, it does not make sense to keep increasing the number of estimators as the marginal rate of return (in terms of improving accuracy) peaks at a certain level and then goes down.

Ah… there it is… the famous Marginal Rate of Return, which is so near and dear to business folks.

Machine learning scientists, engineers, and business development teams finally have a common concept to ponder over, a common metric to plot and make decisions with.

To illustrate this behavior more clearly, we make up an objective function — which integrates both the accuracy and computing cost in a single scalar output by combining the validation set accuracy and the compute time in a linear function with proper weights. The MRR behavior can be clearly seen if we plot the objective function,

It is easy to see that the coefficient of accuracy is a positive number in this equation whereas the coefficient of the compute time is a negative number to reflect the true nature of the objective — cost subtracted from the revenue.

In the Github repo, I also show how to solve the optimization by calling a function from the Scipy package. For this particular example, the objective function is extremely simple and a simple plot will show the evolution to determine that the optimum number of trees to include in the ML algorithm is around 10 or 11. So, the use of a Scipy function is really not needed.

But the same idea can be extended to a more complicated objective function which encompasses a plethora of ML hyperparameters. And then, the full power of optimization algorithms can be brought to bear to solve the business-centric optimization.

See this article for Scipy optimization algorithms discussion,

Summary

In this article, we talked about business-centric optimization. A simple demo was discussed to illustrate the idea clearly — that often, the choice of ML algorithms and their settings, need to be guided by an overarching goal of business metric optimization. In this endeavor, wrapping the core ML algorithm with an optimization loop could be extremely handy for quick decision making.

In fact, in one of my previous articles, I talked about how this idea does not need to be restricted to a single type of machine learning, or even a single type of analytics process but can span over a variety of quantitive disciplines — ML model, statistical estimation, stochastic simulation, etc. — all feeding to a common optimization engine.","['machine', 'algorithm', 'business', 'number', 'learning', 'ml', 'simple', 'integrating', 'model', 'objective', 'optimization', 'revenue', 'accuracy']","Integrating business optimization with a machine learning modelIn this article, we illustrate the concept of integrating the goal of optimizing a business metric with a machine learning pipeline.
Specifically, we show a case illustration of how a simple optimization loop can be wrapped around a core ML algorithm for guiding it toward achieving a specific business objective.
The business objective: Maximize profitProfit is the king (in most business situations anyway) and a very good indicator of the economic value added for most types of business.
Thereby, we identify the key relationships — between a single hyperparameter of the ML algorithm and two business metrics — revenue and cost.
In this endeavor, wrapping the core ML algorithm with an optimization loop could be extremely handy for quick decision making.",en,['Tirthajyoti Sarkar'],2019-08-26 19:48:42.555000+00:00,"{'Business', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Technology'}","{'https://miro.medium.com/max/1018/1*rKMiB0GSBmWXkH7vhBBNdg.png', 'https://miro.medium.com/max/60/1*JOb4Gv28YCDvEunUjYAeRQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*dROuRoTytntKE6LLBKKzKA.jpeg', 'https://miro.medium.com/max/60/1*ugbAaSRW2q-N7vd_h7MvKQ.png?q=20', 'https://miro.medium.com/max/60/1*rKMiB0GSBmWXkH7vhBBNdg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*f83IpFHwt-t_4oOE8UFh8Q.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*dROuRoTytntKE6LLBKKzKA.jpeg', 'https://miro.medium.com/max/1404/1*H6baP7OgjLma4oMRlTEqzg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*AVVFZWE1ycfsxUnc08HdsQ.jpeg', 'https://miro.medium.com/max/60/1*8q7bRWI7wMRyPrm4SyNfBw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*ItzlX6eYJClYKJhSo1b_JQ.png?q=20', 'https://miro.medium.com/max/2320/1*ugbAaSRW2q-N7vd_h7MvKQ.png', 'https://miro.medium.com/max/10944/1*AVVFZWE1ycfsxUnc08HdsQ.jpeg', 'https://miro.medium.com/max/60/1*AVVFZWE1ycfsxUnc08HdsQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*UaWd-fWn7UDNBUEVLG6D1Q.png?q=20', 'https://miro.medium.com/max/1598/1*ItzlX6eYJClYKJhSo1b_JQ.png', 'https://miro.medium.com/max/3510/1*UaWd-fWn7UDNBUEVLG6D1Q.png', 'https://miro.medium.com/max/60/1*H6baP7OgjLma4oMRlTEqzg.png?q=20', 'https://miro.medium.com/max/1254/1*8q7bRWI7wMRyPrm4SyNfBw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1004/1*f83IpFHwt-t_4oOE8UFh8Q.png', 'https://miro.medium.com/max/1144/1*JOb4Gv28YCDvEunUjYAeRQ.png'}",2020-03-05 00:07:25.878425,9.979159832000732
https://towardsdatascience.com/learning-in-graphs-with-python-part-3-8d5513eef62d,Learning in Graphs with Python (Part 3),"There are three main tasks in graph learning that we will cover in this article:

Link prediction

Node label prediction

Graph embedding

Let’s start with Link prediction!

I. Link prediction

In Link Prediction, given a graph G, we aim to predict new edges. Predictions are useful to predict future relations or missing edges when the graph is not fully observed for example, or when new customers join a platform (e.g. a new LinkedIn user).

Link prediction for a new LinkedIn user would simply be a suggestion of people he might know.

In link prediction, we simply try to build a similarity measure between pairs of nodes and link the most similar nodes. The question is now to identify and compute the right similarity scores!

To illustrate the different similarity scores, let’s consider the following graph :

Initial graph

Let N(i) be a set of neighbors of node i. On the graph above, the neighbors of both nodes i and j can be represented as :

Neighbors of j

And the neighbors of i :

Neighbors of i

1. Similarity Scores

We can build several similarity scores for these two nodes based on their neighborhoods.

Common Neighbors: S(i,j)=∣N(i)∩N(j)∣, i.e the number of common neighbors. In this example, the score would be simply 12, since they share only 2 common neighbors.

Common Neighbors

Jaccard Coefficient: A normalized common neighbors version.

The intersection is the Common Neighbors, and the union is :

Union

Therefore, the Jaccard Coefficient is given by the ratio of the pink over the yellow:

Jaccard Coefficient

And the value is 2/7.

Adamic-Adar index: For each common neighbor of nodes i and j, we add 1 divided by the total number of neighbors of that node. The concept is that common elements with very large neighborhoods are less significant when predicting a connection between two nodes compared to elements shared between a small number of nodes.

Preferential attachment : S(i,j)=∣N(i,j)∣∗∣N(j)∣

: S(i,j)=∣N(i,j)∣∗∣N(j)∣ We can also use community information when it is available.

2. Performance metrics

How do we perform the evaluation of the link prediction? We must hide a subset of node pairs, and predict their links based on the rules defined above. This is the equivalent of the train-test-split in supervised learning.

We then evaluate the proportion of correct predictions for dense graphs, or use Area under the Curve criteria for Sparse graphs.

3. Implementation

Let’s implement this in Python on the Karate graph we used in the 2 first articles! First of all, print the information about the graph :

n = G_karate.number_of_nodes()

m = G_karate.number_of_edges()

print(""Number of nodes :"", str(n))

print(""Number of edges :"", str(m))

print(""Number of connected components :"" str(nx.number_connected_components(G_karate)))

Then, plot the graph itself :

plt.figure(figsize=(12,8))

nx.draw(G_karate)

Now, let’s remove some connections, e.g 25% of the nodes:

# Take a random sample of edges

edge_subset = random.sample(G_karate.edges(), int(0.25 * G_karate.number_of_edges())) # Remove some edges

G_karate_train = G_karate.copy()

G_karate_train.remove_edges_from(edge_subset)

And plot the partially observed graph :

plt.figure(figsize=(12,8))

nx.draw(G_karate_train)

Partially observed graph

You can print the number of edges we deleted and the number of edges remaining :

edge_subset_size = len(list(edge_subset))

print(""Deleted : "", str(edge_subset_size))

print(""Remaining : "", str((m - edge_subset_size)))

This heads :

Deleted : 15

Remaining : 63

Jaccard Coefficient

We can first make a prediction using the Jaccard Coefficient :

prediction_jaccard = list(nx.jaccard_coefficient(G_karate_train))

score, label = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_jaccard])

The prediction looks like this, a first node, a second node, and a Jaccard score (or a label directly):

[(0, 32, 0.15),

(0, 33, 0.125),

(0, 3, 0.21428571428571427),

(0, 9, 0.0),

(0, 14, 0.0),

(0, 15, 0.0),

...

We can use the ROC-AUC criteria to compare the performance of the different models since we have both a label and probabilities.

Adamic-Adar

We can now repeat this for the Adamic-Adar Index :

prediction_adamic = list(nx.adamic_adar_index(G_karate_train))

score, label = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_adamic])

Preferential Attachment

And for the preferential attachment score :

prediction_pref = list(nx.preferential_attachment(G_karate_train))

score, label = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_pref])

We then select the model that maximizes the area under the curve for example, or any other criteria that you chose. So far, we covered the most common similarity scores for link prediction.

We’ll now cover into more details the node labeling algorithms.

II. Node Labeling

Given a graph where some nodes are not labeled, we want to predict their labels. This is in some sense a semi-supervised learning problem.

One common way to deal with such problems is to make the assumption that there is a certain smoothness on the graph. The Smoothness assumption states that points connected via a path through high-density regions on the data are likely to have similar labels. This is the main hypothesis behind the Label Propagation Algorithm.

The Label Propagation Algorithm (LPA) is a fast algorithm for finding communities in a graph using network structure alone as its guide, without any predefined objective function or prior information about the communities.

Neo4J Illustration of LPA

A single label can quickly become dominant in a densely connected group of nodes, but it will have trouble crossing a sparsely connected region.

How does the semi-supervised label propagation work?

First, we have some data : x1, …, xl, xl+1, …, xn ∈ Rp, and labels for the first l points: y1,…,yl ∈ 1…C.

We define the initial label matrix Y ∈ R(n×C) such that Yij=1 if xi has label yi=j and 0 otherwise.

The algorithm will generate a prediction matrix F∈R(n×C) which we’ll detail below. Then, we predict the label of a node by finding the most likely label :

What is the prediction matrix F ?

The prediction matrix is a matrix F* that minimizes both smoothness and accuracy criteria. Therefore, there is a tradeoff to make between the smoothness and the accuracy of our result.

The problem expression is quite complex, so I won’t go into details. However, the solution is given by :

If you’d like to go further on this topic, check the notions of smoothness of a graph function and manifold regularization. I won’t cover the implementation of this part, but if you are interested, Stanford has a great set of labeled graphs that you can download: https://snap.stanford.edu/data/, and Networkx has a direct implementation of label propagation: https://networkx.github.io/documentation/latest/reference/algorithms/generated/networkx.algorithms.community.label_propagation.label_propagation_communities.html

III. Graph Embedding

When dealing with NLP or computer vision problems, we are used to working with embeddings of images or texts in a Deep Neural Net. One of the limitations of graphs with what we have seen so far is the absence of vector features. However, we can learn an embedding of the graph! There are several levels of embedding in a graph :

Embedding graph components (nodes, edges, features…) (Node2Vec)

Embedding sub-parts of a graph or a whole graph (Graph2Vec)

1. Node Embedding

We will first focus on the embedding of graph components. Several approaches are possible to embed a node or an edge. For example, DeepWalk uses short random walks to learn representations for edges in graphs. We will talk about Node2Vec , a paper that was published by Aditya Grover and Jure Leskovec from Stanford University in 2016.

According to the authors: “node2vec is an algorithmic framework for representational learning on graphs. Given any graph, it can learn continuous feature representations for the nodes, which can then be used for various downstream machine learning tasks.”

The model learns low-dimensional representations for nodes by optimizing a neighborhood preserving objective, using random walks.

The code of Node2Vec is available on GitHub: https://github.com/eliorc/node2vec

To install the package, simply run:

pip install node2vec

Then, in your notebook, we’ll embed the Karate graph :

from node2vec import Node2Vec

Then, pre-compute the probabilities and generate walks :

node2vec = Node2Vec(G_karate, dimensions=64, walk_length=30, num_walks=200, workers=4)

We can then embed the nodes :

model = node2vec.fit(window=10, min_count=1, batch_words=4)

To get the vector of a node, say node ‘2’, use get_vector :

model.wv.get_vector(‘2’)

The outcome has the following form :

array([-0.03066591, 0.52942747, -0.14170371, 0.5471569 , 0.07588464, -0.5693364 , -0.3017375 , 0.21902356, 0.05244258 ...

It has a length of 64 since we defined the dimension as 64 above. What can we do with this embedding? One of the first options is for example to identify the most similar node!

model.wv.most_similar(‘2’)

It returns a list of the most similar nodes and the corresponding probabilities :

[('3', 0.6494477391242981),

('13', 0.6262941360473633),

('7', 0.6137452721595764),

...

If the nodes have labels, we can train an algorithm based on the embedding and attach a label (node labeling, most similar node…)

2. Edge Embedding

Edges can also be embedded, and the embedding can be further used for classification.

from node2vec.edges import HadamardEmbedder

edges_embs = HadamardEmbedder(keyed_vectors=model.wv)

Then, retrieve the vectors by specifying the name of the 2 linked nodes :

edges_embs[(‘1’, ‘2’)]

Which heads :

array([-8.1781112e-03, -1.8037426e-01, 4.9451444e-02, 2.8731486e-01...

Again, we can retrieve the most similar edge, which can be used for missing edges prediction for example :

edges_kv = edges_embs.as_keyed_vectors()

edges_kv.most_similar(str((‘1’, ‘2’)))

This heads :

[(""('2', '21')"", 0.8726599216461182),

(""('2', '7')"", 0.856759786605835),

(""('2', '3')"", 0.8566413521766663),

...

3. Graph Embedding

There are also ways to embed a graph or a sub-graph directly. This approach has been developed in the Graph2Vec paper and is useful to represent graphs or sub-graphs as vectors, thus allowing graph classification or graph similarity measures for example. I won’t dive deeper into this technique, but feel free to check the Github of this project :

To run the embedding, it’s as easy as :

python src/graph2vec.py --input-path data_folder/ --output-path output.csv

IV. Going further

We have now covered the introduction to graphs, the main types of graphs, the different graph algorithms, their implementation in Python with Networkx, and graph learning techniques for node labeling, link prediction, and graph embedding.

Needless to say, this is only the tip of the iceberg. Graph Theory is ever-expanding, and I thought that listing some resources to go further could be useful :

Graph Convolution Networks : https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780

Geometric Deep Learning on Graphs and Manifolds : http://www.geometricdeeplearning.com/

A MOOC that could help: https://www.edx.org/course/advanced-algorithmics-and-graph-theory-with-python

If you have any other resources in mind which you feel could be useful, don’t hesitate to leave a comment and I’ll add it to the list!

I hope this series of articles was interesting to follow! Feel free to comment if you have any question or remark.

NB: This article was originally published on my personal blog: https://maelfabien.github.io/

Sources :","['graph', 'graphs', 'edges', 'prediction', 'nodes', 'link', 'python', 'learning', 'neighbors', 'node', 'embedding', 'label']","There are three main tasks in graph learning that we will cover in this article:Link predictionNode label predictionGraph embeddingLet’s start with Link prediction!
In link prediction, we simply try to build a similarity measure between pairs of nodes and link the most similar nodes.
We then evaluate the proportion of correct predictions for dense graphs, or use Area under the Curve criteria for Sparse graphs.
There are several levels of embedding in a graph :Embedding graph components (nodes, edges, features…) (Node2Vec)Embedding sub-parts of a graph or a whole graph (Graph2Vec)1.
Going furtherWe have now covered the introduction to graphs, the main types of graphs, the different graph algorithms, their implementation in Python with Networkx, and graph learning techniques for node labeling, link prediction, and graph embedding.",en,['Maël Fabien'],2019-09-02 06:43:24.050000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/1200/1*DDAX3wQfIKOKWgoZkF12Wg.png', 'https://miro.medium.com/max/1288/1*hvRWvDAv0XvB23GsYnuzOA.png', 'https://miro.medium.com/max/3348/1*Abws_-eSApriKsxLeCXzJg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*2ZuW6l9zdQyGQeBewTIdIw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*IPyIiYV0MIXQUPnTLUW-ZA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*gWI5gCeeoZfvoknpgxdNDg.jpeg', 'https://miro.medium.com/max/60/1*6rFC3f4MerE3F9IeY-jdHA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2504/1*kJf4iuXMQEtGv4Hg6UkNJw.png', 'https://miro.medium.com/max/60/1*BZHBFFhX9ZHAgpetIrduzA.png?q=20', 'https://miro.medium.com/max/60/1*DDAX3wQfIKOKWgoZkF12Wg.png?q=20', 'https://miro.medium.com/max/60/1*JLnyyuTRe7LgQ89IDP0ElA.png?q=20', 'https://miro.medium.com/max/60/1*VeKA_M0JdGKgZzAQh-oyWQ.png?q=20', 'https://miro.medium.com/max/60/1*ItWMMgh4fKBdfSGGPoDWyw.png?q=20', 'https://miro.medium.com/max/60/1*D1SCbN2qOSgpQ6U1iRXw9w.png?q=20', 'https://miro.medium.com/max/3744/1*ItWMMgh4fKBdfSGGPoDWyw.png', 'https://miro.medium.com/max/832/1*PPw0Syg3F3RcMLOfN6-dpA.png', 'https://miro.medium.com/max/60/1*6IeCwlborbi2GcvzLjytEQ.png?q=20', 'https://miro.medium.com/max/60/1*PPw0Syg3F3RcMLOfN6-dpA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2996/1*BZHBFFhX9ZHAgpetIrduzA.png', 'https://miro.medium.com/max/60/1*hvRWvDAv0XvB23GsYnuzOA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*gWI5gCeeoZfvoknpgxdNDg.jpeg', 'https://miro.medium.com/max/60/1*kJf4iuXMQEtGv4Hg6UkNJw.png?q=20', 'https://miro.medium.com/max/60/1*Abws_-eSApriKsxLeCXzJg.png?q=20', 'https://miro.medium.com/max/3196/1*IPyIiYV0MIXQUPnTLUW-ZA.png', 'https://miro.medium.com/max/4544/1*DDAX3wQfIKOKWgoZkF12Wg.png', 'https://miro.medium.com/max/2524/1*2ZuW6l9zdQyGQeBewTIdIw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3656/1*D1SCbN2qOSgpQ6U1iRXw9w.png', 'https://miro.medium.com/max/3356/1*JLnyyuTRe7LgQ89IDP0ElA.png', 'https://miro.medium.com/max/2836/1*VeKA_M0JdGKgZzAQh-oyWQ.png', 'https://miro.medium.com/max/3044/1*6IeCwlborbi2GcvzLjytEQ.png', 'https://miro.medium.com/max/3564/1*6rFC3f4MerE3F9IeY-jdHA.png'}",2020-03-05 00:07:28.105091,2.225653886795044
https://towardsdatascience.com/understanding-entity-embeddings-and-its-application-69e37ae1501d,Understanding Entity Embeddings and It’s Application,"As of late I’ve been reading a lot on entity embeddings after being tasked to work on a forecasting problem.

The task at hand was to predict the salary of a given job title, given the historical job ads data that we have in our data warehouse. Naturally, I just had to seek out how this can be solved using deep learning — since it’s a lot more sexier nowadays to do stuff in deep learning instead of plain ’ol linear regression (if you’re reading this, and since only a data scientist would ever be reading this post, I’m sure you’d understand :) ).

And what better way to go about learning deep learning than to look for examples from fast.ai.

Lo and behold — they actually do have an example of that.

Complete with codes and all for a very similar problem, the Kaggle Rossman Challenge. Guess that wraps up my task then.

Having ran through the codes and retrofitting it to my data though, there’s clearly a lot more that one needs to understand to be able to make it run as smoothly as the author had intended.

I was intrigued with the idea that one can not only represent words into vectors, but even those data that are time based? — it was news to me at that time. I knew then that I had to look further into this notion of representing something as a vector, and what more can be accomplished using it.

Hence this post.

Entity Embedding

What is it?

Loosely speaking, entity embedding is a vector (a list of real numbers) representation of something (aka an entity). That something (again, the entity), in Natural Language Processing (NLP) for instance, can be a word, or a sentence, or a paragraph.

“ The GloVe word embedding of the word “stick” — a vector of 200 floats (rounded to two decimals). It goes on for two hundred values. “. [1]

However, it really isn’t limited to just that.

Entities (the vector) can also be thought of as an object, a context or an idea, of a thing. In the case of the popular Word2Vec model [8], that thing — are words. In recent times though, researchers have taken the idea of creating embeddings given a context (recall that Word2Vec creates embedding given the context of it’s surrounding words) and applied it to other kinds of objects (of which I’ll go through later).

For example, in the diagram below, a visualization of a Twitter user entity embedding of demonstrates that users with similar characteristics are closer to each other in the given vector space.

From Twitter [2]

Why is it important?

In NLP, being able to represent words into embeddings allow us to use much less memory as the vector length would typically be much shorter than the language’s vocabulary (traditionally one could perform one-hot encoding to represent each word in the vocabulary). In addition, the Word2Vec paper also showed that the embeddings also store some sense of semantic meaning of the word.

But more importantly, being able to represent entities (ie words, Twitter users, purchasing behaviour over time) into vectors opens up the possibility for us to perform various operation on it (the typical example being, using it as input to a machine learning model).

Applications

The following are some examples of how some well known organizations are using it to their advantage. It’s not meant to be exhaustive nor in-depth. Readers are advised to check out the respective source materials (links provided in the reference section) should more detail be required.

Model features

Embeddings are mostly created first and foremost as a way to represent something in vector format, to be used in a deep learning model. They’re useful to have since it allows us to capture and forward only the most salient information needed from the original data into our model.

A model using embeddings as input features will benefit from their encoded knowledge, and therefore improve performance. On top of that, assuming compactness of the embeddings, the model itself will require fewer parameters, resulting in faster iteration speed and cost savings in terms of infrastructure during both training and serving. [2]

For text based data, there are many forms of model architectures that has been developed to better capture the different kinds of information that text can contain. From mere one-hot encoding, TF-IDF, to neural based architectures like Word2Vec, GloVe to current state of the arts like ELMo, ULMFiT and BERT — today’s word embeddings have evolved from merely storing a binary yes or no status to capturing syntactic relationships and context.

For other types of information, like Twitter users, Pinterest pins or historical product sales revenue — a whole new kinds of model architecture are usually required.

For Instacart, in trying to optimize the efficiency of their personal shoppers; have built a deep learning model to predict the fastest sequence to sort the items of them. For the model, they embed their store locations, shopping items and shoppers into a 10-dimensional vector space by using the Embedding method in Keras [3].

The store location embedding enables the model to learn store layouts and generalize learnings across retailers and locations. The shopper embedding learns that shoppers may take consistently different routes through stores.

Looking deeper at the embedding layer afterwards reveal some interesting insights.

2-dimensional space representation of the embeddings using t-SNE [3]

Most of these clusters (above) correspond to departments, even though the department data was never used to learn the embeddings. Further, we can zoom into a region, like the blue meat and seafood department in the upper left. There are other products that appear near the meat and seafood, but aren’t meat and seafood. Instead, these are products (like spices, marinades, deli or other items) that are sold at the meat and seafood counter. The model is learning the organization of the store better than the department and aisle encoding data we have. [3]

Similar technique was also used in the Taxi Destination Prediction Kaggle competition [5], where the authors opted to use the neural network approach to eventually win the competition.

Categorical data (ie Client ID, Taxi ID, and Stand IDD) are represented in 10-dimension. Time is broken down to several bucket types and later embedded with the same dimension count.

Feature compression

In the Taxi Destination Prediction example earlier, we saw that there were 6 items being embedded — the 3 different ID types and 3 time buckets. If such model deployed in production, and as we continually add more and more features into the model to improve it’s performance — they will be a time when the time taken to process everything during inference time will be too slow.

As such, what we could do is to store the embeddings after the model have been trained, and load it back later (like Word2Vec embeddings). Note though that one should only store embeddings that are mostly static in nature (like the IDs). Effective usage of loading already pretrained embeddings would reduce the computation and memory load taken during inference time.

Nearest neighbour search

As we saw earlier in Instacart’s visualized embedding, similar items are in fact closer to each other in the multi dimensional vector space that they exist in. Exploiting this behaviour, we could in fact look for items of similar attributes given that we have a sample vector of what we’re interested in based on their distance.

Pinterest for instance, created a 128-dimensional embeddings for it’s pins (aka Pin2Vec) to capture the context of how each Pin relates to the Pinners [4]. It’s creation isn’t as straightforward (unlike Instacart or the Taxi competition) however, as they’ve adopted a similar method to Word2Vec in coming up with the represention.

“The learned Pin2Vec groups Pins with respect to a Pinner’s recent engagement” [4]

Pin2Vec architecture is inspired by Word2Vec [4]

The result was a more relevant recommendations as compared to the predecessor. However, they are still use the latter for long tail Pins with sparse data.

In the Pinterest app, retrieving related pins (ie search result) isn’t only based on tapping a pin. One can also make use of the visual search feature, or the text bar at the top of the app. Can a nearest neighbour based search still be used in such cases?

In his lecture at Berkeley, Dan Gillick of Google proposes that it could be done provided that we are able to place all of the different objects/entities, coming from text, images, video or audio; in the same vector space. [6 (42:05) ]

By training all the model together, we can ensure that the embeddings reside in the same space.

Considering the above diagram for example, there are 2 labelled dataset: (i) Question — Image and (ii) Question — Document. By training both model datasets together we can ensure that the question, image and document — all exist in a single vector space.

Transfer learning

Transfer learning is another common use of entity embeddings. Essentially, what it means is that we train a model (ie. a language model), and use it for another type of problem (ie. text classification).

Training a classifier using BERT [1].

In general models that are built using a pretrained model are faster to train and can achieve better results. In my current organization, we leverage BERT for one of our text classification task where it quickly achieved state of the art results for the given problem.

Summary

From merely representing words and it’s semantics, to representing time and spatial locations — there seem to be a clear advantage in being able to come up with good representations of entities into vectors.

The keyword there however is — “being able to”.

While there has been a lot of development in representing words or text in general into embeddings, the same cannot be said to other types of entities. As we’ve seen in the case of Pin2Vec, coming up with the embedding does require some understanding of the problem and creativity in solving it.

And even then, it is important that we don’t make the assumption the learned representations are 100% correct. Even Word2Vec, for all it’s hype, isn’t all that reliable in some of the cases. In the diagram below for example, while the relationship Obama is to Barack as to Sarkozy is to Nicolas make sense since the context is first names and last names; the same can’t be said for Putin and Medvedev — who are two separate individuals. For more details on the caveats of word embeddings, check out the article in [7].","['text', 'vector', 'entity', 'learning', 'embeddings', 'model', 'data', 'word2vec', 'application', 'embedding', 'words', 'understanding', 'using']","As of late I’ve been reading a lot on entity embeddings after being tasked to work on a forecasting problem.
Loosely speaking, entity embedding is a vector (a list of real numbers) representation of something (aka an entity).
A model using embeddings as input features will benefit from their encoded knowledge, and therefore improve performance.
2-dimensional space representation of the embeddings using t-SNE [3]Most of these clusters (above) correspond to departments, even though the department data was never used to learn the embeddings.
As such, what we could do is to store the embeddings after the model have been trained, and load it back later (like Word2Vec embeddings).",en,['Hafidz Zulkifli'],2019-01-28 14:56:02.230000+00:00,"{'Data Science', 'Deep Learning', 'Machine Learning', 'Embedding', 'NLP'}","{'https://miro.medium.com/fit/c/160/160/1*2zQ9d0myivvoyHckRRllcw.png', 'https://miro.medium.com/max/520/1*99Lk64_eBtK_MPOD5BUfMw.png', 'https://miro.medium.com/max/1092/1*WcJ8zp9AHnTeiangcKWPiA.png', 'https://miro.medium.com/max/60/1*BLczqN9K14EVahZZOaum7g.png?q=20', 'https://miro.medium.com/max/1458/1*Kx3pT6_xZP2BjpH20Y1Biw.png', 'https://miro.medium.com/max/1138/1*3G6HKgqY1dz7ODERnlI11g.png', 'https://miro.medium.com/max/60/1*L9mtyQ14UdCV5vrjuczPKA.png?q=20', 'https://miro.medium.com/max/60/1*YbnSS_m2NOpX5UGetBtX8w.png?q=20', 'https://miro.medium.com/max/1888/1*LJoc_6ou76u0onD9sJRORQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*GJOfmEsrtrayaoMoOK1qtw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*2zQ9d0myivvoyHckRRllcw.png', 'https://miro.medium.com/max/2276/1*3G6HKgqY1dz7ODERnlI11g.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*LJoc_6ou76u0onD9sJRORQ.png?q=20', 'https://miro.medium.com/max/60/1*3G6HKgqY1dz7ODERnlI11g.png?q=20', 'https://miro.medium.com/max/60/1*Kx3pT6_xZP2BjpH20Y1Biw.png?q=20', 'https://miro.medium.com/max/776/1*L9mtyQ14UdCV5vrjuczPKA.png', 'https://miro.medium.com/max/60/1*99Lk64_eBtK_MPOD5BUfMw.png?q=20', 'https://miro.medium.com/max/916/1*GJOfmEsrtrayaoMoOK1qtw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*QXph9gYL39itXl4E125xiQ.png?q=20', 'https://miro.medium.com/max/60/1*WcJ8zp9AHnTeiangcKWPiA.png?q=20', 'https://miro.medium.com/max/60/1*ZNBE4IcqsQrvrAOpMs8Lvw.png?q=20', 'https://miro.medium.com/max/1812/1*BLczqN9K14EVahZZOaum7g.png', 'https://miro.medium.com/max/60/1*h_33ougKyOP5OadptiulvA.png?q=20', 'https://miro.medium.com/max/928/1*h_33ougKyOP5OadptiulvA.png', 'https://miro.medium.com/max/1088/1*QXph9gYL39itXl4E125xiQ.png', 'https://miro.medium.com/max/60/0*Y-S-kMC1zVfFjZuH?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1818/1*YbnSS_m2NOpX5UGetBtX8w.png', 'https://miro.medium.com/max/1756/1*ZNBE4IcqsQrvrAOpMs8Lvw.png', 'https://miro.medium.com/max/1200/0*Y-S-kMC1zVfFjZuH'}",2020-03-05 00:07:37.369492,9.262643337249756
https://towardsdatascience.com/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b,Optimizing deep learning trading bots using state-of-the-art techniques,"In the last article, we used deep reinforcement learning to create Bitcoin trading bots that don’t lose money. Although the agents were profitable, the results weren’t all that impressive, so this time we’re going to step it up a notch and massively improve our model’s profitability.

As a reminder, the purpose of this series of articles is to experiment with state-of-the-art deep reinforcement learning technologies to see if we can create profitable Bitcoin trading bots. It seems to be the status quo to quickly shut down any attempts to create reinforcement learning algorithms, as it is “the wrong way to go about building a trading algorithm”. However, recent advances in the field have shown that RL agents are often capable of learning much more than supervised learning agents within the same problem domain. For this reason, I am writing these articles to see just how profitable we can make these trading agents, or if the status quo exists for a reason.

We will first improve our model’s policy network and make the input data set stationary, so we can learn more from less data. Next, we will use advanced feature engineering to improve our agent’s observation space and fine tune our reward function to produce more attractive strategies. Finally, we will use a technique called Bayesian optimization to zone in on the most profitable hyper-parameters, before training and testing the final agents profitablity. Hold on to your seats everyone, this is going to be a wild ride.

When you’ve read this article, check out TensorTrade — the successor framework to the codebase produced in this article.

Modifications

The first thing we need to do to improve the profitability of our model, is make a couple improvements on the code we wrote in the last article. If you do not yet have the code, you can grab it from my GitHub.

Recurrent Networks

The first change we need to make is to update our policy to use a recurrent, Long Short-Term Memory (LSTM) network, in place of our previous, Multi-Layer Perceptron (MLP) network. Since recurrent networks are capable of maintaining internal state over time, we no longer need a sliding “look-back” window to capture the motion of the price action. Instead, it is inherently captured by the recursive nature of the network. At each time step, the input from the data set is passed into the algorithm, along with the output from the last time step.

This allows the LSTM to maintain an internal state that gets updated at each time step as the agent “remembers” and “forgets” specific data relationships.

Here we update our PPO2 model to use the MlpLstmPolicy, to take advantage of its recurrent nature.

Stationary Data

It was also pointed out to me on the last article that our time series data is not stationary, and therefore, any machine learning model is going to have a hard time predicting future values.

A stationary time series is one whose mean, variance, and auto-correlation (lagged correlation with itself) are constant.

The bottom line is that our time series contains an obvious trend and seasonality, which both impact our algorithms ability to predict the time series accurately. We can fix this by using differencing and transformation techniques to produce a more normal distribution from our existing time series.

Differencing is the process of subtracting the derivative (rate of return) at each time step from the value at that time step. This has the desired result of removing the trend in our case, however, the data still has a clear seasonality to it. We can attempt to remove that by taking the logarithm at each time step before differencing, which produces the final, stationary time series, shown below on the right.

We can verify the produced time series is stationary by running it through an Augmented Dickey-Fuller Test. Doing this gives us a p-value of 0.00, allowing us to reject the test’s null hypothesis and confirm our time series is stationary.

Here we run the Augmented Dicker-Fuller Test on our transformed data set to ensure stationarity.

Now that we’ve got that out of the way, we are going to further update our observation space using a bit of feature engineering.

Feature Engineering

To further improve our model, we are going to be doing a bit of feature engineering.

Feature engineering is the process of using domain-specific knowledge to create additional input data that improves a machine learning model.

In our case, we are going to be adding some common, yet insightful technical indicators to our data set, as well as the output from the StatsModels SARIMAX prediction model. The technical indicators should add some relevant, though lagging information to our data set, which will be complimented well by the forecasted data from our prediction model. This combination of features should provide a nice balance of useful observations for our model to learn from.

Technical Analysis

To choose our set of technical indicators, we are going to compare the correlation of all 32 indicators (58 features) available in the ta library. We can use pandas to find the correlation between each indicator of the same type (momentum, volume, trend, volatility), then select only the least correlated indicators from each type to use as features. That way, we can get as much benefit out of these technical indicators as possible, without adding too much noise to our observation space.

Seaborn heatmap of technical indicator correlation on BTC data set.

It turns out that the volatility indicators are all highly correlated, as well as a couple of the momentum indicators. When we remove all duplicate features (features with an absolute mean correlation > 0.5 within their group), we are left with 38 technical features to add to our observation space. This is perfect, so we’ll create a utility method named add_indicators to add these features to our data frame, and call it within our environment’s initialization to avoid having to calculate these values on each time step.

Here we initialize our environment, adding the indicators to our data frame before making it stationary.

Statistical Analysis

Next we need to add our prediction model. We’ve chosen to use the Seasonal Auto Regressive Integrated Moving Average (SARIMA) model to provide price predictions because it can be calculated very quickly at each step, and it is decently accurate on our stationary data set. As a bonus, it‘s pretty simple to implement and it allows us to create a confidence interval for its future predictions, which is often much more insightful than a single value. For example, our agent can be learn to be more cautious trusting predictions when the confidence interval is small and take more risk when the interval is large.

Here we add the SARIMAX predictions and confidence intervals to our observation space.

Now that we’ve updated our policy to use a more applicable, recurrent network and improved our observation space through contextual feature engineering, it’s time to optimize all of the things.

Reward Optimization

One might think our reward function from the previous article (i.e. rewarding incremental net worth gains) is the best we can do, however, further inspection shows this is far from the truth. While our simple reward function from last time was able to profit, it produced volatile strategies that often lead to stark losses in capital. To improve on this, we are going to need to consider other metrics to reward, besides simply unrealized profit.

A simple improvement to this strategy, as mentioned by Sean O’Gordman in the comments of my last article, is to not only reward profits from holding BTC while it is increasing in price, but also reward profits from not holding BTC while it is decreasing in price. For example, we could reward our agent for any incremental increase in net worth while it is holding a BTC/USD position, and again for the incremental decrease in value of BTC/USD while it is not holding any positions.

While this strategy is great at rewarding increased returns, it fails to take into account the risk of producing those high returns. Investors have long since discovered this flaw with simple profit measures, and have traditionally turned to risk-adjusted return metrics to account for it.

Volatility-Based Metrics

The most common risk-adjusted return metric is the Sharpe ratio. This is a simple ratio of a portfolio’s excess returns to volatility, measured over a specific period of time. To maintain a high Sharpe ratio, an investment must have both high returns and low volatility (i.e. risk). The math for this goes as follows:

This metric has stood the test of time, however it too is flawed for our purposes, as it penalizes upside volatility. For Bitcoin, this can be problematic as upside volatility (wild upwards price movement) can often be quite profitable to be a part of. This leads us to the first rewards metric we will be testing with our agents.

The Sortino ratio is very similar to the Sharpe ratio, except it only considers downside volatility as risk, rather than overall volatility. As a result, this ratio does not penalize upside volatility. Here’s the math:

Additional Metrics

The second rewards metric that we will be testing on this data set will be the Calmar ratio. All of our metrics up to this point have failed to take into account drawdown.

Drawdown is the measure of a specific loss in value to a portfolio, from peak to trough.

Large drawdowns can be detrimental to successful trading strategies, as long periods of high returns can be quickly reversed by a sudden, large drawdown.

To encourage strategies that actively prevent large drawdowns, we can use a rewards metric that specifically accounts for these losses in capital, such as the Calmar ratio. This ratio is identical to the Sharpe ratio, except that it uses maximum drawdown in place of the portfolio value’s standard deviation.

Our final metric, used heavily in the hedge fund industry, is the Omega ratio. On paper, the Omega ratio should be better than both the Sortino and Calmar ratios at measuring risk vs. return, as it is able to account for the entirety of the risk over return distribution in a single metric. To find it, we need to calculate the probability distributions of a portfolio moving above or below a specific benchmark, and then take the ratio of the two. The higher the ratio, the higher the probability of upside potential over downside potential.

If this looks complicated, don’t worry. It get’s simpler in code.

The Code

While writing the code for each of these rewards metrics sounds really fun, I have opted to use the empyrical library to calculate them instead. Luckily enough, this library just happens to include the three rewards metrics we’ve defined above. Getting a ratio at each time step is as simple as providing the list of returns and benchmark returns for a time period to the corresponding Empyrical function.

Here we set the reward at each time step based on our pre-defined reward function

Now that we’ve decided how to measure a successful trading strategy, it’s time to figure out which of these metrics produces the most appealing results. Let’s plug each of these reward functions into Optuna and use good old Bayesian optimization to find the best strategy for our data set.

The Toolset

Any great technician needs a great toolset. Instead of re-inventing the wheel, we are going to take advantage of the pain and suffering of the programmers that have come before us. For today’s job, our most important tool is going to be the optuna library, which implements Bayesian optimization using Tree-structured Parzen Estimators (TPEs). TPEs are parallelizable, which allows us to take advantage of our GPU, dramatically decreasing our overall search time. In a nutshell,

Bayesian optimization is a technique for efficiently searching a hyperspace to find the set of parameters that maximize a given objective function.

In simpler terms, Bayesian optimization is an efficient method for improving any black box model. It works by modeling the objective function you want to optimize using a surrogate function, or a distribution of surrogate functions. That distribution improves over time as the algorithm explores the hyperspace and zones in on the areas that produce the most value.

How does this apply to our Bitcoin trading bots? Essentially, we can use this technique to find the set of hyper-parameters that make our model the most profitable. We are searching for a needle in a haystack and Bayesian optimization is our magnet. Let’s get started.

Implementing Optuna

Optimizing hyper-parameters with Optuna is fairly simple. First, we’ll need to create an optuna study, which is the parent container for all of our hyper-parameter trials. A trial contains a specific configuration of hyper-parameters and its resulting cost from the objective function. We can then call study.optimize() and pass in our objective function, and Optuna will use Bayesian optimization to find the configuration of hyper-parameters that produces the lowest cost.

In this case, our objective function consists of training and testing our PPO2 model on our Bitcoin trading environment. The cost we return from our function is the average reward over the testing period, negated. We need to negate the average reward, because Optuna interprets lower return value as better trials. The optimize function provides a trial object to our objective function, which we then use to specify each variable to optimize.

The optimize_ppo2() and optimize_envs() methods take in a trial object and return a dictionary of parameters to test. The search space for each of our variables is defined by the specific suggest function we call on the trial, and the parameters we pass in to that function.

For example, trial.suggest_loguniform('n_steps', 16, 2048) will suggest a new float between 16–2048 in a logarithmic manner (16, 32, 64, …, 1024, 2048). Further, trial.suggest_uniform('cliprange’, 0.1, 0.4) will suggest floats in a simple, additive manner (0.1, 0.2, 0.3, 0.4). We don’t use it here, but Optuna also provides a method for suggesting categorical variables: suggest_categorical('categorical', ['option_one', ‘option_two']) .

Later, after running our optimization function overnight with a decent CPU/GPU combination, we can load up the study from the sqlite database we told Optuna to create. The study keeps track of the best trial from its tests, which we can use to grab the best set of hyper-parameters for our environment.

We’ve revamped our model, improved our feature set, and optimized all of our hyper-parameters. Now it’s time to see how our agents do with their new reward mechanisms. I have trained an agent to optimize each of our four return metrics: simple profit, the Sortino ratio, the Calmar ratio, and the Omega ratio. Let’s run each of these optimized agents on a test environment, which is initialized with price data they’ve not been trained on, and see profitable they are.

Benchmarking

Before we look at the results, we need to know what a successful trading strategy looks like. For this treason, we are going to benchmark against a couple common, yet effective strategies for trading Bitcoin profitably. Believe it or not, one of the most effective strategies for trading BTC over the last ten years has been to simply buy and hold. The other two strategies we will be testing use very simple, yet effective technical analysis to create buy and sell signals.

Buy and hold

The idea is to buy as much as possible and Hold On for Dear Life (HODL). While this strategy is not particularly complex, it has seen very high success rates in the past.

2. RSI divergence

When consecutive closing price continues to rise as the RSI continues to drop, a negative trend reversal (sell) is signaled. A positive trend reversal (buy) is signaled when closing price consecutively drops as the RSI consecutively rises.

3. Simple Moving Average (SMA) Crossover

When the longer-term SMA crosses above the shorter-term SMA, a negative trend reversal (sell) is signaled. A positive trend reversal (buy) is signaled when the shorter-term SMA crosses above the longer-term SMA.

The purpose of testing against these simple benchmarks is to prove that our RL agents are actually creating alpha over the market. If we can’t beat these simple benchmarks, then we are wasting countless hours of development time and GPU cycles, just to make a cool science project. Let’s prove that this is not the case.

The Results

I must preface this section by stating that the positive profits in this section are the direct result of incorrect code. Due to the way dates were being sorted at the time, the agent was able to see the price 12 hours in advance at all times, an obvious form of look-ahead bias. This has since been fixed, though the time has yet to be invested to replace each of the result sets below. Please understand that these results are completely invalid and highly unlikely to be reproduced. That being said, there is still a large amount of research that went into this article and the purpose was never to make massive amounts of money, rather to see what was possible with the current state-of-the-art reinforcement learning and optimization techniques. So in attempt to keep this article as close to the original as possible, I will leave the old (invalid) results here until I have the time to replace them with new, valid results.

The agents were trained on the first 80% of the data set (hourly OHCLV data from CryptoDataDownload), and tested on the final 20% to see how the strategies generalize to fresh data. This simple cross validation is enough for what we need, as when we eventually release these algorithms into the wild, we can train on the entire data set and treat new incoming data as the new test set.

Let’s quickly move through the losers so we can get to the good stuff. First, we’ve got the Omega strategy, which ends up being fairly useless trading against our data set.

Average net worth of Omega-based agents over 3500 hours of trading

Watching this agent trade, it was clear this reward mechanism produces strategies that over-trade and are not capable of capitalizing on market opportunities.

The Calmar-based strategies came in with a small improvement over the Omega-based strategies, but ultimately the results were very similar. It’s starting to look like we’ve put in a ton of time and effort, just to make things worse…

Average net worth of Calmar-based agents over 3500 hours of trading

Remember our old friend, simple incremental profit? While this reward mechanism didn’t prove to be too successful in our last article, all the modifications and optimizations we’ve done seem to have massively improved the success of the agents.

The average profit is just over 350% of the initial account balance, over our four month test period. If you are unaware of average market returns, these kind of results would be absolutely insane. Surely this is the best we can do with reinforcement learning… right?

Average net worth of Profit-based agents over 3500 hours of trading

Wrong. The average profit produced by agents rewarded by the Sortino ratio was nearly 850%. When I saw the success of these strategies, I had to quickly check to make sure there were no bugs. [editors note: Brace yourself for the irony of the following sentence.] After a thorough inspection, it is clear that the code is bug free and these agents are just very good at trading Bitcoin.

Average net worth of Sortino-based agents over 3500 hours of trading

Instead of over-trading and under-capitalizing, these agents seem to understand the importance of buying low and selling high, while minimizing the risk of holding BTC. Regardless of what specific strategy the agents have learned, our trading bots have clearly learned to trade Bitcoin profitably. If you don’t believe me, see for yourself.

One of the Sortino-based agents trading BTC/USD. Green triangles signal buys, red triangles signal sells.

Now, I am no fool. I understand that the success in these tests may not [read: will not] generalize to live trading. That being said, these results are far more impressive than any algorithmic trading strategies I’ve seen to date (this should have been the first clue that something was wrong…). It is truly amazing considering these agents were given no prior knowledge of how markets worked or how to trade profitably, and instead learned to be massively successful through trial and error alone (along with some good old look-ahead bias). Lots, and lots, of trial and error.

Conclusion

In this article, we’ve optimized our reinforcement learning agents to make even better decisions while trading Bitcoin, and therefore, make a ton more money! It took quite a bit of work, but we’ve managed to accomplish it by doing the following:

Upgrade the existing model to use a recurrent, LSTM policy network with stationary data

Engineer 40+ new features for the agent to learn from using domain-specific technical and statistical analysis

Improve the agent’s reward system to account for risk, instead of simply profit

Fine tuned the model’s hyper-parameters using Bayesian optimization

Benchmarked against common trading strategies to ensure the bots are always beating the market

A highly profitable trading bot is great, in theory. However, I’ve received quite a bit of feedback claiming these agents are simply learning to fit a curve, and therefore, would never be profitable trading on live data. While our method of training/testing on separate data sets should address this issue, it is true that our model could be overfitting to this data set and might not generalize to new data very well. That being said, I’ve got a feeling these agents are learning quite a bit more than simple curve fitting, and as a result, will be able to profit in live trading situations.

To experiment on this hypothesis, with the help of the community I’ve built a fully-fledged reinforcement learning framework for trading stocks, forex, cryptocurrency, and any other financial instrument with an API. Check it out below.","['ratio', 'optimizing', 'set', 'agents', 'bots', 'reward', 'simple', 'learning', 'trading', 'deep', 'data', 'model', 'strategies', 'techniques', 'stateoftheart', 'using']","In the last article, we used deep reinforcement learning to create Bitcoin trading bots that don’t lose money.
However, recent advances in the field have shown that RL agents are often capable of learning much more than supervised learning agents within the same problem domain.
We will first improve our model’s policy network and make the input data set stationary, so we can learn more from less data.
Feature engineering is the process of using domain-specific knowledge to create additional input data that improves a machine learning model.
This is a simple ratio of a portfolio’s excess returns to volatility, measured over a specific period of time.",en,['Adam King'],2019-10-17 04:38:48.440000+00:00,"{'Cryptocurrency', 'Machine Learning', 'Towards Data Science', 'Bitcoin', 'Trading'}","{'https://miro.medium.com/max/2176/1*bdX7OXXn87uJAlemweGDBg@2x.png', 'https://miro.medium.com/max/60/1*y3YRhBl7hybWTEmot8NfJQ@2x.png?q=20', 'https://miro.medium.com/max/1232/1*395WJHWafgPI5XdOOuXhOQ.gif', 'https://miro.medium.com/max/60/1*uyGValvpt4Ty8gJC5YUnuQ.png?q=20', 'https://miro.medium.com/max/1716/1*Xkw2kfEzWuY4TzZD_Caa4Q.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*mAwaGvqs0FPcT_Sum0YuMw.png?q=20', 'https://miro.medium.com/max/1524/1*mAwaGvqs0FPcT_Sum0YuMw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1548/1*E4wNc6xRniE_iYOq1QVqEw.png', 'https://miro.medium.com/max/4000/1*kOeJ0KKlmbNA_iYBYopk3w.jpeg', 'https://miro.medium.com/max/60/1*Xkw2kfEzWuY4TzZD_Caa4Q.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*vuWWTDpcP95w3jVETcLCAQ.png?q=20', 'https://miro.medium.com/max/60/1*E4wNc6xRniE_iYOq1QVqEw.png?q=20', 'https://miro.medium.com/max/60/1*bdX7OXXn87uJAlemweGDBg@2x.png?q=20', 'https://miro.medium.com/max/60/1*cI9NBfsxSZxnY_Fd_CBb7A@2x.png?q=20', 'https://miro.medium.com/max/60/0*aHo9xDRcfJ6_A8vU.png?q=20', 'https://miro.medium.com/max/1792/1*y3YRhBl7hybWTEmot8NfJQ@2x.png', 'https://miro.medium.com/max/1544/0*aHo9xDRcfJ6_A8vU.png', 'https://miro.medium.com/max/60/1*KJeb4w_--IgyzSSQTsuMQQ.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*H3ljCW5r8nFtNQSqY-7GPQ.jpeg', 'https://miro.medium.com/max/5000/1*KJeb4w_--IgyzSSQTsuMQQ.jpeg', 'https://miro.medium.com/max/2192/1*2GnQ7vz5Xpy37NPCrEolrg@2x.png', 'https://miro.medium.com/max/60/1*kOeJ0KKlmbNA_iYBYopk3w.jpeg?q=20', 'https://miro.medium.com/max/60/1*2KmDUdc29rP3s6m5wXrSUQ.jpeg?q=20', 'https://miro.medium.com/max/1184/0*Vl6K9kS7k2B2FqGY', 'https://miro.medium.com/max/1536/1*uyGValvpt4Ty8gJC5YUnuQ.png', 'https://miro.medium.com/max/60/0*Vl6K9kS7k2B2FqGY?q=20', 'https://miro.medium.com/freeze/max/60/1*395WJHWafgPI5XdOOuXhOQ.gif?q=20', 'https://miro.medium.com/max/11520/1*2KmDUdc29rP3s6m5wXrSUQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*nZW6VASAaCVwXsfyR5N8Ow.jpeg?q=20', 'https://miro.medium.com/max/11232/1*H3ljCW5r8nFtNQSqY-7GPQ.jpeg', 'https://miro.medium.com/max/60/1*2GnQ7vz5Xpy37NPCrEolrg@2x.png?q=20', 'https://miro.medium.com/max/1544/1*vuWWTDpcP95w3jVETcLCAQ.png', 'https://miro.medium.com/max/60/1*H3ljCW5r8nFtNQSqY-7GPQ.jpeg?q=20', 'https://miro.medium.com/max/2192/1*cI9NBfsxSZxnY_Fd_CBb7A@2x.png', 'https://miro.medium.com/fit/c/96/96/2*GnXdYdgLDvWhInOVFHxikA.jpeg', 'https://miro.medium.com/max/3200/1*nZW6VASAaCVwXsfyR5N8Ow.jpeg', 'https://miro.medium.com/fit/c/160/160/2*GnXdYdgLDvWhInOVFHxikA.jpeg'}",2020-03-05 00:07:46.757401,9.387909889221191
https://towardsdatascience.com/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29,Creating Bitcoin trading bots don’t lose money,"Life Through The Agent’s Eyes

It can often be helpful to visual an environment’s observation space, in order to get an idea of the types of features your agent will be working with. For example, here is a visualization of our observation space rendered using OpenCV.

OpenCV visualization of the environment’s observation space

Each row in the image represents a row in our observation_space . The first 4 rows of frequency-like red lines represent the OHCL data, and the spurious orange and yellow dots directly below represent the volume. The fluctuating blue bar below that is the agent’s net worth, and the lighter blips below that represent the agent’s trades.

If you squint, you can just make out a candlestick graph, with volume bars below it and a strange morse-code like interface below that shows trade history. It looks like our agent should be able to learn sufficiently from the data in our observation_space , so let’s move on. Here we’ll define our _next_observation method, where we’ll scale the observed data from 0 to 1.

It’s important to only scale the data the agent has observed so far to prevent look-ahead biases.

def _next_observation(self):

end = self.current_step + self.lookback_window_size + 1 obs = np.array([

self.active_df['Open'].values[self.current_step:end],

self.active_df['High'].values[self.current_step:end],

self.active_df['Low'].values[self.current_step:end],

self.active_df['Close'].values[self.current_step:end],

self.active_df['Volume_(BTC)'].values[self.current_step:end],

]) scaled_history = self.scaler.fit_transform(self.account_history) obs = np.append(obs, scaled_history[:, -(self.lookback_window_size

+ 1):], axis=0) return obs

Taking Action

Now that we’ve set up our observation space, it’s time to write our step function, and in turn, take the agent’s prescribed action. Whenever self.steps_left == 0 for our current trading session, we will sell any BTC we are holding and call _reset_session() . Otherwise, we set the reward to our current net worth and only set done to True if we’ve run out of money.

def step(self, action):

current_price = self._get_current_price() + 0.01

self._take_action(action, current_price)

self.steps_left -= 1

self.current_step += 1 if self.steps_left == 0:

self.balance += self.btc_held * current_price

self.btc_held = 0

self._reset_session() obs = self._next_observation()

reward = self.net_worth

done = self.net_worth <= 0 return obs, reward, done, {}

Taking an action is as simple as getting the current_price , determining the specified action, and either buying or selling the specified amount of BTC. Let’s quickly write _take_action so we can test our environment.

def _take_action(self, action, current_price):

action_type = action[0]

amount = action[1] / 10 btc_bought = 0

btc_sold = 0

cost = 0

sales = 0 if action_type < 1:

btc_bought = self.balance / current_price * amount

cost = btc_bought * current_price * (1 + self.commission)

self.btc_held += btc_bought

self.balance -= cost elif action_type < 2:

btc_sold = self.btc_held * amount

sales = btc_sold * current_price * (1 - self.commission)

self.btc_held -= btc_sold

self.balance += sales

Finally, in the same method, we will append the trade to self.trades and update our net worth and account history.

if btc_sold > 0 or btc_bought > 0:

self.trades.append({

'step': self.frame_start+self.current_step,

'amount': btc_sold if btc_sold > 0 else btc_bought,

'total': sales if btc_sold > 0 else cost,

'type': ""sell"" if btc_sold > 0 else ""buy""

}) self.net_worth = self.balance + self.btc_held * current_price

self.account_history = np.append(self.account_history, [

[self.net_worth],

[btc_bought],

[cost],

[btc_sold],

[sales]

], axis=1)

Our agents can now initiate a new environment, step through that environment, and take actions that affect the environment. It’s time to watch them trade.

Watching Our Bots Trade

Our render method could be something as simple as calling print(self.net_worth) , but that’s no fun. Instead we are going to plot a simple candlestick chart of the pricing data with volume bars and a separate plot for our net worth.

We are going to take the code in StockTradingGraph.py from the last article I wrote, and re-purposing it to render our Bitcoin environment. You can grab the code from my GitHub.

The first change we are going to make is to update self.df['Date'] everywhere to self.df['Timestamp'] , and remove all calls to date2num as our dates already come in unix timestamp format. Next, in our render method we are going to update our date labels to print human-readable dates, instead of numbers.

from datetime import datetime

First, import the datetime library, then we’ll use the utcfromtimestamp method to get a UTC string from each timestamp and strftime to format the string in Y-m-d H:M format.

date_labels = np.array([datetime.utcfromtimestamp(x).strftime(

'%Y-%m-%d %H:%M') for x in self.df['Timestamp'].values[step_range]])

Finally, we change self.df['Volume'] to self.df['Volume_(BTC)'] to match our data set, and we’re good to go. Back in our BitcoinTradingEnv , we can now write our render method to display the graph.

def render(self, mode='human', **kwargs):

if mode == 'human':

if self.viewer == None:

self.viewer = BitcoinTradingGraph(self.df,

kwargs.get('title', None)) self.viewer.render(self.frame_start + self.current_step,

self.net_worth,

self.trades,

window_size=self.lookback_window_size)

And voila! We can now watch our agents trade Bitcoin.

Matplotlib visualization of our agent trading Bitcoin

The green ghosted tags represent buys of BTC and the red ghosted tags represent sells. The white tag on the top right is the agent’s current net worth and the bottom right tag is the current price of Bitcoin. Simple, yet elegant. Now, it’s time to train our agent and see how much money we can make!

Training Time

One of the criticisms I received on my first article was the lack of cross-validation, or splitting the data into a training set and test set. The purpose of doing this is to test the accuracy of your final model on fresh data it has never seen before. While this was not a concern of that article, it definitely is here. Since we are using time series data, we don’t have many options when it comes to cross-validation.

For example, one common form of cross validation is called k-fold validation, in which you split the data into k equal groups and one by one single out a group as the test group and use the rest of the data as the training group. However time series data is highly time dependent, meaning later data is highly dependent on previous data. So k-fold won’t work, because our agent will learn from future data before having to trade it, an unfair advantage.

This same flaw applies to most other cross-validation strategies when applied to time series data. So we are left with simply taking a slice of the full data frame to use as the training set from the beginning of the frame up to some arbitrary index, and using the rest of the data as the test set.

slice_point = int(len(df) - 100000)

train_df = df[:slice_point]

test_df = df[slice_point:]

Next, since our environment is only set up to handle a single data frame, we will create two environments, one for the training data and one for the test data.

train_env = DummyVecEnv([lambda: BitcoinTradingEnv(train_df,

commission=0, serial=False)])

test_env = DummyVecEnv([lambda: BitcoinTradingEnv(test_df,

commission=0, serial=True)])

Now, training our model is as simple as creating an agent with our environment and calling model.learn .

model = PPO2(MlpPolicy,

train_env,

verbose=1,

tensorboard_log=""./tensorboard/"")

model.learn(total_timesteps=50000)

Here, we are using tensorboard so we can easily visualize our tensorflow graph and view some quantitative metrics about our agents. For example, here is a graph of the discounted rewards of many agents over 200,000 time steps:

Wow, it looks like our agents are extremely profitable! Our best agent was even capable of 1000x’ing his balance over the course of 200,000 steps, and the rest averaged at least a 30x increase!

It was at this point that I realized there was a bug in the environment… Here is the new rewards graph, after fixing that bug:

As you can see, a couple of our agents did well, and the rest traded themselves into bankruptcy. However, the agents that did well were able to 10x and even 60x their initial balance, at best. I must admit, all of the profitable agents were trained and tested in an environment without commissions, so it is still entirely unrealistic for our agent’s to make any real money. But we’re getting somewhere!

Let’s test our agents on the test environment (with fresh data they’ve never seen before), to see how well they’ve learned to trade Bitcoin.

Our trained agents race to bankruptcy when trading on fresh, test data

Clearly, we’ve still got quite a bit of work to do. By simply switching our model to use stable-baseline’s A2C, instead of the current PPO2 agent, we can greatly improve our performance on this data set. Finally, we can update our reward function slightly, as per Sean O’Gorman’s advice, so that we reward increases in net worth, not just achieving a high net worth and staying there.

reward = self.net_worth - prev_net_worth

These two changes alone greatly improve the performance on the test data set, and as you can see below, we are finally able to achieve profitability on fresh data that wasn’t in the training set.

However, we can do much better. In order for us to improve these results, we are going to need to optimize our hyper-parameters and train our agents for much longer. Time to break out the GPU and get to work!

However, this article is already a bit long and we’ve still got quite a bit of detail to go over, so we are going to take a break here. In my next article, we will use Bayesian optimization to zone in on the best hyper-parameters for our problem space, and improve the agent’s model to achieve highly profitable trading strategies.","['agent', 'bitcoin', 'set', 'lose', 'agents', 'net', 'bots', 'environment', 'money', 'trading', 'data', 'dont', 'method', 'worth', 'training', 'test', 'creating']","The fluctuating blue bar below that is the agent’s net worth, and the lighter blips below that represent the agent’s trades.
Otherwise, we set the reward to our current net worth and only set done to True if we’ve run out of money.
date_labels = np.array([datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M') for x in self.df['Timestamp'].values[step_range]])Finally, we change self.df['Volume'] to self.df['Volume_(BTC)'] to match our data set, and we’re good to go.
The white tag on the top right is the agent’s current net worth and the bottom right tag is the current price of Bitcoin.
Finally, we can update our reward function slightly, as per Sean O’Gorman’s advice, so that we reward increases in net worth, not just achieving a high net worth and staying there.",en,['Adam King'],2019-10-17 04:42:23.718000+00:00,"{'Deep Learning', 'Reinforcement Learning', 'Machine Learning', 'Bitcoin', 'Trading'}","{'https://miro.medium.com/freeze/max/60/1*DWO8W-DyghiZbgBstJZjWQ.gif?q=20', 'https://miro.medium.com/max/1200/1*r7XItmcyWv76mso08vncpw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2376/1*wz5XAg-8PYRDmzBMKdakHw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*C3Z4y4EUeN8mLpmdbLPUZA.png?q=20', 'https://miro.medium.com/max/5164/1*SFNha2nSRaeE100dTCIXLQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/13440/1*r7XItmcyWv76mso08vncpw.jpeg', 'https://miro.medium.com/max/60/1*hor57pXvQR42QmW-mIS5ew.jpeg?q=20', 'https://miro.medium.com/max/60/1*wz5XAg-8PYRDmzBMKdakHw.png?q=20', 'https://miro.medium.com/max/5168/1*C3Z4y4EUeN8mLpmdbLPUZA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1640/1*DWO8W-DyghiZbgBstJZjWQ.gif', 'https://miro.medium.com/max/60/1*SFNha2nSRaeE100dTCIXLQ.png?q=20', 'https://miro.medium.com/freeze/max/60/1*f8gvwrKvpij6m-KCL6wxGA.gif?q=20', 'https://miro.medium.com/max/800/0*IeiYxZVLPlPmbG38.png', 'https://miro.medium.com/max/60/0*IeiYxZVLPlPmbG38.png?q=20', 'https://miro.medium.com/max/1280/1*f8gvwrKvpij6m-KCL6wxGA.gif', 'https://miro.medium.com/max/60/1*r7XItmcyWv76mso08vncpw.jpeg?q=20', 'https://miro.medium.com/max/60/1*UCtL7UMAHKnx4ePoP-0p2w.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/2*GnXdYdgLDvWhInOVFHxikA.jpeg', 'https://miro.medium.com/max/11624/1*hor57pXvQR42QmW-mIS5ew.jpeg', 'https://miro.medium.com/max/2396/1*UCtL7UMAHKnx4ePoP-0p2w.png', 'https://miro.medium.com/fit/c/160/160/2*GnXdYdgLDvWhInOVFHxikA.jpeg'}",2020-03-05 00:07:49.044198,2.2858331203460693
https://towardsdatascience.com/guide-to-r-and-python-in-a-single-jupyter-notebook-ff12532eb3ba,Guide to R and Python in a Single Jupyter Notebook,"Linear/Polynomial Regression, but make it R

After this section, we’ll know everything we need to in order to work with R models. The rest of the lab is just applying these concepts to run particular models. This section, therefore, is your ‘cheat sheet’ for working in R.

What we need to know:

Importing (base) R functions

Importing R Library functions

Populating vectors R understands

Populating DataFrames R understands

Populating Formulas R understands

Running models in R

Getting results back to Python

Getting model predictions in R

Plotting in R

Reading R’s documentation

Importing R functions

To import R functions we need the rpy2 package. Depending on your environment, you may also need to specify the path to the R home directory. I have given an example below for how to specify this.

# if you're on JupyterHub you may need to specify the path to R



#import os

#os.environ['R_HOME'] = ""/usr/share/anaconda3/lib/R""



import rpy2.robjects as robjects

To specify an R function, simply use robjects.r followed by the name of the package in square brackets as a string. To prevent confusion, I like to use r_ for functions, libraries, and other objects imported from R.

r_lm = robjects.r[""lm""]

r_predict = robjects.r[""predict""]

#r_plot = robjects.r[""plot""] # more on plotting later



#lm() and predict() are two of the most common functions we'll use

Importing R libraries

We can import individual functions, but we can also import entire libraries too. To import an entire library, you can extract the importr package from rpy2.robjects.packages .

from rpy2.robjects.packages import importr

#r_cluster = importr('cluster')

#r_cluster.pam;

Populating vectors R understands

To specify a float vector that can interface with Python packages, we can use the robjects.FloatVector function. The argument to this function references the data array that you wish to convert to an R object, in our case, the age and y variables from our diabetes dataset.

r_y = robjects.FloatVector(diab['y'])

r_age = robjects.FloatVector(diab['age'])

# What happens if we pass the wrong type?

# How does r_age display?

# How does r_age print?

Populating Dataframes R understands

We can specify individual vectors, and we can also specify entire dataframes. This is done by using the robjects.DataFrame function. The argument to this function is a dictionary specifying the name and the vector (obtained from robjects.FloatVector ) associated with the name.

diab_r = robjects.DataFrame({""y"":r_y, ""age"":r_age})

# How does diab_r display?

# How does diab_r print?

Populating formulas R understands

To specify a formula, for example, for regression, we can use the robjects.Formula function. This follows the R syntax dependent variable ~ independent variables . In our case, the output y is modeled as a function of the age variable.

simple_formula = robjects.Formula(""y~age"")

simple_formula.environment[""y""] = r_y #populate the formula's .environment, so it knows what 'y' and 'age' refer to

simple_formula.environment[""age""] = r_age

Notice in the above formula we had to specify the FloatVector’s associated with each of the variables in our formula. We have to do this as the formula does not automatically relate our variable names to variables that we have previously specified — they have not yet been associated with the robjects.Formula object.

Running Models in R

To specify a model, in this case a linear regression model using our previously imported r_lm function, we need to pass our formula variable as an argument (this will not work unless we pass an R formula object).

diab_lm = r_lm(formula=simple_formula) # the formula object is storing all the needed variables

Instead of specifying each of the individual float vectors related to the robjects.Formula object, we can reference the dataset in the formula itself (as long as this has been made into an R object itself).

simple_formula = robjects.Formula(""y~age"") # reset the formula

diab_lm = r_lm(formula=simple_formula, data=diab_r) #can also use a 'dumb' formula and pass a dataframe

Getting results back to Python

Using R functions and libraries is great, but we can also analyze our results and get them back to Python for further processing. To look at the output:

diab_lm #the result is already 'in' python, but it's a special object

We can also check the names in our output:

print(diab_lm.names) # view all names

To take the first element of our output:

diab_lm[0] #grab the first element

To take the coefficients:

diab_lm.rx2(""coefficients"") #use rx2 to get elements by name!

To put the coefficients in a Numpy array:

np.array(diab_lm.rx2(""coefficients"")) #r vectors can be converted to numpy (but rarely needed)

Getting Predictions

To get predictions using our R model, we can create a prediction dataframe and use the r_predict function, similar to how it is done using Python.

# make a df to predict on (might just be the validation or test dataframe)

predict_df = robjects.DataFrame({""age"": robjects.FloatVector(np.linspace(0,16,100))}) # call R's predict() function, passing the model and the data

predictions = r_predict(diab_lm, predict_df)

We can use the rx2 function to extract the ‘age’ values:

x_vals = predict_df.rx2(""age"")

We can also plot our data using Python:

ax = diab.plot.scatter(x='age',y='y',c='Red',title=""Diabetes data"")

ax.set_xlabel(""Age at Diagnosis"")

ax.set_ylabel(""Log C-Peptide Concentration""); ax.plot(x_vals,predictions); #plt still works with r vectors as input!

We can also plot using R, although this is slightly more involved.

Plotting in R

To plot in R, we need to turn on the %R magic function using the following command:

%load_ext rpy2.ipython

The above turns on the %R “magic”.

R’s plot() command responds differently based on what you hand to it; different models get different plots!

For any specific model search for plot.modelname. For example, for a GAM model, search plot.gam for any details of plotting a GAM model.

for any details of plotting a GAM model. The %R “magic” runs R code in ‘notebook’ mode, so figures display nicely

“magic” runs R code in ‘notebook’ mode, so figures display nicely Ahead of the plot(<model>) code we pass in the variables R needs to know about ( -i is for ""input"")

%R -i diab_lm plot(diab_lm);

Reading R’s documentation

The documentation for the lm() function is here, and a prettier version (same content) is here. When Googling, prefer rdocumentation.org when possible. Sections:

Usage : gives the function signature, including all optional arguments

: gives the function signature, including all optional arguments Arguments : What each function input controls

: What each function input controls Details : additional info on what the function does and how arguments interact. Often the right place to start reading

: additional info on what the function does and how arguments interact. Value : the structure of the object returned by the function

: the structure of the object returned by the function References : The relevant academic papers

: The relevant academic papers See Also: other functions of interest

Example

As an example to test our newly acquired knowledge, we will try the following:

Add confidence intervals calculated in R to the linear regression plot above. Use the interval= argument to r_predict() (documentation here). You will have to work with a matrix returned by R.

argument to (documentation here). You will have to work with a matrix returned by R. Fit a 5th-degree polynomial to the diabetes data in R. Search the web for an easier method than writing out a formula with all 5 polynomial terms.

Confidence intervals:

CI_matrix = np.array(r_predict(diab_lm, predict_df, interval=""confidence""))



ax = diab.plot.scatter(x='age',y='y',c='Red',title=""Diabetes data"")

ax.set_xlabel(""Age at Diagnosis"")

ax.set_ylabel(""Log C-Peptide Concentration"");



ax.plot(x_vals,CI_matrix[:,0], label=""prediction"")

ax.plot(x_vals,CI_matrix[:,1], label=""95% CI"", c='g')

ax.plot(x_vals,CI_matrix[:,2], label=""95% CI"", c='g')

plt.legend();

5-th degree polynomial:","['r', 'object', 'function', 'single', 'vectors', 'python', 'need', 'does', 'formula', 'model', 'specify', 'jupyter', 'notebook', 'using', 'guide']","This section, therefore, is your ‘cheat sheet’ for working in R.What we need to know:Importing (base) R functionsImporting R Library functionsPopulating vectors R understandsPopulating DataFrames R understandsPopulating Formulas R understandsRunning models in RGetting results back to PythonGetting model predictions in RPlotting in RReading R’s documentationImporting R functionsTo import R functions we need the rpy2 package.
from rpy2.robjects.packages import importr#r_cluster = importr('cluster')#r_cluster.pam;Populating vectors R understandsTo specify a float vector that can interface with Python packages, we can use the robjects.FloatVector function.
We can also plot using R, although this is slightly more involved.
Plotting in RTo plot in R, we need to turn on the %R magic function using the following command:%load_ext rpy2.ipythonThe above turns on the %R “magic”.
Often the right place to start reading: additional info on what the function does and how arguments interact.",en,"['Matthew Stewart', 'Phd Researcher']",2020-01-30 13:46:58.394000+00:00,"{'Data Science', 'Artificial Intelligence', 'Python', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*sWDHGs0AaOfLrWP8paaqlQ.png?q=20', 'https://miro.medium.com/max/60/1*Q3VD4d4IoNME-3oSSanYAA.png?q=20', 'https://miro.medium.com/max/60/1*ALLDYoZSOrsF8z_8AWLRpg.png?q=20', 'https://miro.medium.com/max/1636/1*hXTVL9XTVd7xNNG2TSUpdg.png', 'https://miro.medium.com/max/60/1*Fxd-AKxDsK5HnKhLuApxvw.png?q=20', 'https://miro.medium.com/max/687/0*xmql88Y4JK9AK992.png', 'https://miro.medium.com/max/1536/1*PQRU00M8ye806dy-a-ThTA.png', 'https://miro.medium.com/max/1636/1*DZR4Xk8lZWAjKTQjeXvZfw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1636/1*pX4qZBhwgur0c2Q2sIPFCA.png', 'https://miro.medium.com/max/1536/1*Fxd-AKxDsK5HnKhLuApxvw.png', 'https://miro.medium.com/max/1536/1*sWDHGs0AaOfLrWP8paaqlQ.png', 'https://miro.medium.com/max/1536/1*_i3HhKMDXLFNJthpBlXUfQ.png', 'https://miro.medium.com/max/40/1*Svg-Xik2XhdDNJDP7zWSoQ.png?q=20', 'https://miro.medium.com/max/60/0*xmql88Y4JK9AK992.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1572/1*XTpWiqebnkRdJFxlsOMadQ.png', 'https://miro.medium.com/fit/c/160/160/2*AiKFMI6FEyDWnNMtiPAr1A.jpeg', 'https://miro.medium.com/max/1374/0*xmql88Y4JK9AK992.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*_i3HhKMDXLFNJthpBlXUfQ.png?q=20', 'https://miro.medium.com/max/60/1*pX4qZBhwgur0c2Q2sIPFCA.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*AiKFMI6FEyDWnNMtiPAr1A.jpeg', 'https://miro.medium.com/max/2148/1*uxZA_FuuUgwEtbQ2dyG0qQ.png', 'https://miro.medium.com/max/60/1*_0Oo2HhLzg5z5Ds-dVXBxg.png?q=20', 'https://miro.medium.com/max/60/1*uxZA_FuuUgwEtbQ2dyG0qQ.png?q=20', 'https://miro.medium.com/max/60/1*hXTVL9XTVd7xNNG2TSUpdg.png?q=20', 'https://miro.medium.com/max/1536/1*Svg-Xik2XhdDNJDP7zWSoQ.png', 'https://miro.medium.com/max/40/1*PQRU00M8ye806dy-a-ThTA.png?q=20', 'https://miro.medium.com/max/60/1*YLzAKjXkq87tgmcLvCwYFA.png?q=20', 'https://miro.medium.com/max/1636/1*Q3VD4d4IoNME-3oSSanYAA.png', 'https://miro.medium.com/max/60/1*XTpWiqebnkRdJFxlsOMadQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*DZR4Xk8lZWAjKTQjeXvZfw.png?q=20', 'https://miro.medium.com/max/60/1*X8hJdOLdzugEjBxHwyiymQ.png?q=20', 'https://miro.medium.com/max/1576/1*_0Oo2HhLzg5z5Ds-dVXBxg.png', 'https://miro.medium.com/max/1536/1*X8hJdOLdzugEjBxHwyiymQ.png', 'https://miro.medium.com/max/1752/1*ALLDYoZSOrsF8z_8AWLRpg.png', 'https://miro.medium.com/max/1636/1*YLzAKjXkq87tgmcLvCwYFA.png'}",2020-03-05 00:08:00.653075,11.607888221740723
https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03,BERT for dummies — Step by Step Tutorial,"Motivation

Chatbots, virtual assistant, and dialog agents will typically classify queries into specific intents in order to generate the most coherent response. Intent classification is a classification problem that predicts the intent label for any given user query. It is usually a multi-class classification problem, where the query is assigned one unique label. For example, the query “how much does the limousine service cost within pittsburgh” is labeled as “groundfare” while the query “what kind of ground transportation is available in denver” is labeled as “ground_service”. The query “i want to fly from boston at 838 am and arrive in Denver at 1110 in the morning” is a “flight” intent, while “ show me the costs and times for flights from san francisco to atlanta” is an “airfare+flight_time” intent.

The examples above show how ambiguous intent labeling can be. Users might add misleading words, causing multiple intents to be present in the same query. Attention-based learning methods were proposed for intent classification (Liu and Lane, 2016; Goo et al., 2018). One type of network built with attention is called a Transformer. It applies attention mechanisms to gather information about the relevant context of a given word, and then encode that context in a rich vector that smartly represents the word.

In this article, we will demonstrate Transformer, especially how its attention mechanism helps in solving the intent classification task by learning contextual relationships. After demonstrating the limitation of a LSTM-based classifier, we introduce BERT: Pre-training of Deep Bidirectional Transformers, a novel Transformer-approach, pre-trained on large corpora and open-sourced. The last part of this article presents the Python code necessary for fine-tuning BERT for the task of Intent Classification and achieving state-of-art accuracy on unseen intent queries. We use the ATIS (Airline Travel Information System) dataset, a standard benchmark dataset widely used for recognizing the intent behind a customer query.

Intent classification with LSTM

Data

In one of our previous article, you will find the Python code for loading the ATIS dataset. In the ATIS training dataset, we have 26 distinct intents, whose distribution is shown below. The dataset is highly unbalanced, with most queries labeled as “flight” (code 14).

Multi-class classifier

Before looking at Transformer, we implement a simple LSTM recurrent network for solving the classification task. After the usual preprocessing, tokenization and vectorization, the 4978 samples are fed into a Keras Embedding layer, which projects each word as a Word2vec embedding of dimension 256. The results are passed through a LSTM layer with 1024 cells. This produces 1024 outputs which are given to a Dense layer with 26 nodes and softmax activation. The probabilities created at the end of this pipeline are compared to the original labels using categorical crossentropy.

As we can see in the training output above, the Adam optimizer gets stuck, the loss and accuracy do not improve. The model appears to predict the majority class “flight” at each step.

When we use the trained model to predict the intents on the unseen test dataset, the confusion matrix clearly shows how the model overfits to the majority “flight” class.

Data augmentation

Dealing with an imbalanced dataset is a common challenge when solving a classification task. Data augmentation is one thing that comes to mind as a good workaround. Here, it is not rare to encounter the SMOTE algorithm, as a popular choice for augmenting the dataset without biasing predictions. SMOTE uses a k-Nearest Neighbors classifier to create synthetic datapoints as a multi-dimensional interpolation of closely related groups of true data points. Unfortunately, we have 25 minority classes in the ATIS training dataset, leaving us with a single overly representative class. SMOTE fails to work as it cannot find enough neighbors (minimum is 2). Oversampling with replacement is an alternative to SMOTE, which also does not improve the model’s predictive performance either.

The SNIPS dataset, which is collected from the Snips personal voice assistant, a more recent dataset for natural language understanding, is a dataset which could be used to augment the ATIS dataset in a future effort.

Binary classifier

Since we were not quite successful at augmenting the dataset, now, we will rather reduce the scope of the problem. We define a binary classification task where the “flight” queries are evaluated against the remaining classes, by collapsing them into a single class called “other”. The distribution of labels in this new dataset is given below.

We can now use a similar network architecture as previously. The only change is to reduce the number of nodes in the Dense layer to 1, activation function to sigmoid and the loss function to binary crossentropy. Surprisingly, the LSTM model is still not able to learn to predict the intent, given the user query, as we see below.

After 10 epochs, we evaluate the model on an unseen test dataset. This time, we have all samples being predicted as “other”, although “flight” had more than twice as many samples as “other” in the training set.

Intent Classification with BERT

The motivation why we are now looking at Transformer is the poor classification result we witnessed with sequence-to-sequence models on the Intent Classification task when the dataset is imbalanced. In this section, we introduce a variant of Transformer and implement it for solving our classification problem. We will look especially at the late 2018 published Bidirectional Encoder Representations from Transformers (BERT).

What is BERT?

BERT is basically a trained Transformer Encoder stack, with twelve in the Base version, and twenty-four in the Large version, compared to 6 encoder layers in the original Transformer we described in the previous article.

BERT encoders have larger feedforward networks (768 and 1024 nodes in Base and Large respectively) and more attention heads (12 and 16 respectively). BERT was trained on Wikipedia and Book Corpus, a dataset containing +10,000 books of different genres. Below you can see a diagram of additional variants of BERT pre-trained on specialized corpora.

BERT was released to the public, as a new era in NLP. Its open-sourced model code broke several records for difficult language-based tasks. The pre-trained model on massive datasets enables anyone building natural language processing to use this free powerhouse. BERT theoretically allows us to smash multiple benchmarks with minimal task-specific fine-tuning.

BERT works similarly to the Transformer encoder stack, by taking a sequence of words as input which keep flowing up the stack from one encoder to the next, while new sequences are coming in. The final output for each sequence is a vector of 728 numbers in Base or 1024 in Large version. We will use such vectors for our intent classification problem.

Why do we need BERT?

Proper language representation is key for general-purpose language understanding by machines. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same representation in “bank deposit” and in “riverbank”. Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT, as a contextual model, captures these relationships in a bidirectional way. BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.

We will use BERT to extract high-quality language features from the ATIS query text data, and fine-tune BERT on a specific task (classification) with own data to produce state of the art predictions.

Preparing BERT environment

Feel free to download the original Jupyter Notebook, which we will adapt for our goal in this section.

As for development environment, we recommend Google Colab with its offer of free GPUs and TPUs, which can be added by going to the menu and selecting: Edit -> Notebook Settings -> Add accelerator (GPU). An alternative to Colab is to use a JupyterLab Notebook Instance on Google Cloud Platform, by selecting the menu AI Platform -> Notebooks -> New Instance -> Pytorch 1.1 -> With 1 NVIDIA Tesla K80 after requesting Google to increase your GPU quota. This will cost ca. $0.40 per hour (current pricing, which might change). Below you find the code for verifying your GPU availability.

We will use the PyTorch interface for BERT by Hugging Face, which at the moment, is the most widely accepted and most powerful PyTorch interface for getting on rails with BERT. Hugging Face provides pytorch-transformers repository with additional libraries for interfacing more pre-trained models for natural language processing: GPT, GPT-2, Transformer-XL, XLNet, XLM.

As you can see below, in order for torch to use the GPU, you have to identify and specify the GPU as the device, because later in the training loop, we load data onto that device.

Now we can upload our dataset to the notebook instance. BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). Furthermore, we need to tokenize our text into tokens that correspond to BERT’s vocabulary.

'[CLS] i want to fly from boston at 838 am and arrive in denver at 1110 in the morning [SEP]' ['[CLS]', 'i', 'want', 'to', 'fly', 'from', 'boston', 'at', '83', '##8', 'am', 'and', 'arrive', 'in', 'denver', 'at', '111', '##0', 'in', 'the', 'morning', '[SEP]']

For each tokenized sentence, BERT requires input ids, a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary.

BERT’s clever language modeling task masks 15% of words in the input and asks the model to predict the missing word. To make BERT better at handling relationships between multiple sentences, the pre-training process also included an additional task: given two sentences (A and B), is B likely to be the sentence that follows A? Therefore we need to tell BERT what task we are solving by using the concept of attention mask and segment mask. In our case, all words in a query will be predicted and we do not have multiple sentences per query. We define the mask below.

Now it is time to create all tensors and iterators needed during fine-tuning of BERT using our data.

Finally, it is time to fine-tune the BERT model so that it outputs the intent class given a user query string. For this purpose, we use the BertForSequenceClassification, which is the normal BERT model with an added single linear layer on top for classification. Below we display a summary of the model. The encoder summary is shown only once. The same summary would normally be repeated 12 times. We display only 1 of them for simplicity sake. We can see the BertEmbedding layer at the beginning, followed by a Transformer architecture for each encoder layer: BertAttention, BertIntermediate, BertOutput. At the end, we have the Classifier layer.

As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. Training the classifier is relatively inexpensive. The bottom layers have already great English words representation, and we only really need to train the top layer, with a bit of tweaking going on in the lower levels to accommodate our task. This is a variant of transfer learning.

BERT fine-tuned on the Intent Classification task for Natural Language Understanding

The training loss plot from the variable train_loss_set looks awesome. The whole training loop took less than 10 minutes.

Now, it is the moment of truth. Is BERT overfitting? Or is it doing better than our previous LSTM network? We now load the test dataset and prepare inputs just as we did with the training set. We then create tensors and run the model on the dataset in evaluation mode.

With BERT we are able to get a good score (95.93%) on the intent classification task. This demonstrates that with a pre-trained BERT model it is possible to quickly and effectively create a high-quality model with minimal effort and training time using the PyTorch interface.

Conclusion

In this article, we demonstrated how to load the pre-trained BERT model in a PyTorch notebook and fine-tune it on your own dataset for solving a specific task. Attention matters when dealing with natural language understanding tasks. When combined with powerful words embedding from Transformer, an intent classifier can significantly improve its performance, as we successfully exposed.

This area opens a wide door for future work, especially because natural language understanding is at the core of several technologies including conversational AI (chatbots, personal assistants) and upcoming augmented analytics which was ranked by Gartner as a top disruptive challenge that organizations will face very soon. Understanding natural language has an impact on traditional analytical and business intelligence since executives are rapidly adopting smart information retrieval by text queries and data narratives instead of dashboards with complex charts.

Thank you for reading.","['task', 'bert', 'transformer', 'dataset', 'dummies', 'tutorial', 'model', 'step', 'query', 'classification', 'training', 'language', 'intent']","Attention-based learning methods were proposed for intent classification (Liu and Lane, 2016; Goo et al., 2018).
In the ATIS training dataset, we have 26 distinct intents, whose distribution is shown below.
Unfortunately, we have 25 minority classes in the ATIS training dataset, leaving us with a single overly representative class.
BERT fine-tuned on the Intent Classification task for Natural Language UnderstandingThe training loss plot from the variable train_loss_set looks awesome.
With BERT we are able to get a good score (95.93%) on the intent classification task.",en,['Michel Kana'],2019-10-26 06:17:22.301000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Bert'}","{'https://miro.medium.com/max/1516/1*I-xkHXKejHFcr4tzQsnhog.png', 'https://miro.medium.com/max/60/1*V4dDPeyCHnNH8YsfBsqt3g.png?q=20', 'https://miro.medium.com/max/60/1*kOmdaoN2TKR3rlUcJA1MFg.png?q=20', 'https://miro.medium.com/max/1312/1*0DzFbN7t65ldXVIdmTYtCA.png', 'https://miro.medium.com/max/60/1*H8Dn6qxeTM6XeHm-Zqe4Dw.png?q=20', 'https://miro.medium.com/max/60/1*I-xkHXKejHFcr4tzQsnhog.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*xLLr7-d5D1yToCNP.jpg?q=20', 'https://miro.medium.com/max/60/1*ovBCVu-rVs_w3GwLP2csMQ.png?q=20', 'https://miro.medium.com/max/1530/1*ovBCVu-rVs_w3GwLP2csMQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*iVsZZRSqynrKb5sQ39OWbQ.png?q=20', 'https://miro.medium.com/max/2800/0*xLLr7-d5D1yToCNP.jpg', 'https://miro.medium.com/max/1202/1*D6EzZrbT5vsF8z3ylBt1qQ.png', 'https://miro.medium.com/max/60/1*0DzFbN7t65ldXVIdmTYtCA.png?q=20', 'https://miro.medium.com/max/1740/1*J0iYjvaYBUvJcpjc8X6HKw.png', 'https://miro.medium.com/max/2800/0*XSr4xRO5nhVpjuos.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*C0SEiFsbr0Ee_8ht.png?q=20', 'https://miro.medium.com/max/60/0*XSr4xRO5nhVpjuos.png?q=20', 'https://miro.medium.com/max/1132/1*kOmdaoN2TKR3rlUcJA1MFg.png', 'https://miro.medium.com/max/60/1*D6EzZrbT5vsF8z3ylBt1qQ.png?q=20', 'https://miro.medium.com/max/870/1*iVsZZRSqynrKb5sQ39OWbQ.png', 'https://miro.medium.com/fit/c/96/96/1*aVyIg7GQbGFxFphcjUqVAw.jpeg', 'https://miro.medium.com/max/802/1*V4dDPeyCHnNH8YsfBsqt3g.png', 'https://miro.medium.com/fit/c/160/160/1*aVyIg7GQbGFxFphcjUqVAw.jpeg', 'https://miro.medium.com/max/2800/0*C0SEiFsbr0Ee_8ht.png', 'https://miro.medium.com/max/60/1*J0iYjvaYBUvJcpjc8X6HKw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1782/1*H8Dn6qxeTM6XeHm-Zqe4Dw.png', 'https://miro.medium.com/max/1200/0*xLLr7-d5D1yToCNP.jpg'}",2020-03-05 00:08:02.946323,2.292281150817871
https://towardsdatascience.com/tricks-for-postgres-and-docker-that-will-make-your-life-easier-fc7bfcba5082,Tricks for Postgres and Docker that will make your life easier,"Nowadays, everybody is trying to run everything in container and I don’t blame them, I do the same thing, because running applications, databases or other tools in Docker container is super nice and we all know why (isolation, easy setup, security…). However, sometimes debugging, accessing, or in general interacting with containers can be pretty annoying. This includes accessing, modifying or querying databases. So, as I used PostgreSQL extensively and have been running it inside containers for a while now, I — over time — made a list of few commands that can help immensely when doing simple and also not so simple operation with a database server.

Log Into PSQL

The most basic thing you will always need to do if you want to interact with your database server is to connect to the database itself (using PSQL):

So for Docker container called db , default user postgres and database name blog it would be

Running Command Against Database

It’s nice that you can log in and then execute whatever commands you need, but it’s often more convenient to do it in one go, especially if you want to run just a single command or query:

So if we wanted to list all tables in database using same parameters as in previous example:

Here, \l lists all tables in current database, if you are not familiar with psql ""backslash"" commands, then I highly recommend this cheatsheet.

Apart from psql commands you can run any SQL query like so:

Backing up Your Data

From time to time I need to backup data or whole schema of the database, sometimes just as an “insurance policy” and sometimes so I can make changes recklessly and restore everything afterwards, so here how to do it:

In this example, we are making use of pg_dump utility, which allows us to extract PostgreSQL databases. I'm using --column-inserts and --data-only flags to get only table rows, but quite often all that is needed is schema, for that you can use -s flag.

Execute whole SQL files

Sometimes you need to populate existing database with enough data for testing (please don’t do this with production databases) or it’s just easier to use data from file then to copy and paste them into command above.

Here we first need to copy the file itself into the running container and then execute it using the -f flag.

Prepopulating Database on the Start

Previous example was good enough if you need to execute it from time to time, but it can become annoying if you have to do it every time you start the database. So, in case you decide that it’s better to just populate the database on the start, then here is solution for that. It just requires little more work:

We will need following files:

Dockerfile - Dockerfile for your Postgres image

- Dockerfile for your Postgres image create_db.sh - Script that creates database, schema and populates it.

- Script that creates database, schema and populates it. schema.sql - SQL file containing your database schema

- SQL file containing your database schema data.sql - SQL file containing data used to populate your database

- SQL file containing data used to populate your database .env - File with environment variables, to make your life easier

First, the Dockerfile :

This is very simple Dockerfile, all we need to do here is to copy our script and schema/data into the image so they can be on run start-up. You may be asking, There is no ENTRYPOINT or COMMAND , how do we run it on start-up? - the answer is, that base postgres image runs on start any scripts present in docker-entrypoint-initdb.d directory, so all we need to do is copy our script to this directory and PostgreSQL takes care of it.

But what is in the script ( create_db.sh )?

Start-up script first logs into psql with specified username ( POSTGRES_USER ), then it creates your database ( DB_NAME ). Next it create database schema using file we copied into the image and finally it populates the database with data. All the variables here are coming from the .env file mentioned before, which makes it very easy to change your database name or username at any time without modifying script itself.

For more complete example please see my repository here

What About docker-compose ?

In my experience, most of the time I end up running database in conjunction with the application that is using it and the simplest way to do it is docker-compose. Usually I prefer to refer to the docker-compose service by service name, rather then container name which might or might not be the same thing. In case it isn’t same, you can just following command:

Only real difference here from the previous examples is the docker-compose part, which looks up information of the specified service. The -q flag make it so, that only container IDs are displayed, which is all we need.

Conclusion

I hope at least some of these little hacks will make your life easier when dealing with Docker and PostgreSQL or maybe if you were avoiding Docker just because it might be little annoying when working with databases, then I hope you will give it a shot after reading this article. 🙂

Note: This was originally posted at martinheinz.dev","['file', 'life', 'script', 'postgres', 'docker', 'need', 'sql', 'run', 'container', 'data', 'database', 'tricks', 'easier', 'schema', 'using']","Here we first need to copy the file itself into the running container and then execute it using the -f flag.
It just requires little more work:We will need following files:Dockerfile - Dockerfile for your Postgres image- Dockerfile for your Postgres image create_db.sh - Script that creates database, schema and populates it.
- Script that creates database, schema and populates it.
Next it create database schema using file we copied into the image and finally it populates the database with data.
The -q flag make it so, that only container IDs are displayed, which is all we need.",en,['Martin Heinz'],2019-09-30 17:06:27.452000+00:00,"{'Postgres', 'Database', 'Programming', 'Docker', 'DevOps'}","{'https://miro.medium.com/fit/c/160/160/2*p3sIQ1Ga2bfAbZYzoCcACw.jpeg', 'https://miro.medium.com/max/2560/1*Wg7pRIUc7uRO23ZcoY6U_w.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*Wg7pRIUc7uRO23ZcoY6U_w.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/2*p3sIQ1Ga2bfAbZYzoCcACw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*Wg7pRIUc7uRO23ZcoY6U_w.jpeg?q=20'}",2020-03-05 00:08:14.462434,11.515102624893188
https://medium.com/sfu-big-data/predicting-stable-portfolios-using-machine-learning-f2e27d6dbbec,Predicting Stable Portfolios Using Machine Learning,"Muhammad Rafay Aleem, Nandita Dwivedi, Kiran

1. Motivation and Background

Portfolio Management is the process of maximizing the return on a portfolio. Portfolio managers make trading decisions on behalf of their clients depending on their appetite for risk. They analyze different assets, their strengths and weaknesses before making a decision about which equities they should hold in a portfolio for balancing the risk and drawing maximum returns. This makes portfolio management a difficult process. We aim to make this process better and simpler by using predictive modeling and deep learning techniques. We generate stable portfolios on predicted stock prices for next quarter.

Related Work

In past a lot of attempts have been made to predict the stock prices for firms, and also to understand how text of news articles, twitter posts, and other platforms influence stock prices. These attempts have involved analysing the impact of sentiments of the mentioned resources and predicting value stock prices. What still remains unexplored is the effect of sentiments of financial reports on these stock markets. Harvard Business School published a working paper, ‘Lazy Prices’ claiming that these financial reports have influence in a firm’s stock price. They stated that a drastic change in similarity (Jaccard and Cosine scores) of these filings can increase or decrease a firm’s market value [1].

2. Problem Statement

The project required us to first explore finance domain and gather relevant information about stock price movement, trading methods and portfolio management techniques. We understood how impactful data science has been in finance so far and came up with three main problems we wanted to focus -

To what extent are the quarterly and annual financial reports filed by firms influencing their stock prices? How does including the sentiments of historical financial reports change prediction of stock prices? How can we built a stable portfolio using the predicted values for a quarter?

These problems are challenging because we had to do a lot of background research on domain. Using right features and doing proper feature engineering to come up with best predictions had a significant learning curve. In addition to that, the data source and format for these filings is not consistent and therefore scraping and parsing the data was a big challenge. There are two different data sources involved here — one is data from financial reports and other is stock price values. In order to prepare dataset for analysis and to train our prediction model we had to look into various methods to integrate these two sources. These datasets are not a direct match between each other as OHLC data is released daily while SEC filings are released every quarter. To merge them together, we had to carefully match the date ranges and thoughtfully engineer features to map predictions for the next quarter.

3. Data Science Pipeline

3.1 Data Collection

SEC 10-Q and 10-K filings from EDGAR

The very first step in our data pipeline was to scrape SEC Edgar (Electronic Data Gathering, Analysis, and Retrieval) database. This is an online database maintained by the U.S. Securities and Exchange Commission (the “SEC”) to track all SEC filings by public companies and now contains over 12 million such filings.

Since EDGAR doesn’t support any filtering options other than company tickers and central index keys (CIK) at the moment of writing this, we had to scrape all 10-K and 10-Q filings for the S&P 500 [2] companies, regardless of the time periods that we were interested in.

We collected around 60 GB of this data for S&P 500 tickers from the database. Since EDGAR limits the number of requests per user to 10 per second, we had to add some extra primitives to accommodate for this limitation. Furthermore, given the huge size of dataset, we maintained checkpoints for each downloaded ticker in order to allow resuming at later time in case of failures and avoid any data loss.

OHLC data from Quandl API

OHLC (open-high-low-close) is a type of dataset that provides open, high, low, and close prices for a stock for each and every business day. This dataset was one of the most crucial information used to train our models in later parts of our pipeline. We leveraged Stocker [3], a Python abstraction on Quandl API, to retrieve this OHLC data for each S&P 500 company.

3.2 Data Preparation

SEC 10-Q and 10-K filings from EDGAR

We found that the scraped 10-K and 10-Q SEC filings were highly unstructured as it contains HTML tags, symbols and numerical tables. We cleaned these files by removing unrelated data and then used them to parse HTML into clean text data which was then used to analyse the sentiments. For cleaning and parsing, we used regex, NumPy and BeautifulSoup.

As part of data preparation, we also generated CikList list mapping files for 10-K and 10-Q filings for each CIK which contained mappings for dates, SEC types, filenames, CIKs and tickers. Date field was the filing date of the SEC form; SEC type was 10-Q or 10-K, and the filename was the SEC file.

Each CikList mapping file had approximately 12 rows for 10-K and 34 rows for 10-Q SEC files for a 10 year window. We used these mapping files to keep track of SEC files for every CIK. These saved mappings were later used to calculate sentiments for all SEC files corresponding to each CIK.

OHLC data from Quandl API

Python Stocker module makes it relatively easy to get OHLC data for each ticker as a Pandas dataframe. However, since our method makes prediction for the next quarter using a 90 day prior window (reasons described in Methodology section), we had to do some data wrangling before we could feed it to our deep learning models.

We used a 90 day window (a quarter is 90 days) to construct each row as X containing all OHLC data for that period and put Adj. Close as Y. The method we used to prepare this data is visually explained in Figure 1.

Figure 1: 90 day windows created on OHLC data

3.3 Data Integration

Combining Sentiment Scores with OHLC data

Integrating OHLC data obtained from Quandl API with sentiments from SEC filings based on dates was complicated because of the difference in SEC filing dates and OHLC quarterly dates for the stocks.

For OHLC data, quarterly stock returns were in months 3rd, 6th, 9th and 12th whereas, for SECs, quarterly data was for months 2nd, 5th, 7th and 10th.

Thus, to resolve this issue, we used Panda’s “forward” merge method which mapped/joined the sentiment scores for the SEC filing for one quarter with the stock’s price data for the next nearest date. The purpose behind this mapping was to analyse how were the closing prices of tickers affected following the release of a financial report. We have shown first 5 rows of the joined dataframe in Figure 2.

Figure 2: OHLC data joined with SEC sentiment scores for GWW.

Standardizing OHLC data for S&P 500 stocks

We only took 10 years of stock data for the S&P 500 companies. The reason for limiting this data to a recent time window is that it is widely believed that financial data is ‘non-stationary’ and prone to regime changes which might render old data less relevant for prediction [4, 5]. Therefore, we defined this period to be in the range of 1-Jan-2008 to 31-Dec-2018.

After retrieving this data and limiting it within a 10 year period, we found out that a few companies went public after 1st Jan 2008 and they had to be filtered out to maintain consistency. This filtering reduced the dataset to around 300 tickers. Moreover, there were some cases where few rows of OHLC data were missing for certain tickers. Further investigation didn’t reveal any particular event in those companies’ history so it could have been some bug in Quandl API. To fix this inconsistency, we used Pandas ‘interpolate’ method to construct those missing rows.

We also used fastai library to convert dates into features such as ‘day of week’, ‘month of year’ and ‘week’ in order to allow the model to learn if there were any temporal relations in stock prices.

4. Methodology

Sentiment Analysis Using NLTK VADER

We used NLTK VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analyser which is a lexicon and rule-based sentiment analysis tool. In this approach, each of the words in the lexicon is rated as to whether it is positive, negative or neutral and scores are calculated based on how positive, negative or neutral a sentiment is.

Vader lexicons do not have the financial dictionaries, so we updated it with financial dictionaries from Loughran-McDonald Financial Sentiment Word Lists [6, 7].

Since the MD&A section of SEC filings focuses on the historical description of the financial indicators from the management’s perspective, if processed correctly, this section can help investors give a better prediction for a company’s future return. In the beginning, we decided to focus on the MD&A section and tried to fetch the content from that section using regex, but due to the disorganised structure of SEC filings, we failed to achieve it.

Instead, we obtained positive, negative and neutral sentiment scores for the entire sections of each SEC filings (approx. 4600 SEC files). VADER is quite slow with large texts so we split the content into batches of 2000 words which resulted in reduced sentiment calculation time. For each of these batches, we calculated the positive, negative and neutral sentiment scores and took the mean, which was the final sentiment scores for each SEC filing.

Stock Price Prediction

Initial ideas, failures and success

As mentioned in section 3.3, we had around 300 tickers to work with, given the constraints that we have described in the earlier section. We started training a single model using all 300 tickers by feature engineering them as follows:

Constructed pandas dataframes for each stock using a 90 day window as described in Figure 1.

Fit separate scalers on whole data set for each stock to scale values between 0 and 1. Fitting separate scalers was essential because each stock can have different minimum and maximum prices.

These scalers were then used to transform the respective train, validation and test dataset for each of these stocks.

After all these transformations we used Keras TimeseriesGenerator to cascade OHLC data for all stocks as described in Figure 3.

Figure 3: Cascaded OHLC data for each ticker. Each layer denotes dataset for one ticker windowed using the method shown in Figure 1.

Due to constraints on hardware resources, we failed to train a single model using all the 300 stocks data and had to eventually limit it to the first 50 stocks.

LSTM models have been studied well and proven to be very effective on time-series data. We compiled neural network models using Keras with two LSTM layers, two dropout layers and a dense layer for the output.

We obtained very poor results after training a single model over data from all 50 stocks (constructed as described in Figure 3). To investigate it further, we also trained separate models for each of the 50 stocks without the cascading method. Results obtained using the latter proved much more accurate with trend lines that were reflective of the real trends for those stock. Each of these 50 models were trained for 20 epochs and without sentiment scores. Results obtained using this method are discussed in Section 5.

Training with sentiments

Sentiment scores can only be calculated for each quarter, mapping to three 10-Q and one 10-K filing for each financial year considered for a company. Therefore, they needed to be mapped to the daily OHLC data for each stock. We achieved this by considering 90 day OHLC data as one quarter and replicating prior quarter’s sentiment scores for all the rows. Separate models, with exactly same architecture as described above, were trained for 20 epochs for each of the 50 stocks. First training was done with positive sentiment scores and the second was done with negative. Results are discussed in the later section.

Predicting Stable Portfolios

We decided to create stable portfolios because we want to hold the portfolio for the predicted next quarter. The strategy we follow in order to create stable portfolios is to keep uncorrelated stocks in same portfolio. We do this to avoid any huge loss in a portfolio, because if one stock goes down the others will balance the loss. For example: If a portfolio has 2 stocks [AAPL, FB] in case they are correlated, if AAPL falls this would mean FB also falls and we get overall loss in portfolio. But if the stocks were uncorrelated, AAPL and FB will not fall together and thus prevent any heavy loss. This creates stable portfolios with balanced risk on which continuous earnings can be drawn for the quarter.

We started with evaluating the correlation and covariance of each pair combination of stocks, and kept the pairs with weak correlation in same set. We considered a correlation value less than 0.5, and a covariance value less than mean covariance. Then for each set we checked which other stock can be added based on low correlation/covariance values.

The Sharpe ratio is calculated by subtracting the risk-free rate from the return of the portfolio and dividing that result by the standard deviation of the portfolio’s excess return.

A higher value of Sharpe ratio means better risk-adjusted return. A sharpe ratio greater than 1 is considered good, greater than 2 is considered very good and greater than 3 is considered excellent. We considered risk-free rate of 2% i.e. current risk free rate of US market. We calculated the best sharpe ratio value for each portfolio among randomly selected 200 weights. And based on the Sharpe Ratio obtained we divided our portfolios as good, better and best.

5. Evaluation

Evaluating the effect of SEC sentiments on the stock prices

We mapped quarterly OHLC data with the quarterly sentiment scores to analyze how positive and negative sentiments are influencing the closing prices.

From Figure 4, it can be seen that when positive sentiment increased, GWW’s closing price in the following quarter was trailing the trend and going high. And the opposite trend had been noticed in case of negative sentiments in Figure 5.

Figure 4 : Time series effect of the positive sentiments of SEC files on subsequent close prices for GWW.

Figure 5 : Time series effect of the negative sentiments of SEC files on subsequent close prices for GWW.

We were able to see the positive and negative trend between the stock’s closing price and the SEC sentiment scores. It showed that the sentiment scores of the previous quarter had an impact on the stock’s closing price of the next quarter.

Since stock prices depend on various other factors as well, we were not able to see the relation between sentiments and stock closing price for each quarter.

Evaluation on stock predictions

We trained each of our 50 models for three cases; no sentiments, positive sentiments as features and negative sentiments as features. Results for one of the stocks (W W Grainger Inc) for all three cases is shown in Table 1.

Table 1: Results for W W Grainger Inc (GWW)

Although we obtained promising results for the GWW stock, this wasn’t always the case and including sentiments for some of the tickers had little to no effect on the RMSE. This suggests that sentiments from SEC filings are not the only factors affecting stock prices and there can be other potential factors that can be investigated. For example, a sudden announcement of change in energy policy can positively or negatively impact clean energy companies and this change can reflect for the next quarter. If such changes were never addressed in a prior SEC filing, a trained model won’t be able to pick that information from just the sentiment scores of that SEC filing. Improvements and suggestions to circumvent this are discussed in the Future Work section.

Plots in Figure 6 show Adj. Closing prices for GWW. Green line shows the historical stock performance over a period of 10 years (model training was done using data from this time period). Blue line shows predicted trend without sentiments as features and yellow line shows predicted trend after including negative sentiment scores.

A magnified view over the test window is shown in Figure 7 and includes actual and predicted trends. Note that although both predicted trends are similar, one that includes negative sentiments appears closer to the real values. This is confirmed by the lower RMSE given in Table 1.

Figure 6: Plots showing Ajd. close prices for GWW.

Figure 7: Plots comparing actual Adj. Closing prices for GWW with predicted prices with and without negative sentiment scores.

Evaluation on Portfolio Optimization

We analyzed the correlation between 50 stocks using python colormaps. Plot in Figure 8 shows the colormap obtained:

Figure 8: Correlation matrix for 50 stocks

Blue color is relative to minimum correlation and red is relative to highest. This means that the stocks towards blue can be in the same portfolio. For example: [sbac, msci]

After getting the stock pairs that have correlation less than 0.5 and covariance less than mean covariance, we analyzed the “pairable” stocks and “unpairable” stocks using the plot in Figure 9:

Figure 9: Plot showing pairable and unpairable stocks

The plot shows pairable stocks in blue and unpairable stocks in green. For Example: [amgn, mat] qualify to be in same portfolio based on their covariance and correlation values. We generated portfolios based on the above plot and calculated Sharpe Ratio and weight distribution for each portfolio. First few rows of the final dataframe is shown for reference:

Figure 10: Final dataframe of constructed portfolios

Finally, based on Sharpe Ratio, portfolios were divided into good, better and best. The following plots show the Return and Volatility values of Portfolios in “good”, “better” and “best” range. These plots are useful to evaluate the trade-offs between Volatility, Return and Risk and select a portfolio.

Plot in Figure 11 is generated for “good” portfolios. An example of trade-off by looking at the plot below is [ma, zion, mat, vno, sna, flir, aee, duk, etr, pnr, akam] and [ndaq, mat]. It can be seen that latter gives a better return of 20% with similar risk of 1.7 Sharpe Ratio.

Figure 11: Plot of ‘good’ portfolios

Plot in Figure 12 is for portfolios with Sharpe Ratio >=2 and < 3 (better). Greater sharpe ratio here means that the portfolios are less risky compared to “good” ones. As above, similar interesting trade-offs can be derived in this set as well. The plot shows one such trade-off between [ndaq,flir] and [wfc,rtn] choosing either gives almost the same return but latter is less riskier because of higher Sharpe Ratio.

Figure 12: Plot of ‘better’ portfolios

Similar conclusions can be drawn from “best” Sharpe ratio range (Figure 13) as well. This set of portfolios have minimum risk involved.

Figure 13: Plot of ‘best’ portfolios

6. Data Product

Our final product is a collection of Jupyter Notebooks. We have divided the product into four modules: SEC Scraper, Sentiment Analyzer, Stock Predictor and Portfolio Generator.

SEC Scraper: This module scrapes SEC Edgar website to extract 10-Q and 10-K filings for S&P 500 companies. Implements checkpointing so failed downloads can resume later. It also performs all the cleaning on HTML and generates raw text files out of them.

Sentiment Analyzer: This module contains two notebooks.

Sentiment Analysis Notebook: Extracts scores for positive, negative and neutral sentiments from SEC files provided in a directory. It saves these extracted sentiments into CSV files which can later be used for model training, visualizations, etc.

Sentiment Visualization Notebook: Generic notebook to plot sentiment trends together with closing prices for a stock.

Stock Predictor: This module contains three notebooks

LSTM Stocks with no sentiments: Performs feature engineering on stock data and trains and evaluates LSTM models for each of the given stocks. Each model is then saved in its respective directory along with plots comparing actual prices with the predicted ones. Training epochs and all directories are configurable.

LSTM Stocks with sentiments: Performs feature engineering on stock data as well as sentiments extracted from the Sentiment Analyzer module. It then trains and saves each model along with comparison plots.

Predictions Generator: This notebook can be used after training to load saved models and perform predictions on test data. Predicted data frames can then be exported to CSV files to be used later for various use cases.

Portfolio Generator and Optimizer: Constructs portfolios using pairwise selection of weakly correlated stocks. Optimizes portfolios to give highest Sharpe Ratio with respect to the optimal weights assigned to stocks in the portfolio.

7. Lessons Learnt

Learning about finance domain to understand what can potentially work was definitely a challenge. Moreover, working on how to manipulate time-series data, windowing methods, and training LSTM models using that proved interesting. Our findings also suggested that training a single model for data from all companies is fruitful because of issues such as correlation and covariance.

Sentiments extracted from SEC filings are significant to predict future stock trends. We learnt to extract sentiments on very large text data (100,000 words in some case) using NTLK VADER.

We learned how to construct portfolios by leveraging concepts such as correlation, covariance, Sharpe ratio and volatility. Relevant visualizations such as colormaps and correlation matrix were very useful in confirming the obtained results.

8. Summary

Machine learning in finance is a relatively recent development and it is hard to find the right resources. Moreover, stock price prediction is usually considered a hard problem and one of the reasons why most portfolio managers do not completely rely on it. We have shown neural networks constructed using LSTMs to be reliable for this problem. Sentiment analysis on SEC filings has also proven to be a reliable technique.With cleaner data and more evaluations using tools described in Section 9, we believe that the results can be improved further. Lastly, portfolio construction is hard and stock markets can be volatile. We have shown a method that can be utilized to mitigate risk and construct diversified portfolios based on predicted results.

9. Future Work

Sentiment Analysis

For sentiment analysis, we can consider word-embedding models namely: Word2Vec, FastText and Universal Sentence Encoder. We can evaluate if these models can give us more accurate sentiment scores and evaluate if they can help predict better trends on the stock market.

We extracted only positive, negative and neutral sentiments from SEC filings. There are a lot of other sentiments tied to the financial industry, like uncertainty, litigious, constraining, superfluous, which can be extracted and analyzed further.

Stock Price Prediction

Currently, we have trained separate models for each of the stocks considered. This was done because we obtained very poor results after training a single model using the method described in section 4. However, this method of training separate models for each stock can become unscalable if the number of stocks is large. To address this issue, we can consider training models for a set of highly correlated stocks instead of single stock. This can reduce the number of models trained. Moreover, we can leverage transfer learning to retrain the last layer of such models for every new stock that lies within a certain range of colinearity applying to a model.

We explained earlier that including sentiment scores from SEC filings didn’t necessarily give lower RMSE and better trend lines. To further establish this claim, we can augment our features by including news sentiments for every quarter for each of the respective stocks and evaluate if these scores together with SEC scores will help us make better predictions.

Portfolio Optimization

There are a lot of other portfolio construction methods that can be evaluated using our predictions. We can evaluate industry standard techniques such as ‘Global Minimum Variance Portfolio (GMV)’ and ‘Inverse Volatility Portfolio IVP’ [8] and compare those results from real world portfolios for a particular time window.

Finally, portfolio rebalancing is a standard practice for investment firms. We can extend our methods to implement rebalancing of existing portfolios and compare their performance with the ones that we are constructing for each quarter.","['machine', 'stocks', 'stock', 'sentiment', 'portfolios', 'sec', 'portfolio', 'learning', 'scores', 'predicting', 'prices', 'stable', 'data', 'sentiments', 'using']","We generate stable portfolios on predicted stock prices for next quarter.
Related WorkIn past a lot of attempts have been made to predict the stock prices for firms, and also to understand how text of news articles, twitter posts, and other platforms influence stock prices.
How can we built a stable portfolio using the predicted values for a quarter?
Standardizing OHLC data for S&P 500 stocksWe only took 10 years of stock data for the S&P 500 companies.
Predicting Stable PortfoliosWe decided to create stable portfolios because we want to hold the portfolio for the predicted next quarter.",en,['Nandita Dwivedi'],2019-05-10 16:52:28.022000+00:00,"{'Sharpe Ratio', 'Deep Learning', 'Machine Learning', 'Lstm', 'Portfolio Management'}","{'https://miro.medium.com/max/60/0*-hudUe1csueDlhXe?q=20', 'https://miro.medium.com/max/2916/0*Iurul09VjqZaZI-9', 'https://miro.medium.com/max/756/0*JORHoZfZz6hlnEnE', 'https://miro.medium.com/max/3080/0*j-J90sEVod3_g6Bm', 'https://miro.medium.com/max/3156/0*bAoQ1UUMm7O4vqjM', 'https://miro.medium.com/max/2520/1*z18X2GQXlqSi4-0-zlGC0w.png', 'https://miro.medium.com/max/3200/0*ENZeaOXm2UfErtgp', 'https://miro.medium.com/fit/c/80/80/1*dmbNkD5D-u45r44go_cf0g.png', 'https://miro.medium.com/max/60/0*IBK7nirG4_nXL08s?q=20', 'https://miro.medium.com/max/1996/0*GnGBjoBjQCnu-N2U', 'https://miro.medium.com/fit/c/160/160/1*1lrFbLjEEvq3WyeXzgJWkw.jpeg', 'https://miro.medium.com/max/60/0*OvwgH4VLgzcW_KvU?q=20', 'https://miro.medium.com/max/60/0*j-J90sEVod3_g6Bm?q=20', 'https://miro.medium.com/fit/c/96/96/1*1lrFbLjEEvq3WyeXzgJWkw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*LX--hnrQwZiaVUxhbmalJA.jpeg', 'https://miro.medium.com/max/2128/0*-hudUe1csueDlhXe', 'https://miro.medium.com/max/60/0*Fl0-uQYOVgNKGGGS?q=20', 'https://miro.medium.com/max/1488/0*IBK7nirG4_nXL08s', 'https://miro.medium.com/max/60/0*RTmLYow6ajZPm0pq?q=20', 'https://miro.medium.com/max/1732/0*XqlRFf6iAVup_TGS', 'https://miro.medium.com/max/60/0*Iurul09VjqZaZI-9?q=20', 'https://miro.medium.com/max/60/0*XqlRFf6iAVup_TGS?q=20', 'https://miro.medium.com/max/2320/0*RTmLYow6ajZPm0pq', 'https://miro.medium.com/max/3200/0*x0ELS_QgHHJg1bPg', 'https://miro.medium.com/max/3200/0*Q2cTXkr_PtkPQ3J3', 'https://miro.medium.com/max/60/0*x0ELS_QgHHJg1bPg?q=20', 'https://miro.medium.com/max/480/1*j8rE8JWnchD0fXa63wHH0g.png', 'https://miro.medium.com/max/1512/0*JORHoZfZz6hlnEnE', 'https://miro.medium.com/max/60/0*JORHoZfZz6hlnEnE?q=20', 'https://miro.medium.com/max/60/1*z18X2GQXlqSi4-0-zlGC0w.png?q=20', 'https://miro.medium.com/max/2320/0*OvwgH4VLgzcW_KvU', 'https://miro.medium.com/max/60/0*Q2cTXkr_PtkPQ3J3?q=20', 'https://miro.medium.com/max/60/0*bAoQ1UUMm7O4vqjM?q=20', 'https://miro.medium.com/max/60/0*ENZeaOXm2UfErtgp?q=20', 'https://miro.medium.com/fit/c/80/80/2*Pa2IwwprVIjIM3ZNy9TH7w.jpeg', 'https://miro.medium.com/fit/c/160/160/1*d8C4Rm6xwPFrkfVOYImyow.jpeg', 'https://miro.medium.com/max/60/0*GnGBjoBjQCnu-N2U?q=20', 'https://miro.medium.com/max/1164/0*Fl0-uQYOVgNKGGGS'}",2020-03-05 00:08:15.504471,1.042037010192871
https://towardsdatascience.com/cryptocurrency-analysis-with-python-buy-and-hold-c3b0bc164ffa,Cryptocurrency Analysis with Python — Buy and Hold,"In this part, I am going to analyze which coin (Bitcoin, Ethereum or Litecoin) was the most profitable in the last two months using buy and hold strategy. We’ll go through the analysis of these 3 cryptocurrencies and try to give an objective answer.

To run this code download the Jupyter notebook.

Bitcoin, Ethereum, and Litecoin

Disclaimer

I am not a trader and this blog post is not a financial advice. This is purely introductory knowledge. The conclusion here can be misleading as we analyze the period with immense growth.

Requirements

For other requirements, see my previous blog post in this series.

Getting the data

To get the latest data, go to the previous blog post, where I described how to download it using Cryptocompare API. You can also use the data I work within this example.

First, let’s download hourly data for BTC, ETH, and LTC from Coinbase exchange. This time we work with an hourly time interval as it has higher granularity. Cryptocompare API limits response to 2000 samples, which is 2.7 months of data for each coin.

import pandas as pd def get_filename(from_symbol, to_symbol, exchange, datetime_interval, download_date):

return '%s_%s_%s_%s_%s.csv' % (from_symbol, to_symbol, exchange, datetime_interval, download_date) def read_dataset(filename):

print('Reading data from %s' % filename)

df = pd.read_csv(filename)

df.datetime = pd.to_datetime(df.datetime) # change to datetime

df = df.set_index('datetime')

df = df.sort_index() # sort by datetime

print(df.shape)

return df

Load the data

df_btc = read_dataset(get_filename('BTC', 'USD', 'Coinbase', 'hour', '2017-12-24'))

df_eth = read_dataset(get_filename('ETH', 'USD', 'Coinbase', 'hour', '2017-12-24'))

df_ltc = read_dataset(get_filename('LTC', 'USD', 'Coinbase', 'hour', '2017-12-24')) df_btc.head()

Few entries in the dataset.

Extract closing prices

We are going to analyze closing prices, which are prices at which the hourly period closed. We merge BTC, ETH and LTC closing prices to a Dataframe to make analysis easier.

df = pd.DataFrame({'BTC': df_btc.close,

'ETH': df_eth.close,

'LTC': df_ltc.close}) df.head()

Analysis

Basic statistics

In 2.7 months, all three cryptocurrencies fluctuated a lot as you can observe in the table below.

For each coin, we count the number of events and calculate mean, standard deviation, minimum, quartiles, and maximum closing price.

Observations

The difference between the highest and the lowest BTC price was more than $15000 in 2.7 months.

The LTC surged from $48.61 to $378.66 at a certain point, which is an increase of 678.98%.

df.describe()

Let’s dive deeper into LTC

We visualize the data in the table above with a box plot. A box plot shows the quartiles of the dataset with points that are determined to be outliers using a method of the inter-quartile range (IQR). In other words, the IQR is the first quartile (25%) subtracted from the third quartile (75%).

On the box plot below, we see that LTC closing hourly price was most of the time between $50 and $100 in the last 2.7 months. All values over $150 are outliers (using IQR). Note that outliers are specific to this data sample.

import seaborn as sns ax = sns.boxplot(data=df['LTC'], orient=""h"")

LTC Boxplot with closing hourly prices

Histogram of LTC closing price

Let’s estimate the frequency distribution of LTC closing prices. The histogram shows the number of hours LTC had a certain value.

Observations

LTC closing price was not over $100 for many hours.

it has right-skewed distribution because a natural limit prevents outcomes on one side.

blue dashed line (median) shows that half of the time closing prices were under $63.50.

df['LTC'].hist(bins=30, figsize=(15,10)).axvline(df['LTC'].median(), color='b', linestyle='dashed', linewidth=2)

Histogram for LTC with the median

Visualize absolute closing prices

The chart below shows the absolute closing prices. It is not of much use as BTC closing prices are much higher than prices of ETH and LTC.

df.plot(grid=True, figsize=(15, 10))

Absolute closing price changes of BTC, ETH and LTC

Visualize relative changes in closing prices

We are interested in a relative change of the price rather than in absolute price, so we use three different y-axis scales.

We see that closing prices move in tandem. When one coin closing price increases so do the other.

import matplotlib.pyplot as plt

import numpy as np fig, ax1 = plt.subplots(figsize=(20, 10))

ax2 = ax1.twinx()

rspine = ax2.spines['right']

rspine.set_position(('axes', 1.15))

ax2.set_frame_on(True)

ax2.patch.set_visible(False)

fig.subplots_adjust(right=0.7) df['BTC'].plot(ax=ax1, style='b-')

df['ETH'].plot(ax=ax1, style='r-', secondary_y=True)

df['LTC'].plot(ax=ax2, style='g-') # legend

ax2.legend([ax1.get_lines()[0],

ax1.right_ax.get_lines()[0],

ax2.get_lines()[0]],

['BTC', 'ETH', 'LTC'])

Relative closing price changes of BTC, ETH and LTC

Measure the correlation of closing prices

We calculate the Pearson correlation between the closing prices of BTC, ETH, and LTC. Pearson correlation is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is the total positive linear correlation, 0 is no linear correlation, and −1 is the total negative linear correlation. The correlation matrix is symmetric so we only show the lower half.

Sifr Data daily updates Pearson correlations for many cryptocurrencies.

Observations

Closing prices aren’t normalized, see Log Returns, where we normalize closing prices before calculating correlation,

BTC, ETH and LTC were highly correlated in the past 2 months. This means, when BTC closing price increased, ETH and LTC followed.

ETH and LTC were even more correlated with 0.9565 Pearson correlation coefficient.

import seaborn as sns

import matplotlib.pyplot as plt # Compute the correlation matrix

corr = df.corr() # Generate a mask for the upper triangle

mask = np.zeros_like(corr, dtype=np.bool)

mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure

f, ax = plt.subplots(figsize=(10, 10)) # Draw the heatmap with the mask and correct aspect ratio

sns.heatmap(corr, annot=True, fmt = '.4f', mask=mask, center=0, square=True, linewidths=.5)

Pearson Correlation for BTC, ETH and LTC

Buy and hold strategy

Buy and hold is a passive investment strategy in which an investor buys a cryptocurrency and holds it for a long period, regardless of fluctuations in the market.

Let’s analyze returns using the Buy and hold strategy for the past 2.7 months. We calculate the return percentage, where t represents a certain period and price0 is the initial closing price:

df_return = df.apply(lambda x: x / x[0])

df_return.head()

Visualize returns

We show that LTC was the most profitable for the period between October 2, 2017 and December 24, 2017.

df_return.plot(grid=True, figsize=(15, 10)).axhline(y = 1, color = ""black"", lw = 2)

The factor of returns for BTC, ETH and LTC in 2.7 months

Conclusion

The cryptocurrencies we analyzed fluctuated a lot but all gained in a given 2.7 months period.

What about the percentage increase?

The percentage increase for BTC, ETH and LTC in 2.7 months

How many coins could we buy for $1000?

The number of coins we could buy with $1000 a while ago

How much money would we make?

The amount of money we would make if we invested $1000 a while ago

Originally published on my website romanorac.github.io.","['27', 'buy', 'cryptocurrency', 'correlation', 'months', 'eth', 'hold', 'python', 'price', 'prices', 'btc', 'data', 'ltc', 'closing', 'analysis']","Extract closing pricesWe are going to analyze closing prices, which are prices at which the hourly period closed.
We merge BTC, ETH and LTC closing prices to a Dataframe to make analysis easier.
import seaborn as sns ax = sns.boxplot(data=df['LTC'], orient=""h"")LTC Boxplot with closing hourly pricesHistogram of LTC closing priceLet’s estimate the frequency distribution of LTC closing prices.
Let’s analyze returns using the Buy and hold strategy for the past 2.7 months.
The percentage increase for BTC, ETH and LTC in 2.7 monthsHow many coins could we buy for $1000?",en,['Roman Orac'],2019-08-30 23:48:27.542000+00:00,"{'Cryptocurrency', 'Programming', 'Data Science'}","{'https://miro.medium.com/max/3196/1*vfGyf8TznAvFan7FaGbipA.png', 'https://miro.medium.com/max/608/1*IFKYVOKc_8M_P5XAottrbw.png', 'https://miro.medium.com/max/744/0*pyISXYMjQ3uBuF50.png', 'https://miro.medium.com/max/3200/1*sulRpl_P7FyZoxzZZX5LGA.png', 'https://miro.medium.com/max/60/1*9EyFDLDsRm_bKh7EDX_03A.png?q=20', 'https://miro.medium.com/max/60/0*TMjwNEq0OCcBzt1y.png?q=20', 'https://miro.medium.com/max/3204/1*APiqylDjW5-kD9w7QJW5wA.png', 'https://miro.medium.com/max/1124/0*TMjwNEq0OCcBzt1y.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/600/0*GAm7ubYgPzUzcZ_M.png', 'https://miro.medium.com/max/60/0*fNWfl-kL-wrV4Rk2.png?q=20', 'https://miro.medium.com/max/3208/1*9EyFDLDsRm_bKh7EDX_03A.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/0*Xh8bVz7iakj13Sw0.png?q=20', 'https://miro.medium.com/max/748/0*Yzajyxhmc36A4cvO.png', 'https://miro.medium.com/max/60/0*GAm7ubYgPzUzcZ_M.png?q=20', 'https://miro.medium.com/max/60/1*IFKYVOKc_8M_P5XAottrbw.png?q=20', 'https://miro.medium.com/max/60/0*Yzajyxhmc36A4cvO.png?q=20', 'https://miro.medium.com/max/1786/0*Xh8bVz7iakj13Sw0.png', 'https://miro.medium.com/max/60/1*APiqylDjW5-kD9w7QJW5wA.png?q=20', 'https://miro.medium.com/max/60/0*pyISXYMjQ3uBuF50.png?q=20', 'https://miro.medium.com/max/60/0*WwHCS1vkuDA8LREQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*b9r4UOJik2ArFpqDyxRJ9A.jpeg', 'https://miro.medium.com/max/2062/0*fNWfl-kL-wrV4Rk2.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1736/0*WwHCS1vkuDA8LREQ.png', 'https://miro.medium.com/max/730/0*rpZ1aPDYAc0RES9X.png', 'https://miro.medium.com/max/1760/0*Os5Av4wgniWuznvJ.png', 'https://miro.medium.com/max/738/0*4yZkaKbIwgHm6hAL.png', 'https://miro.medium.com/max/60/1*sulRpl_P7FyZoxzZZX5LGA.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*b9r4UOJik2ArFpqDyxRJ9A.jpeg', 'https://miro.medium.com/max/60/0*rpZ1aPDYAc0RES9X.png?q=20', 'https://miro.medium.com/max/60/1*vfGyf8TznAvFan7FaGbipA.png?q=20', 'https://miro.medium.com/max/1200/0*GAm7ubYgPzUzcZ_M.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*4yZkaKbIwgHm6hAL.png?q=20', 'https://miro.medium.com/max/60/0*Os5Av4wgniWuznvJ.png?q=20'}",2020-03-05 00:08:24.690289,9.184818267822266
https://towardsdatascience.com/algorithmic-trading-based-on-technical-analysis-in-python-80d445dc6943,Algorithmic trading based on Technical Analysis in Python,"Algorithmic trading based on Technical Analysis in Python

Learn how to create and implement trading strategies based on Technical Analysis!

This is the fourth part of a series of articles on backtesting trading strategies in Python. The previous ones described the following topics:

introducing the zipline framework and presenting how to test basic strategies (link)

importing custom data to use with zipline (link)

evaluating the performance of trading strategies (link)

This time, the goal of the article is to show how to create trading strategies based on Technical Analysis (TA in short). Quoting Wikipedia, technical analysis is a “methodology for forecasting the direction of prices through the study of past market data, primarily price, and volume”.

In this article, I show how to use a popular Python library for calculating TA indicators — TA-Lib — together with the zipline backtesting framework. I will create 5 strategies and then investigate which one performs best over the investment horizon.

The Setup

For this article I use the following libraries:

pyfolio 0.9.2

numpy 1.14.6

matplotlib 3.0.0

pandas 0.22.0

json 2.0.9

empyrical 0.5.0

zipline 1.3.0

Helper Functions

Before creating the strategies, I define a few helper functions (here I only describe one of them, as it is the most important one affecting the backtests).

The function is used for getting the modified start date of the backtest. That is because I would like all the strategies to start working on the same day — the first day of 2016. However, some strategies based on technical indicators require a certain number of past observations — the so-called “warm-up period”. That is why using this function I calculate the date the backtest should start so that on the first day of the investment horizon I already have enough past observations to calculate the indicators. Please bear in mind that no trading decision can happen before the true start date of the backtest!

UPDATE: Actually there is no explicit need to use this approach. When we specify the bar_count in the data.history , zipline will automatically take the previous number of bars available, even when they are from a period before the backtest. I used this approach here.

Strategies

In this article we use the following problem setting:

the investor has a capital of 10000$

the investment horizon covers years 2016–2017

the investor can only invest in Tesla’s stock

we assume no transactions costs — zero-commission trading

there is no short selling (the investor can only sell what he/she currently owns)

when the investor opens a position (buys the stock), the investor goes “all in” — allocates all available resources for the purchase

One of the reasons for selecting this range of dates is the fact that from mid-2018 the Quandl dataset was not updated and we want to keep the code as simple as possible. For details on how to load custom data (including the latest stock prices) into zipline , please refer to my previous article.

Buy And Hold Strategy

We start with the most basic strategy — Buy and Hold. The idea is that we buy a certain asset and do not do anything for the entire duration of the investment horizon. So at the first possible date, we buy as much Tesla stock as we can with our capital and do nothing later.

This simple strategy can also be considered a benchmark for more advanced ones — because there is no point in using a very complex strategy that generates less money (in general or due to transaction costs) than buying once and doing nothing.

We load the performance DataFrame:

buy_and_hold_results = pd.read_pickle('buy_and_hold.pkl')

What can happen here (it did not, but it is good to be aware of the possibility) is the sudden appearance of negative ending_cash . The reason for that could be the fact that the amount of shares we want to buy is calculated at the end of the day, using that day’s (closing) price. However, the order is executed on the next day, and the price can change significantly. In zipline the order is not rejected due to insufficient funds, but we can end up with a negative balance. This can happen mostly with strategies that go “all-in”. We could come up with some ways to avoid it — for example manually calculating the number of shares we can buy the next day and also including some markup to prevent such a situation from occurring, however, for simplicity we accept that this can happen.

We use a helper function to visualize some details of the strategy: the evolution of the portfolio’s value, the transactions on top of the price series, and the daily returns.

visualize_results(buy_and_hold_results, 'Buy and Hold Strategy - TSLA')

We also create the performance summary (using another helper function), which will be used in the last section:

buy_and_hold_perf = get_performance_summary(buy_and_hold_results.returns)

For brevity, we will not show all these steps (such as loading the performance DataFrame or getting the performance summary) for each strategy, because they are done in the same manner each time.

Simple Moving Average Strategy

The second strategy we consider is based on the simple moving average (SMA). The logic of the strategy can be summarized by the following:

when the price crosses the 20-day SMA upwards — buy shares

when the price crosses the 20-day SMA downwards — sell all the shares

the moving average uses 19 prior days and the current day — the trading decision is for the next day

This is the first time we need to use the previously defined helper function to calculate the adjusted starting date, which will enable the investor to make trading decisions on the first trading day of 2016.

get_start_date('TSLA', '2016-01-04', 19)

# '2015-12-04'

In the strategy below, we use the adjusted date as the start date.

Note: data.current(context.asset, ‘price’) is equivalent to price_history[-1] .

Below we illustrate the strategy:

The plot below shows the price series together with the 20-day moving average. We have additionally marked the orders, which are executed on the next trading day after the signal was generated.

Moving Average Crossover

This strategy can be considered an extension of the previous one — instead of a single moving average, we use two averages of different window sizes. The 100-day moving average is the one that takes longer to adjust to sudden price changes, while the 20-day one is much faster to account for sudden changes.

The logic of the strategy is as follows:

when the fast MA crosses the slow one upwards, we buy the asset

when the slow MA crosses the fast one upwards, we sell the asset

Bear in mind that many different window-lengths combinations defining the fast and slow MA can be considered for this strategy.

For this strategy, we need to additionally take 100 days of data in order to counter the “warm-up period”.

Below we plotted the two moving averages on top of the price series. We see that the strategy generated much fewer signals than the one based on SMA.

MACD

MACD stands for Moving Average Convergence/Divergence and is an indicator/oscillator used in technical analysis of stock prices.

MACD is a collection of three time-series calculated using historical close prices:

the MACD series — the difference between the fast (shorter period) and slow (longer period) exponential moving averages

the signal — EMA on the MACD series

the divergence — the difference between the MACD series and the signal

MACD is parametrized by the number of days used to calculate the three moving averages — MACD(a,b,c). The parameter a corresponds to the fast EMA, b to the slow EMA, and c to the MACD signal EMA. The most common setup, also used in this article, is MACD(12,26,9). Historically, these numbers corresponded to 2 weeks, 1 month and 1.5 weeks based on a 6-day working week.

One thing to remember is that MACD is a lagging indicator, as it is based on moving averages. That is why the MACD is less useful for stocks that do not exhibit a trend or are trading with erratic price action.

The strategy we use in this article can be described by:

buy shares when the MACD crosses the signal line upwards

sell shares when the MACD crosses the signal line downwards

As before, to counter the warm-up period we need to ascertain that we have 34 observations to calculate the MACD.

Below we plot the MACD and the signal lines, where the crossovers indicate buy/sell signals. Additionally, one could plot the MACD divergence in the form of a barplot (it is commonly referred to as the MACD histogram).

RSI

RSI stands for the Relative Strength Index, which is another technical indicator we can use to create trading strategies. The RSI is classified as a momentum oscillator and it measures the velocity and magnitude of directional price movements. Momentum describes the rate at which the price of the asset rises or falls.

Without going into too many technical details, the RSI measures momentum as the ratio of higher closes to lower closes. Assets with more/stronger positive changes have higher RSI than assets with more/stronger negative changes.

The output of the RSI is a number on a scale from 0 to 100 and it is typically calculated on a 14-day basis. To generate the trading signals, it is common to specify the low and high levels of the RSI at 30 and 70, respectively. The interpretation of the thresholds is that the lower one indicates that the asset is oversold, and the upper one that the asset is overbought.

Sometimes, a medium level (halfway between low and high) is also specified, for example in case of strategies which also allow for short-selling. We can also select more extreme thresholds such as 20 and 80, which would then indicate stronger momentum. However, this should be specified using domain knowledge or by running backtests.

The strategy we consider can be described as:

when the RSI crosses the lower threshold (30) — buy the asset

when the RSI crosses the upper threshold (70) — sell the asset

Below we plot the RSI together with the upper and lower threshold.

Evaluating the performance

The last step involves putting all the performance metrics into one DataFrame and inspecting the results. We can see that in the case of our backtest, the strategy based on the simple moving average performed best in terms of generated returns. It also had the highest Sharpe ratio — the highest excess return (in this case return, as we do not consider a risk-free asset) per unit of risk. The second-best strategy turned out to be the one based on the MACD. It is also good to notice that only these two performed better than the benchmark buy and hold strategy.

perf_df = pd.DataFrame({'Buy and Hold': buy_and_hold_perf,

'Simple Moving Average': sma_perf,

'Moving Average Crossover': mac_perf,

'MACD': macd_perf,

'RSI': rsi_perf})

perf_df.transpose()

Conclusions

In this short article, I showed how to combine zipline with talib in order to backtest trading strategies based on popular technical indicators such as moving averages, the MACD, the RSI, etc. But this was only the beginning, as it is possible to create much more sophisticated strategies.

Some of the possible future directions:

include multiple assets into the strategies

allow for short-selling

mix the indicators

backtest different parameters for each strategy to find the best-performing ones

We must also remember that the fact that the strategy performed well in the past is no guarantee that this will happen again in the future.

As always, any constructive feedback is welcome. You can reach out to me on Twitter or in the comments. You can find the code used for this article on my GitHub.

Below you can find the other articles in the series:","['buy', 'average', 'macd', 'python', 'price', 'trading', 'based', 'technical', 'strategies', 'algorithmic', 'moving', 'strategy', 'analysis']","Algorithmic trading based on Technical Analysis in PythonLearn how to create and implement trading strategies based on Technical Analysis!
This is the fourth part of a series of articles on backtesting trading strategies in Python.
However, some strategies based on technical indicators require a certain number of past observations — the so-called “warm-up period”.
Simple Moving Average StrategyThe second strategy we consider is based on the simple moving average (SMA).
RSIRSI stands for the Relative Strength Index, which is another technical indicator we can use to create trading strategies.",en,['Eryk Lewinson'],2020-02-14 10:02:47.114000+00:00,"{'Quantitative Finance', 'Data Science', 'Algorithmic Trading', 'Towards Data Science', 'Trading'}","{'https://miro.medium.com/max/1200/1*wheisdexXZTJByLF3zloWA.jpeg', 'https://miro.medium.com/max/60/1*zPS-FEUPafp4CjT2qrmNuw.png?q=20', 'https://miro.medium.com/max/3924/1*m3QNlMtH3jHcGEnKO1VwYw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*uTXiDxgucP-1DwGHV7fPpw.png?q=20', 'https://miro.medium.com/max/60/1*R81b0ki86ha0PblxNU9i-A.png?q=20', 'https://miro.medium.com/max/4040/1*OmFxHEQrfxYZ-kFeWZNFYg.png', 'https://miro.medium.com/max/4500/1*wheisdexXZTJByLF3zloWA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*KUipyYqi56L5EZiI9qe6JQ.png', 'https://miro.medium.com/max/3996/1*DQ3H32xQh8GMpIWRHN7vHg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/488/1*zPB7oDayV2o3vhxMRoAuUw.png', 'https://miro.medium.com/max/60/1*W-Jyts_YuLySPwlHY-mLKQ.png?q=20', 'https://miro.medium.com/max/3996/1*nEGyekMSYlMJ3SDVuTGEqQ.png', 'https://miro.medium.com/max/48/1*zPB7oDayV2o3vhxMRoAuUw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*KUipyYqi56L5EZiI9qe6JQ.png', 'https://miro.medium.com/max/60/1*kpm8K9339E0TqieTDy_mig.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/3988/1*zPS-FEUPafp4CjT2qrmNuw.png', 'https://miro.medium.com/max/3984/1*R81b0ki86ha0PblxNU9i-A.png', 'https://miro.medium.com/max/60/1*m3QNlMtH3jHcGEnKO1VwYw.png?q=20', 'https://miro.medium.com/max/60/1*OmFxHEQrfxYZ-kFeWZNFYg.png?q=20', 'https://miro.medium.com/max/60/1*nEGyekMSYlMJ3SDVuTGEqQ.png?q=20', 'https://miro.medium.com/max/4016/1*JrhomuWgITFJdWWvC6kLbQ.png', 'https://miro.medium.com/max/4016/1*kpm8K9339E0TqieTDy_mig.png', 'https://miro.medium.com/max/60/1*wheisdexXZTJByLF3zloWA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3564/1*uTXiDxgucP-1DwGHV7fPpw.png', 'https://miro.medium.com/max/60/1*JrhomuWgITFJdWWvC6kLbQ.png?q=20', 'https://miro.medium.com/max/60/1*DQ3H32xQh8GMpIWRHN7vHg.png?q=20', 'https://miro.medium.com/max/3960/1*W-Jyts_YuLySPwlHY-mLKQ.png'}",2020-03-05 00:08:27.964236,3.272948741912842
https://towardsdatascience.com/algorithmic-trading-based-on-mean-variance-optimization-in-python-62bdf844ac5b,Algorithmic trading based on mean-variance optimization in Python,"Algorithmic trading based on mean-variance optimization in Python

Learn how to create and implement trading strategies using Markowitz’s optimization!

This is the fifth part of a series of articles on backtesting trading strategies in Python. The previous ones described the following topics:

introducing the zipline framework and presenting how to test basic strategies (link)

importing custom data to use with zipline (link)

(link) evaluating the performance of trading strategies (link)

implementing trading strategies based on Technical Analysis (link)

This time, the goal of the article is to show how to create trading strategies using Markowitz’s portfolio optimization and the Modern Portfolio Theory.

In this article, I first give a brief introduction/reminder on the mean-variance optimization and then show how to implement it into trading strategies. Just as before, I will backtest them using the zipline framework.

The Setup

For this article I use the following libraries:

zipline 1.3.0

matplotlib 3.0.0

json 2.0.9

empyrical 0.5.0

numpy 1.14.6

pandas 0.22.0

pyfolio 0.9.2

Primer on mean-variance optimization

In 1952 Harry Markowitz published the ‘Portfolio Selection’, which described an investment theory now known as the Modern Portfolio Theory (MPT in short). Some of the key takeaways are:

the portfolio return is the weighted average of the individual portfolio constituents, however, the volatility is also impacted by the correlation between the assets

the investors should not evaluate the performance of the assets separately, but see how they would influence the performance of a portfolio

diversification (spreading the allocation over multiple assets instead of one or very few) can greatly decrease the portfolio’s volatility

We will not go deeply into the assumptions of MPT, but the main ones are that all investors share the goal of maximizing the returns on investment while avoiding as much risk as possible, they can borrow and lend money at the risk-free rate (without limits) and the transaction costs are not taken into account.

Based on all of the above, the mean-variance analysis is the process of finding optimal asset allocation that provides the best trade-off between the expected return and risk (measured as the variance of returns). A key concept connected to the mean-variance analysis is the Efficient Frontier — a set of optimal portfolios providing the highest expected portfolio return for a given level of risk — or framing it differently — providing the minimum level of risk for the expected portfolio return.

Visualization of the Efficient Frontier — Source: wikipedia

Mathematically, one of the possible formulations of the problem is the following:

where w is the vector of weights, μ is a vector of asset returns, Σ is the covariance matrix, μ_p is the target expected portfolio return. Two of the constraints are:

non-negative weights 0 short-selling is not allowed

weights must sum up to 1 — no leverage is allowed

To solve the problem and obtain the Efficient Frontier, we could define a range of possible expected portfolio returns and then for each value find the weights that minimize the variance. Fortunately, there is a library that makes the process very simple.

PyPortfolioOpt makes it possible to solve the entire optimization problem with only a few lines of code. In this article, we will create portfolios that either maximize the expected Sharpe ratio (portfolio’s excess return per unit of risk) or minimize the overall volatility. Both of these portfolios lie on the Efficient Frontier.

We present how to work with pypfopt in the following short example. First, we download the historical stock prices using yahoofinancials .

pypfopt allows us to easily calculate the expected returns and the covariance matrix directly from the prices, with no need for converting to returns beforehand.

# calculate expected returns and sample covariance amtrix

avg_returns = expected_returns.mean_historical_return(prices_df)

cov_mat = risk_models.sample_cov(prices_df)

We obtain the weights maximizing the Sharpe ratio by running the following lines of code:

# get weights maximizing the Sharpe ratio

ef = EfficientFrontier(avg_returns, cov_mat)

weights = ef.max_sharpe()

cleaned_weights = ef.clean_weights()

cleaned_weights

Which results in the following weights:

{'FB': 0.03787, 'MSFT': 0.83889, 'TSLA': 0.0, 'TWTR': 0.12324}

For convenience, we use the clean_weights() method, as it truncates very small weights to zero and rounds the rest.

Strategies

In this article we use the following problem setting:

the investor has a capital of 50000$

the investment horizon covers years 2016–2017

the investor can only invest in the following stocks: Tesla, Microsoft, Facebook, Twitter

we assume no transactions costs

there is no short selling (the investor can only sell what he/she currently owns)

when performing optimization, the investor considers 252 past trading days to calculate the historical returns and covariance matrix

the first trading decision is made on the last day of December, but the orders are executed on the first trading day of January 2016

Benchmark 1/n strategy

We start by creating a simple benchmark strategy — the **1/n portfolio**. The idea is very simple — on the first day of the test, we allocate 1/n % of our total capital to each of the considered n assets. To keep it simple, we do not do any rebalancing.

What often happens in practice is that the portfolio is rebalanced every X days to bring the allocation back to 1/n. Why? We can imagine that we hold a portfolio of two assets X and Y and at the beginning of the investment horizon the allocation is 50–50. Over a month, the price of X increased sharply while the price of Y decreased. As a result, asset X constitutes 65% of our portfolio’s worth, while Y has only 35%. We might want to rebalance back to 50–50 by selling some of X and buying more Y.

The following plot shows the cumulative returns generated by the strategy.

We store some results to compare with the other strategies.

benchmark_perf = qf.get_performance_summary(returns)

Maximum Sharpe ratio portfolio — rebalancing every 30 days

In this strategy, the investor selects such weights that maximize the portfolio’s expected Sharpe ratio. The portfolio is rebalanced every 30 trading days.

We determine if a given day is a rebalancing day by using the modulo operation ( % in Python) on the current trading day’s number (stored in context.time ). We rebalance on days when the reminder after the division by 30 is 0.

We will inspect the results of all the strategies at the end of the article. However, an interesting thing to see here is the weights allocation over time.

Some of the insights from the plot:

there is virtually no investment in Twitter in this strategy

sometimes entire months are skipped like Jan 2016 or April 2016. That is because we are rebalancing every 30 trading days and there are on average 21 trading days in a month.

Maximum Sharpe ratio portfolio — rebalancing every month

This strategy is very similar to the previous one — we also select weights maximizing the portfolio’s expected Sharpe ratio. The difference is in the rebalancing scheme. First, we define the rebalance method which calculates the optimal weights and executes orders accordingly. Then, we schedule it using schedule_function . With the current setting, the rebalancing happens on the last trading day of the month ( date_rules.month_end ) after the market closes ( time_rules.market_close ).

We also inspect the weights over time:

When rebalancing monthly, we do have entries for all the months. Also, in this case, there are some small investments in Twitter in mid-2017.

Minimum volatility portfolio — rebalancing every month

This time the investors select the portfolio weights by minimizing the volatility. Thanks to PyPortfolioOpt , this is as easy as changing weights = ef.max_sharpe() to weights = ef.min_volatility() in the previous code snippet.

The weights generated by the minimum volatility strategy are definitely most stable over time — there is not so much rebalancing between two consecutive periods. That is certainly important when we account for transaction costs.

Comparing the performance

From the comparison below we see that over the duration of the backtest the strategy minimalizing volatility achieved the best returns together with the lowest portfolio volatility. It also performed much better than the strategies maximizing the Sharpe ratio.

Another interesting observation is that all of the custom strategies created using optimization resulted in performance better than naive 1/n allocation combined with buy and hold.

Conclusions

In this article, I showed how to combine zipline with pypfopt in order to backtest trading strategies based on mean-variance optimization. We only covered portfolios either maximizing the Sharpe ratio or minimizing the overall volatility, however, there are definitely more possibilities.

Some of the possible future directions:

in the optimization scheme, account for maximum potential changes in the allocation. With the zero commission setup, this is not a problem, however, in the presence of transaction costs we would like to avoiding spending too much on the fees if we completely rebalance every X days.

allow for short-selling

use custom objective functions in the optimization problem — optimizing using different evaluation metrics

It is crucial to remember that the fact that the strategy performed well in the past is no guarantee that this will happen again in the future.

As always, any constructive feedback is welcome. You can reach out to me on Twitter or in the comments. You can find the code used for this article on my GitHub.","['expected', 'portfolios', 'python', 'portfolio', 'trading', 'based', 'returns', 'sharpe', 'strategies', 'rebalancing', 'optimization', 'algorithmic', 'meanvariance', 'weights']","Algorithmic trading based on mean-variance optimization in PythonLearn how to create and implement trading strategies using Markowitz’s optimization!
This is the fifth part of a series of articles on backtesting trading strategies in Python.
In this article, I first give a brief introduction/reminder on the mean-variance optimization and then show how to implement it into trading strategies.
Minimum volatility portfolio — rebalancing every monthThis time the investors select the portfolio weights by minimizing the volatility.
ConclusionsIn this article, I showed how to combine zipline with pypfopt in order to backtest trading strategies based on mean-variance optimization.",en,['Eryk Lewinson'],2020-02-14 10:02:28.246000+00:00,"{'Quantitative Finance', 'Data Science', 'Algorithmic Trading', 'Python', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*G8E229bOCQk11M_2DQIeTw.png?q=20', 'https://miro.medium.com/max/60/1*Rp3HLqdCT7FlxuZ_JtwHeg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*fB1HMYI3-LtGTIgbmJQvsQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*TMfjKugnfj2Frg3iadejsw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*KUipyYqi56L5EZiI9qe6JQ.png', 'https://miro.medium.com/max/1724/1*23T48NknhbzzHorBVlJu2g.png', 'https://miro.medium.com/max/2560/1*TMfjKugnfj2Frg3iadejsw.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1640/1*n4Z4vec4dpa_T7RDUKgeuA.png', 'https://miro.medium.com/max/488/1*zPB7oDayV2o3vhxMRoAuUw.png', 'https://miro.medium.com/max/3964/1*Rp3HLqdCT7FlxuZ_JtwHeg.png', 'https://miro.medium.com/max/48/1*zPB7oDayV2o3vhxMRoAuUw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*KUipyYqi56L5EZiI9qe6JQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*n4Z4vec4dpa_T7RDUKgeuA.png?q=20', 'https://miro.medium.com/max/60/1*viW4qhTVm813V0OkE-aKLg.png?q=20', 'https://miro.medium.com/max/3712/1*fB1HMYI3-LtGTIgbmJQvsQ.png', 'https://miro.medium.com/max/60/1*vxcEYBbhbgBUxYehfKzrDA.png?q=20', 'https://miro.medium.com/max/60/1*TMfjKugnfj2Frg3iadejsw.jpeg?q=20', 'https://miro.medium.com/max/3936/1*vxcEYBbhbgBUxYehfKzrDA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*23T48NknhbzzHorBVlJu2g.png?q=20', 'https://miro.medium.com/max/3956/1*viW4qhTVm813V0OkE-aKLg.png', 'https://miro.medium.com/max/3780/1*G8E229bOCQk11M_2DQIeTw.png'}",2020-03-05 00:08:37.818301,9.854064464569092
https://towardsdatascience.com/stock-price-prediction-intervals-using-monte-carlo-simulation-6b52b8ac9c27,Stock Price Prediction Intervals Using Monte Carlo Simulation,"Rolling forecast for the validation data set with Monte Carlo generated prediction intervals

Stock Price Prediction Intervals Using Monte Carlo Simulation

Combining time series modeling and Monte Carlo simulation to generate empirically derived forecast distributions

Edit: One should use the CDF of the chosen distribution to derive the probability of being below or above a certain threshold, instead of Monte Carlo simulation. One should also use the quantiles (say 0.0001 and 0.9999 or some other wide margin) to be the outer bounds of the prediction intervals. Despite that methodological error, I think the broader approach is a good way of creating empirical prediction intervals in the time-series context.

Note: Please don’t start trading securities using this technique. Any investment strategies should be rigorously back-tested prior to use. Paper trading is another option for testing strategies and models. Keep in mind that structural breaks exist as fundamentals in the market change and can cause a model that used to work to start performing poorly, resulting in lost money. This article is about pairing two statistical techniques to generate empirical prediction intervals. IT IS NOT INVESTMENT ADVICE! Also, all images are generated by myself in python.

Most time-series libraries for your preferred programming language will give you point predictions and prediction intervals when forecasting. This is of course very useful for making predictions about the future. But what if you want to know the probability that a future value is above or below some important threshold? An example would be that a stock price in the future is above or below the strike price of an option. That’s were pairing Monte Carlo simulation and time series modeling can come in handy.

To perform this analysis, I got daily closing price data for Netflix from Yahoo Finance for the past 7 years. Then I took the last 500 trading days and split that into two equally sized groups. The first I used to pick a distribution for my predictions and set its parameters. The second I used for validating this approach.

Daily closing prices for Netflix over the past 7 years

Establishing the Order of Integration

Before modeling, the order of integration of our time series needs to be established. If it is not stationary, aka I(0), then we will have issues with auto-correlation in our residuals, which violates an essential assumption made by many statistical models (meaning our model would be bad). To determine the order of integration, Auto-Correlation Functions and Augmented-Dickey Fuller Tests can help us out.

Note: stationary is a short way of saying that the mean and variance of the series do not vary over time.

Auto-Correlation Function Plots

The line of the ACF shows strong postive auto-correlation for the first 450, or so, lags. Then there is a long period of negative auto-correlation. This is a good indication that the stock price is non-stationary.

ACF for the non-differenced stock prices

The ACF plot for the differenced stock prices, however, looks stationary. This suggests the stock price is I(1)

ACF for the differenced stock prices

Augmented Dickey-Fuller Tests

The ADF tests confirm that the stock price is not stationary, but its first difference is. This means the stock price is integrated to the order 1 and will need to be differenced one time for modeling.

ADF tests with no constant, constant, and constant + trend terms for the non-differenced and differenced closing stock prices

Estimating a Simple ARIMA Model

Here I estimated a simple ARIMA model of order (1, 1, 1) on the differenced stock prices transformed by the natural logarithm to control for heteroskedasticity. The residuals on the training data set appear stationary, but there are some big spikes giving long tails to the distribution of residuals. Because the distribution of residuals has long tails and the center of the distribution is a sharp peak around zero, the residuals may not be normally distributed. However, the mean of the distributions is roughly zero, so it seems reasonable to proceed with this simple model for now. We just need to keep in mind the residuals may not be distributed normally.","['differenced', 'monte', 'stock', 'order', 'prediction', 'price', 'stationary', 'intervals', 'distribution', 'residuals', 'using', 'simulation', 'carlo']","Rolling forecast for the validation data set with Monte Carlo generated prediction intervalsStock Price Prediction Intervals Using Monte Carlo SimulationCombining time series modeling and Monte Carlo simulation to generate empirically derived forecast distributionsEdit: One should use the CDF of the chosen distribution to derive the probability of being below or above a certain threshold, instead of Monte Carlo simulation.
Despite that methodological error, I think the broader approach is a good way of creating empirical prediction intervals in the time-series context.
Most time-series libraries for your preferred programming language will give you point predictions and prediction intervals when forecasting.
An example would be that a stock price in the future is above or below the strike price of an option.
This suggests the stock price is I(1)ACF for the differenced stock pricesAugmented Dickey-Fuller TestsThe ADF tests confirm that the stock price is not stationary, but its first difference is.",en,['John Clements'],2019-12-14 17:20:41.404000+00:00,"{'Data Science', 'Economics', 'Machine Learning', 'Programming', 'Finance'}","{'https://miro.medium.com/max/60/1*xrwfTEIkVtxX2r4AWWCYDQ.png?q=20', 'https://miro.medium.com/max/1454/1*o8OqeztNUE6MztmsSVQ_eg.png', 'https://miro.medium.com/max/60/1*7Gz_jeKYR6Izn5pxJTyanw.png?q=20', 'https://miro.medium.com/max/60/1*-MIKPm8ebgml2Dy1v91AKA.png?q=20', 'https://miro.medium.com/max/1478/1*7Gz_jeKYR6Izn5pxJTyanw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*Z0gqzTbNMZ54TZyPeQ6TKA.png?q=20', 'https://miro.medium.com/max/60/1*ZRgKmm0HqbHPfLv98SDZOQ.png?q=20', 'https://miro.medium.com/max/60/1*iVRaaKv8UTxdQtF_icXVjg.png?q=20', 'https://miro.medium.com/max/1478/1*Y4y7sUQkgvt_NWqWJyJJ6g.png', 'https://miro.medium.com/fit/c/96/96/2*0uYENDwkxd1_0bpW__l5tQ.png', 'https://miro.medium.com/max/1848/1*4p2uaE48I_Gz4QkhjvfYIA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*sHMBGz554M6c7FB0VIAaUQ.png?q=20', 'https://miro.medium.com/max/1460/1*vMgpUtFmCm7wKGCMOPLafQ.png', 'https://miro.medium.com/max/60/1*4p2uaE48I_Gz4QkhjvfYIA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1128/1*ZRgKmm0HqbHPfLv98SDZOQ.png', 'https://miro.medium.com/fit/c/160/160/2*0uYENDwkxd1_0bpW__l5tQ.png', 'https://miro.medium.com/max/727/1*o8OqeztNUE6MztmsSVQ_eg.png', 'https://miro.medium.com/max/1460/1*iVRaaKv8UTxdQtF_icXVjg.png', 'https://miro.medium.com/max/1464/1*xrwfTEIkVtxX2r4AWWCYDQ.png', 'https://miro.medium.com/max/60/1*vMgpUtFmCm7wKGCMOPLafQ.png?q=20', 'https://miro.medium.com/max/1838/1*C88O2kUYurGCQUmslKGo8w.png', 'https://miro.medium.com/max/60/1*Y4y7sUQkgvt_NWqWJyJJ6g.png?q=20', 'https://miro.medium.com/max/60/1*o8OqeztNUE6MztmsSVQ_eg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1496/1*Z0gqzTbNMZ54TZyPeQ6TKA.png', 'https://miro.medium.com/max/1454/1*sHMBGz554M6c7FB0VIAaUQ.png', 'https://miro.medium.com/max/60/1*C88O2kUYurGCQUmslKGo8w.png?q=20', 'https://miro.medium.com/max/1280/1*-MIKPm8ebgml2Dy1v91AKA.png'}",2020-03-05 00:08:40.306335,2.4870758056640625
https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3,"Simple Transformers — Introducing The Easiest Way To Use BERT, RoBERTa, XLNet, and XLM","Preface

The Simple Transformers library is built as a wrapper around the excellent Transformers library by Hugging Face. I am eternally grateful for the hard work done by the folks at Hugging Face to enable the public to easily access and use Transformer models. I don’t know what I’d have done without you guys!

Introduction

I believe it’s fair to say that the success of Transformer models have been nothing short of phenomenal in advancing the field of Natural Language Processing. Not only have they shown staggering leaps in performance on many NLP tasks they were designed to solve, pre-trained Transformers are also almost uncannily good at Transfer Learning. This means that anyone can take advantage of the long hours and the mind-boggling computational power that has gone into training these models to perform a countless variety of NLP tasks. You don’t need the deep pockets of Google or Facebook to build a state-of-the-art model to solve your NLP problem anymore!

Or so one might hope. The truth is that getting these models to work still requires substantial technical know-how. Unless you have expertise or at least experience in deep learning, it can seem a daunting challenge. I am happy to say that my previous articles on Transformers (here and here) seem to have helped a lot of people get a start on using Transformers. Interestingly, I noticed that people of various backgrounds (linguistics, medicine, and business to name but a few) were attempting to use these models to solve problems in their own domain. However, the technical barriers that need to be overcome in order to adapt Transformers to specific tasks are non-trivial and may even be rather discouraging.

Simple Transformers

This conundrum was the main motivation behind my decision to develop a simple library to perform (binary and multiclass) text classification (the most common NLP task that I’ve seen) using Transformers. The idea was to make it as simple as possible, which means abstracting away a lot of the implementational and technical details. The implementation of the library can be found on Github. I highly encourage you to look at it to get a better idea of how everything works, although it is not necessary to know the inner details to use the library.

To that end, the Simple Transformers library was written so that a Transformer model can be initialized, trained on a given dataset, and evaluated on a given dataset, in just 3 lines of code! Let’s see how it’s done, shall we?

Installation

Install Anaconda or Miniconda Package Manager from here Create a new virtual environment and install the required packages.

conda create -n transformers python pandas tqdm

conda activate transformers

If using cuda:

conda install pytorch cudatoolkit=10.0 -c pytorch

else:

conda install pytorch cpuonly -c pytorch

conda install -c anaconda scipy

conda install -c anaconda scikit-learn

pip install transformers

pip install tensorboardx Install simpletransformers.

pip install simpletransformers

Usage

A quick look at how to use this library on the Yelp Reviews dataset.

Download Yelp Reviews Dataset. Extract train.csv and test.csv and place them in the directory data/ .

(Bash users can use this script to download the dataset)

Nothing fancy here, we are just getting the data in the correct form. This is all you have to do for any dataset.

Create two pandas DataFrame objects for the train and eval portions.

objects for the train and eval portions. Each DataFrame should have two columns. The first column contains the text that you want to train or evaluate and has the datatype str . The second column has the corresponding label and has the datatype int .

With the data in order, it’s time to train and evaluate the model.

That’s it!

For making predictions on other text, TransformerModel comes with a predict(to_predict) method which given a list of text, returns the model predictions and the raw model outputs.

For more details on all available methods, please see the Github repo. The repo also contains a minimal example of using the library.

Default settings and how to change them

The default args used are given below. Any of these can be overridden by passing a dict containing the corresponding key/value pairs to the init method of TransformerModel. (See example below)

self.args = {

'model_type': 'roberta',

'model_name': 'roberta-base',

'output_dir': 'outputs/',

'cache_dir': 'cache/', 'fp16': True,

'fp16_opt_level': 'O1',

'max_seq_length': 128,

'train_batch_size': 8,

'eval_batch_size': 8,

'gradient_accumulation_steps': 1,

'num_train_epochs': 1,

'weight_decay': 0,

'learning_rate': 4e-5,

'adam_epsilon': 1e-8,

'warmup_ratio': 0.06,

'warmup_steps': 0,

'max_grad_norm': 1.0, 'logging_steps': 50,

'evaluate_during_training': False,

'save_steps': 2000,

'eval_all_checkpoints': True,

'use_tensorboard': True, 'overwrite_output_dir': False,

'reprocess_input_data': False,

}

To override any of these, simply pass in a dictionary with the appropriate key/value pair to the TransformerModel.

For an explanation of what each argument does, please refer to the Github repo.

Conclusion

That’s all folks! The easiest way to use Transformer models that I know of.","['way', 'introducing', 'models', 'bert', 'easiest', 'transformer', 'train', 'install', 'nlp', 'xlnet', 'transformers', 'simple', 'roberta', 'library', 'xlm', 'text', 'using']","PrefaceThe Simple Transformers library is built as a wrapper around the excellent Transformers library by Hugging Face.
I am eternally grateful for the hard work done by the folks at Hugging Face to enable the public to easily access and use Transformer models.
IntroductionI believe it’s fair to say that the success of Transformer models have been nothing short of phenomenal in advancing the field of Natural Language Processing.
I am happy to say that my previous articles on Transformers (here and here) seem to have helped a lot of people get a start on using Transformers.
The easiest way to use Transformer models that I know of.",en,['Thilina Rajapakse'],2019-10-11 08:00:54.900000+00:00,"{'Data Science', 'Artificial Intelligence', 'Naturallanguageprocessing', 'Pytorch', 'Programming'}","{'https://miro.medium.com/max/60/1*HmTmqEntaryJpH5MNgTsdg.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*HmTmqEntaryJpH5MNgTsdg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/96/96/2*oBM4rR-W3VS_mtPKrbcIpA.jpeg', 'https://miro.medium.com/fit/c/160/160/2*oBM4rR-W3VS_mtPKrbcIpA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/9018/1*HmTmqEntaryJpH5MNgTsdg.jpeg'}",2020-03-05 00:08:49.971490,9.665154695510864
https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146,RL — Policy Gradient Explained,"Policy Gradient Methods (PG) are frequently used algorithms in reinforcement learning (RL). The principle is very simple.

We observe and act.

A human takes actions based on observations. As a quote from Stephen Curry:

You have to rely on the fact that you put the work in to create the muscle memory and then trust that it will kick in. The reason you practice and work on it so much is so that during the game your instincts take over to a point where it feels weird if you don’t do it the right way.

Constant practice is the key to build muscle memory for athletes. For PG, we train a policy to act based on observations. The training in PG makes actions with high rewards more likely, or vice versa.

We keep what is working and throw away what is not.

In policy gradients, Curry is our agent.

He observes the state of the environment (s). He takes action (u) based on his instinct (a policy π) on the state s. He moves and the opponents react. A new state is formed. He takes further actions based on the observed state. After a trajectory τ of motions, he adjusts his instinct based on the total rewards R(τ) received.

Curry visualizes the situation and instantly knows what to do. Years of training perfects the instinct to maximize the rewards. In RL, the instinct may be mathematically described as:

the probability of taking the action u given a state s. π is the policy in RL. For example, what is the chance of turning or stopping when you see a car in front:

Objective

How can we formulate our objective mathematically? The expected rewards equal the sum of the probability of a trajectory × corresponding rewards:

And our objective is to find a policy θ that create a trajectory τ

that maximizes the expected rewards.

Input features & rewards

s can be handcrafted features for the state (like the joint angles/velocity of a robotic arm) but in some problem domains, RL is mature enough to handle raw images directly. π can be a deterministic policy which output the exact action to be taken (move the joystick left or right). π can be a stochastic policy also which outputs the possibility of an action that it may take.

We record the reward r given at each time step. In a basketball game, all are 0 except the terminate state which equals 0, 1, 2 or 3.

Let’s introduce one more term H called the horizon. We can run the course of simulation indefinitely (h→∞) until it reaches the terminate state, or we set a limit to H steps.

Optimization

First, let’s identify a common and important trick in Deep Learning and RL. The partial derivative of a function f(x) (R.H.S.) is equal to f(x) times the partial derivative of the log(f(x)).

Replace f(x) with π.

Also, for a continuous space, expectation can be expressed as:

Now, let’s formalize our optimization problem mathematically. We want to model a policy that creates trajectories that maximize the total rewards.

However, to use gradient descent to optimize our problem, do we need to take the derivative of the reward function r which may not be differentiable or formalized?

Let’s rewrite our objective function J as:

The gradient (policy gradient) becomes:

Great news! The policy gradient can be represented as an expectation. It means we can use sampling to approximate it. Also, we sample the value of r but not differentiate it. It makes sense because the rewards do not directly depend on how we parameterize the model. But the trajectories τ are. So what is the partial derivative of the log π(τ).

π(τ) is defined as:

Take the log:

The first and the last term does not depend on θ and can be removed.

So the policy gradient

becomes:

And we use this policy gradient to update the policy θ.

Intuition

How can we make sense of these equations? The underlined term is the maximum log likelihood. In deep learning, it measures the likelihood of the observed data. In our context, it measures how likely the trajectory is under the current policy. By multiplying it with the rewards, we want to increase the likelihood of a policy if the trajectory results in a high positive reward. On the contrary, we want to decrease the likelihood of a policy if it results in a high negative reward. In short, keep what is working and throw out what is not.

If going up the hill below means higher rewards, we will change the model parameters (policy) to increase the likelihood of trajectories that move higher.

There is one thing significant about the policy gradient. The probability of a trajectory is defined as:

States in a trajectory are strongly related. In Deep Learning, a long sequence of multiplication with factors that are strongly correlated can trigger vanishing or exploding gradient easily. However, the policy gradient only sums up the gradient which breaks the curse of multiplying a long sequence of numbers.

The trick

creates a maximum log likelihood and the log breaks the curse of multiplying a long chain of policy.

Policy Gradient with Monte Carlo rollouts

Here is the REINFORCE algorithm which uses Monte Carlo rollout to compute the rewards. i.e. play out the whole episode to compute the total rewards.

Policy gradient with automatic differentiation

The policy gradient can be computed easily with many Deep Learning software packages. For example, this is the partial code for TensorFlow:

Yes, as often, coding looks simpler than the explanations.

Continuous control with Gaussian policies

How can we model a continuous control?

Let’s assume the values for actions are Gaussian distributed

and the policy is defined using a Gaussian distribution with means computed from a deep network:

With

We can compute the partial derivative of the log π as:

So we can backpropagate

through the policy network π to update the policy θ. The algorithm will look exactly the same as before. Just start with a slightly different way in calculating the log of the policy.

Policy Gradients improvements

Policy Gradients suffer from high variance and low convergence.

Monte Carlo plays out the whole trajectory and records the exact rewards of a trajectory. However, the stochastic policy may take different actions in different episodes. One small turn can completely alter the result. So Monte Carlo has no bias but high variance. Variance hurts deep learning optimization. The variance provides conflicting descent direction for the model to learn. One sampled rewards may want to increase the log likelihood and another may want to decrease it. This hurts the convergence. To reduce the variance caused by actions, we want to reduce the variance for the sampled rewards.

Increasing the batch size in PG reduces variance.

However, increasing the batch size significantly reduces sample efficiency. So we cannot increase it too far, we need additional mechanisms to reduce the variance.

Baseline

We can always subtract a term to the optimization problem as long as the term is not related to θ. So instead of using the total reward, we subtract it with V(s).

We define the advantage function A and rewrite the policy gradient in terms of A.

In deep learning, we want input features to be zero-centered. Intuitively, RL is interested in knowing whether an action is performed better than the average. If rewards are always positive (R>0), PG always try to increase a trajectory probability even if it receives much smaller rewards than others. Consider two different situations:

Situation 1: Trajectory A receives+10 rewards and Trajectory B receives -10 rewards.

Situation 2: Trajectory A receives +10 rewards and Trajectory B receives +1 rewards.

In the first situation, PG will increase the probability of Trajectory A while decreasing B. In the second situation, it will increase both. As a human, we will likely decrease the likelihood of trajectory B in both situations.

By introducing a baseline, like V, we can recalibrate the rewards relative to the average action.

Vanilla Policy Gradient Algorithm

Here is the generic algorithm for the Policy Gradient Algorithm using a baseline b.

Causality

Future actions should not change past decision. Present actions only impact the future. Therefore, we can change our objective function to reflect this also.

Reward discount

Reward discount reduces variance which reduces the impact of distant actions. Here, a different formula is used to compute the total rewards.

And the corresponding objective function becomes:

Part 2

This ends part 1 of the policy gradient methods. In the second part, we continue on the Temporal Difference, Hyperparameter tuning, and importance sampling. Temporal Difference will further reduce the variance and the importance sampling will lay down the theoretical foundation for more advanced policy gradient methods like TRPO and PPO.

Credit and references

UCL RL course

UC Berkeley RL course

UC Berkeley RL Bootcamp

A3C paper

GAE paper","['state', 'rl', 'actions', 'log', 'likelihood', 'trajectory', 'policy', 'variance', 'rewards', 'explained', 'gradient']","Let’s rewrite our objective function J as:The gradient (policy gradient) becomes:Great news!
So the policy gradientbecomes:And we use this policy gradient to update the policy θ.IntuitionHow can we make sense of these equations?
However, the policy gradient only sums up the gradient which breaks the curse of multiplying a long sequence of numbers.
Policy gradient with automatic differentiationThe policy gradient can be computed easily with many Deep Learning software packages.
Vanilla Policy Gradient AlgorithmHere is the generic algorithm for the Policy Gradient Algorithm using a baseline b.CausalityFuture actions should not change past decision.",en,['Jonathan Hui'],2018-09-30 20:20:15.909000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Reinforcement Learning', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*4Dl4KD6Hk3dqrAPcv-1zTw.png?q=20', 'https://miro.medium.com/max/60/1*qF_agjFzPMwYfafsVJMo9A.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/60/1*HDGxQ2SgZng8DZ6Mzp0meQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*m5--PCQa5gvU0Z7e8d-E2A.png?q=20', 'https://miro.medium.com/max/1200/0*ep1_h3aZgY5MKFsm', 'https://miro.medium.com/max/3000/1*wZU7sKpHjqCGZYNw9RNSsQ.jpeg', 'https://miro.medium.com/max/2800/1*8f8-JeLOAdgLWcIQglzoGQ.png', 'https://miro.medium.com/max/60/1*0A4M301ZwWzn1JyGAyHapQ.png?q=20', 'https://miro.medium.com/max/3996/1*VszBEU86sub-vmO58sJeRg.png', 'https://miro.medium.com/max/2728/1*3Rw6OuCLrjpk28rr2GH2-A.png', 'https://miro.medium.com/max/60/1*QCCr4lzSXBSi-YGZKGZHQg.jpeg?q=20', 'https://miro.medium.com/max/60/1*1z0DeYDd_uhkdNw3jE2aWw.png?q=20', 'https://miro.medium.com/max/4008/1*ZPYbuDkPodvY3sNHVL4Cxw.png', 'https://miro.medium.com/max/2752/1*VJefxhGskx_wu_BiqaNe2g.png', 'https://miro.medium.com/max/60/1*0VAsJ3HDSi4mvDnBJFmyeQ.png?q=20', 'https://miro.medium.com/max/60/1*YFddfJV2mieJ2IQTw4srmQ.png?q=20', 'https://miro.medium.com/max/60/1*M9SeXz21wvhsGS9o5rBQ8w.png?q=20', 'https://miro.medium.com/max/3200/1*nFBKqcUyE5Qzji3AQWl2Xg.png', 'https://miro.medium.com/max/60/1*cb00sUNRtKP2ieBV1pVnkQ.png?q=20', 'https://miro.medium.com/max/60/1*94EI9DpoXnWa6oLHvh14pw.jpeg?q=20', 'https://miro.medium.com/max/3200/1*IP0dEvYryFrLqww5Oxay1g.jpeg', 'https://miro.medium.com/max/3464/1*zkOBQ9Izq28yXCANTmdKtA.png', 'https://miro.medium.com/max/2732/1*JmVoAoQ6opm_q669_-CV9A.png', 'https://miro.medium.com/max/60/1*nLiB_BSY7uCIGbJIZULeRg.png?q=20', 'https://miro.medium.com/max/3100/1*m5--PCQa5gvU0Z7e8d-E2A.png', 'https://miro.medium.com/max/60/1*Xsb11fW8GluAt0tiYi2uuQ.png?q=20', 'https://miro.medium.com/max/3036/1*sFyXY3IVUFBGNCdmIA-11A.png', 'https://miro.medium.com/max/3200/1*1z0DeYDd_uhkdNw3jE2aWw.png', 'https://miro.medium.com/max/60/1*JmVoAoQ6opm_q669_-CV9A.png?q=20', 'https://miro.medium.com/max/60/1*8f8-JeLOAdgLWcIQglzoGQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/6370/1*HDGxQ2SgZng8DZ6Mzp0meQ.jpeg', 'https://miro.medium.com/max/3200/1*2Zs3yqlYflf4uqssMNDTcg.png', 'https://miro.medium.com/max/60/0*ep1_h3aZgY5MKFsm?q=20', 'https://miro.medium.com/max/2904/1*HjG9oKTtPtULEim4t2p5bQ.png', 'https://miro.medium.com/max/60/1*ZPYbuDkPodvY3sNHVL4Cxw.png?q=20', 'https://miro.medium.com/max/60/1*3Rw6OuCLrjpk28rr2GH2-A.png?q=20', 'https://miro.medium.com/max/60/1*VDCraEnXZtx-Q1BBEvSVRQ.jpeg?q=20', 'https://miro.medium.com/max/4028/1*2r_qbs-dhkKuqv8UW9Ih_A.png', 'https://miro.medium.com/max/2904/1*0A4M301ZwWzn1JyGAyHapQ.png', 'https://miro.medium.com/max/60/1*2Zs3yqlYflf4uqssMNDTcg.png?q=20', 'https://miro.medium.com/max/60/1*4y8n5BohMTpqR_xhLjyizQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*CoigUy974OVb4AGk2PwMrQ.png?q=20', 'https://miro.medium.com/max/3200/1*4y8n5BohMTpqR_xhLjyizQ.jpeg', 'https://miro.medium.com/max/2800/1*Xsb11fW8GluAt0tiYi2uuQ.png', 'https://miro.medium.com/max/60/1*sFyXY3IVUFBGNCdmIA-11A.png?q=20', 'https://miro.medium.com/max/60/1*nFBKqcUyE5Qzji3AQWl2Xg.png?q=20', 'https://miro.medium.com/max/2800/1*0aDOHM18NWdeRx3JmVUxlw.jpeg', 'https://miro.medium.com/max/60/1*cglnX3JI1XcBxik_IPjzqw.png?q=20', 'https://miro.medium.com/max/3200/1*cglnX3JI1XcBxik_IPjzqw.png', 'https://miro.medium.com/max/2400/1*CoigUy974OVb4AGk2PwMrQ.png', 'https://miro.medium.com/fit/c/96/96/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/60/1*2r_qbs-dhkKuqv8UW9Ih_A.png?q=20', 'https://miro.medium.com/max/60/1*HO4qzrqEuCb4wZ1-NW9n4Q.png?q=20', 'https://miro.medium.com/max/60/1*pzW16xJ4_U_Qsjlw1t8F0Q.png?q=20', 'https://miro.medium.com/max/2620/1*7EMqIf41obEaRAywBEMWpQ.png', 'https://miro.medium.com/max/2132/1*94EI9DpoXnWa6oLHvh14pw.jpeg', 'https://miro.medium.com/max/3600/1*qF_agjFzPMwYfafsVJMo9A.png', 'https://miro.medium.com/max/60/1*FyHy0PM_Tj3UKvlpbw_GWQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*9CVaS8Yn30i5wb8sZ91oEA.jpeg', 'https://miro.medium.com/max/60/1*PRYEapEDt0A4Uh8iFN_fxw.png?q=20', 'https://miro.medium.com/max/3700/1*pzW16xJ4_U_Qsjlw1t8F0Q.png', 'https://miro.medium.com/max/2400/1*M9SeXz21wvhsGS9o5rBQ8w.png', 'https://miro.medium.com/max/3200/1*0VAsJ3HDSi4mvDnBJFmyeQ.png', 'https://miro.medium.com/max/60/1*7EMqIf41obEaRAywBEMWpQ.png?q=20', 'https://miro.medium.com/max/60/1*wZU7sKpHjqCGZYNw9RNSsQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*HjG9oKTtPtULEim4t2p5bQ.png?q=20', 'https://miro.medium.com/max/3000/1*EWuvBxscWd7cJlYC7oXLWA.png', 'https://miro.medium.com/max/60/1*VJefxhGskx_wu_BiqaNe2g.png?q=20', 'https://miro.medium.com/max/60/1*dSO_ST85b_VdaghwBrX9TA.png?q=20', 'https://miro.medium.com/max/2280/1*VDCraEnXZtx-Q1BBEvSVRQ.jpeg', 'https://miro.medium.com/max/3400/1*cb00sUNRtKP2ieBV1pVnkQ.png', 'https://miro.medium.com/max/60/1*0aDOHM18NWdeRx3JmVUxlw.jpeg?q=20', 'https://miro.medium.com/max/2800/1*5xSqPc0ekeL9XLpPKq1BIw.png', 'https://miro.medium.com/max/3600/1*dSO_ST85b_VdaghwBrX9TA.png', 'https://miro.medium.com/max/2800/1*HO4qzrqEuCb4wZ1-NW9n4Q.png', 'https://miro.medium.com/max/60/1*5xSqPc0ekeL9XLpPKq1BIw.png?q=20', 'https://miro.medium.com/max/3200/1*PRYEapEDt0A4Uh8iFN_fxw.png', 'https://miro.medium.com/max/2800/1*FyHy0PM_Tj3UKvlpbw_GWQ.png', 'https://miro.medium.com/max/3200/1*4Dl4KD6Hk3dqrAPcv-1zTw.png', 'https://miro.medium.com/max/60/1*EWuvBxscWd7cJlYC7oXLWA.png?q=20', 'https://miro.medium.com/max/60/1*DkFVqjX4CVnSNjjcfIOKXw.png?q=20', 'https://miro.medium.com/max/60/1*VszBEU86sub-vmO58sJeRg.png?q=20', 'https://miro.medium.com/max/3036/1*QCCr4lzSXBSi-YGZKGZHQg.jpeg', 'https://miro.medium.com/max/60/1*zkOBQ9Izq28yXCANTmdKtA.png?q=20', 'https://miro.medium.com/max/2904/1*DkFVqjX4CVnSNjjcfIOKXw.png', 'https://miro.medium.com/max/3800/1*nLiB_BSY7uCIGbJIZULeRg.png', 'https://miro.medium.com/max/11706/0*ep1_h3aZgY5MKFsm', 'https://miro.medium.com/max/60/1*IP0dEvYryFrLqww5Oxay1g.jpeg?q=20', 'https://miro.medium.com/max/3200/1*YFddfJV2mieJ2IQTw4srmQ.png', 'https://miro.medium.com/fit/c/80/80/2*GljF8L_ld119qC4OywbOHg.png'}",2020-03-05 00:08:53.080496,3.109006643295288
https://towardsdatascience.com/netflix-open-sources-polynote-to-make-data-science-notebooks-better-8d6820535b25,Netflix Open Sources Polynote to Make Data Science Notebooks Better,"Netflix Open Sources Polynote to Make Data Science Notebooks Better

The new notebook environment provides substantial improvements to streamline experimentation in machine learning workflows.

Notebooks are the data scientist best friend and can also be a nightmare to work with. For someone accustomed to work with modern integrated develop environments(IDEs), working with notebooks feels like going back decades. Furthermore, modern notebook environments is mostly constrained to Python programs and lack first-class support for other programming languages. A few days ago, Netflix open sourced Polynote, a new notebook environment that addresses some of those challenges.

Polynote was born out of the necessity to accelerate data science experimentation at Netflix. Over the years, Netflix has built a world-class machine learning platform mostly based on JVM languages like Scala. The support for those languages in mainstream notebook technologies such as Jupyter is fundamentally basic so they needed a better solutions. Polynote was initiated by that basic requirement but incorporated the lessons learned building one of the most ambitious notebook-based experimentation platforms in the data science world.

Inside Netflix’ Notebook Drive Architecture

Over the last few years, Netflix has transformed its use of data science notebooks from an experimentation artifact to a key component of the lifecycle of machine learning solutions. Initially, Netflix adopted Jupyter Notebooks like a data exploration and analysis tools. However, the engineering team quickly realized that Jupyter offered tangible advantages in terms of runtime abstraction, extensibility, interpretability of the code and debugging that could have a major impact in data science workloads if used correctly. In order to expand the use of Jupyter as a data science runtime, the Netflix team needed to solve a few major challenges:

· The Code-Output Mismatch: Notebooks are frequently changed and, many times, the output you are seeing in the environment does not correspond to the current code.

· The Server Requirement: Notebooks typically require a Notebook server runtime to run which represents an architecture challenge when adopted at scale.

· Scheduling: Most data science models need to be executed on a periodic basics but the tools for scheduling Notebooks are still fairly limited.

· Parametrizing: Notebooks are fairly static code-environments and the processes for passing input parameters are far from trivial.

· Integration Testing: Notebooks are isolated code- environments which notoriously difficult to integrate with other Notebooks. As a result, tasks like integration testing become a nightmare when using Notebooks.

To address those requirements, Netflix built a very ambitious architecture that enable the operationalization of Jupyter notebooks. The initial implementation included technologies such as Papermill which enables the parametrization of notebooks.

While the initial notebook architecture at Netflix was certainly ambitious, it was also constrained Python programs. Now it was time to expand.

Entering Polynote

Polynote is a multi-language notebook experimentation environment. In addition to Python, the current release supports languages such as SQL, Vega(visualizations) and, of course, Scala. The platform is also integrated with data science infrastructures such as Apache Spark. At its core, Polynote includes the following capabilities:

a) Improved Editing Experience: Polynote tries to enable an editing experience closer to modern IDEs.

b) Multi-Language Support: Polynote introduces first-class support for Scala and other languages used in data science environmenhts.

c) Data Visualization Improvements: Polynote integrates native data visualizations into notebooks’ dataset without the need of adding a lot of code.

d) Configuration and Dependency Management: Languages like Scala require complex package dependencies in its programs. Polynote saves the package dependency configuration within the notebook itself addressing some of the common challenges in this area experienced by JVM developers.

e) Reproducibility: The combination of code, data and execution results into a single document makes notebooks powerful, but also difficult to reproduce. Polynote includes reproducibility as a first-class capability of the framework.

Improved Editing Experience

Polynote includes common features in IDEs such as code auto-completion or syntax error highlighting which improves the experience for data scientists and researchers building Notebooks. More of the editing capabilities are powered by the Monaco editor which powers the experience of Visual Studio Code.

Multi-Language Support

Polynote does not only provide support for multiple languages but it also allows those languages to be combined in a single program. In Polynote, every cell can be based on a different language. When a cell is run, the kernel provides the available typed input values to the cell’s language interpreter. In turn, the interpreter provides the resulting typed output values back to the kernel. This allows cells in Polynote notebooks to operate within the same context. The example below shows a Python library, to compute an isotonic regression of a dataset generated with Scala.

Data Visualization Improvements

Data visualizations are a common component of most notebook environment. However, Polynote takes the visualization value proposition to another level by including it as a native component of the platform which does not require developers to write any code in order to visually explore a dataset.

Configuration and Dependency Management

Most of the time, data scientists working on notebooks can enjoy the efficiency of Python’s package management model to handle the dependencies of a program. However, in JVM-languages like Scala dependency management can become a total night mare. Polynote addresses that challenge by storing the configuration and dependency information directly in the notebook itself, rather than relying on external files. Additionally, Polynote provides a user-friendly Configuration section where users can set dependencies for each notebook.

Reproducibility

With Polynote, Netflix a new code interpretation block instead of relying on a REPL model like a traditional notebook. One of the key capabilities of the new interpretation model is that it removes hidden states which allows data scientists to copy cells within a notebook without introducing any state from the previous position.

Polynote is a new release in the ambitious competitive of data science notebooks but one that stands in its own merits. The support for JVM-based languages could make Polynote a favorite of developers working on Spark infrastructures. Also the editing and reproducatility capabilities are definitely welcomed enhancements to traditional notebook environments. Polynote is available in Github and you can also follow the project’s website.","['polynote', 'code', 'sources', 'scala', 'science', 'notebook', 'better', 'data', 'languages', 'support', 'netflix', 'notebooks', 'open']","Netflix Open Sources Polynote to Make Data Science Notebooks BetterThe new notebook environment provides substantial improvements to streamline experimentation in machine learning workflows.
Polynote was born out of the necessity to accelerate data science experimentation at Netflix.
Inside Netflix’ Notebook Drive ArchitectureOver the last few years, Netflix has transformed its use of data science notebooks from an experimentation artifact to a key component of the lifecycle of machine learning solutions.
b) Multi-Language Support: Polynote introduces first-class support for Scala and other languages used in data science environmenhts.
Polynote is a new release in the ambitious competitive of data science notebooks but one that stands in its own merits.",en,['Jesus Rodriguez'],2019-10-24 13:44:56.497000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Invector Labs', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*dIdayOOAAJ9ItdKmtkQnww.png?q=20', 'https://miro.medium.com/max/3200/1*9zxfagBBKrbKSCROsB8-Aw.png', 'https://miro.medium.com/freeze/max/60/1*Wzz9Uvn7UHW4Nf0KDSlN4Q.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/freeze/max/60/1*eCHyvwcG-yjGpRdETtd2Mg.gif?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*9hmRpqiPP9vEjlGS2AJnaw.jpeg', 'https://miro.medium.com/max/1600/1*Wzz9Uvn7UHW4Nf0KDSlN4Q.gif', 'https://miro.medium.com/max/60/1*9zxfagBBKrbKSCROsB8-Aw.png?q=20', 'https://miro.medium.com/max/60/1*03PYOXkjCRp1FEmU2epwYQ.png?q=20', 'https://miro.medium.com/max/60/1*k1N50ELecQ-RcxSom2siXA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/1*9hmRpqiPP9vEjlGS2AJnaw.jpeg', 'https://miro.medium.com/max/1600/1*eCHyvwcG-yjGpRdETtd2Mg.gif', 'https://miro.medium.com/max/1200/1*03PYOXkjCRp1FEmU2epwYQ.png', 'https://miro.medium.com/max/2490/1*03PYOXkjCRp1FEmU2epwYQ.png', 'https://miro.medium.com/max/1600/1*k1N50ELecQ-RcxSom2siXA.png', 'https://miro.medium.com/max/568/1*dIdayOOAAJ9ItdKmtkQnww.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1920/1*QSm4UtxbRfkm0T5eR5s40Q.gif', 'https://miro.medium.com/freeze/max/60/1*QSm4UtxbRfkm0T5eR5s40Q.gif?q=20'}",2020-03-05 00:08:59.845276,6.763788938522339
https://medium.com/tribalscale/why-are-you-still-doing-batch-processing-etl-is-dead-62e75d47ecb9,Why are you still doing batch processing? “ETL is dead”,"By Bill Scott

ETL: Rest In Peace

It was about a year ago that a few colleagues suggested I research Apache Kafka for an application that I was designing. I watched the re-run video from QCon 2016 titled “ETL is Dead; Long Live Streams.” In that video, Neha Narkhede (CTO of Confluent), describes the concept of replacing ETL batch data processing with messaging and micro-services. It took some time for the paradigm to really sink in, but after designing and writing an event-sourced data streaming system, I can say that I am a believer. I will describe the difference between ETL batch processing and a data streaming process.

Every company is still doing batch processing, it’s just a fact of life. A file of data is received, it must be processed: parsed, validated, cleansed, calculated, organized, aggregated, then eventually delivered to some downstream system. Most companies are using some sort of workflow tool such as Microsoft Integration Services or Informatica; tools that can intuitively wrap these tasks into a scheduled “package” contained on a single server.

M odern business demands are outgrowing the paradigm of “batch processing,” most data can’t wait for a “batch” to be scheduled, analysts want that data now, and downstream systems want an instant response.

In many cases, batch data has grown so large that these ETL tools just can’t process the data fast enough.

What is data streaming? We can think of data streaming (or data flow) as the “Henry Ford” of the batch processing. It breaks the “batch” process into individual applications that perform a single action, and data flows between applications using a type of messaging server. The data streaming server is a new class of server: a resilient, low latency, high throughput messaging server that can accept huge volumes of data records, and can publish the records in the form of an event to any application that subscribes to the stream.

Apache Kafka is probably the most popular and proven of this class of emerging technology. Developed by LinkedIn, in response to the throughput limitations of RabbitMQ, Kafka boasts message throughput rates of 2M messages per second. Kafka is one of many tools with similar goals of data streaming: Apache Beam, Apache Flume, Apache Spark Streaming, Apache Pulsar, Amazon Kinesis, and ZeroMQ, to name a few.

Consider this fictional case study:

There are 2 startup companies that make cupcakes: Crusty Cupcakes and Castle Cupcakes.

The Crusty Cupcakes startup introduced a great cupcake product. They have one very talented baker in a specially rigged kitchen that can cook a batch of 1,000 cupcakes per day. The baker collects the ingredients, mixes the recipe, bakes the delicious items, artistically decorates them, then packages them and hands them off to the delivery person.

Crusty’s Custom Kitchen, the baker does everything.

Crusty Cupcakes are a hit! The demand grows to 2,000 cupcakes per day!

The CEO decides to add 1 more baker, but the kitchen is customized for only 1 baker at a time so they must also add a new kitchen… It’s hard to find a talented baker with the skills required but they finally find one.

Demand grows again to 4,000 cupcakes per day. Crusty Cupcakes is going strong. They decide they must add 2 more bakers and 2 more kitchens.

Crusty must keep adding kitchens and bakers to scale up to meet demand.

If demand grows to 1,000,000 cupcakes per day, Crusty Cupcakes must have 1,000 bakers and 1,000 kitchens… A very large infrastructure investment for cupcakes. What happens if Crusty creates a new cupcake flavor? You guessed it, new baker and a new kitchen.

Castle Cupcakes is the second startup, they also have a great cupcake product. They decided to plan for growth by using a conveyor belt and job stations.

Castle Cupcakes planned to scale.

Belt 1: Individual measurements of ingredients are set out. Belt 1: Handled by Mixing-Baker. When the ingredients arrive, she knows how to mix the ingredients and then put the mixture onto Belt 2. Belt 2: The perfectly whisked mixture is delivered. Belt 2: Handled by Pan-Pour-Baker. When the mixture arrives, she can delicately measure and pour the mixture into the pan and then puts the pan onto Belt 3. Belt 3: The pan is delivered with the exact measurement of mixture. Belt 3: Handled by Oven-Baker. When the pan arrives, she puts the pan in the oven and waits the specific amount of time until it’s done. When it’s done, she puts the cooked item on the next belt. Belt 4: The cooked item is delivered. Belt 4: Handled by Decorator. Wen the cooked item arrives, she applies the frosting in an interesting and beautiful way. She then puts it on the next belt. Belt 5: A completely decorated cupcake is delivered. Belt 5: Handled by the Packager. When the decorated cupcake arrives, she collects each cupcake and puts them each into a basket package.

You can see that once the infrastructure is put into place (the belts), you could easily add more bakers to handle extra work (if the belt moves faster). It would also be easy to add a new frosting design or new cupcake flavor by adding different types of belts and bakers.

Castle Cupcakes can speed up the belts and add more workers to increase output, or can add new belts to add new flavors.

OK, but how does that relate to ETL processing?

The Crusty Cupcakes’ approach resembles the old-world ETL approach, a workflow of tasks is wrapped in a rigid environment. It is easy to create, will work for smaller batches, but is hard to grow on demand. Scaling this monolithic process requires “sharding” or copying the entire process and running it in parallel in a separate environment.

The Castle Cupcakes’ approach resembles data streaming: using Kafka, the belts represent a “topic” or a stream of similar data items. The “bakers” are applications (consumers) subscribing to events raised on the “topic.” The consumers are simple tasks performed on the data. When that task is complete, it can add the resulting item(s) to the other topic(s) in the workflow sequence. The system can scale at any point just by adding more consumers, and is Agile as it adds different consumers or topics.

Castle Cupcakes is approaching their business in a “cupcake first” model, which allows them to respond to change by adding a new baker at any step of the process. Let’s say they want to add a new frosting design, they just add a new “Decorator.” Crusty Cupcakes is trapped in a “batch first” model, where the process is more important than the cupcakes. Crusty Cupcakes was an easy start, but harder to grow.

The moral of the story: Don’t be Crusty

Originally published at medium.com on March 26, 2019.","['baker', 'etl', 'doing', 'batch', 'cupcake', 'crusty', 'process', 'belt', 'streaming', 'data', 'dead', 'cupcakes', 'add', 'processing']","I watched the re-run video from QCon 2016 titled “ETL is Dead; Long Live Streams.” In that video, Neha Narkhede (CTO of Confluent), describes the concept of replacing ETL batch data processing with messaging and micro-services.
I will describe the difference between ETL batch processing and a data streaming process.
In many cases, batch data has grown so large that these ETL tools just can’t process the data fast enough.
We can think of data streaming (or data flow) as the “Henry Ford” of the batch processing.
Consider this fictional case study:There are 2 startup companies that make cupcakes: Crusty Cupcakes and Castle Cupcakes.",en,['Tribalscale Inc.'],2019-04-02 20:09:49.939000+00:00,"{'Digital Transformation', 'Software Development', 'Development', 'Etl', 'Batch Processing'}","{'https://miro.medium.com/fit/c/96/96/1*Soary0hbIwjq6asph_9LFQ.jpeg', 'https://miro.medium.com/proxy/1*nEQB3rVuy90bX0xIarh2Bw.png', 'https://miro.medium.com/fit/c/80/80/1*TQuev-t8CMka_pDxlPFHlQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*Soary0hbIwjq6asph_9LFQ.jpeg', 'https://miro.medium.com/fit/c/80/80/0*RE5AOb7ZQpKv0Jgb', 'https://miro.medium.com/fit/c/160/160/1*9q0wyiYEBD_HH277kqH42Q.jpeg', 'https://miro.medium.com/max/600/1*S6GSGRNSOSSvsaJutNxLRA.png', 'https://miro.medium.com/proxy/1*u8GtxOjEAbw_sMP1X4MBOw.png', 'https://miro.medium.com/fit/c/80/80/1*mm1eWBY4TZbncymiNckSqw.jpeg', 'https://miro.medium.com/proxy/1*fnzlGrZAEKuJJtkMTO94fw.png', 'https://miro.medium.com/max/1200/1*nEQB3rVuy90bX0xIarh2Bw.png', 'https://miro.medium.com/proxy/1*0UaYJd5l7fMnzug0PnTQ9g.png', 'https://miro.medium.com/proxy/1*4X27SgOkDctbefVknm5HdA.png', 'https://miro.medium.com/proxy/1*elyMbehr-k-lQSKYrXIiRA.png'}",2020-03-05 00:09:01.261882,1.4166061878204346
https://towardsdatascience.com/a-quick-introduction-to-tensorflow-2-0-for-deep-learning-e740ca2e974c,A Quick Introduction to TensorFlow 2.0 for Deep Learning,"After much community hype and anticipation, TensorFlow 2.0 was finally released by Google on September 30, 2019.

TensorFlow 2.0 represents a major milestone in the library’s development. Over the past few years, one of TensorFlow’s main weaknesses, and a big reason many people switched over to PyTorch, was its very complicated API.

Defining deep neural networks required far more work than was reasonable. This led to the development of several high-level APIs that sat on-top of TensorFlow including TF Slim and Keras.

Now things have come full circle as Keras will be the official API of TensorFlow 2.0. Loading data, defining models, training, and evaluating are all now much easier to do, with cleaner Keras style code and faster development time.

This article will be a quick introduction to the new TensorFlow 2.0 way of doing Deep Learning using Keras. We’ll go through an end-to-end pipeline of loading our dataset, defining our model, training, and evaluating, all with the new TensorFlow 2.0 API. If you’d like to run the entire code yourself, I’ve set up a Google Colab Notebook with the whole thing!

Import and Setup

We’ll start off by importing TensorFlow, Keras, and Matplotlib. Notice how we pull our Keras directly from TensorFlow using tensorflow.keras , as it’s now bundled right within it. We also have an if statement to install version 2.0 in case our notebook is running an older version.

Next, we’ll load up our dataset. For this tutorial, we’re going to use the MNIST dataset which contains 60,000 training images and 10,000 test images of digits from 0 to 9, size 28x28. It’s a pretty basic dataset that used all the time for quick tests and PoCs. There’s also some visualization code using Matplotlib so we can take a look at the data.

Visualizing MNIST digits

Creating a Convolutional Neural Network for Image Classification

The best way to do image classification is of course to use a Convolutional Neural Network (CNN). The tensorflow.keras.layers API will have everything we need to build such a network. Since MNIST is quite small — images of size 28x28 and only 60,000 training images — we don’t need a super huge network, so we’ll keep it simple.

The formula for building a good CNN has largely remained the same over the past few years: stack convolution layers (typically 3x3 or 1x1) with non-linear activations in-between (typically ReLU), add a couple of fully connected layers and a Softmax function at the very end to get the class probabilities. We’ve done all of that in the network definition below.

Our model has a total of 6 convolutional layers with a ReLU activation after each one. After the convolutional layers, we have a GlobalAveragePooling to get our data into a dense vector. We finish off with our fully-connected (Dense) layers, with the last one having a size of 10 for the 10 classes of MNIST.

Again, notice how all of our model layers come right from tensorflow.keras.layers and that we’re using the functional API of Keras. With the functional API, we build our model as a series of sequential functions. The first layer takes the input image as an input variable. Following that, each subsequent layer takes the output of the previous layer as its input. Our model.Model() simply connects the “pipeline” from the input to the output tensors.

For a more detailed description of the model, check out the print out of model.summary() down below.

Model: ""model_1"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d_12 (Conv2D) (None, 28, 28, 32) 320 _________________________________________________________________ activation_16 (Activation) (None, 28, 28, 32) 0 _________________________________________________________________ conv2d_13 (Conv2D) (None, 14, 14, 32) 9248 _________________________________________________________________ activation_17 (Activation) (None, 14, 14, 32) 0 _________________________________________________________________ conv2d_14 (Conv2D) (None, 14, 14, 64) 18496 _________________________________________________________________ activation_18 (Activation) (None, 14, 14, 64) 0 _________________________________________________________________ conv2d_15 (Conv2D) (None, 7, 7, 64) 36928 _________________________________________________________________ activation_19 (Activation) (None, 7, 7, 64) 0 _________________________________________________________________ conv2d_16 (Conv2D) (None, 7, 7, 64) 36928 _________________________________________________________________ activation_20 (Activation) (None, 7, 7, 64) 0 _________________________________________________________________ conv2d_17 (Conv2D) (None, 7, 7, 64) 36928 _________________________________________________________________ activation_21 (Activation) (None, 7, 7, 64) 0 _________________________________________________________________ global_average_pooling2d_2 ( (None, 64) 0 _________________________________________________________________ dense_4 (Dense) (None, 32) 2080 _________________________________________________________________ activation_22 (Activation) (None, 32) 0 _________________________________________________________________ dense_5 (Dense) (None, 10) 330 _________________________________________________________________ activation_23 (Activation) (None, 10) 0 ================================================================= Total params: 141,258 Trainable params: 141,258 Non-trainable params: 0 _________________________________________________________________

Training and Testing

Here comes the best part: training and getting our actual results!

First off, we’ll need to do a bit of data preprocessing to have the data properly formatted for training. Our training images need to be in an array of 4 dimensions with the format of:

(batch_size, width, height, channels)

We convert the images to type of float32, a requirement for proper training, and normalize such that each pixel has a value between 0.0 and 1.0

As for the labels, since we are using Softmax activation, we’ll want our target output to be in the form of one-hot encoded vectors. To do so, we use the tf.keras.utils.to_categorical() function. The second variable in the function is set to 10 since we have 10 classes.

We select Adam as our optimizer of choice — it’s super easy to use and works well out of the box. We set the loss function to be categorical_crossentropy which is compatible with our Softmax. Training the CNN is then as easy as calling the Keras .fit() function with our data as input!

Notice how all of this is almost purely Keras. Really the only difference is that we are using the Keras library from TensorFlow, i.e tensorflow.keras . It’s incredibly convenient as it comes in one nice package — the power of TensorFlow with the ease of Keras. Brilliant!

MNIST is an easy dataset so our CNN should reach high accuracy quite quickly. In my own experiments, it got to about 97% within 5 epochs.

Once training is complete, we can plot the history of the loss and accuracy. Once again, we use pure Keras code to pull the loss and accuracy information from the history. Matplotlib is used for easy plotting.","['introduction', '20', 'tensorflow', '64', 'quick', '14', 'learning', 'keras', 'activation', 'deep', 'layers', 'images', 'training', 'using', '_________________________________________________________________']","This article will be a quick introduction to the new TensorFlow 2.0 way of doing Deep Learning using Keras.
Import and SetupWe’ll start off by importing TensorFlow, Keras, and Matplotlib.
Notice how we pull our Keras directly from TensorFlow using tensorflow.keras , as it’s now bundled right within it.
For this tutorial, we’re going to use the MNIST dataset which contains 60,000 training images and 10,000 test images of digits from 0 to 9, size 28x28.
Since MNIST is quite small — images of size 28x28 and only 60,000 training images — we don’t need a super huge network, so we’ll keep it simple.",en,['George Seif'],2019-11-13 02:35:48.706000+00:00,"{'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Programming', 'Technology'}","{'https://miro.medium.com/fit/c/160/160/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/790/1*sfgwR_WkxeyEougFGIbFMg.png', 'https://miro.medium.com/max/778/1*brKKDMYK3xQnJe2fCrbvjQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/0*yz4OzmTYGW6M-UKV.jpg', 'https://miro.medium.com/max/60/1*sfgwR_WkxeyEougFGIbFMg.png?q=20', 'https://miro.medium.com/max/600/0*yz4OzmTYGW6M-UKV.jpg', 'https://miro.medium.com/max/60/1*YVKBAbXQR7rcDMZ1HBGhGw.png?q=20', 'https://miro.medium.com/max/60/0*yz4OzmTYGW6M-UKV.jpg?q=20', 'https://miro.medium.com/fit/c/96/96/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/max/1152/1*YVKBAbXQR7rcDMZ1HBGhGw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*brKKDMYK3xQnJe2fCrbvjQ.png?q=20'}",2020-03-05 00:09:08.716786,7.454903841018677
https://medium.com/swlh/building-an-artificial-neural-network-in-less-than-10-minutes-cbe59dbb903c,Deep Learning: Predicting Customer Churn,"In this ANN model that we’ll be looking at, I used the Rectifier and the Sigmoid function. How did I use both? Here’s the intuition:

Since my output variable is binary, I use the Rectifier function to classify that in my hidden layers, and then I use the Sigmoid function to determine the probability of whether the output will 1 or 0.

The output value and the predicted value will generally be differentiated by a cost function (error).

The goal is to minimize the loss function (cost) since this would bring the predicted value closer to the actual value. This is usually done by changing the weights of the input variables. Sometimes it can take a lot of time and computational power to calculate the actual or global cost function, and it makes sense to use a gradient descent approach to make this process much faster.

A Gradient Descent takes the lowest point of the cost function

A Gradient descent uses the slope of a loss function at a certain point and tries to move downwards to find the lowest point of the function. However, if my function is not convex (with higher degrees freedom), I could end up at a local minimum rather than the global minimum of the function, and the network wouldn’t be as efficient.

Therefore, I use the stochastic gradient descent method, which runs the function for each and every row and keeps updating the minimum of the cost function. This way, I have a higher chance of finding the global minimum. It is also actually faster than the gradient function since it is running smaller algorithms.

II. || The problem ||

This is a dataset of a firm’s customers, it’s not a real dataset but resembles real-like data. The aim is to build an ANN to predict whether a customer will leave the company or not given certain demographic characteristics such as age, gender, salary, credit score, whether they are active or not, etc.

A subset of my Dataset for this project

Thus, we can classify this problem as a demographic segmentation model.

Such a model could be used to predict anything, not just customer churn. You could also try to predict things like whether the customer should get a loan, or if the customer is more likely to purchase a new product. The only change would be to relabel the variables so that we’re predicting the right thing.

Below, you can see a workflow of the ANN model I created in this project. This includes all the steps required to build such a model. After you learn it, you can refer to this diagram to help you remember the steps.

III. || Importing Data and Preprocessing ||

As you saw in the previous section, the dataset has 10 Columns (demographic parameters), and 10000 rows (customers). I selected the X (input variables) variables based on their importance in predicting the y variable (output variable), which is the last column “Churn”.

Note: in the “Churn” column, 1 is if a customer left the firm (is no longer a customer) and 0 refers to if they chose to stay with the firm.

For the X values, I chose columns 2–9, ranging from credit score to estimated salary, since these could actually be important predictors for why a customer chose to leave or stay with the firm. The column I excluded was Customer ID (since it wouldn’t have any effect on the output).

[3]: # Importing the libraries

import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

In [4]: # Importing the dataset

df = pd.read_csv(‘Churn_Customers.csv’)

X = df.iloc[:, 1:9].values

y = df.iloc[:, 9].values In [5]: dataset.head(3)

+------+-----+-------+------+---+-------+----+------+------+-----+

|CustID|Score|Country|Gender|Age|Balance|Card|Active|Salary|Churn|

+------+-----+-------+------+---+-------+----+------+------+-----+

| 15634| 619 |France |Female|42 | 0.00 | 1 | 1 |101348| 1 |

+------+-----+-------+------+---+-------+----+------+------+-----+

| 15647| 608 | Spain |Female|41 | 83807 | 0 | 1 |112542| 0 |

+------+-----+-------+------+---+-------+----+------+------+-----+

| 15619| 502 | France|Female|42 |159660 | 1 | 0 |113931| 1 |

+------+-----+-------+------+---+-------+----+------+------+-----+ In [7]: X

Out[7]: array([[619, ‘France’, ‘Female’, …, 1, 1, 101348.88],

[608, ‘Spain’, ‘Female’, …, 0, 1, 112542.58],

[502, ‘France’, ‘Female’, …, 1, 0, 113931.57],

…,

[709, ‘France’, ‘Female’, …, 0, 1, 42085.58],

[772, ‘Germany’, ‘Male’, …, 1, 0, 92888.52],

[792, ‘France’, ‘Female’, …, 1, 0, 38190.78]], dtype=object) In [8]: y

Out[8]: array([1, 0, 1, …, 1, 1, 0])

For mobile users: The code is easier to understand on a desktop. The output of dataset.head(3) is the same as the subset table picture in section II.

X and Y are just arrays of the Predictor and Result variables.

III.i ~ Encoding categorical variables

Photo by Andrew Butler on Unsplash

To intuitively think about it, categorical variables are inherently different from numerical variables. However, Python will read a categorical variable as a string and will exclude such variables in making calculations.

Thus, we need to encode these variables as numerical in order for the machine to understand and use them in the model.

In this model, There are only 2 categorical variables:

- Country (France, Spain, Germany)

- Gender (Male or Female).

Below, I use two classes from sklearn: —

Labelencoder : Which converts the string into a numerical label as 0, 1, 2 etc. The way it works is that it creates separate columns for each variable and assigns them a value of 0 or 1 to denote whether they are male/female or not (0 = no, 1 = yes)

: Which converts the string into a numerical label as 0, 1, 2 etc. The way it works is that it creates separate columns for each variable and assigns them a value of 0 or 1 to denote whether they are male/female or not (0 = no, 1 = yes) OneHotEncoder : I use this class to tell the machine that my categorical variables are not ordered. In some cases, if you would encode something like “Large, Medium, Small”, those would be ordered categories. However, in this case, the categories follow no order and I have to specify that using the OneHotEncoder .

Note: it is also super important to keep the dummy variable trap in mind. Normally, I would remove one of the columns to avoid getting caught in a dummy variable trap. This usually happens because the machine knows that if X0 = 0, then X1 has to be 1. It is a form of redundancy, and so to make the model better suited, I’ll add one line of code to remove the first column.

In [9]: # Encoding the Independent Variables

from sklearn.preprocessing import LabelEncoder, OneHotEncoder # This line is for encoding the Geography

labelencoder_X_1 = LabelEncoder()

X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) # This line is for encoding the Gender

labelencoder_X_2 = LabelEncoder()

X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) #one-hot encoding the columns

onehotencoder = OneHotEncoder(categorical_features = [1])

X = onehotencoder.fit_transform(X).toarray() In [10]: X = X[:, 1:] #Avoiding the dummy variable trap

In [11]: X[:, 0:10]

Out[11]: array([[ 0., 0., 619., …, 1., 1., 1.],

[ 0., 1., 608., …, 1., 0., 1.],

[ 0., 0., 502., …, 3., 1., 0.],

…,

[ 0., 0., 709., …, 1., 0., 1.],

[ 1., 0., 772., …, 2., 1., 0.],

[ 0., 0., 792., …, 1., 1., 0.]])

III.ii ~ Splitting the Data into Train and Test sets

The lines of code below show the shape of X_train , X_test , y_train , and y_test . As you can see, the data was split with a test size of 0.2, which means that Training sets have 8000 points of data, while Test sets have 2000.

In [12]: from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) In [13]: X_train.shape #shape of X_train

Out[13]: (8000, 11) In [14]: X_test.shape #shape of X_test

Out[14]: (2000, 11) In [15]: y_train.shape #shape of y_train

Out[15]: (8000,) In [16]: y_test.shape #shape of y_test

Out[16]: (2000,)

III.iii ~ Feature Scaling

Photo by James Pond on Unsplash

Now that the data has been fit into training and test sets, I will feature scale the data manually, so that the machine doesn’t anchor on higher values and give us a biased prediction.

For example, Salary is a higher number than age, which can cause the machine to put more weight on Salary in the model. Therefore, we want to scale all values between -1 and 1, so it is comprehensible to the model.

In [17]: from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train = sc.fit_transform(X_train)

X_test = sc.transform(X_test)

IV. || Importing Keras and Libraries for the ANN ||

In this next section, I’m going to import Keras, which is the most important package I need for my ANN. I’ll use two classes that will help us define the ANN throughout the next section: Sequential and Dense . I won’t go into much detail about these classes, as the code will show what they are doing.

Note: This is an old version of Keras that runs with backend Tensorflow 1. So if you installed Tensorflow 2, you can either downgrade or see the documentation to learn about any changes.

In [18]: #Importing Keras & classes

import keras

from keras.models import Sequential

from keras.layers import Dense >>> Using TensorFlow backend.

V. || Building the ANN ||

The following steps will show how I went about building this artificial neural network:

Step 1: I will start by initializing the ANN using the sequential class

Step 2: I will add the input layer along with the first hidden layer.

Step 3: I will add another second hidden layer

Step 4: Now I will add the output layer.

Step 5: After adding the layers, I will compile the ANN model

Step 6: Finally, I will fit the ANN model to the training set. The model will then train itself based on the number of epochs I mention.

Step 7: Evaluation: I will create a predictor variable and a confusion matrix to evaluate the results predicted by the machine and compare them with the actual results.

V.i ~ How to mathematically create an ANN:

Randomly initialize the weights to small numbers close to 0 (but not 0). Input the first observation of your dataset in the input layer. each feature in one input node. Forward-Propagation: from left to right. the neurons are activated in a way that the impact of each neuron’s activation is limited by the weights. Propagate the activations until getting the predicted result y. Compare the predicted result to the actual result. Measure the generated error. Back-Propagation: from right to left, the error is back-propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights. Repeat Steps 1 to 5 and update the weights after each observation (Reinforcement Learning). Or: Repeat Steps 1 to 5 but updates the weights only after a batch of observations (Batch Learning). When the whole training set passed through the ANN. that makes an epoch. Redo more epochs.

V.ii ~ Initialization

Photo by Niels And Marco on Unsplash

So there are actually 2 ways of initializing a model: either with Sequential Layers, like I did above, or the other method is to do it by a graph. The step below is essentially initializing the model as a sequence of layers.

I create the object, Classifier which is basically the Artificial Neural Network that I’m about to build.

In [19]: #Initializing the Artificial Neural Network

classifier = Sequential()

V.iii ~ Adding the input layer and the first hidden layer

In the steps below, I used the add method of the object to include the Dense class in the classifier object. Dense is essentially what is allowing us to create the layers for the model.

Now, upon inspecting the Dense class, I can see there are a number of parameters, but as the mathematical steps above show us, I know already which parameters to input for the model.

So I will use the following for the input layer and the first hidden layer:

• output_dim (output dimensions):

This is simply the number of nodes I want to add in the hidden layer. I had previously learned that there is no right answer to this as experimentation can allow us to choose the right number of nodes, however, in this project, I took the average sum of the number of input and output layers, (8 + 1)/2 = 4.5 and round it off to 5.

• init (random initialization):

This is the first step of the stochastic gradient descent. I need to initialize the weights to small numbers close to 0. The default value for this parameter is given as ""glorot_uniform"" , but for simplification, I will use the ""uniform"" function, which will initialize the weights according to a uniform distribution.

• activation :

As the name suggests, this is the activation function. In the first hidden layer, we want to use the rectifier activation function as I had mentioned in the introduction and that's why I input relu in this parameter.

• input_dim (input dimensions):

This is the number of nodes in the input layer, which I already know is 8.

In [20]: #Adding the input layer and a hidden layer

classifier.add(Dense(output_dim = 5, init = ‘uniform’, activation = ‘relu’,

input_dim = 8))

V.iv ~ Adding the second hidden layer

For this hidden layer, I use the add method on the classifier object again.

Using the dense function, I have a similar line of code, but the only difference is that this time there’s no need to specify the number of input layers since the model already knows how many layers to expect as I have already added the input layer to the model.

In [27]: #Adding second hidden layer

classifier.add(Dense(output_dim = 5, init = ‘uniform’, activation = ‘relu’))

V.v ~ Adding the output layer

Photo by Erik Mclean on Unsplash

The final layer that we need to code into the model is the output layer. This process will again use the same add method with the Dense class.

However, this time the number of nodes is changed to 1 since there is only one binary output variable (1 or 0) in this layer, it will only have 1 node.

The activation function will also change to ‘ Sigmoid ’ since we want to determine the probability that this output function will 1 or 0.

In [21]: #Adding output layer

classifier.add(Dense(output_dim = 1, init = ‘uniform’, activation = ‘sigmoid’)) /Users/rohangupta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=”sigmoid”, units=1, kernel_initializer=”uniform”)`

V.vi ~ Compiling the ANN model

This time I use the Compile method on the classifier object and I input the following parameters:

Optimizer : This the algorithm I want to use to find the optimal set of weights for the ANN model. The model’s layers have been built, but the weights have only been initialized. Therefore, it is important to use an optimizer to find the right combination of weights. Adam is one of the stochastic gradient descent algorithms, and that is the one I will use to find the optimal set of weights for this model.

: This the algorithm I want to use to find the optimal set of weights for the ANN model. The model’s layers have been built, but the weights have only been initialized. Therefore, it is important to use an optimizer to find the right combination of weights. is one of the stochastic gradient descent algorithms, and that is the one I will use to find the optimal set of weights for this model. Loss : This corresponds to the loss function within the Stochastic gradient descent algorithm.

The basic idea of this is that we need to optimize this loss function within the algorithm to find the optimal weights. For example, in linear regression, I use the sum-of-squares loss function to optimize the model. However, for the stochastic gradient descent, I use a logarithmic function known as binary_crossentropy (Binary Cross-Entropy) since we have a binary output layer.

: This corresponds to the loss function within the Stochastic gradient descent algorithm. The basic idea of this is that we need to optimize this loss function within the algorithm to find the optimal weights. For example, in linear regression, I use the sum-of-squares loss function to optimize the model. However, for the stochastic gradient descent, I use a logarithmic function known as (Binary Cross-Entropy) since we have a binary output layer. Metrics : Just the criterion metric I use to evaluate the model. I can use the accuracy model (which sees correct predictions over total predictions). So, I input 'accuracy' in the metrics parameter. Since this is expecting a list, I would have to put it in square brackets.

In [22]: #Compiling the artificial neural network

classifier.compile(optimizer = ‘adam’, loss = ‘binary_crossentropy’,

metrics = [‘accuracy’])

V.vii~ Fitting the ANN model to the training set

Photo by Markus Spiske on Unsplash

Now I will fit the model to the training dataset and will run the model to a certain number of epochs.

I start by using the fit method to fit the classifier model to X_Train and y_train . Then, I add two more parameters, which are the batch size and the number of epochs. If you look back at the beginning of this section, steps 6 and 7 refer to these parameters.

In step 6, we can choose to update the weights after every observation or every batch. So for this step, I’ll use batches of 10 to update the weights.

Step 7 tells us that we need to pass the whole training set to more than just 1 epoch. Epoch refers to one round of the entire dataset going through the ANN. I chose 100 epochs for this as choosing these values can be an experimentative process.

In [23]: #Fitting artifical neural network to the training set

classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100) >>>

Epoch 1/100

8000/8000 [==============================] — 2s 279us/step — loss: 0.4822 — acc: 0.7956

...................................................................

Epoch 98/100

8000/8000 [==============================] — 2s 277us/step — loss: 0.3866 — acc: 0.8412

Epoch 99/100

8000/8000 [==============================] — 2s 245us/step — loss: 0.3829 — acc: 0.8405

Epoch 100/100

8000/8000 [==============================] — 2s 259us/step — loss: 0.3767 — acc: 0.8420

Out[23]: <keras.callbacks.History at 0x1a28680cc0>

VI. || Predicting Results & Evaluating the Model ||

Now the model has already run, and I will create a variable, y_pred to store the machine’s predictions. For this, I used the Predict method on the X_test dataset to get values corresponding to y_test .

In [24]: # Predicting the Test set results

y_pred = classifier.predict(X_test)

y_pred[:10] Out[24]: array([[0.29425734],

[0.2981767 ],

[0.16820437],

[0.03832313],

[0.14470574],

[0.6088282 ],

[0.08877337],

[0.22763023],

[0.18400699],

[0.750462 ]], dtype=float32)

Now, the y_pred variable above shows values between 0 and 1. This is because I used the sigmoid function and the prediction function gives us the probabilities of whether a customer left or not. When in fact, I want binary values such as 0 or 1, True or false, yes or no. These would tell us whether a customer left the firm or chose to stay according to my prediction.

In [25]: #Converting probabilities into a binary result

y_pred = (y_pred > 0.5)

y_pred[:10] Out[25]: array([[False],

[False],

[False],

[False],

[False],

[ True],

[False],

[False],

[False],

[ True]])

The output above makes a lot more sense. I basically used the code y_pred > 0.5 so that the model would tell us if it is true or false depending on whether the probability of the customer leaving was above or below 50%.

Note: It is True if the customer left the firm and False if the customer chose to stay.

VI.i ~ Making the Confusion Matrix

A Confusion Matrix

Finally, I’m going to create a confusion matrix to evaluate the model. This will tell us how many incorrect and correct predictions the model had. The matrix will be a 2X2 box.

The image on the left shows us what the confusion matrix would look like. We get an instant view of the model’s true and false positives and negatives.

In [26]: # Making the Confusion Matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

cm Out[27]: array([[1538, 57],

[ 241, 164]]) In [28]: TP = 164 #True Positives

TN = 1538 #True Negatives

FP = 241 #False Positives

FN = 57 #False Negatives

VI.ii ~ The accuracy, precision, recall and F1 score of the model.

The descriptions for these metrics are given as the following:

Accuracy is as the name goes. It measures the percentage of correct predictions made by the model.

Sum all True predictions and divide by the total number of predictions.

is as the name goes. It measures the percentage of correct predictions made by the model. Sum all True predictions and divide by the total number of predictions. Precision is the closeness of two or more measurements to each other.

Divide True Positives by Total Positives.

is the closeness of two or more measurements to each other. Divide True Positives by Total Positives. Recall is the ratio of correctly predicted positive observations to all observations in the actual class.

Divide True Positives by the sum of True Positives and False Negatives.

is the ratio of correctly predicted positive observations to all observations in the actual class. Divide True Positives by the sum of True Positives and False Negatives. F1 Score is the weighted average of Precision and Recall. F1 Score might be a better measure to use if I need to seek a balance between Precision &Recall, AND if there is an uneven class distribution.

Multiply Precision and Recall, divide the result by the sum of Precision and Recall, and then multiply the final result by 2.

In [29]: #Accuracy

Accuracy = (TP + TN)/(TP + TN + FN + FP)

Accuracy

Out[29]: 0.851 In [30]: #Precision

Precision = TP / (TP + FP)

Precision

Out[30]: 0.4049382716049383 In [31]: #Recall

Recall = TP / (TP + FN)

Recall

Out[31]: 0.7420814479638009 In [32]: #F1 Score

F1_Score = 2 * Precision * Recall / (Precision + Recall)

F1_Score

Out[32]: 0.523961661341853

VI.iii ~ Graphing the Evaluation Metrics (Conclusion)

Finally, I will graph each of the metrics calculated above, so I can visualize how the effective the model really is. You’ll use matplotlib for this, so make sure you have imported matplotlib.pyplot .

In section III, I did it this way: import matplotlib.pyplot as plt

In [33]: Eval_Metrics = [Accuracy, Precision, Recall, F1_Score]

Metric_Names = [‘Accuracy’, ‘Precision’, ‘Recall’, ‘F1 Score’]

Metrics_pos = np.arange(len(Metric_Names))

plt.bar(Metrics_pos, Eval_Metrics)

plt.xticks(Metrics_pos, Metric_Names)

plt.title(‘Accuracy v Precision v Recall v F1 Score of the ANN model’)

plt.show()","['function', 'loss', 'true', 'variable', 'churn', 'customer', 'predicting', 'deep', 'learning', 'model', 'output', 'layer', 'input', 'weights', 'ann']","The goal is to minimize the loss function (cost) since this would bring the predicted value closer to the actual value.
Below, you can see a workflow of the ANN model I created in this project.
I selected the X (input variables) variables based on their importance in predicting the y variable (output variable), which is the last column “Churn”.
• input_dim (input dimensions):This is the number of nodes in the input layer, which I already know is 8.
The activation function will also change to ‘ Sigmoid ’ since we want to determine the probability that this output function will 1 or 0.",en,['Rohan Gupta'],2019-10-09 14:35:07.791000+00:00,"{'Neural Networks', 'Deep Learning', 'Data Science', 'Predictions', 'Machine Learning'}","{'https://miro.medium.com/max/5998/1*7wN5t9ILU0fpnhbMX2vtng.jpeg', 'https://miro.medium.com/max/60/0*ayQC9rQeWzN3OEIp?q=20', 'https://miro.medium.com/max/60/1*7jKV29t1B30wcFkqaN94Og.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*Xd2uZaVHfrGOP14W_3UQRg.jpeg', 'https://miro.medium.com/max/10912/0*ayQC9rQeWzN3OEIp', 'https://miro.medium.com/max/1958/1*zQ5X5aqP7t2AZCTxGzrkYg.png', 'https://miro.medium.com/max/60/0*XE1td0vbTyHmMlgB?q=20', 'https://miro.medium.com/max/1250/1*LHpUtKeKvwjmXAErwE7Xpw.png', 'https://miro.medium.com/fit/c/80/80/1*iUOmQH3pEEytA5Mwwuclxg.jpeg', 'https://miro.medium.com/freeze/max/60/1*UI6iybVcYfapg8IvZbNLuA.gif?q=20', 'https://miro.medium.com/fit/c/96/96/2*Nsp9KMZowVMIn1Lff8x6kg.png', 'https://miro.medium.com/max/60/1*zQ5X5aqP7t2AZCTxGzrkYg.png?q=20', 'https://miro.medium.com/max/422/1*IOJrKVmLnRcFz3E_KrrN_Q.png', 'https://miro.medium.com/max/6528/0*HUcVIwUaSVHJPH7f', 'https://miro.medium.com/max/60/1*vNytNnS3Y9hBtnw3lzamqw.jpeg?q=20', 'https://miro.medium.com/freeze/max/60/1*6UTv7y8nwQQSwZI9TXCj2A.gif?q=20', 'https://miro.medium.com/max/1336/1*UI6iybVcYfapg8IvZbNLuA.gif', 'https://miro.medium.com/max/1762/1*7jKV29t1B30wcFkqaN94Og.png', 'https://miro.medium.com/max/60/1*B-LOoowO0Br7o1bW4CVipA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*D4KAER7h6bCkTU-m7Aw9DA.jpeg', 'https://miro.medium.com/max/60/1*LHpUtKeKvwjmXAErwE7Xpw.png?q=20', 'https://miro.medium.com/max/1200/1*7wN5t9ILU0fpnhbMX2vtng.jpeg', 'https://miro.medium.com/max/10544/0*XE1td0vbTyHmMlgB', 'https://miro.medium.com/max/60/1*7wN5t9ILU0fpnhbMX2vtng.jpeg?q=20', 'https://miro.medium.com/max/11520/0*UC622UZO3kgavKlz', 'https://miro.medium.com/max/60/0*oLShH19ZnbXGqTKk?q=20', 'https://miro.medium.com/max/12000/0*oLShH19ZnbXGqTKk', 'https://miro.medium.com/max/40/0*HUcVIwUaSVHJPH7f?q=20', 'https://miro.medium.com/max/60/0*UC622UZO3kgavKlz?q=20', 'https://miro.medium.com/fit/c/80/80/1*vTG13sWBDAFir-PlEvsg9w.jpeg', 'https://miro.medium.com/max/1992/1*vNytNnS3Y9hBtnw3lzamqw.jpeg', 'https://miro.medium.com/max/2018/1*B-LOoowO0Br7o1bW4CVipA.png', 'https://miro.medium.com/max/1200/1*6UTv7y8nwQQSwZI9TXCj2A.gif', 'https://miro.medium.com/fit/c/160/160/2*Nsp9KMZowVMIn1Lff8x6kg.png'}",2020-03-05 00:09:10.377848,1.6604118347167969
https://towardsdatascience.com/just-keep-guessing-the-power-of-the-monte-carlo-method-f06bc6f33d19,Just Keep Guessing: The Power of the Monte Carlo Method,"The Monte Carlo method is an incredibly powerful tool used in a wide variety of fields. From mathematics to science to finance, the Monte Carlo method can be used to solve a variety of unique and interesting problems.

The idea of the Monte Carlo method is farily stright forward. It relies on a large amount of repeated random sampling to obtain numerical results, and takes the average of all of these results to find the final answer to the problem.

Let’s attempt to find the numerical solution to an integral using the Monte Carlo method. Before we begin, it should be noted that Monte Carlo simulations are not optimal for single variable functions. They do their best work when solving multi variable functions, but for the sake of clarity and simplicity, we’ll stick to one for now.

Let’s start with the definition of the average value of a function. The average value can be expressed as

where a and b are our starting and ending integration points and x is the value that we are integrating over. So from here we can calculate the integral by multiplying both sides of the equation by the range (b-a).

So now the integral is equal to the average value of a function times the range that we are interested in integrating over.

From here we have to find the average of F. We can do this taking a large sample of random numbers, finding the corresponding value for each random number, add those values together, and then divide the sum by the total number of random numbers. Mathematically, this relation looks like

It is important to note that this only works if the random numbers are uniformally distributed. With this in mind, the final equation for the integral of interest is

All that is left to do now is to plug in the function we want to integrate, our limits of integration, and how many guesses we want to take at evaluating the integral.

Let’s pick a simple example. Let’s integrate the function sin(x) from 0 to pi. Analytically, this integration isn’t too difficult, the calculation is just

but our interest is in seeing how well the Monte Carlo method can perform this calculation, so the equation we’ll be solving is

Here is all the code you need to run your own Monte Carlo calculations.

import numpy as np

from scipy import random a, b = 0, np.pi # limits of integration

N = 100_000 # Number of random points

integral = 0.0 # initialize the value of the integral x_random = random.uniform(a,b,N) # array of random numbers def func(x):

'''This is the function that we want to integrate'''

return np.sin(x) # Makes a guess at the integral, adds that guess to the other guesses, and prints intermediate answers

for x in range(N):

integral += func(x_random[x])

if (x % 10_000 == 0) & (x != 0):

intermediate_answer = (b-a)/float(x)*integral

print(f'Value after {x} iterations is {intermediate_answer}')



answer = (b-a)/float(N)*integral

print(f'The value of the integral from {a} to {b} is: ', answer)

And the result is

We can see that our Mote Carlo calculation is slowly converging toward the correct answer!","['monte', 'function', 'average', 'guessing', 'integral', 'numbers', 'value', 'method', 'random', 'x', 'power', 'carlo']","The Monte Carlo method is an incredibly powerful tool used in a wide variety of fields.
From mathematics to science to finance, the Monte Carlo method can be used to solve a variety of unique and interesting problems.
The idea of the Monte Carlo method is farily stright forward.
Let’s attempt to find the numerical solution to an integral using the Monte Carlo method.
Analytically, this integration isn’t too difficult, the calculation is justbut our interest is in seeing how well the Monte Carlo method can perform this calculation, so the equation we’ll be solving isHere is all the code you need to run your own Monte Carlo calculations.",en,['Steven Dye'],2019-11-08 00:46:34.022000+00:00,"{'Machine Learning', 'Python', 'Monte Carlo', 'Data Science'}","{'https://miro.medium.com/max/376/1*1sbldYNtTCEq3T2ILaIUrQ.png', 'https://miro.medium.com/max/1324/1*pKwXRZ4-yoX0vC2K6_5bbA.png', 'https://miro.medium.com/fit/c/96/96/2*4nUPdHSD5e4v2rpNDylvTw.jpeg', 'https://miro.medium.com/max/60/1*DfT4vHYNWrtxFAp7lendWg.png?q=20', 'https://miro.medium.com/max/442/1*AuQSFEiZCTzfe1pPSo4R3w.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/2*4nUPdHSD5e4v2rpNDylvTw.jpeg', 'https://miro.medium.com/max/60/1*pKwXRZ4-yoX0vC2K6_5bbA.png?q=20', 'https://miro.medium.com/max/60/1*HmO3ijKQejzUXOPXc0O6Dg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/482/1*-OXvEXFJP34LNTDxgv7NxQ.png', 'https://miro.medium.com/max/60/1*rCmGN4-CEG0ag7micf4chw.png?q=20', 'https://miro.medium.com/max/986/1*km2cavQ5SGD8lfrYz92Lhg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*AuQSFEiZCTzfe1pPSo4R3w.png?q=20', 'https://miro.medium.com/max/262/1*rCmGN4-CEG0ag7micf4chw.png', 'https://miro.medium.com/max/1500/1*DfT4vHYNWrtxFAp7lendWg.png', 'https://miro.medium.com/max/350/1*HmO3ijKQejzUXOPXc0O6Dg.png', 'https://miro.medium.com/max/60/1*1sbldYNtTCEq3T2ILaIUrQ.png?q=20', 'https://miro.medium.com/max/60/1*km2cavQ5SGD8lfrYz92Lhg.png?q=20', 'https://miro.medium.com/max/60/1*-OXvEXFJP34LNTDxgv7NxQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/750/1*DfT4vHYNWrtxFAp7lendWg.png'}",2020-03-05 00:09:17.095004,6.716165542602539
https://towardsdatascience.com/data-version-control-machine-learning-process-control-using-dvc-github-c629511e95b5,Data Version control & machine learning process control using DVC & GitHub,"DVC workflow

Even with all the success we’ve seen today in machine learning (ML), specifically deep learning and its applications in business, the data science community still lacks good practices for organizing their projects and effectively collaborating across their varied ML projects.

Improving the productivity of machine learning workflow is necessary. As a Data Scientist at Group K Diagnostics, I deal with the hundreds of gigabytes of data. It is indeed very tough for data scientist to manage and track the data. The data tracking is necessary thing for any data science workflow. The need for data versioning is extreme, thus Data Version control (https://dvc.org) is very crucial tool for any data scientist. This is very handy and useful tool for your data science projects. The main reasons are,

DVC tracks ML models and data sets: DVC is built to make ML models shareable and reproducible.

DVC is built to make ML models shareable and reproducible. ML project version control: Version control machine learning models, data sets and intermediate files.

Version control machine learning models, data sets and intermediate files. ML experiment management: Harness the full power of Git branches to try different ideas instead of sloppy file suffixes and comments in code.

Harness the full power of Git branches to try different ideas instead of sloppy file suffixes and comments in code. Deployment & Collaboration: Instead of ad-hoc scripts, use push/pull commands to move consistent bundles of ML models, data, and code into production, remote machines, or a colleague’s computer.

Major advantage here is that, we can use it with Amazon AWS S3, Microsoft Azure and other data/resource management systems. We can configure those systems to store, read, write and snap data. The setup and usage is very simple,

Install DVC using, (assuming you have python environment ready)

pip install dvc

2. Perform the DVC initialization, (assuming you are in Github repository, if not then perform)

git init dvc init

3. Now let’s assume that our project has ‘data’ folder which has various data directory (connected & downloaded through AWS S3)

├── data

│ ├── exp1

│ ├── x1.png

│ ├── y1.png

│ ├── exp2

│ ├── x2.png

│ ├── y2.png

│ └── exp3

│ ├── x3.png

│ ├── y3.png

│ └── exp4

│ ├── x4.png

│ ├── y4.png

Now after using this data first time, we will perform the following steps.

dvc add ./data/exp1 && git add ./data/exp1.dvc dvc add ./data/exp2 && git add ./data/exp2.dvc dvc add ./data/exp3 && git add ./data/exp3.dvc dvc add ./data/exp4 && git add ./data/exp4.dvc

This will create .dvc file for each folder and that file will be committed to GitHub so we know which version of model uses which version of data.

After this we need to push the dvc changes if we are using remote data storages,

dvc push filename

This command will push the recent changes and will update the cache, Let’s do,

dvc status

This will check for any potential change in data, if any then it will track it and we can commit it.

DVC makes collaboration very easy. Collaborators can set it up in the same way or they can pull up the data with, ‘ dvc pull filename’ command.

This procedure also works for data versions containing a lot more data than currently persistent in the data folder as dvc stores differences of arbitrary size between different versions in its cache and can therefore recreate older or newer states of the data directories by its checkout command. The checkout is of course also possible in the other direction.

For more information on commands visit their documentation here.

For using DVC with Amazon AWS S3, please visit https://dvc.org/doc/user-guide/external-dependencies

The best overall highlights and features are,

It is Git-compatible! DVC runs on top of any Git repository and is compatible with any standard Git server or provider (GitHub, GitLab, etc). Data file contents can be shared by network-accessible storage or any supported cloud solution. DVC offers all the advantages of a distributed version control system — lock-free, local branching, and versioning. It’s storage agnoistic. You can work with S3, Google cloud and Azure. It comes with Metric tracking and includes a command to list all branches, along with metric values, to track the progress or pick the best version. Language- & framework-agnostic Provides end to end support for ML pipeline framework. It can track failures! It is also compatible with HDFS, Hive & Apache Spark.

So give it a shot and make your life easy! Click here to get started! As a Data Scientist, I highly recommend it to each and every data scientist to use it within their ML pipeline.","['machine', 'file', 'models', 'version', 'scientist', 'control', 'process', 'github', 'learning', 'ml', 'data', 'git', 'dvc', 'add', 'using']","The need for data versioning is extreme, thus Data Version control (https://dvc.org) is very crucial tool for any data scientist.
The main reasons are,DVC tracks ML models and data sets: DVC is built to make ML models shareable and reproducible.
ML project version control: Version control machine learning models, data sets and intermediate files.
dvc add ./data/exp1 && git add ./data/exp1.dvc dvc add ./data/exp2 && git add ./data/exp2.dvc dvc add ./data/exp3 && git add ./data/exp3.dvc dvc add ./data/exp4 && git add ./data/exp4.dvcThis will create .dvc file for each folder and that file will be committed to GitHub so we know which version of model uses which version of data.
As a Data Scientist, I highly recommend it to each and every data scientist to use it within their ML pipeline.",en,['Nisarg Dave'],2020-02-06 13:56:44.023000+00:00,"{'Github', 'Machine Learning', 'Dvc', 'Data Science'}","{'https://miro.medium.com/max/60/1*B3NAZY4D3BN6kPdrATt5Bw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*B3NAZY4D3BN6kPdrATt5Bw.png', 'https://miro.medium.com/fit/c/96/96/1*DsFruBKtzMg8jHWxXIH3-w.jpeg', 'https://miro.medium.com/fit/c/160/160/1*DsFruBKtzMg8jHWxXIH3-w.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/4032/1*B3NAZY4D3BN6kPdrATt5Bw.png'}",2020-03-05 00:09:23.657069,6.561082124710083
https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0,A Minimalist End-to-End Scrapy Tutorial (Part I),"A Minimalist End-to-End Scrapy Tutorial (Part I)

Systematic Web Scraping for Beginners

Photo by Paweł Czerwiński on Unsplash

Part I, Part II, Part III, Part IV

Web scraping is an important skill for data scientists to have. I have developed a number of ad hoc web scraping projects using Python, BeautifulSoup, and Scrapy in the past few years and read a few books and tons of online tutorials along the way. However, I have not found a simple beginner level tutorial that is end-to-end in the sense that covers all basic steps and concepts in a typical Scrapy web scraping project (therefore Minimalist in the title) — that’s why I am writing this and hope the code repo can serve as a template to help jumpstart your web scraping projects.

Many people ask: should I use BeautifulSoup or Scrapy? They are different things: BeautifulSoup is a library for parsing HTML and XML and Scrapy is a web scraping framework. You can use BeautifulSoup instead of Scrapy build-in selectors if you want but comparing BeautifulSoup to Scrapy is like comparing the Mac keyboard to the iMac or a better metaphor as stated in the official documentation “like comparing jinja2 to Django” if you know what they are :) — In short, you should learn Scrapy if you want to do serious and systematic web scraping.

TL;DR, show me the code:

In this tutorial series, I am going to cover the following steps:

(This tutorial) Start a Scrapy project from scratch and develop a simple spider. One important thing is the use of Scrapy Shell for analyzing pages and debugging, which is one of the main reasons you should use Scrapy over BeautifulSoup. (Part II) Introduce Item and ItemLoader and explain why you want to use them (although they make your code seem more complicated at first). (Part III) Store the data to the database using ORM (SQLAlchemy) via Pipelines and show how to set up the most common One-to-Many and Many-to-Many relationships. (Part IV) Deploy the project to Scrapinghub (you have to pay for service such as scheduled crawling jobs) or set up your own servers completely free of charge by using the great open source project ScrapydWeb and Heroku.

In addition, I also created a separate repo (Scrapy + Selenium) to show how to crawl dynamic web pages (such as a page that loads additional content via scrolling) and how to use proxy networks (ProxyMesh) to avoid getting banned.

Some prerequisites:

Basic knowledge on Python (Python 3 for this tutorial), virtual environment, Homebrew, etc., see my other article for how to set up the environment: How to Setup Mac for Python Development

Basic knowledge of Git and Github. I recommend the Pro Git book.

Basic knowledge of database and ORM, e.g., Introduction to Structured Query Language (SQL).

Let’s get started!

First, create a new folder, setup Python 3 virtual environment inside the folder, and install Scrapy. To make this step easy, I created a starter repo, which you can fork and clone (see Python3 virtual environment documentation if needed):



$ cd scrapy-tutorial-starter

$ python3.6 -m venv venv

$ source venv/bin/activate

$ pip install -r requirements.txt $ git clone https://github.com/yourusername/scrapy-tutorial-starter.git $ cd scrapy-tutorial-starter$ python3.6 -m venv venv$ source venv/bin/activate$ pip install -r requirements.txt

Your folder should look like the following and I assume we always work in the virtual environment. Note that we only have one package in the requirements.txt so far.

run scrapy startproject tutorial to create an empty scrapy project and your folder looks like:

Two identical “tutorial” folders were created. We don’t need the first level “tutorial” folder — delete it and move the second level “tutorial” folder with its contents one-level up — I know this is confusing but that’s all you have to do with the folder structure. Now, your folder should look like:

Don’t worry about the auto-generated files so far, we will come back to those files later. This tutorial is based on the official Scrapy tutorial. Therefore, the website we are going to crawl is http://quotes.toscrape.com, which is quite simple: there are pages of quotes with authors and tags:

When you click the author, it goes to the author detail page with name, birthday, and bio.

Now, create a new file named “quotes-spider.py” in the “spider” folder with the following content:

You just created a spider named “quotes”, which sends a request to http://quotes.toscrape.com and gets the response from the server. However, the spider does not do anything so far when parsing the response and simply outputs a string to the console. Let’s run this spider: scrapy crawl quotes , you should see the output like:

Next, let’s analyze the response, i.e., the HTML page at http://quotes.toscrape.com using Scrapy Shell by running:

$ scrapy shell http://quotes.toscrape.com/ ... 2019-08-21 20:10:40 [scrapy.core.engine] INFO: Spider opened 2019-08-21 20:10:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None) 2019-08-21 20:10:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None) [s] Available Scrapy objects: [s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc) [s] crawler <scrapy.crawler.Crawler object at 0x105d01dd8> [s] item {} [s] request <GET http://quotes.toscrape.com/> [s] response <200 http://quotes.toscrape.com/> [s] settings <scrapy.settings.Settings object at 0x106ae34e0> [s] spider <DefaultSpider 'default' at 0x106f13780> [s] Useful shortcuts: [s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed) [s] fetch(req) Fetch a scrapy.Request and update local objects [s] shelp() Shell help (print this help) [s] view(response) View response in a browser >>>

You can select elements using either Xpath selector or CSS selector and Chrome DevTools is often used to analyze the page (we won’t cover the selector details, please read the documents to learn how to use them):

For example, you can test the selector and see the results in Scrapy Shell — assume we want to get the quote block shown above:

You can either use Xpath response.xpath(“//div[@class=’quote’]”).get() ( .get() shows the first selected element, use .getall() to show all) or CSS response.css(“div .quote”).get() . I bolded the quote text, author, and tags we want to get from this quote block:

>>> response.xpath(""//div[@class='quote']"").get() '<div class=""quote"" itemscope itemtype=""http://schema.org/CreativeWork"">

<span class=""text"" itemprop=""text"">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>

<span>by <small class=""author"" itemprop=""author"">Albert Einstein</small>

<a href=""/author/Albert-Einstein"">(about)</a>

</span>

<div class=""tags"">

Tags:

<meta class=""keywords"" itemprop=""keywords"" content=""change,deep-thoughts,thinking,world"">



<a class=""tag"" href=""/tag/change/page/1/"">change</a>



<a class=""tag"" href=""/tag/deep-thoughts/page/1/"">deep-thoughts</a>



<a class=""tag"" href=""/tag/thinking/page/1/"">thinking</a>



<a class=""tag"" href=""/tag/world/page/1/"">world</a>



</div>

</div>'

We can proceed in the shell to get the data as follows:

get all quote blocks into “quotes”

use the first quote in “quotes”: quotes[0]

try the css selectors

>>> quotes = response.xpath(""//div[@class='quote']"")

>>> quotes[0].css("".text::text"").getall() ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'] >>> quotes[0].css("".author::text"").getall() ['Albert Einstein'] >>> quotes[0].css("".tag::text"").getall() ['change', 'deep-thoughts', 'thinking', 'world']

It seems that the selectors shown above get what we need. Note that I am mixing Xpath and CSS selectors for the demonstration purpose here — no need to use both in this tutorial.

Now, let’s revise the spider file and use keyword yield to output the selected data to the console (note that each page has many quotes and we use a loop to go over all of them):

import scrapy class QuotesSpider(scrapy.Spider):

name = ""quotes"" start_urls = [' http://quotes.toscrape.com' def parse(self, response):

self.logger.info('hello this is my first spider')

quotes = response.css('div.quote')

for quote in quotes:

yield {

'text': quote.css('.text::text').get(),

'author': quote.css('.author::text').get(),

'tags': quote.css('.tag::text').getall(),

}

Run the spider again: scrapy crawl quotes and you can see the extracted data in the log:

You can save the data in a JSON file by running: scrapy crawl quotes -o quotes.json

So far, we get all quote information from the first page, and our next task is to crawl all pages. You should notice a “Next” button at the bottom of the front page for page navigation — the logic is: click the Next button to go to the next page, get the quotes, click Next again till the last page without the Next button.

Via Chrome DevTools, we can get the URL of the next page:

Let’s test it out in Scrapy Shell by running scrapy shell http://quotes.toscrape.com/ again:

Now we can write the following code for the spider to go over all pages to get all quotes:

next_page = response.urljoin(next_page) gets the full URL and yield scrapy.Request(next_page, callback=self.parse) sends a new request to get the next page and use a callback function to call the same parse function to get the quotes from the new page.

Shortcuts can be used to further simplify the code above: see this section. Essentially, response.follow supports relative URLs (no need to call urljoin ) and automatically uses the href attribute for <a> . So, the code can be shortened further:

for a in response.css('li.next a'):

yield response.follow(a, callback=self.parse)

Now, run the spider again scrapy crawl quotes you should see quotes from all 10 pages have been extracted. Hang in there — we are almost done for this first part. The next task is to crawl the individual author's page.

As shown above, when we process each quote, we can go to the individual author’s page by following the highlighted link — let’s use Scrapy Shell to get the link:

So, during the loop of extracting each quote, we issue another request to go to the corresponding author’s page and create another parse_author function to extract the author’s name, birthday, born location and bio and output to the console. The updated spider looks like the following:

Run the spider again scrapy crawl quotes and double-check that everything you need to extract is output to the console correctly. Note that Scrapy is based on Twisted, a popular event-driven networking framework for Python and thus is asynchronous. This means that the individual author page may not be processed in sync with the corresponding quote, e.g., the order of the author page results may not match the quote order on the page. We will discuss how to link the quote with its corresponding author page in the later part.

Congratulations, you have finished Part I of this tutorial.

Learn more about Item and ItemLoader in Part II.

Part I, Part II, Part III, Part IV","['httpquotestoscrapecom', 'scrapy', 'shell', 'folder', 's', 'endtoend', 'page', 'minimalist', 'quote', 'quotes', 'tutorial', 'spider']","A Minimalist End-to-End Scrapy Tutorial (Part I)Systematic Web Scraping for BeginnersPhoto by Paweł Czerwiński on UnsplashPart I, Part II, Part III, Part IVWeb scraping is an important skill for data scientists to have.
One important thing is the use of Scrapy Shell for analyzing pages and debugging, which is one of the main reasons you should use Scrapy over BeautifulSoup.
run scrapy startproject tutorial to create an empty scrapy project and your folder looks like:Two identical “tutorial” folders were created.
We don’t need the first level “tutorial” folder — delete it and move the second level “tutorial” folder with its contents one-level up — I know this is confusing but that’s all you have to do with the folder structure.
This tutorial is based on the official Scrapy tutorial.",en,['Harry Wang'],2020-02-06 14:02:35.402000+00:00,"{'Web Crawler', 'Scrapy', 'Data Science', 'Python', 'Web Scraping'}","{'https://miro.medium.com/max/60/1*Lpqu_VWTgiNFYpXip_CfWA.png?q=20', 'https://miro.medium.com/max/60/1*oimG6llPwkQjgELJcSV6qg.png?q=20', 'https://miro.medium.com/max/60/1*7Oh1JM4hPUdVaZXRFGId3Q.png?q=20', 'https://miro.medium.com/max/1590/1*AXkxnsl61RuOIP5_qGE0eA.png', 'https://miro.medium.com/fit/c/96/96/1*4mlEX4QSrIB-qTX3IXAbYA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/494/1*K61KJLWgTBSfH0zWDA9dPQ.png', 'https://miro.medium.com/max/1696/1*LYk0X4YcY3kaRHp37Kr7OA.png', 'https://miro.medium.com/max/1600/1*oimG6llPwkQjgELJcSV6qg.png', 'https://miro.medium.com/max/1400/1*dN15tIZ0LeISCUYbMTAXzA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2226/1*Lpqu_VWTgiNFYpXip_CfWA.png', 'https://miro.medium.com/max/60/1*LYk0X4YcY3kaRHp37Kr7OA.png?q=20', 'https://miro.medium.com/max/2868/1*idTIgtpUhMtbeLErYRAvFg.png', 'https://miro.medium.com/max/60/1*dN15tIZ0LeISCUYbMTAXzA.png?q=20', 'https://miro.medium.com/max/1966/1*WIBE22Qu-wJaxkPabYUfoA.png', 'https://miro.medium.com/max/1200/0*2OeoRtfHganECeFf', 'https://miro.medium.com/max/26/1*4sdDP1t3okK6p5p_94VtyA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/442/1*4sdDP1t3okK6p5p_94VtyA.png', 'https://miro.medium.com/max/60/0*2OeoRtfHganECeFf?q=20', 'https://miro.medium.com/max/1742/1*kZVnS5mYVm89DXvaHQDPJQ.png', 'https://miro.medium.com/max/60/1*IuinM1PhEOtuDrO1unb7cA.png?q=20', 'https://miro.medium.com/max/60/1*kZVnS5mYVm89DXvaHQDPJQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*4mlEX4QSrIB-qTX3IXAbYA.jpeg', 'https://miro.medium.com/max/60/1*idTIgtpUhMtbeLErYRAvFg.png?q=20', 'https://miro.medium.com/max/1588/1*IuinM1PhEOtuDrO1unb7cA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/32/1*K61KJLWgTBSfH0zWDA9dPQ.png?q=20', 'https://miro.medium.com/max/60/1*AXkxnsl61RuOIP5_qGE0eA.png?q=20', 'https://miro.medium.com/max/60/1*WIBE22Qu-wJaxkPabYUfoA.png?q=20', 'https://miro.medium.com/max/1590/1*7Oh1JM4hPUdVaZXRFGId3Q.png', 'https://miro.medium.com/max/12000/0*2OeoRtfHganECeFf'}",2020-03-05 00:09:31.336588,7.678504467010498
https://medium.com/analytics-vidhya/knowledge-graph-a-powerful-data-science-technique-to-mine-information-from-text-with-python-f8bfd217accc,Knowledge Graph — A Powerful Data Science Technique to Mine Information from Text (with Python code),"Overview

Knowledge graphs are one of the most fascinating concepts in data science

Learn how to build a knowledge graph using text from Wikipedia data

We will be working hands-on in Python to build our knowledge graph using the popular spaCy library

Introduction

Lionel Messi needs no introduction. Even folks who don’t follow football have heard about the brilliance of one of the greatest players to have graced the sport. Here’s his Wikipedia page:

Quite a lot of information there! We have text, tons of hyperlinks, and even an audio clip. That’s a lot of relevant and potentially useful information on a single page. The possibilities of putting this into a use case are endless.

However, there is a slight problem. This is not an ideal source of data to feed to our machines. Not in its current form anyway.

Can we find a way to make this text data readable for machines? Essentially, can we transform this text data into something that can be used by the machines and also can be interpreted easily by us?

Yes, we can! We can do it with the help of Knowledge Graphs (KG), one of the most fascinating concepts in data science. I have been blown away by the sheer potential and applications of knowledge graphs and I am sure you will as well.

In this article, you will learn what knowledge graphs are, why they’re useful, and then we’ll dive into code by building our own knowledge graph on data extracted from Wikipedia.

Table of Contents

What is a Knowledge Graph? How to Represent Knowledge in a Graph?

- Sentence Segmentation

- Entities Extraction

- Relations Extraction Build a Knowledge Graph from Text Data

What is a Knowledge Graph?

Let’s get one thing out of the way — we will see the term “graphs” a lot in this article. We do not mean bar charts, pie charts, and line plots when I say graphs. Here, we are talking about interconnected entities which can be people, locations, organizations, or even an event.

We can define a graph as a set of nodes and edges.

Take a look at the figure below:

Node A and Node B here are two different entities. These nodes are connected by an edge that represents the relationship between the two nodes. Now, this is the smallest knowledge graph we can build — it is also known as a triple.

Knowledge Graph’s come in a variety of shapes and sizes. For example, the knowledge graph of Wikidata had 59,910,568 nodes by October 2019.

How to Represent Knowledge in a Graph?

Before we get started with building Knowledge Graphs, it is important to understand how information or knowledge is embedded in these graphs.

Let me explain this using an example. If Node A = Putin and Node B = Russia, then it is quite likely that the edge would be “president of”:

A node or an entity can have multiple relations as well. Putin is not only the President of Russia, he also worked for the Soviet Union’s security agency, KGB. But how do we incorporate this new information about Putin in the knowledge graph above?

It’s actually pretty simple. Just add one more node for the new entity, KGB:

The new relationships can emerge not only from the first node but from any node in a knowledge graph as shown below:

Russia is a member of the Asia Pacific Economic Cooperation (APEC).

Identifying the entities and the relation between them is not a difficult task for us. However, manually building a knowledge graph is not scalable. Nobody is going to go through thousands of documents and extract all the entities and the relations between them!

That’s why machines are more suitable to perform this task as going through even hundreds or thousands of documents is child’s play for them. But then there is another challenge — machines do not understand natural language. This is where Natural Language Processing (NLP) comes into the picture.

To build a knowledge graph from the text, it is important to make our machine understand natural language. This can be done by using NLP techniques such as sentence segmentation, dependency parsing, parts of speech tagging, and entity recognition. Let’s discuss these in a bit more detail.

Sentence Segmentation

The first step in building a knowledge graph is to split the text document or article into sentences. Then, we will shortlist only those sentences in which there is exactly 1 subject and 1 object. Let’s look at a sample text below:

“Indian tennis player Sumit Nagal moved up six places from 135 to a career-best 129 in the latest men’s singles ranking. The 22-year-old recently won the ATP Challenger tournament. He made his Grand Slam debut against Federer in the 2019 US Open. Nagal won the first set.”

Let’s split the paragraph above into sentences:

Indian tennis player Sumit Nagal moved up six places from 135 to a career-best 129 in the latest men’s singles ranking The 22-year-old recently won the ATP Challenger tournament He made his Grand Slam debut against Federer in the 2019 US Open

Out of these four sentences, we will shortlist the second and the fourth sentences because each of them contains 1 subject and 1 object.

In the second sentence, “22-year-old” is the subject and the object is “ATP Challenger tournament”. In the fourth sentence, the subject is “Nagal” and “first set” is the object:

The challenge is to make your machine understand the text, especially in the cases of multi-word objects and subjects. For example, extracting the objects in both the sentences above is a bit tricky. Can you think of any method to solve this problem?

Entities Extraction

The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. The nouns and the proper nouns would be our entities.

However, when an entity spans across multiple words, then POS tags alone are not sufficient. We need to parse the dependency tree of the sentence. You can read more about dependency parsing in the following article.

Let’s get the dependency tags for one of the shortlisted sentences. I will use the popular spaCy library for this task:

Output:

The … det

22-year … amod

— … punct

old … nsubj

recently … advmod

won … ROOT

ATP … compound

Challenger … compound

tournament … dobj

. … punct

The subject ( nsubj) in this sentence as per the dependency parser is “old”. That is not the desired entity. We wanted to extract “22-year-old” instead.

The dependency tag of “22-year” is amod which means it is a modifier of “old”. Hence, we should define a rule to extract such entities.

The rule can be something like this — extract the subject/object along with its modifiers and also extract the punctuation marks between them.

But then look at the object (dobj) in the sentence. It is just “tournament” instead of “ATP Challenger tournament”. Here, we don’t have the modifiers but we do have compound words.

Compound words are those words that collectively form a new term with a different meaning. Therefore, we can update the above rule to ⁠- extract the subject/object along with its modifiers, compound words and also extract the punctuation marks between them.

In short, we will use dependency parsing to extract entities.

Extract Relations

Entity extraction is half the job done. To build a knowledge graph, we need edges to connect the nodes (entities) to one another. These edges are the relations between a pair of nodes.

Let’s go back to the example in the last section. We shortlisted a couple of sentences to build a knowledge graph:

Can you guess the relation between the subject and the object in these two sentences?

Both sentences have the same relation — “won”. Let’s see how these relations can be extracted. We will again use dependency parsing:

Output:

Nagal … nsubj

won … ROOT

the … det

first … amod

set … dobj

. … punct

To extract the relation, we have to find the ROOT of the sentence (which is also the verb of the sentence). Hence, the relation extracted from this sentence would be “won”.

Finally, the knowledge graph from these two sentences will be like this:

Build a Knowledge Graph from Text Data

Time to get our hands on some code! Let’s fire up our Jupyter Notebooks (or whatever IDE you prefer).

We will build a knowledge graph from scratch by using the text from a set of movies and films related to Wikipedia articles. I have already extracted around 4,300 sentences from over 500 Wikipedia articles. Each of these sentences contains exactly two entities — one subject and one object. You can download these sentences from here.

I suggest using Google Colab for this implementation to speed up the computation time.

Import Libraries

Read Data

Read the CSV file containing the Wikipedia sentences:

Output: (4318, 1)

Let’s inspect a few sample sentences:

candidate_sentences['sentence'].sample(5)

Output:

Let’s check the subject and object of one of these sentences. Ideally, there should be one subject and one object in the sentence:

Output:

Perfect! There is only one subject (‘process’) and only one object (‘standard’). You can check for other sentences in a similar manner.

Entity Pairs Extraction

To build a knowledge graph, the most important things are the nodes and the edges between them.

These nodes are going to be the entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities to one another. We will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences.

The main idea is to go through a sentence and extract the subject and the object as and when they are encountered. However, there are a few challenges ⁠- an entity can span across multiple words, eg., “red wine”, and the dependency parsers tag only the individual words as subjects or objects.

So, I have created a function below to extract the subject and the object (entities) from a sentence while also overcoming the challenges mentioned above. I have partitioned the code into multiple chunks for your convenience:

Let me explain the code chunks in the function above:

Chunk 1

I have defined a few empty variables in this chunk. prv_tok_dep and prv_tok_text will hold the dependency tag of the previous word in the sentence and that previous word itself, respectively.

prefix and modifier will hold the text that is associated with the subject or the object.

Chunk 2

Next, we will loop through the tokens in the sentence. We will first check if the token is a punctuation mark or not. If yes, then we will ignore it and move on to the next token.

If the token is a part of a compound word (dependency tag = “compound”), we will keep it in the prefix variable. A compound word is a combination of multiple words linked to form a word with a new meaning (example — “Football Stadium”, “animal lover”).

As and when we come across a subject or an object in the sentence, we will add this prefix to it. We will do the same thing with the modifier words, such as “nice shirt”, “big house”, etc.

Chunk 3

Here, if the token is the subject, then it will be captured as the first entity in the ent1 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will be reset.

Chunk 4

Here, if the token is the object, then it will be captured as the second entity in the ent2 variable. Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will again be reset.

Chunk 5

Once we have captured the subject and the object in the sentence, we will update the previous token and its dependency tag.

Let’s test this function on a sentence:

get_entities(""the film had 200 patents"")

Output: [‘film’, ‘200 patents’]

Great, it seems to be working as planned. In the above sentence, ‘film’ is the subject and ‘200 patents’ is the object.

Now we can use this function to extract these entity pairs for all the sentences in our data:

The list entity_pairs contains all the subject-object pairs from the Wikipedia sentences. Let’s have a look at a few of them:

entity_pairs[10:20]

Output:

As you can see, there are a few pronouns in these entity pairs such as ‘we’, ‘it’, ‘she’, etc. We’d like to have proper nouns or nouns instead. Perhaps we can further improve the get_entities( ) function to filter out pronouns. For the time being, let’s leave it as it is and move on to the relation extraction part.

Relation / Predicate Extraction

This is going to be a very interesting aspect of this article. Our hypothesis is that the predicate is actually the main verb in a sentence.

For example, in the sentence — “Sixty Hollywood musicals were released in 1929”, the verb is “released in” and this is what we are going to use as the predicate for the triple generated from this sentence.

The function below is capable of capturing such predicates from the sentences. Here, I have used spaCy’s rule-based matching:

The pattern defined in the function tries to find the ROOT word or the main verb in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (‘prep’) or an agent word. If yes, then it is added to the ROOT word.

Let me show you a glimpse of this function:

get_relation(""John completed the task"")

Output: completed

Similarly, let’s get the relations from all the Wikipedia sentences:

relations = [get_relation(i) for i in

tqdm(candidate_sentences['sentence'])]

Let’s take a look at the most frequent relations or predicates that we have just extracted:

pd.Series(relations).value_counts()[:50]

It turns out that relations like “A is B” and “A was B” are the most common relations. However, there are quite a few relations that are more associated with the overall theme — “the ecosystem around movies”. Some of the examples are “composed by”, “released in”, “produced”, “written by” and a few more.

Build a Knowledge Graph

We will finally create a knowledge graph from the extracted entities (subject-object pairs) and the predicates (relation between entities).

Let’s create a dataframe of entities and predicates:

Next, we will use the networkx library to create a network from this dataframe. The nodes will represent the entities and the edges or connections between the nodes will represent the relations between the nodes.

It is going to be a directed graph. In other words, the relation between any connected node pair is not two-way, it is only from one node to another. For example, “John eats pasta”:

Let’s plot the network:

Output:

🥶 Well, this is not exactly what we were hoping for (still looks quite a sight though!).

It turns out that we have created a graph with all the relations that we had. It becomes really hard to visualize a graph with these many relations or predicates.

So, it’s advisable to use only a few important relations to visualize a graph. I will take one relation at a time. Let’s start with the relation “composed by”:

Output:

That’s a much cleaner graph. Here the arrows point towards the composers. For instance, A.R. Rahman, who is a renowned music composer, has entities like “soundtrack score”, “film score”, and “music” connected to him in the graph above.

Let’s check out a few more relations.

Since writing is an important role in any movie, I would like to visualize the graph for the “written by” relation:

Output:

Awesome! This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.

Let’s see the knowledge graph of another important predicate, i.e., the “released in”:

Output:

I can see quite a few interesting information in this graph. For example, look at this relationship — “several action horror movies released in the 1980s” and “pk released on 4844 screens”. These are facts and it shows us that we can mine such facts from just text. That’s quite amazing!

End Notes

In this article, we learned how to extract information from a given text in the form of triples and build a knowledge graph from it.

However, we restricted ourselves to use sentences with exactly 2 entities. Even then we were able to build quite informative knowledge graphs. Imagine the potential we have here!

I encourage you to explore this field of information extraction more to learn extraction of more complex relationships. In case you have any doubt or you want to share your thoughts, please feel free to use the comments section below.","['powerful', 'graph', 'information', 'code', 'object', 'subject', 'python', 'entities', 'knowledge', 'data', 'sentence', 'sentences', 'relations', 'extract', 'technique', 'text', 'science']","In this article, you will learn what knowledge graphs are, why they’re useful, and then we’ll dive into code by building our own knowledge graph on data extracted from Wikipedia.
- Sentence Segmentation- Entities Extraction- Relations Extraction Build a Knowledge Graph from Text DataWhat is a Knowledge Graph?
Now, this is the smallest knowledge graph we can build — it is also known as a triple.
Finally, the knowledge graph from these two sentences will be like this:Build a Knowledge Graph from Text DataTime to get our hands on some code!
Let’s see the knowledge graph of another important predicate, i.e., the “released in”:Output:I can see quite a few interesting information in this graph.",en,['Prateek Joshi'],2019-10-16 05:06:53.111000+00:00,"{'Text Mining', 'Graph', 'Wikipedia', 'Knowledge Graph', 'NLP'}","{'https://miro.medium.com/max/60/0*rkpBLhXwJlzbA-Nl?q=20', 'https://miro.medium.com/max/1394/0*wpFl1bgxm766W3Ub', 'https://miro.medium.com/max/1834/0*m7pYgIH5WNSPssk8', 'https://miro.medium.com/max/60/0*x8vXDgY7kjHBrPE0?q=20', 'https://miro.medium.com/max/1728/0*BRlp2JB6aUbUEKC3', 'https://miro.medium.com/max/862/0*uS2bfXKMbm_6Bsmx', 'https://miro.medium.com/max/60/0*GMN_SDuFNlJlMjDg?q=20', 'https://miro.medium.com/max/60/1*riL1ULZ0Zaww0ssqZHY68A.png?q=20', 'https://miro.medium.com/max/1940/0*Mm2pITuSaChR_qMP', 'https://miro.medium.com/max/1994/0*x8vXDgY7kjHBrPE0', 'https://miro.medium.com/max/1728/0*rkpBLhXwJlzbA-Nl', 'https://miro.medium.com/max/290/1*cK8jYS5H7rDYhb0vZkW4NA.png', 'https://miro.medium.com/fit/c/160/160/2*QHjdvZCdY28zSqJk6BAqsA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*f0xB_2JvkHaF1zlsF-hsHA.jpeg', 'https://miro.medium.com/max/60/0*2DUxBosim3ShVddX?q=20', 'https://miro.medium.com/max/1440/0*GMN_SDuFNlJlMjDg', 'https://miro.medium.com/max/60/0*EFLVEF3znOyPArBG?q=20', 'https://miro.medium.com/fit/c/96/96/2*QHjdvZCdY28zSqJk6BAqsA.jpeg', 'https://miro.medium.com/max/1050/1*riL1ULZ0Zaww0ssqZHY68A.png', 'https://miro.medium.com/max/60/0*wpFl1bgxm766W3Ub?q=20', 'https://miro.medium.com/fit/c/80/80/1*PFdJBI5MLv6iMemP3QtVlA.jpeg', 'https://miro.medium.com/max/766/0*Q8u4JM9GKYZhC8Z_', 'https://miro.medium.com/max/60/0*vIaSoY5zSVCLsFMb?q=20', 'https://miro.medium.com/max/1160/0*EFLVEF3znOyPArBG', 'https://miro.medium.com/max/60/0*uS2bfXKMbm_6Bsmx?q=20', 'https://miro.medium.com/max/60/0*BRlp2JB6aUbUEKC3?q=20', 'https://miro.medium.com/fit/c/160/160/1*miCA9MEw8TjpXyR0xY1w-A.png', 'https://miro.medium.com/max/2158/0*C2d03pYjQWtHlNek', 'https://miro.medium.com/max/1728/0*8LrBBmzwbPUfw9Lh', 'https://miro.medium.com/max/1280/0*2DUxBosim3ShVddX', 'https://miro.medium.com/max/896/0*32zpEW7toOaojsTC', 'https://miro.medium.com/max/60/0*32zpEW7toOaojsTC?q=20', 'https://miro.medium.com/max/60/0*m1BwP1-YfhiG2S9y?q=20', 'https://miro.medium.com/max/60/0*8LrBBmzwbPUfw9Lh?q=20', 'https://miro.medium.com/max/60/0*Q8u4JM9GKYZhC8Z_?q=20', 'https://miro.medium.com/fit/c/80/80/2*-22Bm0tuymQyxNXw1WBxyQ.jpeg', 'https://miro.medium.com/max/60/0*m7pYgIH5WNSPssk8?q=20', 'https://miro.medium.com/max/60/0*Mm2pITuSaChR_qMP?q=20', 'https://miro.medium.com/max/1230/0*m1BwP1-YfhiG2S9y', 'https://miro.medium.com/max/1958/0*vIaSoY5zSVCLsFMb', 'https://miro.medium.com/max/60/0*3x-eJ3RNGJUqyKE_?q=20', 'https://miro.medium.com/max/2100/1*riL1ULZ0Zaww0ssqZHY68A.png', 'https://miro.medium.com/max/1372/0*3x-eJ3RNGJUqyKE_', 'https://miro.medium.com/max/60/0*C2d03pYjQWtHlNek?q=20'}",2020-03-05 00:09:33.406835,2.0692384243011475
https://towardsdatascience.com/object-oriented-machine-learning-pipeline-with-mlflow-for-pandas-and-koalas-dataframes-ef8517d39a12,Object-Oriented Machine Learning Pipeline with mlflow for Pandas and Koalas DataFrames,"Object-Oriented Machine Learning Pipeline with mlflow for Pandas and Koalas DataFrames

End-to-end process of developing Spark-enabled machine learning pipeline in Python using Pandas, Koalas, scikit-learn, and mlflow

In the article Python Data Preprocessing Using Pandas DataFrame, Spark DataFrame, and Koalas DataFrame, I used a public dataset to evaluate and compare the basic functionality of Pandas, Spark, and Koalas DataFrames in typical data preprocessing steps for machine learning. The main advantage of Koalas is that it supports an easy-to-use API similar to Pandas on Spark.

In this article, I use a more challenging dataset, the Interview Attendance Problem for Kaggle competition to demonstrate an end-to-end process of developing an Object-Oriented machine learning pipeline in Python for both Pandas and Koalas DataFrames using Pandas, Koalas, scikit-learn, and mlflow. This is achieved by:

Developing a data preprocessing pipeline using Pandas DataFrame with scikit-learn pipeline API

Developing a data preprocessing pipeline for Spark by combining scikit-learn pipeline API with Koalas DataFrame

Developing a machine learning pipeline by combining scikit-learn with mlflow

The end-to-end development process is based on the Cross-industry standard process for data mining. As shown in the diagram below, it consists of six major phases:

Business Understanding

Data Understanding

Data Preparation

Modeling

Evaluation

Deployment

Figure 1: CRISP-DM process diagram (refer to source in Wikipedia)

For convenience of discussion, it is assumed that the following Python libraries have been installed on a local machine such as Mac:

Anaconda (conda 4.7.10) with Python 3.6, Numpy, Pandas, Matplotlib, and Scikit-Learn

pyspark 2.4.4

Koalas

mlflow

The reason of using Python 3.6 is that certain functionality (e.g., deployment) of the current release of mlflow does not work with Python 3.7.

1. Business Understanding

The first phase is business understanding. The key point in this phase is to understand the business problem to be solved. As an example, the following is a brief description of the Kaggle interview attendance problem:

Given a set of questions that are asked by a recruiter while scheduling an interview with a candidate, how to use the answers to those questions from the candidate to predict whether the expected attendance will attend a scheduled interview (yes, no, or uncertain).

2. Data Understanding

Once the business problem is understood, the next step is to identify where (i.e., data sources) and how we should collect data from which a machine learning solution to the problem can be built.

The dataset for the Kaggle interview attendance problem has been collected from the recruitment industry in India by the researchers over a period of more than 2 years between September 2014 and January 2017.

This dataset is collected with labels (the column of Observed Attendance holds the labels) and thus it is suitable for supervised machine learning.

The following code imports the necessary Python libraries for all the source code in this article, and loads the dataset into a Koalas DataFrame and displays the first five rows of the DataFrame as shown in the table above.

import numpy as np

import pandas as pd

import databricks.koalas as ks

import matplotlib.pyplot as plt

import matplotlib as mpl

from datetime import datetime

import os from sklearn.base import BaseEstimator, TransformerMixin

from sklearn.pipeline import Pipeline

from sklearn.externals import joblib import mlflow

import mlflow.sklearn from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import GridSearchCV from sklearn.metrics import make_scorer

from sklearn.metrics import accuracy_score

from sklearn.metrics import f1_score

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score %matplotlib inline ks_df = ks.read_csv('Interview_Attendance_Data.csv')

ks_df.head()

3. Data Preparation

The main goal of data preparation is to clean and transform a collected raw dataset into appropriate format so that the transformed data can be effectively consumed by a target machine learning model.

In the interview attendance raw dataset, the column of Name(Cand ID) contains candidate unique identifiers, which do not have much prediction power and thus can be dropped. In addition, all of the columns (i.e., columns from _c22 to _c26 for Koalas DataFrame, or columns from Unnamed: 22 to Unnamed: 26 for Pandas DataFrame) have no data and thus can safely be dropped as well.

Except for the date of interview, all of the other columns in the dataset have categorical (textual) values. In order to use machine learning to solve the problem, those categorical values must be transformed into numeric values because a machine learning model can only consume numeric data.

The column of Date of Interview should be split into day, month, and year to increase prediction power since the information of individual day, month, and year tends to be more strongly correlated with seasonable jobs compared with a string of date as a whole.

The columns of Nature of Skillset and Candidate Native location have a large number of unique entries. These will introduce a large number of new derived features after one-hot encoding. Too many features can lead to a curse of dimensionality problem in the case the size of dataset is not large enough. To alleviate such problem, the values of these two columns are redivided into a smaller number of buckets.

The above data preprocessing/transformation can be summarized as following steps:

Bucketing skillset

Bucketing candidate native location

Parsing interview date

Changing categorical values to uppercase and dropping less useful features

One-Hot Encoding categorical values

These steps are implemented by developing an Object-Oriented data preprocessing pipeline for both Pandas and Koalas DataFrames by combining Pandas and Koalas DataFrames with scikit-learn pipeline API (i.e., BaseEstimator, TransformerMixin, and Pipeline).

3.1 Transforming Column Values

Several data preprocessing steps share a common operation of transforming the values of a particular column in a DataFrame. But, as described in Koalas Series, a Koalas Series does not support some of the common Pandas DataFrame and Series indexing mechanisms such as df.iloc[0]. Because of this, there is no simple method of traversing and changing the values of a column in a Koalas DataFrame.

The other difficulty is that Koalas does not allow to build a new Koalas Series object from scratch and then add it as a new column in an existing Koalas DataFrame. It only allows a new Koalas Series object that is built from the existing columns of a Koalas DataFrame.

The difficulties above are avoided by defining a global function to call the apply() method of a Koalas Series object.

def transformColumn(column_values, func, func_type): def transform_column(column_element) -> func_type:

return func(column_element)



cvalues = column_values

cvalues = cvalues.apply(transform_column)

return cvalues

3.2 Bucketing Skillset

To alleviate the curse of dimensionality issue, the transform() method of the BucketSkillset transformer class divides the unique values of the skillset column into smaller number of buckets by changing those values that appear less than 9 times as one same string value of Others.

class BucketSkillset(BaseEstimator, TransformerMixin):

def __init__(self):

self.skillset = ['JAVA/J2EE/Struts/Hibernate', 'Fresher', 'Accounting Operations', 'CDD KYC', 'Routine', 'Oracle',

'JAVA/SPRING/HIBERNATE/JSF', 'Java J2EE', 'SAS', 'Oracle Plsql', 'Java Developer',

'Lending and Liabilities', 'Banking Operations', 'Java', 'Core Java', 'Java J2ee', 'T-24 developer',

'Senior software engineer-Mednet', 'ALS Testing', 'SCCM', 'COTS Developer', 'Analytical R & D',

'Sr Automation Testing', 'Regulatory', 'Hadoop', 'testing', 'Java', 'ETL', 'Publishing']



def fit(self, X, y=None):

return self



def transform(self, X, y=None):



func = lambda x: x if x in self.skillset else 'Others'



X1 = X.copy()

cname = 'Nature of Skillset'

cvalue = X1[cname]



if type(X1) == ks.DataFrame:

cvalue = transformColumn(cvalue, func, str)

X1[cname] = cvalue

elif type(X1) == pd.DataFrame:

X2 = map(func, cvalue)

X1[cname] = pd.Series(X2)

else:

print('BucketSkillset: unsupported dataframe: {}'.format(type(X1)))

pass



return X1

3.3 Bucketing candidate native location

Similarly to bucketing skillset, to alleviate the curse of dimensionality issue, the transform() method of the BucketLocation transformer class divides the unique values of the candidate native location column into smaller number of buckets by changing those values that appear less than 12 times into one same value of Others.

class BucketLocation(BaseEstimator, TransformerMixin):

def __init__(self):

self.candidate_locations = ['Chennai', 'Hyderabad', 'Bangalore', 'Gurgaon', 'Cuttack', 'Cochin',

'Pune', 'Coimbatore', 'Allahabad', 'Noida', 'Visakapatinam', 'Nagercoil',

'Trivandrum', 'Kolkata', 'Trichy', 'Vellore']





def fit(self, X, y=None):

return self



def transform(self, X, y=None):

X1 = X.copy()



func = lambda x: x if x in self.candidate_locations else 'Others'



cname = 'Candidate Native location'

cvalue = X1[cname]

if type(X1) == ks.DataFrame:

cvalue = transformColumn(cvalue, func, str)

X1[cname] = cvalue

elif type(X1) == pd.DataFrame:

X2 = map(func, cvalue)

X1[cname] = pd.Series(X2)

else:

print('BucketLocation: unsupported dataframe: {}'.format(type(X1)))

pass



return X1

3.4 Parsing Interview Date

The values of the column of Date of Interview are messy in that various formats are used. For instance not only different delimits are used to separate day, month, and year, but also different orders of day, month, and year are followed. This is handled by the _parseDate() and transform_date() methods of the ParseInterviewDate transformer class. The overall functionality of the transform() method is to separate the interview date string into values of individual day, month, and year.

class ParseInterviewDate(BaseEstimator, TransformerMixin):

def __init__(self):

pass

def __parseDate(self, string, delimit):

try:

if ('&' in string):

subs = tuple(string.split('&'))

string = subs[0]

except:

print ('TypeError: {}'.format(string))

return None



string = string.strip()



try:

d = datetime.strptime(string, '%d{0}%m{0}%Y'.format(delimit))

except:

try:

d = datetime.strptime(string, '%d{0}%m{0}%y'.format(delimit))

except:

try:

d = datetime.strptime(string, '%d{0}%b{0}%Y'.format(delimit))

except:

try:

d = datetime.strptime(string, '%d{0}%b{0}%y'.format(delimit))

except:

try:

d = datetime.strptime(string, '%b{0}%d{0}%Y'.format(delimit))

except:

try:

d = datetime.strptime(string, '%b{0}%d{0}%y'.format(delimit))

except:

d = None

return d



def fit(self, X, y=None):

return self



def transform(self, X, y=None):



def transform_date(ditem):

if (isinstance(ditem, str) and len(ditem) > 0):

if ('.' in ditem):

d = self.__parseDate(ditem, '.')

elif ('/' in ditem):

d = self.__parseDate(ditem, '/')

elif ('-' in ditem):

d = self.__parseDate(ditem, '-')

elif (' ' in ditem):

d = self.__parseDate(ditem, ' ')

else:

d = None



if (d is None):

return 0, 0, 0

else:

return d.day, d.month, d.year



def get_day(column_element) -> int:

try:

day, month, year = transform_date(column_element)

return int(day)

except:

return 0



def get_month(column_element) -> int:

try:

day, month, year = transform_date(column_element)

return int(month)

except:

return 0



def get_year(column_element) -> int:

try:

day, month, year = transform_date(column_element)

return int(year)

except:

return 0



def pandas_transform_date(X1):

days = []

months = []

years = []

ditems = X1['Date of Interview'].values

for ditem in ditems:

if (isinstance(ditem, str) and len(ditem) > 0):

if ('.' in ditem):

d = self.__parseDate(ditem, '.')

elif ('/' in ditem):

d = self.__parseDate(ditem, '/')

elif ('-' in ditem):

d = self.__parseDate(ditem, '-')

elif (' ' in ditem):

d = self.__parseDate(ditem, ' ')

else:

d = None



if (d is None):

# print(""{}, invalid format of interview date!"".format(ditem))

days.append(0) # 0 - NaN

months.append(0)

years.append(0)

else:

days.append(d.day)

months.append(d.month)

years.append(d.year)

else:

days.append(0)

months.append(0)

years.append(0)



X1['Year'] = years

X1['Month'] = months

X1['Day'] = days



return X1



X1 = X.copy()



if type(X1) == ks.DataFrame:

X1['Year'] = X1['Date of Interview']

X1['Month'] = X1['Date of Interview']

X1['Day'] = X1['Date of Interview']



func_map = {'Year' : get_year, 'Month' : get_month, 'Day' : get_day}

for cname in func_map:

cvalue = X1[cname]

cvalue = cvalue.apply(func_map[cname])

X1[cname] = cvalue

elif type(X1) == pd.DataFrame:

X1 = pandas_transform_date(X1)

else:

print('ParseInterviewDate: unsupported dataframe: {}'.format(type(X1)))

pass



return X1

3.5 Changing Categorical Values to Uppercase and Dropping Less Useful Features

The transform() method of the FeaturesUppercase transformer class is to change the values of categorical features to uppercase and at the same time drop less useful features.

class FeaturesUppercase(BaseEstimator, TransformerMixin):

def __init__(self, feature_names, drop_feature_names):

self.feature_names = feature_names

self.drop_feature_names = drop_feature_names



def fit(self, X, y=None):

return self



def transform(self, X, y=None):



func = lambda x: x.strip().upper()



X1 = X.copy()



for fname in self.feature_names:

values = X1[fname]

values = values.fillna('NaN')

if type(X1) == ks.DataFrame:

values = transformColumn(values, func, str)

elif type(X1) == pd.DataFrame:

values = map(lambda x: x.strip().upper(), values)

else:

print('FeaturesUppercase: unsupported dataframe: {}'.format(type(X1)))

X1[fname] = values



# drop less important features

X1 = X1.drop(self.drop_feature_names, axis=1)



return X1

3.6 One-Hot Encoding Categorical Values

The transform() method of the OneHotEncodeData transformer class calls the get_dummies() method of DataFrame to one-hot encode the values of categorical values.

class OneHotEncodeData(BaseEstimator, TransformerMixin):

def __init__(self):

self.one_hot_feature_names = ['Client name',

'Industry',

'Location',

'Position to be closed',

'Nature of Skillset',

'Interview Type',

'Gender',

'Candidate Current Location',

'Candidate Job Location',

'Interview Venue',

'Candidate Native location',

'Have you obtained the necessary permission to start at the required time',

'Hope there will be no unscheduled meetings',

'Can I Call you three hours before the interview and follow up on your attendance for the interview',

'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',

'Have you taken a printout of your updated resume. Have you read the JD and understood the same',

'Are you clear with the venue details and the landmark.',

'Has the call letter been shared',

'Marital Status']

self.label_encoders = None

self.one_hot_encoders = None



def fit(self, X, y=None):

return self



def transform(self, X, y=None):

X1 = X.copy()

if type(X1) == ks.DataFrame:

X1 = ks.get_dummies(X1)

elif type(X1) == pd.DataFrame:

X1 = pd.get_dummies(X1)

else:

print('OneHotEncodeData: unsupported dataframe: {}'.format(type(X1)))

pass



return X1

3.7 Combining Transformers into Pipeline

All of the data preprocessing transformers are combined into a scikit-learn pipeline as follows in the PreprocessData() method of the PredictInterview class (see Section 4.3 for details). The fit() and transform() methods of these transformers will be executed sequentially once the fit_transform() method of the pipeline object is called.

self.pipeline = Pipeline([

('bucket_skillset', BucketSkillset()),

('bucket_location', BucketLocation()),

('parse_interview_date', ParseInterviewDate()),

('features_to_uppercase', FeaturesUppercase(self.feature_names, self.drop_feature_names)),

('one_hot_encoder', self.oneHotEncoder)

])

4. Modeling

Once the dataset has been prepared, the next step is modeling. The main goals of modeling include:

Identify machine learning model

Train machine learning model

Tune the hyper-parameters of machine learning model

4.1 Identifying Machine Learning Model

There are three major high-level types of machine learning and deep learning algorithms/models:

supervised machine learning and deep learning

unsupervised machine learning and deep learning

reinforcement learning

Supervised machine learning and deep learning can be divided into subtypes such as regression and classification. Each subtype includes various machine learning and deep learning algorithms/models. For instance, supervised machine learning classification models include Decision Tree classifier, Random Forest classifier, GBM classifier, etc.

Generally speaking, given a business problem, there are many different types of models that can be used as possible solutions. These different models need to be compared to identify the most promising one as the solution to the target business problem. Thus model identification can not be done in isolation. It depends on model training and evaluation/comparison of model performance metrics.

In this article we simply select the scikit-learn RandomForestClassifier model for demonstration purpose.

4.2 Training model and tuning hyper-parameters

Once a model (e.g., RandomForestClassifier) is identified, typically there are multiple hyper-parameters to be tuned. A hyper-parameter is a parameter that needs to be set before a model training can begin and such hyper-parameter value does not change during model training. For example, the Random Forest Classifier has multiple hyper-parameters such as number of estimators, max depth, etc.

The sciket-learn GridSearchCV is a popular library for searching the best combination of hyper-parameters of a given model by automatically executing an instance of the model many times. Each execution corresponds to a unique combination of the selected hyper-parameter values. The GridSearch class is to use this library to find the best combination of number of estimators and max depth:

class GridSearch(object):

def __init__(self, cv=10):

self.grid_param = [

{'n_estimators': range(68,69), # range(60, 70)

'max_depth' : range(8,9)} # range(5, 10)}

]

self.cv = cv

self.scoring_function = make_scorer(f1_score, greater_is_better=True)

self.gridSearch = None



def fit(self, X, y):

rfc = RandomForestClassifier()

self.gridSearchCV = GridSearchCV(rfc, self.grid_param, cv=self.cv, scoring=self.scoring_function)

self.gridSearchCV.fit(X, y)

return self.gridSearchCV.best_estimator_

4.3 Tracking Model Hyper-Parameters and Performance Metrics

One designed functionality of mlflow is to track and compare the hyper-parameters and performance metrics of different model executions.

The method of mlFlow() of the PredictInterview class is to train a model, use the trained model to predict results, obtain various model performance metrics, and then call the mlflow API to track both the hyper-parameters and performance metrics, and at the same time log a trained model into a file for later usage such as deployment.

def mlFlow(self):

np.random.seed(40)

with mlflow.start_run():

self.loadData()

self.PreprocessData()

self.trainModel()

self.predictClasses()

accuracy_score, f1_score, rmse_score, mae_score, r2_score = self.getModelMetrics() best_params = self.gridSearch.gridSearchCV.best_params_ mlflow.log_param(""n_estimators"", best_params[""n_estimators""])

mlflow.log_param(""max_depth"", best_params[""max_depth""])

mlflow.log_metric(""rmse"", rmse_score)

mlflow.log_metric(""r2"", r2_score)

mlflow.log_metric(""mae"", mae_score)

mlflow.log_metric(""accuracy"", accuracy_score)

mlflow.log_metric(""f1"", f1_score) mlflow.sklearn.log_model(self.rfc, ""random_forest_model"")

A Jupyter notebook of the PredictInterview class below and all the other pieces of source code in this article are available in Github [6].

class PredictInterview(object):

def __init__(self, use_koalas=True):

self.use_koalas = use_koalas

self.dataset_file_name = 'Interview_Attendance_Data.csv'

self.feature_names = ['Date of Interview',

'Client name',

'Industry',

'Location',

'Position to be closed',

'Nature of Skillset',

'Interview Type',

'Gender',

'Candidate Current Location',

'Candidate Job Location',

'Interview Venue',

'Candidate Native location',

'Have you obtained the necessary permission to start at the required time',

'Hope there will be no unscheduled meetings',

'Can I Call you three hours before the interview and follow up on your attendance for the interview',

'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',

'Have you taken a printout of your updated resume. Have you read the JD and understood the same',

'Are you clear with the venue details and the landmark.',

'Has the call letter been shared', 'Marital Status']



if self.use_koalas:

self.drop_feature_names = [

'Name(Cand ID)',

'Date of Interview',

'_c22',

'_c23',

'_c24',

'_c25',

'_c26']

else: # use Pandas

self.drop_feature_names = [

'Unnamed: 22',

'Unnamed: 23',

'Unnamed: 24',

'Unnamed: 25',

'Unnamed: 26']



self.dataset = None

self.rfc = None

self.gridSearch = None

self.X_train = None

self.y_train = None

self.X_test = None

self.y_test = None

self.y_pred = None

self.X_clean = None

self.y_clean = None

self.X_train_encoded = None

self.X_test_encoded = None

self.y_train_encoded = None

self.accuracy_score = None

self.f1_score = None

self.oneHotEncoder = None

self.X_test_name_ids = None

self.pipeline = None





def loadData(self, path=None):

if (path != None):

path = os.path.join(path, self.dataset_file_name)

else:

path = self.dataset_file_name



if self.use_koalas:

dataset = ks.read_csv(path)

else:

dataset = pd.read_csv(path)



# shuffle data

self.dataset = dataset.sample(frac=1.0)



return self.dataset



def PreprocessData(self):

y = self.dataset['Observed Attendance'] # extract labels y

if self.use_koalas:

X = self.dataset.drop('Observed Attendance') # extract features X

else:

X = self.dataset.drop(['Observed Attendance'], axis=1)



self.oneHotEncoder = OneHotEncodeData()



self.pipeline = Pipeline([

('bucket_skillset', BucketSkillset()),

('bucket_location', BucketLocation()),

('parse_interview_date', ParseInterviewDate()),

('features_to_uppercase', FeaturesUppercase(self.feature_names, self.drop_feature_names)),

('one_hot_encoder', self.oneHotEncoder)

])



X_1hot = self.pipeline.fit_transform(X)



# fill up missing labels and then change labels to uppercase

y = y.fillna('NaN')



if self.use_koalas:

func = lambda x: x.strip().upper()

y_uppercase = transformColumn(y, func, str)

else:

y_uppercase = map(lambda x: x.strip().upper(), y.values)

y_uppercase = pd.Series(y_uppercase)



# separate labeled records from unlabeled records

self.X_train_encoded = X_1hot[y_uppercase != 'NAN']

self.X_test_encoded = X_1hot[y_uppercase == 'NAN']



# save Names/ID for reporting later one

self.X_test_name_ids = self.dataset['Name(Cand ID)'].loc[y_uppercase == 'NAN']



y_train = y_uppercase.loc[y_uppercase != 'NAN']



# encode labels as follows: 0 - NO, 1 - YES, NAN - NAN

if self.use_koalas:

func = lambda x: 1 if x == 'YES' else 0

y = transformColumn(y_train, func, int)

else:

y = map(lambda x: 1 if x == 'YES' else 0, y_train)

y = pd.Series(y)



self.y_train_encoded = y



self.X_clean = X_1hot

self.y_clean = y_uppercase



return None



def __splitData(self):

if self.use_koalas:

X_train_encoded = self.X_train_encoded.to_numpy()

y_train_encoded = self.y_train_encoded.to_numpy()

else:

X_train_encoded = self.X_train_encoded.values

y_train_encoded = self.y_train_encoded.values



self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_train_encoded,

y_train_encoded,

test_size = 0.25, random_state = 0)

return (self.X_train, self.X_test, self.y_train, self.y_test)



def trainModel(self):

X_train, X_test, y_train, y_test = self.__splitData()

self.gridSearch = GridSearch()

self.rfc = self.gridSearch.fit(X_train, y_train)

return self.rfc



def predictClasses(self):

if (self.rfc is None):

print(""No trained model available, please train a model first!"")

return None



self.y_pred = self.rfc.predict(self.X_test)

return self.y_pred



def getModelMetrics(self):

if (self.y_test is None or self.y_pred is None):

print('Failed to get model performance metrics because y_test is null or y_pred is null!')

return None



self.accuracy_score = accuracy_score(self.y_test, self.y_pred)

self.f1_score = f1_score(self.y_test, self.y_pred)



pred = self.predictAttendanceProbability(self.X_test)[:, 1]

actual = self.y_test.astype(float)



self.rmse_score = np.sqrt(mean_squared_error(actual, pred))

self.mae_score = mean_absolute_error(actual, pred)

self.r2_score = r2_score(actual, pred)



return (self.accuracy_score, self.f1_score, self.rmse_score, self.mae_score, self.r2_score)



def predictNullAttendanceProbability(self):

y_pred = self.rfc.predict_proba(self.X_test_encoded.to_numpy())

return y_pred



def predictNullAttendanceClasses(self):

y_pred = self.rfc.predict(self.X_test_encoded.to_numpy())

return y_pred



def predictAttendanceProbability(self, X):

y_pred = self.rfc.predict_proba(X)

return y_pred



def predictAttendanceClass(self, X):

y_pred = self.rfc.predict(X)

return y_pred



def mlFlow(self):

np.random.seed(40)

with mlflow.start_run():

self.loadData()

self.PreprocessData()

self.trainModel()

self.predictClasses()

accuracy_score, f1_score, rmse_score, mae_score, r2_score = self.getModelMetrics() best_params = self.gridSearch.gridSearchCV.best_params_ mlflow.log_param(""n_estimators"", best_params[""n_estimators""])

mlflow.log_param(""max_depth"", best_params[""max_depth""])

mlflow.log_metric(""rmse"", rmse_score)

mlflow.log_metric(""r2"", r2_score)

mlflow.log_metric(""mae"", mae_score)

mlflow.log_metric(""accuracy"", accuracy_score)

mlflow.log_metric(""f1"", f1_score) mlflow.sklearn.log_model(self.rfc, ""random_forest_model"")

The code below shows how to instantiate an object of the PredictInterview class and then call its mlFlow() method.

predictInterview = PredictInterview(use_koalas=True)

predictInterview.mlFlow()

4.4 Comparing Model Hyper-Parameters and Performance Metrics

Once the hyper-parameters and performance metrics of a model have been tracked in mlflow, we can use a terminal or Jupyter notebook to start the mlflow UI (User Interface) as follows to view the history of model executions:

!mlflow ui # for jupyter notebook

Assuming that the mlflow UI starts on a local machine, the following IP address and port number can be used to view the results in a Web browser:

http://127.0.0.1:5000

The following picture is a snapshot of a model execution history in the mlflow UI:","['machine', 'dataframes', 'dataframe', 'pandas', 'interview', 'learning', 'koalas', 'model', 'data', 'x', 'mlflow', 'values', 'objectoriented', 'pipeline']","Object-Oriented Machine Learning Pipeline with mlflow for Pandas and Koalas DataFramesEnd-to-end process of developing Spark-enabled machine learning pipeline in Python using Pandas, Koalas, scikit-learn, and mlflowIn the article Python Data Preprocessing Using Pandas DataFrame, Spark DataFrame, and Koalas DataFrame, I used a public dataset to evaluate and compare the basic functionality of Pandas, Spark, and Koalas DataFrames in typical data preprocessing steps for machine learning.
In this article, I use a more challenging dataset, the Interview Attendance Problem for Kaggle competition to demonstrate an end-to-end process of developing an Object-Oriented machine learning pipeline in Python for both Pandas and Koalas DataFrames using Pandas, Koalas, scikit-learn, and mlflow.
In order to use machine learning to solve the problem, those categorical values must be transformed into numeric values because a machine learning model can only consume numeric data.
It only allows a new Koalas Series object that is built from the existing columns of a Koalas DataFrame.
The main goals of modeling include:Identify machine learning modelTrain machine learning modelTune the hyper-parameters of machine learning model4.1 Identifying Machine Learning ModelThere are three major high-level types of machine learning and deep learning algorithms/models:supervised machine learning and deep learningunsupervised machine learning and deep learningreinforcement learningSupervised machine learning and deep learning can be divided into subtypes such as regression and classification.",en,['Yuefeng Zhang'],2019-11-15 16:09:50.100000+00:00,"{'Scikit Learn', 'Koalas', 'Pyspark', 'Mlflow', 'Machine Learning Pipeline'}","{'https://miro.medium.com/max/4136/1*jVpAifUQejwP8DaveV-Udg.png', 'https://miro.medium.com/max/5284/1*drUXcQA-RqUGFEbm35PyyQ.png', 'https://miro.medium.com/max/6528/1*t-EB042Z8x5YUCD0cZiGzA.jpeg', 'https://miro.medium.com/max/60/1*WfjlQixYhzQ_6k-2yto-eQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/0*LgcqU4P_enK1TWfp', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3310/1*WfjlQixYhzQ_6k-2yto-eQ.png', 'https://miro.medium.com/max/1198/1*eapYiAVmG5HC9JU0QLZWEw.png', 'https://miro.medium.com/max/4016/1*oSM_m9HjN-8qvr268V-lWA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/536/1*2NiX5sDQw1qlmeUFFvugIw.png', 'https://miro.medium.com/max/60/1*t-EB042Z8x5YUCD0cZiGzA.jpeg?q=20', 'https://miro.medium.com/max/36/1*2NiX5sDQw1qlmeUFFvugIw.png?q=20', 'https://miro.medium.com/max/60/1*eapYiAVmG5HC9JU0QLZWEw.png?q=20', 'https://miro.medium.com/max/60/1*drUXcQA-RqUGFEbm35PyyQ.png?q=20', 'https://miro.medium.com/max/1200/1*t-EB042Z8x5YUCD0cZiGzA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*jVpAifUQejwP8DaveV-Udg.png?q=20', 'https://miro.medium.com/max/60/1*oSM_m9HjN-8qvr268V-lWA.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*LgcqU4P_enK1TWfp'}",2020-03-05 00:09:39.939635,6.531802654266357
https://towardsdatascience.com/visualizing-statistical-plots-with-seaborn-6b6e60ce5e71,Visualizing statistical plots with Seaborn,"Now lets look at visualizing data that has more categorical columns in it. For this we will be using the ‘tips’ dataset. Let load it and have a look at it.

tips = sns.load_dataset('tips')

tips.head()

Tips data

The ‘tips’ dataset contains information regarding the tip paid by diners and their information such as bill amount, sex, time etc. Lets start off by looking at the distribution of the tip amounts and its relationship with the total bill amount.

sns.distplot(tips['tip'], kde = False, bins = 30)

sns.jointplot(x = 'total_bill', y = 'tip', data = tips, kind = 'kde')

Fig 11

Fig 12

Now lets see if the tip given varies by day or not. This can be done with the help of a boxplot.

sns.boxplot(x = 'day',y = 'tip', data = tips, palette = 'coolwarm')

Fig 13

Perhaps we want to compare by the sex of the diners. Like in the pairplot, we can specify that by the ‘hue’ parameter as well

sns.boxplot(x = 'day',y = 'tip', data = tips, hue = 'sex', palette = 'coolwarm')

Fig 14

This creates two boxplots for each day on the basis of sex.

Another way of visualizing the above data in with the help of a violinplot.

sns.violinplot(x = 'day',y = 'tip', data = tips, palette = 'rainbow')

Fig 15

The violin plot can be thought of as a combination of the box and kde plots. The thick line in the center indicates the interquartile range with the kde of the tip on both sides.

Similar to the box plot, we can use ‘sex’ to create two violin plots side by side to compare. But we have a better option in this case.

sns.violinplot(x = 'day',y = 'tip', data = tips, hue = 'sex', split = True, palette = 'rainbow')

Fig 16

By indicating ‘split’ as true we are able to create two different kdes on each side of the central line. This allows us to see the differences side by side and not crowd out the plot.

Previously we saw that we were able to use pair plots to have a look at the whole data and include one categorical column. But in this case we have multiple categorical columns. These can be integrated into the plots using Facet Grid.

g = sns.FacetGrid(tips, col=""time"", row=""smoker"")

Fig 17

The above command created a 2 x 2 plots to account for the four possibilities across ‘time’ and ‘smoker’. That is one plot each for every combination of what time was the meal and whether the diner was a smoker or not.

To these plots we can add the numerical data and compare how they differ across the above two columns of ‘time’ and ‘smoker’.

g = sns.FacetGrid(tips, col=""time"", row=""smoker"")

g = g.map(plt.hist, ""tip"")

Fig 18

I added the distribution for tip values to see how they differ across ‘time’ and ‘smoker’. Above each plot is a line indicating which ‘time’ and ‘smoker’ value it corresponds to.

Similarly we can also plot two numerical columns and see the relationship between the two and across ‘time’ and ‘smoker’.

g = sns.FacetGrid(tips, col=""time"", row=""smoker"",hue='sex')

g = g.map(plt.scatter, ""total_bill"", ""tip"").add_legend()

Fig 18

The above plot shows the relationship between the tip and bill amounts and across ‘time’ and ‘smoker’. In addition, the colour scheme of each plot shows the sex as well. By using facet grids we can create plots involving multiple numerical and categorical columns.

Lastly I want to cover heatmaps and clustermaps.

In the below figure I am using a heatmap to show the correlation of all the numerical values against each other.

sns.heatmap(tips.corr(), cmap = 'coolwarm', annot = True)

Fig 19

The command ‘tips.corr’ gives the correlation between all the numerical variables. By indicating ‘annot’ as true we can have the correlation values on the figure as well.

For building the clustermap I first pivoted the tips dataset to get the value of tip amount across day and sex.

tip_pivot = tips.pivot_table(values='tip',index='day',columns='sex')

Pivoted table

Using this I can create a clustermap on the value of tips and how close are the days to each other.

sns.clustermap(tip_pivot, cmap = 'coolwarm', standard_scale = 1)

Fig 21

As there are only two sexes, there is not much to see in terms of clustering there. But in the case of days we see that Saturday and Thursday are grouped together indicating the similarity in the tip amounts given on those days.

The above clustermap is pretty simple as there are not many categories on each axis. A clustermap can really shine when there are multiple categories allowing to discern similarities and patterns.","['numerical', 'tips', 'sex', 'statistical', 'data', 'plot', 'tip', 'visualizing', 'plots', 'smoker', 'palette', 'using', 'seaborn']","sns.distplot(tips['tip'], kde = False, bins = 30)sns.jointplot(x = 'total_bill', y = 'tip', data = tips, kind = 'kde')Fig 11Fig 12Now lets see if the tip given varies by day or not.
sns.boxplot(x = 'day',y = 'tip', data = tips, palette = 'coolwarm')Fig 13Perhaps we want to compare by the sex of the diners.
sns.violinplot(x = 'day',y = 'tip', data = tips, palette = 'rainbow')Fig 15The violin plot can be thought of as a combination of the box and kde plots.
These can be integrated into the plots using Facet Grid.
To these plots we can add the numerical data and compare how they differ across the above two columns of ‘time’ and ‘smoker’.",en,['Pranav Prathvikumar'],2019-11-11 14:59:00.191000+00:00,"{'Data Analysis', 'Python', 'Data Visualization', 'Data Science'}","{'https://miro.medium.com/max/864/1*C-NluLTROMwDTIRsMxDChQ.jpeg', 'https://miro.medium.com/fit/c/96/96/2*edkIaQIh_02NTA-SJssg6Q.jpeg', 'https://miro.medium.com/max/864/1*03p85FbEIBtZ1wRlYuiGVA.jpeg', 'https://miro.medium.com/max/60/1*HwmZqUOtOa0sJXvuojnUPw.jpeg?q=20', 'https://miro.medium.com/max/864/1*HwmZqUOtOa0sJXvuojnUPw.jpeg', 'https://miro.medium.com/max/60/1*Ph4JWUGzEZ4gok2FpdPUeg.jpeg?q=20', 'https://miro.medium.com/max/60/1*AfOKeVQ2u2QQ5L13iuWEZw.jpeg?q=20', 'https://miro.medium.com/max/864/1*7D9lBUS2hL2zXXMgLrzd7w.jpeg', 'https://miro.medium.com/max/1050/1*UHcJWJ2chmFYjigKXraeRA.jpeg', 'https://miro.medium.com/max/864/1*Qi5qPQHLg4atGSky9h2MWA.jpeg', 'https://miro.medium.com/max/9620/1*y9rN9LBOp3fDp_LyuJWiKQ.jpeg', 'https://miro.medium.com/max/60/1*CCE4SOl-AQPyktvm9Fnq9g.jpeg?q=20', 'https://miro.medium.com/max/864/1*JG-OJrGJP1mhCLGSFHtUJQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*xEAlcLTrMcJWkLpqQCFn5w.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/864/1*CCE4SOl-AQPyktvm9Fnq9g.jpeg', 'https://miro.medium.com/max/60/1*0O1epgoRW77rgbR4nsBSOQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*6FkdkmRLn_o6pxzhnV7KRQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*Qi5qPQHLg4atGSky9h2MWA.jpeg?q=20', 'https://miro.medium.com/max/864/1*Ph4JWUGzEZ4gok2FpdPUeg.jpeg', 'https://miro.medium.com/max/60/1*r4pBZHyJws8k2pVcx0GULQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*C-NluLTROMwDTIRsMxDChQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*y9rN9LBOp3fDp_LyuJWiKQ.jpeg?q=20', 'https://miro.medium.com/max/456/1*py1c2xRQc_0EFIhUbDbeWw.jpeg', 'https://miro.medium.com/max/864/1*cwX_aB1SxLiYe9akxNayFw.jpeg', 'https://miro.medium.com/max/60/1*x0ONSIDDZMbt1JOGG4aTrA.jpeg?q=20', 'https://miro.medium.com/max/1200/1*y9rN9LBOp3fDp_LyuJWiKQ.jpeg', 'https://miro.medium.com/max/864/1*x0ONSIDDZMbt1JOGG4aTrA.jpeg', 'https://miro.medium.com/max/60/1*py1c2xRQc_0EFIhUbDbeWw.jpeg?q=20', 'https://miro.medium.com/max/60/1*asckzOcxVnK2lSnpbk4jhA.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/864/1*xEAlcLTrMcJWkLpqQCFn5w.jpeg', 'https://miro.medium.com/fit/c/160/160/2*edkIaQIh_02NTA-SJssg6Q.jpeg', 'https://miro.medium.com/max/1440/1*r4pBZHyJws8k2pVcx0GULQ.jpeg', 'https://miro.medium.com/max/60/1*7D9lBUS2hL2zXXMgLrzd7w.jpeg?q=20', 'https://miro.medium.com/max/60/1*8caGw3yosvWipj_JFrU31w.jpeg?q=20', 'https://miro.medium.com/max/908/1*6FkdkmRLn_o6pxzhnV7KRQ.jpeg', 'https://miro.medium.com/max/1016/1*iMvjPm4kFva-BNrg608Xxw.jpeg', 'https://miro.medium.com/max/60/1*iMvjPm4kFva-BNrg608Xxw.jpeg?q=20', 'https://miro.medium.com/max/864/1*asckzOcxVnK2lSnpbk4jhA.jpeg', 'https://miro.medium.com/max/60/1*JG-OJrGJP1mhCLGSFHtUJQ.jpeg?q=20', 'https://miro.medium.com/max/864/1*L4_3mada3Xl8agv1LfyOdw.jpeg', 'https://miro.medium.com/max/60/1*UHcJWJ2chmFYjigKXraeRA.jpeg?q=20', 'https://miro.medium.com/max/60/1*cwX_aB1SxLiYe9akxNayFw.jpeg?q=20', 'https://miro.medium.com/max/60/1*03p85FbEIBtZ1wRlYuiGVA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1612/1*AfOKeVQ2u2QQ5L13iuWEZw.jpeg', 'https://miro.medium.com/max/864/1*m2SXzDRyHX1Ye2xR7DC67g.jpeg', 'https://miro.medium.com/max/864/1*8caGw3yosvWipj_JFrU31w.jpeg', 'https://miro.medium.com/max/60/1*m2SXzDRyHX1Ye2xR7DC67g.jpeg?q=20', 'https://miro.medium.com/max/1440/1*0O1epgoRW77rgbR4nsBSOQ.jpeg', 'https://miro.medium.com/max/60/1*L4_3mada3Xl8agv1LfyOdw.jpeg?q=20'}",2020-03-05 00:09:42.021239,2.0806386470794678
https://towardsdatascience.com/random-forest-vs-neural-networks-for-predicting-customer-churn-691666c7431e,Random Forest vs Neural Networks for Predicting Customer Churn,"Random Forest vs Neural Networks for Predicting Customer Churn

Let us see how random forest competes with neural networks for solving a real world business problem

Stuck behind the paywall? Click here to read the full story with my Friend Link!

Customer churn prediction is an essential requirement for a successful business. Most companies with a subscription based business regularly monitors churn rate of their customer base. Statistically 59% of customers don’t return after a bad customer service experience. In addition, cost of acquiring new customers is quite high. This makes predictive models of customer churn appealing as they enable companies to maintain their existing customers at a higher rate. Although defining and predicting customer churn might appear straightforward initially, it involves several practical challenges.

The Challenge

To make a predictive model to anticipate which customers are most likely to churn. This would help the marketing team take appropriate decisions to retain them.

Environment and tools

scikit-learn keras numpy pandas matplotlib

Data

The dataset can be downloaded from the kaggle website which can be found here.

Description of variables in the dataset:

customerID: Customer ID

gender: Whether the customer is a male or a female

SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)

Partner: Whether the customer has a partner or not (Yes, No)

Dependents: Whether the customer has dependents or not (Yes, No)

tenure: Number of months the customer has stayed with the company

PhoneService: Whether the customer has a phone service or not (Yes, No)

MultipleLines: Whether the customer has multiple lines or not (Yes, No, No phone service)

InternetService: Customer’s internet service provider (DSL, Fiber optic, No)

OnlineSecurity: Whether the customer has online security or not (Yes, No, No internet service)

OnlineBackup: Whether the customer has online backup or not (Yes, No, No internet service)

DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)

TechSupport: Whether the customer has tech support or not (Yes, No, No internet service)

StreamingTV: Whether the customer has streaming TV or not (Yes, No, No internet service)

StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)

Contract: The contract term of the customer (Month-to-month, One year, Two year)

PaperlessBilling: Whether the customer has paperless billing or not (Yes, No)

PaymentMethod: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))

MonthlyCharges: The amount charged to the customer monthly

TotalCharges: The total amount charged to the customer

Churn: Whether the customer churned or not (Yes or No)

Where is the code?

Without much ado, let’s get started with the code. The complete project on github can be found here.

I started with loading all the libraries and dependencies required.

import numpy as np

import pandas as pd

from matplotlib import pyplot as plt

from sklearn.model_selection import train_test_split import keras

from keras.models import Sequential

from keras.layers import InputLayer

from keras.layers import Dense

from keras.layers import Dropout

from keras.constraints import maxnorm

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

Let’s see how the dataset looks like.

read_csv is a pandas function to read csv files and do operations on it later. head() method is used to return top n (5 by default) rows of a DataFrame.

data = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')

data.head()

Data Preprocessing

I converted the categorical variables into numerical variables (e.g. Yes/No to 1/0). I ensured that all the values are in numeric format. Also I filled null values with zero.

data.SeniorCitizen.replace([0, 1], [""No"", ""Yes""], inplace= True)

data.TotalCharges.replace(["" ""], [""0""], inplace= True)

data.TotalCharges = data.TotalCharges.astype(float)

data.drop(""customerID"", axis= 1, inplace= True)

data.Churn.replace([""Yes"", ""No""], [1, 0], inplace= True)

pd.get_dummies creates a new dataframe which consists of zeros and ones. The dataframe will have a one depending on the truth of the categorical variables in this case.

data = pd.get_dummies(data)

Next I split the dataset into X and Y.

X contains all the features that are used for making the predictions.

Y contains the outcomes that is whether or not the customer churned

X = data.drop(""Churn"", axis= 1)

y = data.Churn

Next I used train_test_split to split the data into training and testing sets with 20% of the data given to the test set. The training set is used to train the model, while the test set is only used to evaluate the model’s performance.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)

Random Forest

I used random forest classifier with 100 trees and maximum depth of trees as 20.

rf.fit builds a forest of trees from the training set (X, Y). rf.score returns the mean accuracy on the given test data and labels.

rf = RandomForestClassifier(n_estimators=100, max_depth=20,

random_state=42)

rf.fit(X_train, y_train)

score = rf.score(X_train, y_train)

score2 = rf.score(X_test, y_test)

print(""Training set accuracy: "", '%.3f'%(score))

print(""Test set accuracy: "", '%.3f'%(score2))

The accuracy achieved on the training set is 99.8%, while on the test set it is 79%.

rf.predict is used to predict class for X.

rf_predictions = rf.predict(X_test)

rf_probs = rf.predict_proba(X_test)

Let’s evaluate the performance of the model using some other popular classification metrics.

Confusion Matrix

Confusion Matrix is a very important metric when analyzing misclassification. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class. The diagonals represent the classes that have been correctly classified. This helps as we not only know which classes are being misclassified but also what they are being misclassified as.

Precision, Recall and F1-Score

For a better look at misclassification, we often use the following metric to get a better idea of true positives (TP), true negatives (TN), false positive (FP) and false negative (FN).

Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.

Recall is the ratio of correctly predicted positive observations to all the observations in actual class.

F1-Score is the weighted average of Precision and Recall.

y_pred = rf.predict(X_test)

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

print(accuracy_score(y_test, y_pred))

The performance metrics are quite good for predicting customers who dosen’t churn with precision, recall and F1 score values of 0.83, 0.91,0.86. But the problem is that model is not able to accurately predict the customers who will churn with the corresponding values of 0.64, 0.47, 0.54.

I continued with identifying which features are important for the problem in hand. This can help in early detection and maybe even improve the business strategy.

fi = pd.DataFrame({'feature': list(X_train.columns),

'importance': rf.feature_importances_}).\

sort_values('importance', ascending = False)

fi.head()

It can be seen that the most important feature for our prediction problem is TotalCharges followed by tenure and MonthlyCharges.

Neural Networks

Now let’s code a neural network for the same problem. I used a very simple neural network. Please note that the data is in tabular format, hence we don’t need to use complicated architectures which would lead to overfitting.

I used two dense layers with 64 neurons and 8 neurons with relu as the activation function. input_dim argument denotes the number of features in the dataset or in other words the number of columns present in the dataset. In between, I used 20% dropouts to reduce overfitting. The dropout layer ensures that we remove a set percentage of the data each time we iterate through the neural network. kernel_constraint is used for scaling of the weights present in the neural network. The last layer is also a dense layer with 1 neuron and sigmoid as the activation function.

model = Sequential()

model.add(Dense(64, input_dim=46, activation='relu', kernel_constraint=maxnorm(3)))

model.add(Dropout(rate=0.2))

model.add(Dense(8, activation='relu', kernel_constraint=maxnorm(3)))

model.add(Dropout(rate=0.2))

model.add(Dense(1, activation='sigmoid'))

Next I compiled the model using binary_crossentropy as the loss function, adam as the optimizer and accuracy metric to track during training.

model.compile(loss = ""binary_crossentropy"", optimizer = 'adam', metrics=['accuracy'])

I trained the model for 50 epochs with a batch size value of 8. One epoch is when an entire dataset is passed forward and backward through the neural network only once. Batch size is the total number of training examples present in a single batch.

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=8)

Now let’s see how accuracy varies as a function of epochs.

plt.plot(history.history['accuracy'])

plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')

plt.ylabel('accuracy')

plt.xlabel('epoch')

plt.legend(['train', 'test'], loc='upper left')

plt.show()

Accuracy vs epochs

The test accuracy of the neural network after 50 epochs is 78% which is comparable to the 79% accuracy of the random forest.

Conclusions

In this article, I demonstrated how a business can predict and retain their customers. I compared random forest and neural networks for the same. The accuracy of both the algorithms are comparable, hence it is hard to tell which is better. Random forest has proven to be a great algorithm if the dataset is in tabular format. Random Forests requires less preprocessing and the training process is also much simpler. Also hyper-parameter tuning is easier with random forest when compared to neural networks. This gives random forest the edge above neural networks.

References/Further Readings

Before You Go

The corresponding source code can be found here.

Contacts

If you want to keep updated with my latest articles and projects follow me on Medium. These are some of my contacts details:

Happy reading, happy learning and happy coding!","['yes', 'forest', 'import', 'set', 'used', 'neural', 'churn', 'customer', 'predicting', 'networks', 'vs', 'customers', 'random', 'accuracy']","Random Forest vs Neural Networks for Predicting Customer ChurnLet us see how random forest competes with neural networks for solving a real world business problemStuck behind the paywall?
rf = RandomForestClassifier(n_estimators=100, max_depth=20,random_state=42)rf.fit(X_train, y_train)score = rf.score(X_train, y_train)score2 = rf.score(X_test, y_test)print(""Training set accuracy: "", '%.3f'%(score))print(""Test set accuracy: "", '%.3f'%(score2))The accuracy achieved on the training set is 99.8%, while on the test set it is 79%.
I compared random forest and neural networks for the same.
Random forest has proven to be a great algorithm if the dataset is in tabular format.
Also hyper-parameter tuning is easier with random forest when compared to neural networks.",en,['Abhinav Sagar'],2019-11-26 10:59:51.115000+00:00,"{'Neural Networks', 'Data Science', 'Artificial Intelligence', 'Random Forest', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*p_l6Kmhl-mqfv4vh8GoYZA.png?q=20', 'https://miro.medium.com/max/60/1*0aZXAEZp-70QFrSquLL5xw.jpeg?q=20', 'https://miro.medium.com/max/876/1*p_l6Kmhl-mqfv4vh8GoYZA.png', 'https://miro.medium.com/fit/c/160/160/2*ks56lissSz1lWjizk263EQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/2*ks56lissSz1lWjizk263EQ.jpeg', 'https://miro.medium.com/max/1656/1*q6atZ5wgbIH3De_kcrIZgw.png', 'https://miro.medium.com/max/60/1*-USK61ybuAHkH4YUuxZecQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/proxy/1*ll7lPS0HzHl6vYG0Ong-Ow.png', 'https://miro.medium.com/max/482/1*d9HiscAX3Xx5BboP4tpQyQ.png', 'https://miro.medium.com/max/60/1*q6atZ5wgbIH3De_kcrIZgw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/8644/1*0aZXAEZp-70QFrSquLL5xw.jpeg', 'https://miro.medium.com/max/790/1*-USK61ybuAHkH4YUuxZecQ.png', 'https://miro.medium.com/max/60/1*d9HiscAX3Xx5BboP4tpQyQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*0aZXAEZp-70QFrSquLL5xw.jpeg'}",2020-03-05 00:09:48.603384,6.581209659576416
https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e,"An Example of Hyperparameter Optimization on XGBoost, LightGBM and CatBoost using Hyperopt","I promise, this is going to be much more high-level than the previous one:

Introduction

Gradient Boosting Decision Tree (GBDT)

Gradient Boosting is an additive training technique on Decision Trees. The official page of XGBoost gives a very clear explanation of the concepts. Basically, instead of running a static single Decision Tree or Random Forest, new trees are being added iteratively until no further improvement can be achieved. The ensembling technique in addition to regularization are critical in preventing overfitting. Although the model could be very powerful, a lot of hyperparamters are there to be fine-tuned.

XGBoost, LightGBM, and CatBoost

These are the well-known packages for gradient boosting. Compared with the traditional GBDT approach which finds the best split by going through all features, these packages implement histogram-based method that groups features into bins and perform splitting at the bin level rather than feature level. On the other hand they tend to ignore sparse inputs as well. These significantly improve their computational speed (see here for more detail). Few more keys to note:

XGBoost: The famous Kaggle winning package. Tree growing is based on level-wise tree pruning (tree grows across all node at a level) using the information gain from spliting, for which the samples need to be pre-sorted for it to calculate the best score across all possible splits in each step and thus is comparatively time-consuming.

LightGBM: Both level-wise and leaf-wise (tree grows from particular leaf) training are available. It allows user to select a method called Gradient-based One-Side Sampling (GOSS) that splits the samples based on the largest gradients and some random samples with smaller gradients. The assumption behind is that data points with smaller gradients are more well-trained. Another key algorithm is the Exclusive Feature Bundling (EFB), which looks into the sparsity of features and combines multiple features into one without losing any information given that they are never non-zero together. These makes LightGBM a speedier option compared to XGBoost.

CatBoost: Specifically designed for categorical data training, but also applicable to regression tasks. The speed on GPU is claimed to be the fastest among these libraries. It has various methods in transforming catergorical features to numerical. The keys to its speed are linked to two Os: Oblivious Tree and Ordered Boosting. Oblivious Tree refers to the level-wised tree building with symmetric binary splitting (i.e each leaf on each level are split by a single feature), while Ordered Boosting applies permutation and sequential target encoding to convert categorical features. See here and here for more details.

Bayesian Optimization

Compared with GridSearch which is a brute-force approach, or RandomSearch which is purely random, the classical Bayesian Optimization combines randomness and posterior probability distribution in searching the optimal parameters by approximating the target function through Gaussian Process (i.e. random samples are drawn iteratively (Sequential Model-Based Optimization (SMBO)) and the function outputs between the samples are approximated by a confidence region). New samples will be drawn from the parameter space at the high mean and variance over the confidence region for exploration and exploitation. Check this out for more explanation.

Hyperopt

Hyperopt is a python library for search spaces optimizing. Currently it offers two algorithms in optimization: 1. Random Search and 2. Tree of Parzen Estimators (TPE) which is a Bayesian approach which makes use of P(x|y) instead of P(y|x), based on approximating two different distributions separated by a threshold instead of one in calculating the Expected Improvement (see this). It used to accommodate Gaussian processes and regression tress but now these are no longer being implemented.","['speed', 'catboost', 'instead', 'xgboost', 'lightgbm', 'hyperparameter', 'hyperopt', 'example', 'samples', 'tree', 'random', 'optimization', 'features', 'training', 'boosting', 'using', 'level']","I promise, this is going to be much more high-level than the previous one:IntroductionGradient Boosting Decision Tree (GBDT)Gradient Boosting is an additive training technique on Decision Trees.
Basically, instead of running a static single Decision Tree or Random Forest, new trees are being added iteratively until no further improvement can be achieved.
XGBoost, LightGBM, and CatBoostThese are the well-known packages for gradient boosting.
It allows user to select a method called Gradient-based One-Side Sampling (GOSS) that splits the samples based on the largest gradients and some random samples with smaller gradients.
random samples are drawn iteratively (Sequential Model-Based Optimization (SMBO)) and the function outputs between the samples are approximated by a confidence region).",en,[],2019-08-04 10:05:47.026000+00:00,"{'Xgboost', 'Machine Learning', 'Towards Data Science', 'Optimization', 'Gradient Boosting'}","{'https://miro.medium.com/max/2204/1*1_PdcS7tBUi2LTDkzeLRuQ.png', 'https://miro.medium.com/max/1280/1*k3CW3uAd3nqJSJoHegLZuw.png', 'https://miro.medium.com/max/1536/1*oUMDBugjwi3K31I-v2Yg_A.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/2*r1Xiqq17iPdp3Lk2RJEzCw.png', 'https://miro.medium.com/max/694/1*NRQSqkJdSfuw_KsI0Kfk2A.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/2*r1Xiqq17iPdp3Lk2RJEzCw.png', 'https://miro.medium.com/max/60/1*NRQSqkJdSfuw_KsI0Kfk2A.jpeg?q=20', 'https://miro.medium.com/max/11520/1*8QISTMyQ2VkeagBDZvVqWA.jpeg', 'https://miro.medium.com/max/60/1*CrMFlaDfT_CZ-8qYpRRL4w.png?q=20', 'https://miro.medium.com/max/60/1*8QISTMyQ2VkeagBDZvVqWA.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*8QISTMyQ2VkeagBDZvVqWA.jpeg', 'https://miro.medium.com/max/2590/1*5gY5IdU6PO4JCqQoEDtdMA.png', 'https://miro.medium.com/max/1638/1*CrMFlaDfT_CZ-8qYpRRL4w.png', 'https://miro.medium.com/max/60/1*5gY5IdU6PO4JCqQoEDtdMA.png?q=20', 'https://miro.medium.com/max/60/1*oUMDBugjwi3K31I-v2Yg_A.png?q=20', 'https://miro.medium.com/max/60/1*1_PdcS7tBUi2LTDkzeLRuQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*k3CW3uAd3nqJSJoHegLZuw.png?q=20'}",2020-03-05 00:09:55.072638,6.468252420425415
https://towardsdatascience.com/algorithmic-trading-bot-python-ab8f42c37145,Algorithmic Trading Bot: Python,"Trading Bot

At a basic level, the trading bot needs to be able to:

Know how much money we have available to trade with Get the data to use in the strategy Select the stocks we decide we want based on the strategy Buy/sell those stocks to update our portfolio

The entire cloud function is on the longer side so I’ll summarize it here but the full code is on my GitHub. Like I said, the strategy isn’t important here and I am using a simple momentum strategy that selects the ten stocks with the highest momentum over the past 125 of days.

First we download the historical data into a dataframe for the momentum strategy from the BQ API:

Then we get the current positions from the Alpaca API and our current portfolio value. They have an API wrapper which I’m using here. The credentials again are stored in a text file on cloud storage. Notice that the base url we are using is for paper trading. You can set any amount in your paper trading account, here I set it to $10K. Obviously if this is your first time running this you won’t have any positions in Alpaca, so before you run the cloud function, just run the script locally to get your initial portfolio based on the momentum stocks you choose. Then send those tot he Alpaca API to buy them. I’m assuming here you already did that.

Now that we have the historical data and the amount we have to trade with, we can select the stocks based on our strategy. The first step is to identify the stocks with the highest momentum.

The momentum calculation is from the book Trading Evolved from Andreas F. Clenow which I would recommend. It’s very easy to follow and has lot’s of different code examples in it for different types of strategies.

The way it works is that it calculates a linear regression for the log of the closing price for each stock over the past 125 days (minimum number of days is 40). The next step is to make it easier to relate to. It takes the exponent of the slope of the regression line (tells you how much percent up or down it is by day) and then annualizes it (raise to the power of 252 which is the number of trading days in a year) and multiplies it by 100. That is then multiplied by the r squared value which will give weight to models that explain the variance well.

After we identified the top 10 stocks with the highest momentum score, we then need to decide how many shares of each we will buy. Portfolio allocation is a whole topic in and of itself so I won’t get into it here as it’s not important. To allocate here I am using the pyportfolioopt library.

We now have a df with the stocks we want to buy and the quantity. Now we need to figure out if we need to sell any stocks based on what is in our current portfolio. It’s possible that:

Our selection and allocation of momentum stocks today is exactly the same as yesterday and we don’t need to make any sales or buys There are stocks in our current portfolio that we do not want to hold anymore at all The stocks we want to buy today are the same as the ones we currently own but the amount we want to hold has changed (either increased or decreased) There are new stocks we want to buy today, that were not in our portfolio yesterday

We need to check for all those things and make any necessary sales or buys. First we’ll check to see if there’s any stocks in our current portfolio that we do not want anymore.

Now we have a dataframe with any stocks we want to sell and the quantity we need to sell. Next we’ll check to see if the quantity of any stock we currently own has decreased. Again, there may technically be no changes here so we need to check if there are. This will give us a final dataframe with all the stocks we need to sell.

Now that we have the full list of stocks to sell (if there are any), we can send those to the alpaca API to carry out the order.

Finally, we need to see if there are any new stocks we currently own that have increased in quantity or if there are any new stocks we want to buy today that we didn’t own yesterday.

If there are any we need to buy, we send those orders to the API.

It’s also a good idea to log the portfolio once we’re done. Alpaca only allows you to have a single paper trading account, so if you want to run multiple algorithms (which you should), you should create a log so you can track them on your own. We can create a strategy column to identify this strategy from others. Then we can simply add that to another BQ table.

The below SQL query will give you the daily totals with the percent change compared to the previous day for your portfolio.

That’s the bot. You can now schedule it to run everyday in a cloud function. This should give you a good framework in which to run your own trading strategies.","['bot', 'buy', 'python', 'need', 'portfolio', 'trading', 'current', 'momentum', 'run', 'algorithmic', 'alpaca', 'strategy', 'stocks']","You can set any amount in your paper trading account, here I set it to $10K.
The momentum calculation is from the book Trading Evolved from Andreas F. Clenow which I would recommend.
Now we need to figure out if we need to sell any stocks based on what is in our current portfolio.
First we’ll check to see if there’s any stocks in our current portfolio that we do not want anymore.
This should give you a good framework in which to run your own trading strategies.",en,['Rob Salgado'],2019-11-26 08:16:28.832000+00:00,"{'Google Cloud Platform', 'Data Science', 'Algorithmic Trading', 'Python', 'Finance'}","{'https://miro.medium.com/max/1602/1*zK1v-zCqxbxHVp7b6nNNJw.png', 'https://miro.medium.com/max/2914/1*gxaXiLsP-nVEmG_AFll0Gw.png', 'https://miro.medium.com/max/60/1*qb_ZfLREJgXFH9OhM9PJZQ.png?q=20', 'https://miro.medium.com/max/1972/1*lYdva2KhocvMGxngdw9pfg.png', 'https://miro.medium.com/max/52/1*JL234wASF-Zg_rU_En2qeg.png?q=20', 'https://miro.medium.com/max/60/1*DmnyScLohRW2mcuihOzp3A.png?q=20', 'https://miro.medium.com/max/1200/1*0TJmX03vIJHkwVjqH4Fq0g.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1048/1*kSZ83W4G7wkQSyS3Y3rn8A.png', 'https://miro.medium.com/max/1284/1*GCHMsqBMuyaL005ifsgTpw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/746/1*2l0-T-Mf7DDa-ztzM5oArQ.png', 'https://miro.medium.com/max/40/1*nAcmU_kkFAYWLeUJMfICBg.png?q=20', 'https://miro.medium.com/max/46/1*GCHMsqBMuyaL005ifsgTpw.png?q=20', 'https://miro.medium.com/max/46/1*KIAqeKQVMe6dFAgpT1v3iA.png?q=20', 'https://miro.medium.com/max/1116/1*a5EKBEUbV5D2Jy2N3QkdZg.png', 'https://miro.medium.com/max/24/1*p-RqIEzJhZZJH9wTuvfruA.png?q=20', 'https://miro.medium.com/max/60/1*gxaXiLsP-nVEmG_AFll0Gw.png?q=20', 'https://miro.medium.com/max/56/1*O3KRMQ4bI2VnQWOBZyVrxA.png?q=20', 'https://miro.medium.com/max/60/1*lEdCTKetn6_PRApRXVUBQA.png?q=20', 'https://miro.medium.com/max/38/1*e7YuYDspTiXmr5H--rEEQQ.png?q=20', 'https://miro.medium.com/max/1250/1*lEdCTKetn6_PRApRXVUBQA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/8576/1*r5ShILuqiieZhuhOqP21OQ.jpeg', 'https://miro.medium.com/max/1048/1*JL234wASF-Zg_rU_En2qeg.png', 'https://miro.medium.com/max/1066/1*RXcJFPNCmDrWM6GVfhc6-g.png', 'https://miro.medium.com/max/60/1*0TJmX03vIJHkwVjqH4Fq0g.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*Xu5jY5MRE7KW8GBlfAhSUA.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/40/1*2l0-T-Mf7DDa-ztzM5oArQ.png?q=20', 'https://miro.medium.com/max/28/1*zK1v-zCqxbxHVp7b6nNNJw.png?q=20', 'https://miro.medium.com/max/60/1*RXcJFPNCmDrWM6GVfhc6-g.png?q=20', 'https://miro.medium.com/max/1518/1*nAcmU_kkFAYWLeUJMfICBg.png', 'https://miro.medium.com/max/60/1*onx5_D9aWQBkH6if1g_fmg.png?q=20', 'https://miro.medium.com/max/1158/1*O3KRMQ4bI2VnQWOBZyVrxA.png', 'https://miro.medium.com/max/1102/1*qb_ZfLREJgXFH9OhM9PJZQ.png', 'https://miro.medium.com/max/1036/1*KIAqeKQVMe6dFAgpT1v3iA.png', 'https://miro.medium.com/max/1318/1*onx5_D9aWQBkH6if1g_fmg.png', 'https://miro.medium.com/max/12000/1*0TJmX03vIJHkwVjqH4Fq0g.jpeg', 'https://miro.medium.com/max/44/1*lYdva2KhocvMGxngdw9pfg.png?q=20', 'https://miro.medium.com/max/1166/1*e7YuYDspTiXmr5H--rEEQQ.png', 'https://miro.medium.com/max/1686/1*p-RqIEzJhZZJH9wTuvfruA.png', 'https://miro.medium.com/fit/c/160/160/1*Xu5jY5MRE7KW8GBlfAhSUA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1066/1*DmnyScLohRW2mcuihOzp3A.png', 'https://miro.medium.com/max/40/1*kSZ83W4G7wkQSyS3Y3rn8A.png?q=20', 'https://miro.medium.com/max/30/1*a5EKBEUbV5D2Jy2N3QkdZg.png?q=20', 'https://miro.medium.com/max/60/1*r5ShILuqiieZhuhOqP21OQ.jpeg?q=20'}",2020-03-05 00:09:57.839568,2.766930103302002
https://towardsdatascience.com/improving-your-algo-trading-by-using-monte-carlo-simulation-and-probability-cones-abacde033adf,Improving Your Algo Trading By Using Monte Carlo Simulation and Probability Cones,"Improving Your Algo Trading By Using Monte Carlo Simulation and Probability Cones

Using Statistical Techniques To Better Your Odds Of Trading Success

My introduction to the world of applying statistical tools to real world situations was not a good one. I was in charge of Quality Assurance for a medium size aerospace company. Although we extensively used Statistical Process Control (SPC) on the factory floor, we now decided to use some advanced statistical tools, such as Design of Experiments, to rectify a thorny manufacturing contamination problem.

I gave the assignment to my PhD statistician. After reviewing the project (and before conducting any experiments), he returned to me and asked “Now that I understand the project, what conclusion do you want to present to your boss (the CEO)? I can experimentally and statistically make a case for whatever conclusion you choose.”

Wow, I thought. Using statistics not to find the answer, but rather to support a “pet” answer, any pet answer. That made me cringe. Not surprisingly, I turned the project over to someone who wanted the experiment to drive the conclusion, not the other way around.

These days, I like using statistics not to justify a belief, but rather to guide me down a sensible path.

I am an algo trader, and statistical tools help me quite a bit. In this article, I will introduce how I use Monte Carlo simulation and Probability Cones to trade more effectively.

A few words of caution before I jump into the discussion. First off, I am not an expert in statistics by any means. Beyond taking a few undergrad and graduate level Statistics courses, I am clueless about many aspects of statistics. I definitely am not an expert in Monte Carlo analysis or Probability Cones.

What I am adept at is using these tools as a daily practitioner. I may be doing things that a purist would scoff at, but that is OK with me. In the end, I know these tools, and how I deploy them, have helped my trading quite a bit. Trading profits talk, and they tell me I am doing something right — maybe just not theoretically right.

Also, all trading results shown here are hypothetical. Past performance is not indicative of future results, and you should only trade with money you can afford to lose. Trading is very risky.

With those disclaimers out of the way, let’s talk about these tools and how I use them in my algo trading.

What is Monte Carlo Analysis?

Flip a fair coin, and you know the odds of it landing heads is 50%. And let’s say you win $1.25 for every heads that comes up, and lose $1 for every tails. You have a bankroll of $10, and you’ll quit if you ever fall to $6. That is your ruin point. The question is, what are the odds of that happening?

For a simple example such as this, you can find a theoretical equation which approximates the result at Wikipedia: https://en.wikipedia.org/wiki/Risk_of_ruin

But, what if the situation was much more complicated? What if there was a wide variety of possible outcomes, and instead of a two-headed coin, there were 25 sides to the coin, all with a different probability of being chosen? A theoretical calculation, in that case, becomes much more difficult, if not impossible.

Enter Monte Carlo analysis. Monte Carlo analysis utilizes computer simulation to solve problems, calculate probabilities and more — without having to solve theoretical equations. It is especially helpful in situations where theoretical answers are not even possible to derive.

Monte Carlo analysis is used in a wide variety of fields, including some you might not imagine. In the 1940s Monte Carlo simulation was actually used in the development of the first atomic bombs. I use it in trading, a much more benign approach (although trading accounts do blow up on occasion!).

When used in trading, Monte Carlo can help establish probabilities of a number of important performance metrics:

Risk of ruin

Maximum and median drawdowns

Annual rates of return

Return / drawdown ratios

With that information, a trader can make smarter decisions on strategy selection, position sizing and capital deployment and allocation.

How to Use Monte Carlo In Trading

In its typical form, Monte Carlo analysis takes all the trades generated by a hypothetical backtest and randomly selects trade after trade to generate an equity curve. The idea is that with a properly designed backtest, the best estimate of future performance is derived from using past trades (later on, I will discuss many reasons why this might not hold true).

An example of this is shown in Figure 1.

Figure 1- Hypothetical Backtest Curve, With Multiple Possible Future Paths

The thick dark blue curve is the hypothetical backtest, while the thin curves show some of the possible future paths the equity curve could take. The future curves are based on random selection of trades from the backtest curve. (Side Note: In this example, a process called “sampling with replacement” was employed. Each trade in the backtest can be chosen numerous times, based on the random selection process. Some trades may not be chosen at all. This leads to a wide range of ending equity values. If “sampling without replacement” was used, then eventually all equity curves would converge to a common end point, since each backtest trade was used once and only once).

When the Monte Carlo is run, thousands of potential future curves are generated. These results can then be analyzed, with probabilities established for future performance.

I usually think about Monte Carlo in this way: when you perform a historical backtest, before you run the Monte Carlo analysis, you get an equity curve of what happened, such as the equity curve in Figure 2.

Figure 2- Hypothetical Backtest Curve

There is no debate — that is what actually happened. But, what if things were slightly different? What if the trades, for example, came in a different order during the backtest?

In that case, any of the equity curves (paths) shown in Figure 3 COULD have happened.

Figure 3- Slightly Altered History Leads To Different Possible Equity Curves

And of course, many other paths not shown could have happened, too. So, by using the Monte Carlo analysis, and combining the results, we can arrive at probabilities of certain events happening.

Advantages Of Using Monte Carlo

For me, the biggest advantage of Monte Carlo analysis is the way it has changed my mindset about algo trading. When I started to think about probabilities in trading, which Monte Carlo forces you to do, I found myself making better decisions. I no longer thought “the market is definitely going down,” for example. Instead, I thought “the market is likely going down, but will I be safe if it does not?”

Take, for example, the typical trader we have all been at one point or another in our trading career. You start trading a new strategy on Monday, a strategy that historically has performed very well. But after 4 consecutive losses in real time, you scrap the system, since in backtest you only had 3 consecutive losses. You then move on to the next strategy, in a never ending chase for perfection. Of course, shortly after you quit, your original method then recovered from the 4 losers. Unfortunately, you did not get to enjoy that recovery!

If you had run your system thru Monte Carlo simulation, maybe it would have showed a 40% chance of 4 consecutive losers. Then, when the 4 losers actually happened, you would have been dismayed, but you may not have abandoned the strategy. After all, you knew there was a 40% chance of that many consecutive losers occurring.

In this way, Monte Carlo can really help you understand your trading strategy. Although I use it with my algo trading, it can be used by any trader, as long as he or she has a valid and stable trade history. Requiring a valid trade history, however, also leads to some serious disadvantages with the technique.

Disadvantages of Monte Carlo

There are quite a few dangers in using Monte Carlo for algo trading strategy analysis. The first disadvantage is that the historical results need to be stable. By “stable” I am not referring to the actual trade results, but rather the trading method. A trader who consistently trades the same system or approach has a stable method. A trader who tries algo trading on Monday, random guessing on Tuesday and order flow trading on Wednesday does not have a stable approach.

If you constantly tweak your trading approach, your historical trades are meaningless going forward. The historical results should be consistent for a valid Monte Carlo analysis; only one strategy, or one approach, for the whole backtest.

A related concept is if the market changes significantly. When a “step” change in the market occurs, Monte Carlo analysis is no longer valid. An example of this would the Swiss National Bank action in 2015. On January 15, 2015, the Swiss Bank removed a so called “peg” it had established on the Swiss Franc in 2011. This peg tied the value of the Swiss Franc to the Euro.

With the peg suddenly removed, the market reacted violently, and never returned to its previous state. Thus, using Monte Carlo with historical results from 2011–2015 would be pointless, given the structural change to the market in 2015. One could not reasonably expect trades made during the past of 2011–2015 to continue into 2015 and beyond. The market had fundamentally changed.

Another possible disadvantage of Monte Carlo is with serial correlation of trades. Serial correlation is a characteristic of trading histories where the current trade result is dependent (correlated) to a previous trade result. Standard Monte Carlo analysis assumes each trade is independent of the previous trade. My experience has shown that this is generally true (or for all practical purposes true) for many trading systems.

So, if you have a strategy that has serial correlation (either specifically built in or not), Monte Carlo results will be erroneous. An example of this is a simple moving average crossover system, with signals changing based on the previous trade’s profit or loss. Pseudo code for a serially correlated strategy is given below.

Non-Serially Correlated Strategy

If close crosses above the 10 bar moving average then buy

If close crosses below the 10 bar moving average then sell short

Serially Correlated Strategy

If close crosses above the 10 bar moving average then

If the previous trade was profitable then sell short

If the previous trade was not profitable then buy

If close crosses below the 10 bar moving average then

If the previous trade was profitable then buy

If the previous trade was not profitable then sell short

The serially correlated system will contain a history where the result of trade[i] depends on the result of trade[i-1]. This important dependency will be lost when traditional Monte Carlo analysis is conducted. Luckily, in such cases there are ways of both detecting serial correlation, and subsequently dealing with it, with a modified Monte Carlo analysis (beyond the scope of this article).

A final Monte Carlo disadvantage is in the old computer programming adage “Garbage In, Garbage Out.” Except in this case it is more like “Masterpiece In, Masterpiece Out.”

What does this mean? Simply put, if your backtest looks great, it might be that way because of curvefitting, over optimization, hindsight bias, etc. In such cases, the backtest masterpiece will yield a great looking Monte Carlo result — little chance of ruin, small drawdown probabilities and the like.

In this situation, the Monte Carlo simulation can actually give you overconfidence, and lead you to some bad decisions. So, one should be careful if results look too good to be true. In most cases, they probably are!

Getting Started With A Sample Case

There are many free and commercial Monte Carlo simulators on the market, all with different capabilities and features. One good free simulator is available from TickQuest. This particular simulator has some nice ways of visualizing the results.

Another free simulator is one I created for Microsoft Excel, and is available on my website’s Calculator page. The nice thing about the Excel version is that if you can program in Excel Macro language, you can easily extend the capabilities of the simulator. That makes it ideal for do-it-yourself traders wanting to add position sizing and other features.

I’ll use my free Excel Monte Carlo simulator for the example case below.

First, before running the Monte Carlo analysis, you must have a historical backtest of trades. This is generated by the trading platform you use, for a strategy and instrument you select. Trade results are found in the Strategy Performance Report. As cautioned earlier, it is critical that the trade results are not over-optimized or curvefit, since that will give overly optimistic simulation results.

The trades are shown in the equity curve of Figure 4, and the first 10 individual trades are shown in Figure 5.

Figure 4- Sample Equity Curve

Figure 5 — First 10 Trades For Sample Equity Curve (From Strategy Backtest)

These trades represent approximately one year of trading results, with 150 trades, $19,000 profit and a maximum drawdown of about $7,000.

A couple of points to note: first, the trading results used here do not necessarily represent a good or bad strategy — it is merely an example. Second, sound Monte Carlo analysis usually includes 5–10 years of backtest results. This sample strategy only has one year.

To run the simulator, the following numbers are required as shown in Figure 6:

A — The backtested trade results

B — The starting equity for the simulation (the simulator will also run 10 additional starting equity values, all incremented from the value you enter)

C — The quitting equity point –if equity during a simulation drops below this amount, that simulation run is stopped and is considered a case of “ruin.” Many times, this value is set at the trading exchange minimum required margin, since this is the minimum amount required in the account to initiate a new trade.

D — The average number of trades in a year. The simulator runs for a 1 year period, with this many trades taken.

E — System Name identifier

Figure 6- Inputs for Monte Carlo Simulation

Once all the required values are input, the user clicks the “Calculate” button and the simulation begins. The simulator runs 2,500 cases for each of the 11 increasing equity values. Results are shown in the table in Figure 7, section F.

Figure 7- Monte Carlo Output Results

Monte Carlo Results

The yellow table contains the results of the Monte Carlo simulation:

G — Ruin: If you trade this system for one year, what is the probability of dropping below the quitting point equity ( C ) within one year’s time? That is the risk of ruin percentage.

H — Median Drawdown: Based on 2,500 simulation runs, you have a 50% chance that your maximum drawdown in the first year will be greater than the value shown. You want this number to be as low as possible. Remember, even with this drawdown value, you still have a 50% chance that your drawdown will actually be higher. Conversely, you also have a 50% chance that your maximum drawdown will be lower.

I — Median Profit: Based on 2,500 simulation runs, you have a 50% chance that your maximum profit in the first year will be greater than the value shown. You want this number to be as high as possible. Remember, since this is a median profit value, you are running a significant chance that your profit will actually be lower or higher.

J — Median Return: Based on 2,500 simulation runs, you have a 50% chance that your maximum percentage return in the first year will be greater than the value shown. You want this number to be as high as possible. Again, there is a 50% chance of both a higher, or lower, value.

K — Return/DD Ratio: Return to Drawdown ratio. Closely related to the Calmar ratio, which combines return and drawdown into one metric. Higher values of return/dd are generally better. Be careful though — if the return/dd is too high, it may suggest overfitting. If the value is too low, the amount of risk you have to endure for a given return will be too high, and the strategy will not be worth trading.

L — Prob>0: The probability of having a profitable year, if you trade this strategy for a full year.

The astute reader will note that for drawdowns, profit and return, median values from the 2,500 simulation runs are given. This obviously ignores some of the worst case scenarios for drawdown, which many traders are more interested in. The spreadsheet is completely unlocked, so these values can be modified to reflect any percentile desired. The 95th percentile of drawdown is usually pretty popular, for example.

Assuming the Monte Carlo results are acceptable, the trader now may decide to transition to live trading (either simulated or with real money). This live performance can be measured and tracked by the use of probability cones.

What Are Probability Cones?

In manufacturing, one of the best ways to produce quality parts is by utilizing Statistical Process Control (SPC) on the factory floor. SPC helps the production team monitor the manufacturing process, and take action before bad parts can be produced.

In trading, SPC techniques can be used to monitor trading system performance. One way would be to use control charts. An example is shown in Figure 8.

Figure 8- Statistical Process Control Chart

For this approach, each trade result is recorded, and if certain conditions are met (for example, if a part [trade] falls above or below the 3 sigma level), the process (strategy) is considered out of control and action may be warranted. In trading, this could be as simple as turning the strategy off.

There are a few issues with using control charts to monitor trading system performance, though. One issue is there are many rules for an “out of control” situation, and not all them apply to the non-serially correlated trades found in most trading systems. My experience is that control charts can be useful, but they are not ideal.

A better way to monitor real time trading performance is by the use of Probability Cones, also referred to as a cumulative chart. An example chart is shown in Figure 9.

Figure 9- Sample Probability Cone Chart

In this chart, the following values are plotted:

Backtest equity curve (black line)

Average performance of backtest (gray line)

At the end of strategy development, the strategy enters a period of live trading:

Upper probability cone bands (solid and dashed green lines)

Lower probability cone bands (solid and dashed red lines)

Actual trades, real money or simulated (blue line)

If the real time equity curve travels above or below the green and red defined bands, many times it is wise to cease trading, until the trading system returns to a normal area- in between the red and green dashed lines.

How to Use Probability Cones In Trading

Probability Cones can be a relatively easy way to monitor the live performance of a trading system. This is ideal if you have many trading systems to monitor, which is frequently the case for well diversified algo traders.

Only 4 numbers are required to begin the analysis:

M — the average profit per trade from the backtest

N — standard deviation of trade backtest results

O — the inner band threshold, expressed as standard deviations above the mean

P — the outer band threshold, expressed as standard deviations below the mean

Once the analysis is set up, then each individual trade is added to column Q as the trade closes in real time. These trades are depicted in the blue curve.

An example of this is shown in Figure 10.

Figure 10 — Inputs For Probability Cones

Since the Probability Cone analysis is a natural extension of the Monte Carlo analysis, I have included a separate worksheet for it in the Monte Carlo workbook.

Advantages Of Using Probability Cones

The biggest advantage of using probability cones is that there is no hiding from the results. If your live strategy is not performing well, compared to the backtest, it will be readily apparent (see Figure 11).

Figure 11- When Cone Levels Are Crossed, Strategy Should Be Stopped

Probability Cone charts help the trader make unbiased decisions. For instance, this could help in deciding when to stop trading, normally an emotionally charged event (who enjoys turning off a strategy that they spent blood, sweat and tears creating?).

Another advantage of the Probability Cone chart is that it is simple to implement, and easy to understand. If the equity curve is within the upper and lower bands, chances are the trading system is acting normally. If the equity curve exceeds the upper band, caution is warranted, since this good performance is much better than expected. A good thing, to be sure, but has the market changed? Will the system start underperforming soon, and return to its long turn average?

If the equity curve falls below the lower band, maybe the system is broken, or maybe the market has changed. It might be appropriate to cease trading this strategy.

Disadvantages of Probability Cones

Even though these cones are a nice tool, there are some disadvantages. First off, for strategies that have non-normally distributed results, the cones can be inaccurate. In this case, it is recommended that the upper and lower curves be generated via a Monte Carlo simulation with backtest trade results, instead of using just the average and standard deviation. That approach will provide more accurate bands.

A second disadvantage of the tool is in deciding what the upper and lower band values should be. Should 1 standard deviation be used? In that case, the bands will be closer to the average, and strategies may be turned off prematurely as a result of normal fluctuation. On the other hand, if 3 standard deviation bands are used, it might be many trades before the live trading breeches one of the bands. This could represent a significant amount of money lost on a failing strategy.

A final disadvantage of the Probability Cones is that, just as with Monte Carlo analysis, the utility of the analysis is a direct function of the backtest results. In the backtest results are overfit or overoptimized, the cones will not assist a great deal. An overoptimized system will eventually break below the lower band, but only after the trader needlessly loses money trading a system that should never have been traded.

Sample Case

Continuing with the same trading system shown earlier in the Monte Carlo analysis, we can use the Probability Cone analysis to monitor our real time trading performance.

To do this analysis, the only additional numbers needed are the live trade results (Q in Figure 10), and the inner and outer band values (O and P in Figure 10). These are entered in the spreadsheet as shown.

The inner and outer band sigma values, entered as the number of standard deviations, give probabilities based on a normal curve:

1 standard deviation = 68% of the live equity curve should fall within the upper and lower inner bands

2 standard deviation = 95% of the live equity curve should fall within the upper and lower outer bands

A sample of live trading results is shown in Figure 11. In this particular case, the strategy performance is obviously degrading. Many traders would consider turning off the strategy when the lower solid red line is breeched. More conservative traders would turn the strategy off at the dashed red line level.

If such an event happened, though, the strategy should still be tracked. If the strategy performance later improves, then the trader can consider resuming trading the strategy.

Conclusion

Statistical tools such as Monte Carlo analysis and Probability Cones can provide useful information to a trader regarding his or her trading systems. By properly utilizing these tools, a trader can avoid trading a poor strategy, and can also know when to cease live trading an underperforming system. These tools allow the trader to be more in control (no pun intended!) of his trading.

While these tools can be useful, note that they are just that — tools. The trader must still be in charge, and it is up to the trader to determine if the conclusions of each analysis are appropriate and should be followed. Blindly following this (or any analysis) can be dangerous. I personally have found both Monte Carlo and Probability Cones to be useful, but I am cognizant of their limitations and disadvantages. Overall, though, I feel the tools make me a better trader, and that is really the ultimate test.","['analysis', 'monte', 'probability', 'trade', 'improving', 'algo', 'trading', 'cones', 'equity', 'results', 'strategy', 'trades', 'using', 'simulation', 'carlo']","Monte Carlo analysis utilizes computer simulation to solve problems, calculate probabilities and more — without having to solve theoretical equations.
Monte Carlo analysis is used in a wide variety of fields, including some you might not imagine.
Advantages Of Using Monte CarloFor me, the biggest advantage of Monte Carlo analysis is the way it has changed my mindset about algo trading.
Disadvantages of Monte CarloThere are quite a few dangers in using Monte Carlo for algo trading strategy analysis.
Figure 10 — Inputs For Probability ConesSince the Probability Cone analysis is a natural extension of the Monte Carlo analysis, I have included a separate worksheet for it in the Monte Carlo workbook.",en,['Kevin Davey'],2019-12-13 21:04:43.862000+00:00,"{'Trading Strategies', 'Algo Trading', 'Automated Trading Systems', 'Finance', 'Trading'}","{'https://miro.medium.com/max/60/1*Vs5E-rnhJt4hTPBuDNSeJw.jpeg?q=20', 'https://miro.medium.com/max/60/1*O0lhEU_qkJIfDRKHNQfOtQ.jpeg?q=20', 'https://miro.medium.com/max/884/1*I0RvWm0mgnAr-NKPRA5Vpw.jpeg', 'https://miro.medium.com/max/60/1*CK0E3fUlxAiBcumfajmOPg.jpeg?q=20', 'https://miro.medium.com/max/1136/1*G_BAvSTNQ1Bj5Yt2on9iXg.jpeg', 'https://miro.medium.com/max/60/1*BujEevITiCBq54_nurBJbg.jpeg?q=20', 'https://miro.medium.com/max/2138/1*BujEevITiCBq54_nurBJbg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*nX_cPRist7Rbihs1BRuSOA.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*G_BAvSTNQ1Bj5Yt2on9iXg.jpeg?q=20', 'https://miro.medium.com/max/923/1*CK0E3fUlxAiBcumfajmOPg.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*lEL-f88K0oCYVUGTM36Rxw.jpeg?q=20', 'https://miro.medium.com/max/60/1*QlFSBHL0lXQhwyKZLlr6OQ.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/0*wumQiOwT2QQpG0aL', 'https://miro.medium.com/max/38/1*af7YSi5ypHcyowlj-8SShg.jpeg?q=20', 'https://miro.medium.com/max/1846/1*CK0E3fUlxAiBcumfajmOPg.jpeg', 'https://miro.medium.com/max/1206/1*O0lhEU_qkJIfDRKHNQfOtQ.jpeg', 'https://miro.medium.com/max/1646/1*lEL-f88K0oCYVUGTM36Rxw.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*ie6J2TymB1mD-rnDBLDVmQ.jpeg?q=20', 'https://miro.medium.com/max/1614/1*nX_cPRist7Rbihs1BRuSOA.jpeg', 'https://miro.medium.com/max/60/1*qL1SJz3a3ItmDO2sj93Psw.jpeg?q=20', 'https://miro.medium.com/max/264/1*af7YSi5ypHcyowlj-8SShg.jpeg', 'https://miro.medium.com/max/60/1*I0RvWm0mgnAr-NKPRA5Vpw.jpeg?q=20', 'https://miro.medium.com/max/1130/1*QlFSBHL0lXQhwyKZLlr6OQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/936/1*Vs5E-rnhJt4hTPBuDNSeJw.jpeg', 'https://miro.medium.com/max/2138/1*ie6J2TymB1mD-rnDBLDVmQ.jpeg', 'https://miro.medium.com/fit/c/160/160/0*wumQiOwT2QQpG0aL', 'https://miro.medium.com/max/2908/1*qL1SJz3a3ItmDO2sj93Psw.jpeg'}",2020-03-05 00:10:04.302356,6.461787939071655
https://towardsdatascience.com/algo-trading-101-for-dummies-like-me-b3938725d184,Algo Trading 101 for Dummies like Me,"Here’s an attempt to describe the Algo Trading business in layman’s terms. Let’s split the phrase into words — Algo and Trading As you may already know, the word Trading here stands for the action of buying and selling stocks in the capital markets whereas Algo here stands for the term Algorithmic. If you already know what an algorithm is, you can skip the next paragraph.

An algorithm is a clearly defined step-by-step set of operations to be performed. Let’s say if you are assigned a task to drink water from a bottle, the algorithm or set of operations for that will be — to get the water bottle, open the cap, drink the water, close the cap and place the bottle at the right place. Simple. Similarly in a computer system, when you need a machine to do something for you, you explain the job clearly by setting instructions for it to execute. And that process is also called programming a computer.

Now, many of you might already know that before the electronic trading took over, the stock trading was mainly a paper-based activity. There were actual stock certificates and one needed to be physically present there to buy or sell stocks. And then there was dematerialization(DEMAT). Actual certificates were slowly being replaced by their electronic form as they could be registered or transferred electronically. It increased the fluctuations in the stock-prices because now the trading process was faster. But then with the technological developments came the next big thing — ALGO TRADING. Now, you can write an algorithm and instruct a computer to buy or sell stocks for you when the defined conditions are met. These programmed computers can trade at a speed and frequency that is impossible for a human trader. This process can be semi-automated or completely automated and this is why the terms automated trading and algo trading are used interchangeably but are not necessarily the same, in the next section we will discuss how they are different from each other.

“All models are wrong but some are useful” -George Box

Difference between algo trading and automated trading

Automated Trading is often confused with algorithmic trading. Automated Trading is the absolute automation of the trading process. Here decisions about buying and selling are also taken by computer programs. This means the order is automatically created, submitted(to the market) and executed. The automated trading facility is usually utilized by hedge funds that utilize proprietary execution algorithms and trade via Direct-Market Access(DMA) or sponsored access.

High-frequency Trading(HFT) is a subset of automated trading. Technology has made it possible to execute a very large number of orders within seconds. Such speedy trades can last for milliseconds or less. HFT firms earn by trading a really large volume of trades. Clearly speed of execution is the priority here and HFT uses of direct market access to reduce the execution time for transactions. You can also look into this article by Gonçalo Abreu to understand how to assemble an entry-level HFT system.

In short, Algorithmic Trading is basically an execution process based on a written algorithm, Automated Trading does the same job that its name implies and HFT refers to a specific type of ultra-fast automated trading.

Intelligent Algorithmic Trading Systems

Algorithmic trading systems are best understood using a simple conceptual architecture consisting of four components which handle different aspects of the algorithmic trading system namely the data handler, strategy handler, and the trade execution handler. These components map one-for-one with the aforementioned definition of algorithmic trading. We’ll discuss each of the 4 components in detail below:

1.Data Component

Algorithmic Trading systems can use structured data, unstructured data, or both. Data is structured if it is organized according to some pre-determined structure. Examples include spreadsheets, CSV files, JSON files, XML, Databases, and Data-Structures. Market-related data such as inter-day prices, end of day prices, and trade volumes are usually available in a structured format. Economic and company financial data is also available in a structured format. Two good sources for structured financial data are Quandl and Morningstar.

Data is unstructured if it is not organized according to any pre-determined structures. Examples include news, social media, videos, and audio. This type of data is inherently more complex to process and often requires data analytics and data mining techniques to analyze it. Mainstream use of news and data from social networks such as Twitter and Facebook in trading has given rise to more powerful tools that are able to make sense of unstructured data. Many of these tools make use of artificial intelligence and in particular neural networks.

2.Model Component

A model is the representation of the outside world as it is seen by the Algorithmic Trading system. Financial models usually represent how the algorithmic trading system believes the markets work. The ultimate goal of any models is to use it to make inferences about the world or in this case the markets. The most important thing to remember here is the quote from George E.P Box “all models are essentially wrong, but some are useful”.

Models can be constructed using a number of different methodologies and techniques but fundamentally they are all essentially doing one thing: reducing a complex system into a tractable and quantifiable set of rules which describe the behavior of that system under different scenarios. Some approaches include, but are not limited to, mathematical models, symbolic and fuzzy logic systems, decision trees, induction rule sets, and neural networks.

Mathematical Models

The use of mathematical models to describe the behavior of markets is called quantitative finance. Most quantitative finance models work off of the inherent assumptions that market prices (and returns) evolve over time according to a stochastic process, in other words, markets are random. This has been a very useful assumption which is at the heart of almost all derivatives pricing models and some other security valuation models.

Essentially most quantitative models argue that the returns of any given security are driven by one or more random market risk factors. The degree to which the returns are affected by those risk factors is called sensitivity. For example, a well-diversified portfolio’s returns may be driven by the movement of short-term interest rates, various foreign exchange rates, and the returns in the overall stock market. These factors can be measured historically and used to calibrate a model which simulates what those risk factors could do and, by extension, what the returns on the portfolio might be. For more information please see Random Walks Down Wall Street.

Symoblic and Fuzzy Logic Models

Symbolic logic is a form of reasoning which essentially involves the evaluation of predicates (logical statements constructed from logical operators such as AND, OR, and XOR) to either true or false. Fuzzy logic relaxes the binary true or false constraint and allows any given predicate to belong to the set of true and or false predicates to different degrees. This is defined in terms of set membership functions.

In the context of financial markets, the inputs into these systems may include indicators which are expected to correlate with the returns of any given security. These indicators may be quantitative, technical, fundamental, or otherwise in nature. For example, a fuzzy logic system might infer from historical data that if the five days exponentially weighted moving average is greater than or equal to the ten-day exponentially weighted moving average then there is a sixty-five percent probability that the stock will rise in price over the next five days.

A data-mining approach to identifying these rules from a given data set is called rule induction. This is very similar to the induction of a decision tree except that the results are often more human readable.

Decision Tree Models

Decision trees are similar to induction rules except that the rules are structures in the form of a (usually binary) tree. In computer science, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child. In this case, each node represents a decision rule (or decision boundary) and each child node is either another decision boundary or a terminal node which indicates an output.

There are two types of decision trees: classification trees and regression trees. Classification trees contain classes in their outputs (e.g. buy, hold, or sell) whilst regression trees contain outcome values for a particular variable (e.g. -2.5%, 0%, +2.5%, etc.). The nature of the data used to train the decision tree will determine what type of decision tree is produced. Algorithms used for producing decision trees include C4.5 and Genetic Programming.

As with rule induction, the inputs into a decision tree model may include quantities for a given set of fundamental, technical, or statistical factors which are believed to drive the returns of securities.

Neural Network Models

Neural networks are almost certainly the most popular machine learning model available to algorithmic traders. Neural networks consist of layers of interconnected nodes between inputs and outputs. Individual nodes are called perceptrons and resemble a multiple linear regression except that they feed into something called an activation function, which may or may not be non-linear. In non-recurrent neural networks, perceptrons are arranged into layers and layers are connected with other another. There are three types of layers, the input layer, the hidden layer(s), and the output layer. The input layer would receive the normalized inputs which would be the factors expected to drive the returns of the security and the output layer could contain either buy, hold, sell classifications or real-valued probable outcomes such as binned returns. Hidden layers essentially adjust the weightings on those inputs until the error of the neural network (how it performs in a backtest) is minimized. One interpretation of this is that the hidden layers extract salient features in the data which have predictive power with respect to the outputs.

In addition to these models, there are a number of other decision making models which can be used in the context of algorithmic trading (and markets in general) to make predictions regarding the direction of security prices or, for quantitative readers, to make predictions regarding the probability of any given move in a securities price.

The choice of model has a direct effect on the performance of the Algorithmic Trading system. Using multiple models (ensembles) has been shown to improve prediction accuracy but will increase the complexity of the Genetic Programming implementation. The model is the brain of the algorithmic trading system. In order to make the algorithmic trading system more intelligent, the system should store data regarding any and all mistakes made historically and it should adapt to its internal models according to those changes. In some sense, this would constitute self-awareness (of mistakes) and self-adaptation (continuous model calibration). That said, this is certainly not a terminator!

3.Execution Component

The execution component is responsible for putting through the trades that the model identifies. This component needs to meet the functional and non-functional requirements of Algorithmic Trading systems. For example, the speed of the execution, the frequency at which trades are made, the period for which trades are held, and the method by which trade orders are routed to the exchange needs to be sufficient. Any implementation of the algorithmic trading system should be able to satisfy those requirements.

4.Monitor Component

Artificial intelligence learns using objective functions. Objective functions are usually mathematical functions which quantify the performance of the algorithmic trading system. In the context of finance, measures of risk-adjusted return include the Treynor ratio, Sharpe ratio, and the Sortino ratio. The model component in the algorithmic trading system would be “asked” to maximize one or more of these quantities. The challenge with this is that markets are dynamic. In other words, the models, logic, or neural networks which worked before may stop working over time. To combat this the algorithmic trading system should train the models with information about the models themselves. This kind of self-awareness allows the models to adapt to changing environments. I think of this self-adaptation as a form of continuous model calibration for combating market regime changes.

Automated Trading Strategies

Market Analysis

Basic techniques include analyzing transaction volumes for given security to gain a daily profile of trading for that specific security. This enables the trader to start identifying early move, first wave, second wave, and stragglers. This also provides the ability to know what is coming to your market, what participants are saying about your price or what price they advertise, when is the best time to execute and what that price actually means. Collecting, handling and having the right data available is critical, but crucially, depends on your specific business, meaning that you need a complete but flexible platform.

Comparing volumes today vs previous days can give an early indication of whether something is happening in the market. Likewise, looking at trading corridors, i.e. the difference between the best bid and best offer, and how they evolve over the day can provide valuable insights that feed into a trading strategy.

These techniques can start to give the trader a much better understanding of the market activity, and successfully replace trying to piece together data from disparate sources such as trading terminals, repo rates, clients and counterparties. As with the game of poker, knowing what is happening sooner can make all the difference.

Managing Multiple Markets

Simple execution management can be as basic as executing in a way that avoids multiple hits when trading across multiple markets. This can also extend to managing an integrated quote across the markets, rebalancing un-executed quantity on perceived available liquidity.

Passive Aggressive

Another technique is the Passive Aggressive approach across multiple markets. Say you want to buy £100m and put a quote out to market, but the liquidity is not there. If you find a way to source an amount close to the volume you want to buy, you can then execute this ‘almost’ size. The execution system then reduces the quoted amount in the market automatically without trader intervention. This allows you to trade on the basis of your overall objective rather than on a quote by quote basis, and to manage this goal across markets.

Price Divergence From GC

Gaining an immediate notification of what’s trading away from GC gives a real-time view of when a component of the basket is diverging from the basket price. Gaining this understanding more explicitly across markets can provide various opportunities depending on the trading objective.

Trading off Inventory

Integration between the trading system and the global inventory manager can provide major benefits in defining the trading objective in relation to a position, where the position can be updated by another party, for example, a fund manager, or a cash desk.

For example, the position shows a long position of £100m, and the desk objective is to finance that position. As the day progresses, the position changes to £80m and the trading objective are automatically updated to reflect this. This link to inventory can also be enhanced with off-system (behavioral) information: for example, the desk knows that the client will roll-over a position, but the roll-over date is in the future.

Trading Algorithm Pattern Recognition and Randomisers

Counterparty trading activity, including automated trading, can sometimes create a trail that makes it possible to identify the trading strategy. Solutions that can use pattern recognition (something that machine learning is particularly good at) to spot counterparty strategies can provide value to traders.

Conversely, randomizers built into one’s own trading algorithms can cloak one’s strategy, meaning counterparts are unable to spot any discernible logic to the firm’s trading activity and cannot, therefore, start to trade against you.

Sniping Tools

Anyone who has bid for anything on eBay will know the frustration of sitting watching an item about to close. Your bid is winning! But at the last second, another bid suddenly exceeds yours. How is this possible?! You’ve been snipped.

Automated ‘sniping’ tools, widely available on the internet, can auto-exceed the highest bid within a defined limit, allowing the user to avoid having to sit at their PC waiting for a bid to close.

These tools are now coming to the repo market, and mean that correctly timing trading strategies becomes ever more important.

Not using these tools can lose you trades but can also put the firm’s liquidity management under duress during times of market stress and year-end liquidity crunches as the firm repeatedly gets pipped to the post when trying to source liquidity.

Best Execution & Order Slicing

Best Execution can be defined using different dimensions, for example, price, liquidity, cost, speed, execution likelihood, etc. An automated execution tool could, therefore, optimize for whichever of these parameters are most important or some combination of them.

Likewise breaking orders into smaller chunks that will avoid moving the market and then timing those orders in a way that ensures optimum execution can also provide benefits.

Market impact models, increasingly employing artificial intelligence can evaluate the effect of previous trades on a trade and how the impact from each trade decays over time. This allows traders to avoid executing certain trades too closely together, leading to market impact effects that reduce P&L.

What is Technical Analysis?

Technical Analysis is the forecasting of future financial price movements based on an examination of past price movements. Like weather forecasting, technical analysis does not result in absolute predictions about the future. Instead, technical analysis can help investors anticipate what is “likely” to happen to prices over time. Technical analysis uses a wide variety of charts that show price over time.

Technical analysis is applicable to stocks, indices, commodities, futures or any tradable instrument where the price is influenced by the forces of supply and demand. Price data (or as John Murphy calls it, “market action”) refers to any combination of the open, high, low, close, volume, or open interest for a given security over a specific timeframe. The timeframe can be based on intraday (1-minute, 5-minutes, 10-minutes, 15-minutes, 30-minutes or hourly), daily, weekly or monthly price data and last a few hours or many years.

Key Assumptions of Technical Analysis

Technical analysis is applicable to securities where the price is only influenced by the forces of supply and demand. Technical analysis does not work well when other forces can influence the price of the security. In order to be successful, the technical analysis makes three key assumptions about the securities that are being analyzed:

High Liquidity: Liquidity is essentially volume. Heavily-traded stocks allow investors to trade quickly and easily, without dramatically changing the price of the stock. Thinly-traded stocks are more difficult to trade, because there aren’t many buyers or sellers at any given time, so buyers and sellers may have to change their desired price considerably in order to make a trade. In addition, low liquidity stocks are often very low priced (sometimes less than a penny per share), which means that their prices can be more easily manipulated by individual investors. These outside forces acting on thinly-traded stocks make them unsuitable for technical analysis.

Liquidity is essentially volume. Heavily-traded stocks allow investors to trade quickly and easily, without dramatically changing the price of the stock. Thinly-traded stocks are more difficult to trade, because there aren’t many buyers or sellers at any given time, so buyers and sellers may have to change their desired price considerably in order to make a trade. In addition, low liquidity stocks are often very low priced (sometimes less than a penny per share), which means that their prices can be more easily manipulated by individual investors. These outside forces acting on thinly-traded stocks make them unsuitable for technical analysis. No Artificial Price Changes: Splits, dividends, and distributions are the most common “culprits” for artificial price changes. Though there is no difference in the value of the investment, artificial price changes can dramatically affect the price chart and make technical analysis difficult to apply. This kind of price influence from outside sources can be easily addressed by adjusting the historical data prior to the price change.

Splits, dividends, and distributions are the most common “culprits” for artificial price changes. Though there is no difference in the value of the investment, artificial price changes can dramatically affect the price chart and make technical analysis difficult to apply. This kind of price influence from outside sources can be easily addressed by adjusting the historical data prior to the price change. No Extreme News: Technical analysis cannot predict extreme events, including business events such as a company’s CEO dying unexpectedly, and political events such as a terrorist act. When the forces of “extreme news” are influencing the price, technicians have to wait patiently until the chart settles down and starts to reflect the “new normal” that results from such news.

It is important to determine whether or not security meets these three requirements before applying technical analysis. That’s not to say that analysis of any stock whose price is influenced by one of these outside forces is useless, but it will affect the accuracy of that analysis.

The Basis of Technical Analysis

At the turn of the century, the Dow Theory laid the foundations for what was later to become modern technical analysis. Dow Theory was not presented as one complete amalgamation but rather pieced together from the writings of Charles Dow over several years. Of the many theorems put forth by Dow, three stand out:

Price discounts everything: This theorem is similar to the strong and semi-strong forms of market efficiency. Technical analysts believe that the current price fully reflects all information. Because all information is already reflected in the price, it represents the fair value and should form the basis for analysis. After all, the market price reflects the sum knowledge of all participants, including traders, investors, portfolio managers, buy-side analysts, sell-side analysts, market strategist, technical analysts, fundamental analysts, and many others. It would be folly to disagree with the price set by such an impressive array of people with impeccable credentials. Technical analysis utilizes the information captured by the price to interpret what the market is saying with the purpose of forming a view on the future.

This theorem is similar to the strong and semi-strong forms of market efficiency. Technical analysts believe that the current price fully reflects all information. Because all information is already reflected in the price, it represents the fair value and should form the basis for analysis. After all, the market price reflects the sum knowledge of all participants, including traders, investors, portfolio managers, buy-side analysts, sell-side analysts, market strategist, technical analysts, fundamental analysts, and many others. It would be folly to disagree with the price set by such an impressive array of people with impeccable credentials. Technical analysis utilizes the information captured by the price to interpret what the market is saying with the purpose of forming a view on the future. Price movements are not totally random: Most technicians agree that prices trend. However, most technicians also acknowledge that there are periods when prices do not trend. If prices were always random, it would be extremely difficult to make money using technical analysis. In his book, Schwager on Futures: Technical Analysis, Jack Schwager states:

“One way of viewing the situation is that markets may witness extended periods of random fluctuation, interspersed with shorter periods of nonrandom behavior… The goal of the chart analyst is to identify those periods (i.e. major trends).”

A technician believes that it is possible to identify a trend, invest or trade based on the trend and make money as the trend unfolds. Because technical analysis can be applied to many different timeframes, it is possible to spot both short-term and long-term trends. The IBM chart illustrates Schwager’s view on the nature of the trend. The broad trend is up, but it is also interspersed with trading ranges. In between the trading, ranges are smaller uptrends within the larger uptrend. The uptrend is renewed when the stock breaks above the trading range. A downtrend begins when the stock breaks below the low of the previous trading range.

“What” is more important than “Why”: In his book, The Psychology of Technical Analysis, Tony Plummer paraphrases Oscar Wilde by stating,

“A technical analyst knows the price of everything, but the value of nothing”. Technicians, as technical analysts are called, are only concerned with two things:

1. What is the current price?

2. What is the history of the price movement?

The price is the end result of the battle between the forces of supply and demand for the company’s stock. The objective of the analysis is to forecast the direction of the future price. By focusing on price and only price, the technical analysis represents a direct approach. Fundamentalists are concerned with why the price is what it is. For technicians, the “why” portion of the equation is too broad and many times the fundamental reasons given are highly suspect. Technicians believe it is best to concentrate on what and never mind why. Why did the price go up? There were simply more buyers (demand) than sellers (supply). After all, the value of any asset is only what someone is willing to pay for it. Who needs to know why?

Conclusion

Algorithmic Trading has become very popular over the past decade. It now accounts for the majority of trades that are put through exchanges globally and it has attributed to the success of some of the worlds best-performing hedge funds, most notably that of Renaissance Technologies. That having been said, there is still a great deal of confusion and misnomers regarding what Algorithmic Trading is, and how it affects people in the real world. To some extent, the same can be said for Artificial Intelligence.

Too often research into these topics is focussed purely on performance and we forget that it is equally important that researchers and practitioners build stronger and more rigorous conceptual and theoretical models upon which we can further the field in years to come. Whether we like it or not, algorithms shape our modern day world and our reliance on them gives us the moral obligation to continuously seek to understand them and improve upon them. I leave you with a video entitled “How Algorithms shape our world” by Kevin Slavin.

Reference

1.AI for algorithmic trading: rethinking bars, labeling, and stationarity

2.Algorithmic Trading System Architecture

3.Validating Machine Learning and AI Models in Financial Services

4.Machine Learning and AI for trading & execution [Whitepaper]

5.Basics of Algorithmic Trading: Concepts and Examples

6.AI for algorithmic trading: 7 mistakes that could make me broke

7.Trading Systems and Methods [Book]

8.High-frequency trading simulation with Stream Analytics

9.Components of an FX Trading Pattern

10.Quantopian video lecture series to get started with trading [must watch]

11.Zerodha’s varsity set of lecture notes to learn the essentials of trading","['models', 'trade', 'market', 'algo', 'price', 'dummies', 'trading', 'technical', 'data', '101', 'system', 'algorithmic', 'analysis']","“All models are wrong but some are useful” -George BoxDifference between algo trading and automated tradingAutomated Trading is often confused with algorithmic trading.
In short, Algorithmic Trading is basically an execution process based on a written algorithm, Automated Trading does the same job that its name implies and HFT refers to a specific type of ultra-fast automated trading.
2.Model ComponentA model is the representation of the outside world as it is seen by the Algorithmic Trading system.
Trading Algorithm Pattern Recognition and RandomisersCounterparty trading activity, including automated trading, can sometimes create a trail that makes it possible to identify the trading strategy.
Reference1.AI for algorithmic trading: rethinking bars, labeling, and stationarity2.Algorithmic Trading System Architecture3.Validating Machine Learning and AI Models in Financial Services4.Machine Learning and AI for trading & execution [Whitepaper]5.Basics of Algorithmic Trading: Concepts and Examples6.AI for algorithmic trading: 7 mistakes that could make me broke7.Trading Systems and Methods [Book]8.High-frequency trading simulation with Stream Analytics9.Components of an FX Trading Pattern10.Quantopian video lecture series to get started with trading [must watch]11.Zerodha’s varsity set of lecture notes to learn the essentials of trading",en,['Sangeet Moy Das'],2019-06-23 15:18:08.114000+00:00,"{'Quantitative Analysis', 'Algorithmic Trading', 'High Frequency Trading', 'Technical Analysis', 'Machine Learning'}","{'https://miro.medium.com/max/1518/1*hpM43tUaTlBMUYl4qbG4VA.jpeg', 'https://miro.medium.com/max/1038/1*C-1n6MwW2x-lnr1Yyjf6jw.png', 'https://miro.medium.com/max/60/1*Wi8Iu-HF7u9bReYzvQD_tQ.png?q=20', 'https://miro.medium.com/max/60/1*C-1n6MwW2x-lnr1Yyjf6jw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*ogZkOrUkxeSHQXHOOAiR9Q.jpeg?q=20', 'https://miro.medium.com/max/759/1*hpM43tUaTlBMUYl4qbG4VA.jpeg', 'https://miro.medium.com/max/60/1*0l4F7zZsSRV81VL9j3s5Zg.png?q=20', 'https://miro.medium.com/max/1400/1*Wi8Iu-HF7u9bReYzvQD_tQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*P65vXjx6kelEPb-ApvrJkw.jpeg', 'https://miro.medium.com/max/60/1*tfoShg4Bb34FW_GcBF7p-w.png?q=20', 'https://miro.medium.com/max/2232/1*tfoShg4Bb34FW_GcBF7p-w.png', 'https://miro.medium.com/max/1400/1*034gsWM4G2YBoI696x-8Gg.png', 'https://miro.medium.com/max/1500/1*Uohl0PdtT5l4kvehiGzV-g.png', 'https://miro.medium.com/max/60/1*hpM43tUaTlBMUYl4qbG4VA.jpeg?q=20', 'https://miro.medium.com/max/1400/1*WJphhaDzeUHVgi8rx2up0Q.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Uohl0PdtT5l4kvehiGzV-g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*P65vXjx6kelEPb-ApvrJkw.jpeg', 'https://miro.medium.com/max/1202/1*ogZkOrUkxeSHQXHOOAiR9Q.jpeg', 'https://miro.medium.com/max/60/1*034gsWM4G2YBoI696x-8Gg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*WJphhaDzeUHVgi8rx2up0Q.png?q=20', 'https://miro.medium.com/max/2212/1*0l4F7zZsSRV81VL9j3s5Zg.png'}",2020-03-05 00:10:06.657151,2.3547942638397217
https://towardsdatascience.com/introducing-pandas-log-3240a5e57e21,Introducing Pandas-Log,"Introducing Pandas-Log

New Python package for debugging pandas operations

The pandas ecosystem has been invaluable for the data science ecosystem, and thus today most data science tasks consist of series of pandas’ steps to transform raw data into an understandable/usable format.

These steps’ accuracy is crucial, and thus understanding the unexpected results becomes crucial as well. Unfortunately, the ecosystem lacks the tools to understand those unexpected results.

That’s why I created Pandas-log, it provides metadata on each operation which will allow pinpointing the issues. For example, after .query it returns the number of rows being filtered.

As always I believe its easier to understand with an example so I will use the pokemon dataset to find “who is the weakest non-legendary fire pokemon?”.

So who is the weakest fire pokemon?

(Link to the Notebook code can be found here)

First, we will import relevant packages and read our pokemon dataset.

import pandas as pd

import numpy as np

import pandas_log df = pd.read_csv(""pokemon.csv"")

df.head(10)

A sample of our dataset

To answer our question who is the weakest non-legendary fire pokemon we will need to:

Filter out legendary pokemon using .query() . Keep only fire pokemon using .query() . Drop Legendary column using .drop() . Keep the weakest pokemon among them using .nsmallest() .

In code, It will look something like

res = (df.copy()

.query(""legendary==0"")

.query(""type_1=='fire' or type_2=='fire'"")

.drop(""legendary"", axis=1)

.nsmallest(1,""total""))

res

It resulted in empty dataframe

OH NOO!!! Our code does not work !! We got an empty dataframe!!

If only there was a way to track those issues!? Fortunately, that’s what Pandas-log is for!

with just adding a small context manager to our example we will get relevant information that will help us find the issue printed to stdout.

with pandas_log.enable():

res = (df.copy()

.query(""legendary==0"")

.query(""type_1=='fire' or type_2=='fire'"")

.drop(""legendary"", axis=1)

.nsmallest(1,""total""))

After reading the output it’s clear that the issue is in step 2 as we got 0 rows remaining, so something with the predicate “type_1==’fire’ or type_2==’fire’” is wrong. Indeed pokemon type starts with capital letter, so let’s run the fixed code.

res = (df.copy()

.query(""legendary==0"")

.query(""type_1=='Fire' or type_2=='Fire'"")

.drop(""legendary"", axis=1)

.nsmallest(1,""total""))

res

Whoala we got Slugma !!!!!!!!

Few last words to say

The package is still in its early stage so it might contain few bugs. Please have a look at the Github repository and suggest some improvements or extensions of the code. I will gladly welcome any kind of constructive feedback and feel free to contribute to Pandas-log as well! 😉","['pandaslog', 'introducing', 'weakest', 'type_2firedroplegendary', 'pokemon', 'pandas', 'example', 'ecosystem', 'query', 'using', 'code']","Introducing Pandas-LogNew Python package for debugging pandas operationsThe pandas ecosystem has been invaluable for the data science ecosystem, and thus today most data science tasks consist of series of pandas’ steps to transform raw data into an understandable/usable format.
That’s why I created Pandas-log, it provides metadata on each operation which will allow pinpointing the issues.
Keep only fire pokemon using .query() .
Keep the weakest pokemon among them using .nsmallest() .
I will gladly welcome any kind of constructive feedback and feel free to contribute to Pandas-log as well!",en,['Eyal Trabelsi'],2019-10-23 19:03:36.986000+00:00,"{'Pandas', 'Data Science', 'Debugging', 'Python', 'Machine Learning'}","{'https://miro.medium.com/max/2108/1*GvurKVXyyr9BkVogFaMEmw.png', 'https://miro.medium.com/max/1258/0*7zw7-SI3BRMJ49GA.jpg', 'https://miro.medium.com/max/676/1*X3x06jC4gSJGLYtWcU1FlA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2400/1*7fOz2hg8dis04NyLD5lhug.jpeg', 'https://miro.medium.com/max/629/0*7zw7-SI3BRMJ49GA.jpg', 'https://miro.medium.com/max/60/1*sXsiWmjMvf_zymZWjSgIPg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/0*7zw7-SI3BRMJ49GA.jpg?q=20', 'https://miro.medium.com/fit/c/160/160/1*yavjMVASfr5taFbVE2eo7Q.jpeg', 'https://miro.medium.com/max/60/1*vkvklLnx7vPBFmmPb609Og.png?q=20', 'https://miro.medium.com/max/2796/1*fB1TrMcO2G1B29_UOqNBbg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/50/1*X3x06jC4gSJGLYtWcU1FlA.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*yavjMVASfr5taFbVE2eo7Q.jpeg', 'https://miro.medium.com/max/870/1*hm0jEPQjOQLhgM3rKmw19g.gif', 'https://miro.medium.com/max/60/1*fB1TrMcO2G1B29_UOqNBbg.png?q=20', 'https://miro.medium.com/max/3632/1*vkvklLnx7vPBFmmPb609Og.png', 'https://miro.medium.com/max/2708/1*sXsiWmjMvf_zymZWjSgIPg.png', 'https://miro.medium.com/freeze/max/60/1*hm0jEPQjOQLhgM3rKmw19g.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*GvurKVXyyr9BkVogFaMEmw.png?q=20', 'https://miro.medium.com/max/60/1*7fOz2hg8dis04NyLD5lhug.jpeg?q=20'}",2020-03-05 00:10:09.141784,2.483633279800415
https://medium.com/automation-generation/algorithmic-trading-automated-in-python-with-alpaca-google-cloud-and-daily-email-notifications-422b7c6b7c53,Build an Algorithmic Trading Bot in 7 Steps,"How to Build an Algorithmic Trading Bot in 7 Steps

Learn step-by-step how to build a trading bot using python, Alpaca API, Google Cloud Platform, and email notifications.

The purpose of this article is to provide a step-by-step process of how to automate one's algorithmic trading strategies using Alpaca, Python, and Google Cloud. This example utilizes the strategy of pairs trading. Please reference the following GitHub Repo to access the Python script.

Step 1: Create accounts for Alpaca and Google Cloud Platform

Alpaca is a commission-free* brokerage platform that allows users to trade via an API. Once you have created an account you will be given an API Key ID and a secret key which you will reference in the Python script. This will create the bridge to automate your trading strategy.

*Commission-free trading means that there are no commission charges for Alpaca self-directed individual cash brokerage accounts that trade U.S. listed securities through an API. Relevant SEC and FINRA fees may apply.

Get started with Alpaca

Google Cloud Platform (GCP) is a top-tier cloud computing service. They offer hundreds of cloud products. However, for the purpose of this project, you will only need to use two GCP services. If you’re new to Google Cloud, you can take advantage of the free trial for new users that comes with $300 credit (more than you will need to automate this process). Leveraging a cloud service, such as Google, means that you won’t have to manually run your script — or be worried about your computer being on at the correct time each day.

Get started with the Google Cloud Platform.

Once you have created your account, start the free trial. Do this by clicking on “Activate” in the top right corner.

Fill out the necessary information, then create a “New Project”. In this example, it will be labeled, “Algo-trading”.

Step 2: The Python script

The next few steps will go over how to structure the Python script, attach the Alpaca API, send an email notification, and an example of how to build trading logic. The first thing to remember with the Python script is that you will need to create only one function. That function will then be called in Google Cloud. This is crucial to automate the script.

def pairs_trading_algo(self): '''All the code necessary to connect to API, trading logic and sending email notification will go in here''' return done

Step 3: Connect Alpaca API

You can access your Alpaca API keys from the Alpaca Dashboard, once your account is set up. This example will be shown using the paper trading keys. These can be found on the right side of the dashboard, and below the API Key ID is your very own secret key.

You will need to import the following packages: os, and alpaca_trade_api as tradeapi. Follow Alpaca’s documentation on how to download the alpaca_trade_api package. Below is a snippet of the code found in the GitHub Repo.

The os.environ section allows you to specify which environment you are connecting to — paper trading or live trading.

The first blacked out area is where you will place your API Key ID, the second is where you will place your secret key.

The account variable is making sure that you have an account with Alpaca and that it is active. This variable will be used later on.","['account', 'bot', 'key', 'script', 'cloud', 'python', 'steps', 'need', 'trading', 'build', 'api', 'algorithmic', 'alpaca', 'google']","How to Build an Algorithmic Trading Bot in 7 StepsLearn step-by-step how to build a trading bot using python, Alpaca API, Google Cloud Platform, and email notifications.
The purpose of this article is to provide a step-by-step process of how to automate one's algorithmic trading strategies using Alpaca, Python, and Google Cloud.
Step 2: The Python scriptThe next few steps will go over how to structure the Python script, attach the Alpaca API, send an email notification, and an example of how to build trading logic.
The os.environ section allows you to specify which environment you are connecting to — paper trading or live trading.
The first blacked out area is where you will place your API Key ID, the second is where you will place your secret key.",en,['Mcklayne Marshall'],2020-01-08 22:27:41.951000+00:00,"{'Google Cloud Platform', 'Alpaca Tutorial', 'Algorithmic Trading', 'Python', 'Automation'}","{'https://miro.medium.com/max/42/1*S7WASCA37nCoYtgiZk-R7w.png?q=20', 'https://miro.medium.com/max/1482/1*U1O7sOLQGiAoCKUKTuR43A.png', 'https://miro.medium.com/max/60/1*SsU7w4lcGFTdX2zIvdrfcg.png?q=20', 'https://miro.medium.com/max/60/1*23iNfoHz2wFCJl-Dgg_S0Q.png?q=20', 'https://miro.medium.com/max/60/1*zMdqBaCCNVmbfXMIVeQ3xQ.png?q=20', 'https://miro.medium.com/max/60/1*75vay7IyQpl_uklmQs-Y2Q.png?q=20', 'https://miro.medium.com/max/1590/1*zMdqBaCCNVmbfXMIVeQ3xQ.png', 'https://miro.medium.com/max/1424/1*espOumi0cvEEE9QIxoR0vA.png', 'https://miro.medium.com/max/1600/1*23iNfoHz2wFCJl-Dgg_S0Q.png', 'https://miro.medium.com/fit/c/80/80/1*2wHhwPKgAQwB3CnHogIZ3Q.png', 'https://miro.medium.com/max/60/1*U1O7sOLQGiAoCKUKTuR43A.png?q=20', 'https://miro.medium.com/max/60/1*Aw1w2KehnvZi-ZTcKYa6aw.png?q=20', 'https://miro.medium.com/max/38/1*espOumi0cvEEE9QIxoR0vA.png?q=20', 'https://miro.medium.com/max/206/1*eSnULZG7CUfHsVv6azD-GQ.png', 'https://miro.medium.com/max/2798/1*_zFp0lrjpvIh-yIxaVmg3g.png', 'https://miro.medium.com/max/1606/1*Aw1w2KehnvZi-ZTcKYa6aw.png', 'https://miro.medium.com/max/2400/1*SsU7w4lcGFTdX2zIvdrfcg.png', 'https://miro.medium.com/max/60/1*KQmju5ExjiJAt1imrh5F4w.png?q=20', 'https://miro.medium.com/max/38/1*A3myWdu6InWgrZc0z7-EsA.png?q=20', 'https://miro.medium.com/max/774/1*_9OCYvodh9gWFImaO1mfCA.png', 'https://miro.medium.com/max/4508/1*Rmw81fS-Ri_i-MeeauDJJA.png', 'https://miro.medium.com/fit/c/96/96/2*mLKmG6ALXpuAOSGd_CkqMA.jpeg', 'https://miro.medium.com/max/2678/1*qbHOY4F195BgZPQt9Ej14g.png', 'https://miro.medium.com/max/3494/1*75vay7IyQpl_uklmQs-Y2Q.png', 'https://miro.medium.com/max/60/1*_9OCYvodh9gWFImaO1mfCA.png?q=20', 'https://miro.medium.com/max/54/1*kAnKEUDg9kXotAAPGBNDLg.png?q=20', 'https://miro.medium.com/max/3192/1*sc6N_5tSonH9lhsmH9rOrQ.png', 'https://miro.medium.com/max/54/1*Ny2nJP7zjqsaQK6eLyKpDw.png?q=20', 'https://miro.medium.com/max/1200/1*sc6N_5tSonH9lhsmH9rOrQ.png', 'https://miro.medium.com/max/1390/1*KQmju5ExjiJAt1imrh5F4w.png', 'https://miro.medium.com/max/60/1*7yyB5aiQS3OAUdSKnY7E-g.png?q=20', 'https://miro.medium.com/max/60/1*XKVxh4IIPV3Jw9zcQU8wrg.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*mLKmG6ALXpuAOSGd_CkqMA.jpeg', 'https://miro.medium.com/max/60/1*sc6N_5tSonH9lhsmH9rOrQ.png?q=20', 'https://miro.medium.com/max/850/1*JPlSHmIr8ALaunfZQiMP0w.png', 'https://miro.medium.com/max/1326/1*7yyB5aiQS3OAUdSKnY7E-g.png', 'https://miro.medium.com/max/1602/1*Ny2nJP7zjqsaQK6eLyKpDw.png', 'https://miro.medium.com/max/60/1*_zFp0lrjpvIh-yIxaVmg3g.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*X77b7raCcsYFg_1x.jpg', 'https://miro.medium.com/max/60/1*Rmw81fS-Ri_i-MeeauDJJA.png?q=20', 'https://miro.medium.com/max/60/1*JPlSHmIr8ALaunfZQiMP0w.png?q=20', 'https://miro.medium.com/max/4502/1*XKVxh4IIPV3Jw9zcQU8wrg.png', 'https://miro.medium.com/max/806/1*A3myWdu6InWgrZc0z7-EsA.png', 'https://miro.medium.com/fit/c/160/160/1*YIdLyGXl3Ro8JuZcr07ZyQ.png', 'https://miro.medium.com/max/1610/1*S7WASCA37nCoYtgiZk-R7w.png', 'https://miro.medium.com/max/60/1*qbHOY4F195BgZPQt9Ej14g.png?q=20', 'https://miro.medium.com/max/1424/1*kAnKEUDg9kXotAAPGBNDLg.png'}",2020-03-05 00:10:11.138829,1.9961082935333252
https://medium.com/swlh/build-a-trading-simulator-in-python-ebe046949dd9,Build a trading simulator in Python,"Step 0. Preparing the data

For this simulation to work, we need to have data regarding the stock prices and their trading volumes. The following code will extract this data and store it in two different CSV files:

This script will create two CSV files: prices.csv and volume.csv

This works only if we have stored all symbols in a file called symbols.txt. Don’t worry, I have this file ready for you: symbols.txt.

Note: be patient. It took me 40 minutes to get the entire data.

Step 1. Importing modules and initializing global variables

We should start by importing the following modules:

import pandas as pd

import numpy as np

import datetime as dt

import math

import warnings warnings.filterwarnings(""ignore"")

Probably it’s unnecessary to explain using the first four modules. However, the fifth one is simply for ignoring warnings if they ever should pop up.

Let’s move on to initializing global variables.

Global variables

Don’t worry if you don’t know the meaning of each variable. I’ll explain:

prices: this is a Pandas Dataframe where we obtain stock prices;

volumechanges: originally, volume.csv stores daily trading volumes. In our case, we are interested in percentage change. The values are multiplied by 100 to make them more understandable: for example, 0.1 is the same as 10%;

today and simend: both of them are dates. The first one defines the starting point of the whole simulation and changes as the simulation iterates towards its end. The simend variable, however, defines the end of the simulation;

tickers: in this array we will store all the symbols we can trade;

transactionid: when opening/closing position, every transaction will have a unique id. This is important if we want to put our transactions into an Excel file;

money: this is equivalent to cash at hand. In the beginning, we have 1,000,000$;

portfolio: this dictionary is for recording all the symbols and amounts we are currently having in our portfolio. Keys are symbols and values are amounts (integer);

activelog and transactionlog: these two arrays are for keeping track of our transactions. The first one stores open positions only while the latter stores every transaction we make throughout 20 years. I could’ve made them as one but the code runs faster if they are separated.

Step 2. Extracting the current stock price

Assuming that our prices variable is a Pandas Dataframe, accessing a stock price is fairly easy:

Accessing stock prices

To make things clear, every transaction will be with an adjusted close price of a stock (let’s assume that we can buy at that price just a few minutes before the market closes).

Step 3. Transaction function

Every transaction we make (buy or sell) should have a mark. I have implemented a transaction() function that makes a dictionary with the following keys:

id: a unique identifier for each transaction. Corresponding buy/sell transactions will have the same id;

ticker: stock symbol;

amount: the amount that was bought/sold;

price: the price at which stocks were bought/sold;

date: the date our transaction was made;

type: this can either be “buy” or “sell”;

exp_date: if we buy stock, we define exp_date as a date on which we want to close our position. Currently, the difference between date and exp_date is 14 days;

info: any additional information we might want to add, we will leave it empty for our simulation.

If a stock is being bought, a log of this transaction is added to both activelog and transactionlog arrays. If a stock is being sold then it’s log will be deleted from actionlog but still added to a transactionlog.

Below is the code for the transaction() function.

Step 4. Buy function

The buy() function takes two arguments as input: a list of symbols we are interested in buying and the amount of money allocated for the purchase.

For every item in our “shopping list”, the function will find the price of the stock. If this is a number (isn’t a NaN value) then it will find the actual amount of money that will be spent, subtract this from the money variable and add the number of stocks we bought to our portfolio. Finally, it will create a log by calling the transaction function.

Step 5. Sell function

This works pretty much in a similar way as the buy() function but will take positions from the activelog array and try to sell it. It will try to sell only if the exp_date variable is smaller or equal to the today date.

If it was impossible to sell (selling price was NaN) then it will try to postpone the transaction by one day.

Once the positions are closed then the corresponding logs are removed from the activelog array.

Step 6. Simulation iteration

During a simulation iteration, we need to do the following:

Find the symbols that we think are worth buying. Sell stocks that we must sell today (two week holding period is over). Buy stocks that our strategy suggests.

I have written the simulation() function in the following manner:

Lines 79–82 represent selling and buying. Lines 76–78 stand for our strategy.

The strategy is simple: we take the percentage change in trading volumes over the past 14 days and find their mean value. If the mean value is larger than 100 (%) then the strategy suggests that we buy this stock.

Note: mathematically it is nonsense to take the mean value of a percentage change and it would be wiser to use logarithmic changes instead. However, this strategy works well at its current state and I’ll leave it just the way it is.

Let’s take a closer look at line 81. We allocate 500,000$ for making investments through each simulation iteration (assuming that we always have this amount of money available). This is equally divided by all the stocks that are worth buying according to our strategy.

Step 7. Helper functions

There are three minor functions we must implement before moving on to the main body of the simulation: getindices(), tradingday() and currentvalue().

getindices() is a function that takes symbols from symbols.txt file and stores them in the tickers array.

tradingday() is a function that determines whether today is a trading day and returns True/False accordingly.

currentvalue() is a function that returns our capital value (cash + assets). The int(value*100)/100 ensures that we will be returned a value that is rounded to the nearest hundredth.

Step 8. Simulation main body

The simulation main looks as follows:

I’ll try to explain it:

Line 111 fills the tickers array with symbols.

Lines 112 and 113 fill the portfolio dictionary with symbols as keywords and assigning zeros as their values.

Lines 114 to 120 iterate from today to simend. If today is a trading day then the simulation() function is called, current capital value with today’s date is printed out and seven days is added to today. If today wasn’t a trading day then the next trading day will be found.

Now we are more or less ready for executing our code.","['today', 'buy', 'stock', 'function', 'python', 'simulator', 'transaction', 'trading', 'symbols', 'build', 'sell', 'value', 'simulation']","Preparing the dataFor this simulation to work, we need to have data regarding the stock prices and their trading volumes.
I have implemented a transaction() function that makes a dictionary with the following keys:id: a unique identifier for each transaction.
Sell stocks that we must sell today (two week holding period is over).
I have written the simulation() function in the following manner:Lines 79–82 represent selling and buying.
If today wasn’t a trading day then the next trading day will be found.",en,['Markus Rene Pae'],2020-01-06 12:06:12.041000+00:00,"{'Python', 'Programming', 'Investing', 'Technology', 'Trading'}","{'https://miro.medium.com/max/2376/1*YXowyc3yQp9KAZDiVlKiGA.png', 'https://miro.medium.com/max/60/1*YXowyc3yQp9KAZDiVlKiGA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*Xd2uZaVHfrGOP14W_3UQRg.jpeg', 'https://miro.medium.com/max/1200/1*nbfw5CBW8i_XtCenGSGz7Q.png', 'https://miro.medium.com/max/2000/0*xthkL3m5F8NiWgsz', 'https://miro.medium.com/max/60/0*xthkL3m5F8NiWgsz?q=20', 'https://miro.medium.com/max/1276/1*uB3wk8L6tQpFh45ZJhF_Fg.png', 'https://miro.medium.com/max/60/1*QldEiJzIudn6SsN2skBGVg.png?q=20', 'https://miro.medium.com/max/60/1*FLrEyIe27-vqvplHegGtOQ.png?q=20', 'https://miro.medium.com/max/1278/1*gG78r_7_dqPLRRXdEjl-dg.png', 'https://miro.medium.com/max/60/1*jA7kRx7q_uvqCK2im5SGAQ.png?q=20', 'https://miro.medium.com/max/3592/1*nbfw5CBW8i_XtCenGSGz7Q.png', 'https://miro.medium.com/max/3606/1*MyqjgDcfvEL2LxCD2opcXw.png', 'https://miro.medium.com/fit/c/80/80/1*iUOmQH3pEEytA5Mwwuclxg.jpeg', 'https://miro.medium.com/max/60/1*Rxf8yoZVONmX0--ljHN2LA.png?q=20', 'https://miro.medium.com/max/796/1*QldEiJzIudn6SsN2skBGVg.png', 'https://miro.medium.com/max/422/1*IOJrKVmLnRcFz3E_KrrN_Q.png', 'https://miro.medium.com/max/816/1*FLrEyIe27-vqvplHegGtOQ.png', 'https://miro.medium.com/max/60/1*MyqjgDcfvEL2LxCD2opcXw.png?q=20', 'https://miro.medium.com/max/60/1*gG78r_7_dqPLRRXdEjl-dg.png?q=20', 'https://miro.medium.com/max/2046/1*Dp_2CoUwrwaAO9fSz6jcOw.png', 'https://miro.medium.com/max/60/1*Dp_2CoUwrwaAO9fSz6jcOw.png?q=20', 'https://miro.medium.com/max/1070/1*Rxf8yoZVONmX0--ljHN2LA.png', 'https://miro.medium.com/max/60/1*uB3wk8L6tQpFh45ZJhF_Fg.png?q=20', 'https://miro.medium.com/max/60/1*GHk8RC5MZbjHhlYHZGiS_w.png?q=20', 'https://miro.medium.com/max/2070/1*jA7kRx7q_uvqCK2im5SGAQ.png', 'https://miro.medium.com/fit/c/80/80/2*D4KAER7h6bCkTU-m7Aw9DA.jpeg', 'https://miro.medium.com/max/1950/1*GHk8RC5MZbjHhlYHZGiS_w.png', 'https://miro.medium.com/max/934/1*AN3jF6NmM7Y5qcahcPAVfw.png', 'https://miro.medium.com/fit/c/96/96/2*24lZ1IZhRNGIbSb__vvWIQ.jpeg', 'https://miro.medium.com/max/1186/1*yUIrt4KbldnbU-8c-o8wOg.png', 'https://miro.medium.com/max/60/1*nbfw5CBW8i_XtCenGSGz7Q.png?q=20', 'https://miro.medium.com/max/60/1*AN3jF6NmM7Y5qcahcPAVfw.png?q=20', 'https://miro.medium.com/max/60/1*yUIrt4KbldnbU-8c-o8wOg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*vTG13sWBDAFir-PlEvsg9w.jpeg', 'https://miro.medium.com/fit/c/160/160/2*24lZ1IZhRNGIbSb__vvWIQ.jpeg'}",2020-03-05 00:10:12.264835,1.1250059604644775
https://towardsdatascience.com/trading-the-value-area-a2351e4ccc8b,Trading the Value Area,"Trading the Value Area

An intra-day strategy for trading based on volume

Disclaimer: Past performance is NOT indicative of future returns and you should under no circumstances try to match the strategy in this article. Nothing herein is financial advice, and this is NOT a recommendation to trade real money or employ strategies discussed. Many platforms exist for simulated trading (paper trading) which can be used for building and developing the methods discussed and testing your own portfolio without putting real funds at risk. Please use common sense and ALWAYS first consult a professional before trading or investing your hard earned money.

The Value Area is a range of prices where the majority of trading volume took place on the prior trading day. In specific, this area is the range where 70% of the prior day’s volume happened. The value area is approximately one standard deviation above and below the average highest volume price. With this knowledge, there are specific probabilities of market behavior we can understand to digest the value area. The value area gives us an idea of where the smart money is playing ball and where the institutions are guiding the market. From this data, we can derive intra-day strategies that capitalize on market behavior. This idea is derived from a concept known as Market Profiling; you can read about the full context of Marker Profiles on your own.

Like other popularized strategies, I think this is useful to know. However, I wouldn’t hold your breath on returns for this strategy in reality. I’ve done some backtesting and can’t seem to reach the 80% that the authors and evangelists of this strategy claim. Being generous, I’m able to get up to 65%+. It is, however, difficult to spot scenarios where the 80% rule really kicks in for filling. I don’t doubt that accurately trading the value area is profitable. It is a challenge to identify when the trade is available and to execute on it appropriately. Though I’m new to the idea and perhaps a well-established system or algorithmic method could perform well. If you can do so, I can imagine a world where the strategy could play out in your favor.

80% Rule

The 80% Rule states that when the market opens or moves above or below the value area but then returns to the value area twice for two half-hour periods, there is an 80% chance of filling the value area. Reread that if it didn’t make sense before you move on.

If the market opens above the value area and does not return to the value area this is a heavily bullish signal. However, if the market opens above the value area and returns to the value area this is very bearish.

Example Bullish Signal

Similarly, if the market opens below the value area and does not return to the value area this is a bearish signal. If the market opens below the value area but returns to the value area this is a bullish signal.

Example Bearish Signal

This return to the value area from an open above or below the value area is what satisfies the 80% rule.

This rule works because it’s indicative of institutional pressure. When the market opens above or below the value area and stays above or below, this is a signal institutions are buying and it would likely be a fool’s errand to oppose the activity of the smart money.

At their core, the markets are simple. When there are more buyers than sellers, prices go up. When there are more sellers, markets go down. Understanding all of the factors playing into market behavior, however, is mind-numbingly difficult. We can make some interesting observations about the market based on the 80% rule. Simply put, resting inside the value area is indicative of balance in the markets. There is a roughly even amount of buyers and sellers and we’ll see an oscillation between the highs and lows of the value area. Of course, we want to initiate any buying or selling as close to the tops and bottoms of the value area as possible with tight stops in place. If the price falls out of the value area we can quickly stop out and save from further losses. If the price follows the 80% rule we can profit off the filling of the value area.

In this crude graph I show an example of trading the value area. We can see an example where the price shot above the previous day’s average volume and without a return to the value area we are best suited staying out of this trade. The next day we see the stock trade between the value area for the bulk of the day, following the 80% rule. You can see the stock dropped below the value area and then returned to the value area for two 30 minute candles; it performed this action twice. It followed the 80% rule both times.

Charted Using TradingView

Whether you’re trading this algorithmically and have programmed this trade or are trading this manually, you must either program appropriate stops or plan for appropriate manual stops. Stay disciplined and follow your strategy; don’t let emotions guide your trade. It’s interesting to study these various strategies and see how different strategies guide traders’ daily activities. The more strategies we can identify and create algorithmic systems and backtest for the better.

Conclusion

I came across the strategy on a quant finance forum and was fascinated to see how it was being used. I’m doing some programming to try and automatically pull data and calculate the value area for a few stocks. I might try to paper trade it or just track its performance in an excel sheet. Either way, it will be interesting to see how it performs.

You can find some useful quant finance tools here:

Let’s continue the conversation on Twitter!","['80', 'trade', 'market', 'area', 'trading', 'opens', 'strategies', 'rule', 'strategy', 'value']","80% RuleThe 80% Rule states that when the market opens or moves above or below the value area but then returns to the value area twice for two half-hour periods, there is an 80% chance of filling the value area.
If the market opens above the value area and does not return to the value area this is a heavily bullish signal.
However, if the market opens above the value area and returns to the value area this is very bearish.
If the market opens below the value area but returns to the value area this is a bullish signal.
You can see the stock dropped below the value area and then returned to the value area for two 30 minute candles; it performed this action twice.",en,['Luke Posey'],2019-12-24 17:59:13.725000+00:00,"{'Data Science', 'Stock Market', 'Programming', 'Finance', 'Trading'}","{'https://miro.medium.com/focal/774/407/52/50/1*GlvoQ1mWgDqTan4D9Y2COA.png', 'https://miro.medium.com/max/60/1*GlvoQ1mWgDqTan4D9Y2COA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1548/1*GlvoQ1mWgDqTan4D9Y2COA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*BIX0Rb6TnTLYP2eQmgtOpQ.png?q=20', 'https://miro.medium.com/max/60/1*A9EzvnHZl6yD8GNlgz-rIg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*5VsuE0dIXNeg9qhrYfkXWw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1852/1*BIX0Rb6TnTLYP2eQmgtOpQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*5VsuE0dIXNeg9qhrYfkXWw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3668/1*A9EzvnHZl6yD8GNlgz-rIg.png'}",2020-03-05 00:10:18.226688,5.961853504180908
https://towardsdatascience.com/pandas-tips-that-will-save-you-hours-of-head-scratching-31d8572218c9,Pandas tips that will save you hours of head-scratching,"Pandas tips that will save you hours of head-scratching

Making your Data Analysis experiments reproducible saves time for you and others in the long term

When revisiting a problem you’ve worked on in the past and finding out that the code doesn’t work is frustrating. Making your Data Analysis experiments reproducible saves time for you and others in the long term. These tips will help you to write reproducible pandas code, which is important when you are working in a team, on personal projects that you plan to revisit in the future or share with others.

To run the examples download this Jupyter notebook.

To Step Up Your Pandas Game, read:

Output Versions

To make your pandas experiment reproducible, start with outputting the system information and versions of python packages. You (or your colleague) can thank me later. Some functions may be deprecated, broken or are unavailable in older versions of a certain package. Note, Python packages are intentionally printed out with == so that you can use the output directly with pip to install them.

I use the following template to output system information and versions of packages.

import os

import platform

from platform import python_version import jupyterlab

import numpy as np

import pandas as pd print(""System"")

print(""os name: %s"" % os.name)

print(""system: %s"" % platform.system())

print(""release: %s"" % platform.release())

print()

print(""Python"")

print(""version: %s"" % python_version())

print()

print(""Python Packages"")

print(""jupterlab==%s"" % jupyterlab.__version__)

print(""pandas==%s"" % pd.__version__)

print(""numpy==%s"" % np.__version__)

Say NO to automatic imports

Once I had this great idea that instead of writing each time import pandas as pd, import numpy as np , etc. Jupyter Notebook could automatically import them.

Sounds great? Well, it didn’t end well. After a while, I forgot about my custom configuration and soon later I got questions like “Did you even run the code because it fails on the first line!”, “How does this code work on your machine?”.

Say NO to automatic imports.

Set Seeds

When using randomly generated data, setting the seed is a must. In case you are using a Machine Learning model, you should also set the random seed (if available) so that the model returns deterministic output.

Let’s look at the example. We set the random seed and output a random sample with 10 pseudorandom numbers. As expected the second run has different pseudorandom numbers than the first one. Note, pandas uses numpy under the hood so we need to set the seed with numpy.

np.random.seed(42) np.random.random_sample(10) # Output array([0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,

0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258])

np.random.random_sample(10) # Output array([0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,

0.18340451, 0.30424224, 0.52475643, 0.43194502, 0.29122914])

What happens when we set the same seed again? We reset the seed and we get the same sequence of numbers as above. This makes deterministic pseudorandom number generator.

np.random.seed(42) np.random.random_sample(10) # Output array([0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,

0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258])

Commenting

A code block with 10 lines of pandas code can be most probably rewritten in 5 lines by a pandas expert, but code understandability suffers. We tend to do the same as we get better and better with the tool. We may know what the code does today, but will we remember in a month? Will Junior Analysist know what is this mumbo jumbo?

Probably not! When the code gets complex, you should write a comment or two. Not just when doing data analysis with pandas, but when coding in general.

Safety checks

Write safety checks instead of comments, like “This part of the code doesn’t support Null values or duplicates”. It will take the same amount of time, but the user will notice safety checks for sure as they will break execution in the case of a problem.

Let’s look at the example below.

df = pd.DataFrame(index=[1, 2, 3, 4, 4], data={""col1"": [""a"", ""b"", ""c"", ""d"", ""d""]})

The Dataframe has duplicated index value 4. We can use duplicated function to detect duplicated index and then break execution with assert statement.

assert len(df[df.index.duplicated()]) == 0, ""Dataframe has duplicates""

Format the code

Jupyter Notebooks are notorious for unformatted, ugly code. The main reason for this is that early versions didn’t have code formaters. After they did, it wasn’t trivial to install them. But this is not the case anymore.

I use jupyterlab-code-formatter with JupyterLab and it works well. Here is the installation guide. Let me know in the comments if you need any help installing it.

Properly formatted code will increase the chance you won’t throw it away and start over.

Output the shape of a DataFrame

I find it a good practice to output the shape of a DataFrame after each transformation. This helps to spot bugs when there is an incorrect number of rows or columns after reading, merging, etc. This way we can find mistakes by only reviewing the outputs of the shape function without rerunning the notebook.

Let’s inner join the example DataFrame with itself. It has only 5 rows so we expect that the new DataFrame will also have 5 rows.

df.shape (5, 1)

The new DataFrame has 7 rows instead of 5.

df_new = df.join(df, lsuffix='_l', rsuffix='_r')

df_new.shape (7, 2)

We see that the problem occurs because of a duplicated index. We spot a bug right away by using the shape function.

df_new

Asking reproducible questions

There are cases when we have the input data, we know how the output should look, but we don’t know how to write steps in-between (we’ve all been there). The next step is usually to ask a question on Stack Overflow or Reddit or ask a colleague for help.

We can make collaborative problem solving much easier by writing reproducible questions:

describe the core of the problem concisely and don’t dive too deep by copy-pasting half of the experiment,

use small DataFrames that can be initialized in single line (don’t reference local datasets),

when using slack wrap the code in the code block with ```.

A good practice is to make DataFrame output copy-paste friendly. Never heard of it? Let me explain with an example.

df # Copy-paste output col1

1 a

2 b

3 c

4 d

4 d

Pandas has a read_clipboard method which does as the name suggests. We can copy the output from above and run the command below and it will replicate the DataFrame. Try it for yourself. We can also change the separator to tabs \t or any other separator if necessary.

df = pd.read_clipboard(sep='\s\s+')

df

This copy-paste procedure doesn’t work with MultiIndex. So try to avoid them when asking questions.

Conclusion

These were few tips to make your pandas experiments reproducible. Did you enjoy the post? Let me know in the comments below.","['save', 'know', 'versions', 'import', 'tips', 'pandas', 'seed', 'hours', 'reproducible', 'data', 'output', 'headscratching', 'dataframe', 'code']","These tips will help you to write reproducible pandas code, which is important when you are working in a team, on personal projects that you plan to revisit in the future or share with others.
To Step Up Your Pandas Game, read:Output VersionsTo make your pandas experiment reproducible, start with outputting the system information and versions of python packages.
import osimport platformfrom platform import python_version import jupyterlabimport numpy as npimport pandas as pd print(""System"")print(""os name: %s"" % os.name)print(""system: %s"" % platform.system())print(""release: %s"" % platform.release())print()print(""Python"")print(""version: %s"" % python_version())print()print(""Python Packages"")print(""jupterlab==%s"" % jupyterlab.__version__)print(""pandas==%s"" % pd.__version__)print(""numpy==%s"" % np.__version__)Say NO to automatic importsOnce I had this great idea that instead of writing each time import pandas as pd, import numpy as np , etc.
np.random.seed(42) np.random.random_sample(10) # Output array([0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258])CommentingA code block with 10 lines of pandas code can be most probably rewritten in 5 lines by a pandas expert, but code understandability suffers.
ConclusionThese were few tips to make your pandas experiments reproducible.",en,['Roman Orac'],2020-03-02 20:53:10.928000+00:00,"{'Data Science', 'Python', 'Programming', 'Software Engineering', 'Analytics'}","{'https://miro.medium.com/max/60/1*qadgz67If02iu_EwPdGomw.png?q=20', 'https://miro.medium.com/max/60/1*zZTvx9gJlELla1NLxoi-fA.png?q=20', 'https://miro.medium.com/max/4552/1*qadgz67If02iu_EwPdGomw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*hdK7u6AQ76eA4rbJ0fXsAQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1010/0*B-rgeu75PoP4MYmj.jpg', 'https://miro.medium.com/max/4556/1*NpTgaNAZFL70uLh9uLzO3Q.png', 'https://miro.medium.com/max/3808/1*AfEMO9z1E434qbNih7FkHA.png', 'https://miro.medium.com/fit/c/160/160/2*b9r4UOJik2ArFpqDyxRJ9A.jpeg', 'https://miro.medium.com/max/505/0*B-rgeu75PoP4MYmj.jpg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*AfEMO9z1E434qbNih7FkHA.png?q=20', 'https://miro.medium.com/max/60/0*B-rgeu75PoP4MYmj.jpg?q=20', 'https://miro.medium.com/max/4536/1*zZTvx9gJlELla1NLxoi-fA.png', 'https://miro.medium.com/fit/c/96/96/2*b9r4UOJik2ArFpqDyxRJ9A.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*NpTgaNAZFL70uLh9uLzO3Q.png?q=20', 'https://miro.medium.com/max/4540/1*hdK7u6AQ76eA4rbJ0fXsAQ.png'}",2020-03-05 00:10:25.439289,7.211599826812744
https://towardsdatascience.com/the-best-new-geospatial-data-science-libraries-in-2019-7f0174e2a0eb,The Best New Geospatial Data Science Libraries In 2019,"As a Geospatial data scientist, 2019 brought some new tools that made my life easier. In this post, I am sharing the best of these new additions in the Python ecosystem and some resources to get you started. You will find tools that accelerate your Geospatial data science pipelines using GPU, advanced Geospatial Visualization tools and some simple, useful Geoprocessing tools. I hope you will find one or two from the list that can help you.

1. cuSpatial: GPU-Accelerated Spatial and Trajectory Data Management and Analytics Library

It is part of open-sourced libraries with GPU accelerated data science pipelines entirely carried out in the GPU. It was one of the continually developing libraries in 2019 and part of these ecosystems, cuSpatial provides accelerated tools for Geospatial data science processes. A project showing the capability of the GPU accelerated Geospatial data science can be found here: Where should I walk? And the releasing blog.

So far, cuSpatial supports the following features with other features planned in the future:

Spatial window query Point-in-polygon test Haversine distance Hausdorff distance Deriving trajectories from point location data Computing distance/speed of trajectories Computing spatial bounding boxes of trajectories

KeplerGL Jupyter is both convenient and remarkable tool for Geospatial data visualisation inside Jupyter notebooks. This library provided Uber’s advanced Geovisualization tool for Big and was released in June 2019. It allows users to plot maps inside Jupyter Notebook easily and also comes with the Interactive User Interface (UI).

If you want to get started with this tool, here is a step by step guide on how to use in Jupyter notebook.

3. Pysal 2.0: Python Spatial Analysis Library

Pysal is primarily for doing spatial statistics in python and with the release of 2.0 brought it with an improved level of integration with other Geospatial libraries like Geopandas. Pysal now offers a collection of tools for geographic data science packages in Python, including Exploratory Spatial Data Analysis (ESDA).

Here is a guide on how to perform ESDA with Pysal.

Geofeather was small but a great addition to the Python ecosystem. This library brings a faster file-based format for storing geometries with Geopandas, just like a feather is to python. With a simple experiment, I found Geofeather is super fast. I had to wait 15 minutes to write shapefile with Geopandas. It only takes 25 seconds to write that same file in Feather with Geofeather.

PlotlyExpress, although not specifically for Geospatial data visualisation, it brings an easy and intuitive API for Geographic data visualisation with less code. Chances are you have seen PlotlyExpress chart or map already in other articles here in Towards Data Science, as this was immediately adopted in the data science community.

EarthPy promises to integrate both Vector and Raster data processing into one python package. It is built on top of Geopandas and Rasterio. For current spatial analysis, we deal with multiple libraries. Combining some of these functionalities holds some value in many cases. Some resources and examples are available here.

PyGeos was a new addition to the Geospatial Python Ecosystem. It provides vectorised functions to work with arrays of geometries (Numpy), giving better performance (than Shapely) and convenience for such use-cases.

PyGeos Example

8. Momepy: Urban Morphology Measuring Toolkit

MomePy was released late 2019, and it is an excellent addition for the Urban planners and Geospatial data scientists, enabling them to perform advanced quantitative analysis of urban morphology. The library comes with extensive examples using Jupyter notebooks","['spatial', 'jupyter', 'tools', 'geospatial', 'python', 'data', 'gpu', '2019', 'geopandas', 'libraries', 'best', 'science']","You will find tools that accelerate your Geospatial data science pipelines using GPU, advanced Geospatial Visualization tools and some simple, useful Geoprocessing tools.
It was one of the continually developing libraries in 2019 and part of these ecosystems, cuSpatial provides accelerated tools for Geospatial data science processes.
A project showing the capability of the GPU accelerated Geospatial data science can be found here: Where should I walk?
PlotlyExpress, although not specifically for Geospatial data visualisation, it brings an easy and intuitive API for Geographic data visualisation with less code.
Chances are you have seen PlotlyExpress chart or map already in other articles here in Towards Data Science, as this was immediately adopted in the data science community.",en,[],2020-01-04 22:50:38.187000+00:00,"{'Geospatial', 'Data Science', 'Python', 'Programming', 'Geographicdatascience'}","{'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3496/0*r0SNsdLVkG6O0wER', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*VeoHoYuQFCHi9JI7sgUNWA.png', 'https://miro.medium.com/max/3360/1*VeoHoYuQFCHi9JI7sgUNWA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*r0SNsdLVkG6O0wER?q=20', 'https://miro.medium.com/fit/c/160/160/1*7KQZpS8LkIEfZPPHcKfn8Q.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/910/0*Rfb6D7zKapt6p-VU', 'https://miro.medium.com/fit/c/96/96/1*7KQZpS8LkIEfZPPHcKfn8Q.jpeg', 'https://miro.medium.com/max/60/0*Rfb6D7zKapt6p-VU?q=20', 'https://miro.medium.com/max/60/1*VeoHoYuQFCHi9JI7sgUNWA.png?q=20'}",2020-03-05 00:10:27.325610,1.8863210678100586
https://towardsdatascience.com/trading-toolbox-03-ohlc-charts-95b48bb9d748,Python Trading Toolbox: introducing OHLC charts with Matplotlib,"Python Trading Toolbox: introducing OHLC charts with Matplotlib

Unleashing the Power of OHLC data

Photo by Aditya Vyas on Unsplash

In other posts, we started exploring how to compute some basic indicators based on price (simple moving averages and other moving averages) and how to plot them on a chart together with the price. In those examples, we considered daily price data and used the closing price to represent each day of trading. Quite obviously, financial instruments trade throughout the whole day generating more than one price. The closing price is one of the most relevant prices but does not tell the whole story of what happened during the trading day.

OHLC bars and bar charts are a traditional way to capture the range of prices of a financial instrument generated during the entire day of trading: for each single day, four prices are recorded: the opening price (Open), the highest price (High), the lowest price (Low), and the closing price (Close).

Bar charts are not limited to daily prices: weekly and monthly charts can be constructed using open, high, low and close for each period. They can be applied to intraday charts as well by using hourly bars, or bars for any desired interval (e.g. 30 minutes, 10 minutes, down to 1 minute).

Similarly to bar charts, candlestick charts are based on the Open, High, Low, and Close for each day, but use a different visual representation. The range between the Open and the Close is represented by a ‘candle body’ — which takes different colors depending on whether the Close is higher than the Open or the other way round (usually white and black). Highs and Lows are represented by ‘candle wicks’ (called shadows), placed above and below the body respectively. The use of candlestick charting originates from Japan and is associated with a kind of analysis based on patterns.

Creating OHLC Bar Charts with Python

There are several good visualization resources that enable us to create bar and candlestick charts in Python. Two of the best are Plot.ly and Bokeh. Both solutions allow creating professionally looking interactive charts. On the other hand, Matplotlib focuses on static charts and is capable of producing beautiful publication-quality figures. That is usually my first port of call when I have to produce static charts.

While the Matplotlib library is one of those elements that make Python a great environment for data visualization, when it comes to OHLC financial charts it has so far performed below its true potential. The package that handles the drawing of OHLC and candlestick charts within Matplotlib is called mpl-finance, a module that used to be part of the main Matplotlib distribution until it was declared deprecated and became available only as a separate package. That happened, I believe, for a good reason: mpl-finance is not particularly well integrated with pandas nor as easy to use as other plotting features of Matplotlib. More recently, it has found a new maintainer, Daniel Goldfarb, who is working at creating a new API for mpl-finance to make it more usable and consistent with pandas dataframes. The new version should be released at some point during 2020.

A glimpse into the near future

We can have a first look at how the upcoming version of mpl-finance is going to work and look like. To preview the new version (a pre-release, at the time of writing), you just need to run:

pip install mplfinance

Note the spelling of the upcoming version: there is no ‘-’ nor ‘_’ in the name, while the current version is installed as mpl-finance and imported (quite confusingly) as mpl_finance . The new version will put an end to this name mixup.

We can now create our first price bar charts, using the same data for the SPY ETF used in the first post of the series. You can download the CSV file here.

Typing:

import pandas as pd datafile = 'SPY.csv'

data = pd.read_csv(datafile, index_col = 'Date')

data.index = pd.to_datetime(data.index) # Converting the dates from string to datetime format data

shows our OHLC price dataframe:

We can now import the newly installed mpl-finance library:

import mplfinance as mpf

Creating a price bar chart (for the last 50 days of data) is as easy as:

mpf.plot(data[-50:], no_xgaps = True)

If you are not using Jupyter, do not forget to add the following line to visualize this and the next charts:

plt.show()

This is the result:

Creating a candlestick chart is equally straightforward:

mpf.plot(data[-50:], type='candlestick', no_xgaps = True)

The visual results look appealing. The no_xgaps option is a nifty feature that eliminates the gaps usually generated by days with no trading data (e.g. weekends and public holidays).

The current mpl-finance library

The current version of mpl-finance can be installed using:

pip install mpl-finance

or

conda install mpl-finance

if you are using Conda as a package manager. Compared to the version to be released, the current mpl-finance library requires some data manipulation to create a simple OHLC or candlestick chart. In particular:

We need to present the data as a sequence of OHLC price sequences.

The time and dates need to be expressly converted to a format that Matplotlib understands.

Luckily, all those operations will become obsolete once the new version is released, hopefully soon during 2020. For now, I just present my code to perform those tasks without going too much into detail. You can use it as it is:

We can now use the data to plot a bar chart:

Which shows:

By default, bars with a Close higher than the Open are colored in black, while bars with a Close below the Open are colored in red. There are some visible horizontal gaps in the chart: they are generated by non-trading days (weekends and public holidays). Removing them requires some extra non-trivial filtering.

Similarly, a candlestick chart can be generated in the following way:

Which produces:

Unleashing the Power of OHLC data

Using OHLC prices instead of just one single series as the Closing Price opens up a new world of possibilities: we can assess the range that prices covered during each trading day, observe the relationship of the Close versus the Open, check whether prices have risen above previous highs or fallen below previous lows and so on.

Here I will present a fairly simple example of a chart that makes use of High and Low prices as well as Close prices. This chart comes from a trading technique known as Camelback Technique. I am not interested in describing the trading technique in detail (plenty of information can be found on the web): we are just using the chart associated with it as a useful tool to spot existing trends.

We overlay the following moving averages on a daily price bar chart:

40-Day Simple Moving Average of the Highs.

40-Day Simple Moving Average of the Lows.

15-Day Exponential Moving Average of the Closing prices.

This can be coded as follows:

Which gives:

The two simple moving averages drawn in blue create a channel: compared to a single moving average, we now have a grey area when prices are neither above nor below the channel.

As an example, we could now adopt the following trading rules:

Enter long positions (buying) only when the price bars are completely above the higher 40-Day SMA. Enter short positions (selling) only when the price bars are completely below the lower 40-Day SMA. We do not enter any position (we keep flat on the market) when the prices are between the two 40-Day SMAs, or the last bar is crossing any of them.

Another example could be to:

Enter long positions only when the 15-Day EMA is above the higher 40-Day SMA. Enter short positions only when the 15-Day EMA is below the lower 40-Day SMA. Stay flat elsewhere, i.e. when the 15-Day EMA is inside the channel created by the two SMAs.

At this stage, we should be asking whether those rules (or any of the methods mentioned in the article on moving averages) can be used to build a profitable trading system. In other words, we should be asking whether those ideas will help us generate profits instead of making losses, and how to choose the best set of rules. I will try to address this in the next post. We will learn how to backtest a trading system to calculate the profits or losses based on historical data.","['introducing', 'version', 'chart', 'charts', 'ohlc', 'python', 'price', 'prices', 'trading', 'mplfinance', 'data', 'toolbox', 'moving', 'matplotlib']","In those examples, we considered daily price data and used the closing price to represent each day of trading.
Similarly to bar charts, candlestick charts are based on the Open, High, Low, and Close for each day, but use a different visual representation.
Creating OHLC Bar Charts with PythonThere are several good visualization resources that enable us to create bar and candlestick charts in Python.
The no_xgaps option is a nifty feature that eliminates the gaps usually generated by days with no trading data (e.g.
In particular:We need to present the data as a sequence of OHLC price sequences.",en,['Stefano Basurto'],2020-01-07 15:06:09.915000+00:00,"{'Trading Toolbox', 'Data Science', 'Python', 'Data Visualization', 'Programming', 'trading-toolbox'}","{'https://miro.medium.com/max/60/1*dAapBTAlOHNAOmazvKS1vA.png?q=20', 'https://miro.medium.com/max/1184/1*Aw67DaRETdqL-b8Tnr30dQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1626/1*PeILh1HRhHvgmx_7U5gXFA.png', 'https://miro.medium.com/fit/c/160/160/2*HDi27jNyCtKpjIMLRhgzXQ.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1102/1*lqWuG4iBCukjNpEfwPrC3w.png', 'https://miro.medium.com/max/60/0*Z92OqbJLNIOIJ0-i?q=20', 'https://miro.medium.com/max/1626/1*dAapBTAlOHNAOmazvKS1vA.png', 'https://miro.medium.com/max/1626/1*-f8rxpNY-t5Z8_R5slAVzQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Q8vmk28anQLu9dQhx8Q8Hw.png?q=20', 'https://miro.medium.com/max/1184/1*Q8vmk28anQLu9dQhx8Q8Hw.png', 'https://miro.medium.com/max/60/1*PeILh1HRhHvgmx_7U5gXFA.png?q=20', 'https://miro.medium.com/max/1102/1*L4ihi4piVsGmezyXY5iEqw.png', 'https://miro.medium.com/max/60/1*fgqA-Pp2l0Faa_MLK6hvwQ.png?q=20', 'https://miro.medium.com/max/60/1*lqWuG4iBCukjNpEfwPrC3w.png?q=20', 'https://miro.medium.com/max/60/1*L4ihi4piVsGmezyXY5iEqw.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*HDi27jNyCtKpjIMLRhgzXQ.jpeg', 'https://miro.medium.com/max/60/1*-f8rxpNY-t5Z8_R5slAVzQ.png?q=20', 'https://miro.medium.com/max/1200/0*Z92OqbJLNIOIJ0-i', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/10162/0*Z92OqbJLNIOIJ0-i', 'https://miro.medium.com/max/60/1*Aw67DaRETdqL-b8Tnr30dQ.png?q=20', 'https://miro.medium.com/max/1102/1*fgqA-Pp2l0Faa_MLK6hvwQ.png'}",2020-03-05 00:10:34.380311,7.05470085144043
https://towardsdatascience.com/pandas-join-vs-merge-c365fd4fbf49,Pandas Join vs. Merge,"Merge

At a basic level, merge more or less does the same thing as join. Both methods are used to combine two dataframes together, but merge is more versatile at the cost of requiring more detailed inputs. Let’s take a look at how we can create the same combined dataframe with merge as we did with join:

In: joined_df_merge = region_df.merge(sales_df, how='left',

left_index=True,

right_index=True)

print(joined_df_merge) Out: region sales

Tony West 103.0

Sally South 202.0

Carl West NaN

Archie North NaN

Randy East 380.0

Ellen South 101.0

Fred NaN 82.0

Mo East NaN

HanWei NaN NaN

Not that different from when we used join. But merge allows us to specify what columns to join on for both the left and right dataframes. Here by setting “left_index” and “right_index” equal to True, we let merge know that we want to join on the indexes. And we get the same combined dataframe as we obtained before when we used join.

Merge is useful when we don’t want to join on the index. For example, let’s say we want to know, in percentage terms, how much each employee contributed to their region. We can use groupby to sum up all the sales within each unique region. In the code below, the reset_index is used to shift region from being the dataframe’s (grouped_df’s) index to being just a normal column — and yes, we could just keep it as the index and join on it, but I want to demonstrate how to use merge on columns.

In: grouped_df = joined_df_merge.groupby(by='region').sum()

grouped_df.reset_index(inplace=True)

print(grouped_df) Out: region sales

0 East 380.0

1 North 0.0

2 South 303.0

3 West 103.0

Now let’s merge joined_df_merge with grouped_df using the region column. We have to specify a suffix because both of our dataframes (that we are merging) contain a column called sales. The suffixes input appends the specified strings to the labels of columns that have identical names in both dataframes. In our case, since the second dataframe’s sales column is actually sales for the entire region, we can append “_region” to its label to make clear.

In: employee_contrib = joined_df_merge.merge(grouped_df, how='left',

left_on='region',

right_on='region',

suffixes=('','_region'))

print(employee_contrib)

Out: region sales sales_region

0 West 103.0 103.0

1 South 202.0 303.0

2 West NaN 103.0

3 North NaN 0.0

4 East 380.0 380.0

5 South 101.0 303.0

6 NaN 82.0 NaN

7 East NaN 380.0

8 NaN NaN NaN

Oh no, our index disappeared! But we can use set_index to get it back (otherwise we won’t know which employee each row corresponds to):

In: employee_contrib = employee_contrib.set_index(joined_df_merge.index)

print(employee_contrib)

Out: region sales sales_region

Tony West 103.0 103.0

Sally South 202.0 303.0

Carl West NaN 103.0

Archie North NaN 0.0

Randy East 380.0 380.0

Ellen South 101.0 303.0

Fred NaN 82.0 NaN

Mo East NaN 380.0

HanWei NaN NaN NaN

We now have our original sales column and a new column sales_region that tells us the total sales made in a region. Let’s calculate each employees percentage of sales and then clean up our dataframe by dropping observations that have no region (Fred and HanWei) and filling the NaNs in the sales column with zeros:n

In: # Drop NAs in region column

employee_contrib = employee_contrib.dropna(subset=['region']) # Fill NAs in sales column with 0

employee_contrib = employee_contrib.fillna({'sales': 0}) employee_contrib['%_of_sales'] = employee_contrib['sales']/employee_contrib['sales_region'] print(employee_contrib[['region','sales','%_of_sales']]\

.sort_values(by=['region','%_of_sales']))

Out: region sales %_of_sales

Mo East 0.0 0.000000

Randy East 380.0 1.000000

Archie North 0.0 NaN

Ellen South 101.0 0.333333

Sally South 202.0 0.666667

Carl West 0.0 0.000000

Tony West 103.0 1.000000

All done! Notice that the North region has no sales hence the NaN (can’t divide by zero).","['sales', 'east', 'merge', 'south', 'region', 'pandas', 'column', 'north', 'join', 'vs', 'west', 'nan']","MergeAt a basic level, merge more or less does the same thing as join.
But merge allows us to specify what columns to join on for both the left and right dataframes.
Here by setting “left_index” and “right_index” equal to True, we let merge know that we want to join on the indexes.
Merge is useful when we don’t want to join on the index.
In: grouped_df = joined_df_merge.groupby(by='region').sum()grouped_df.reset_index(inplace=True)print(grouped_df) Out: region sales0 East 380.01 North 0.02 South 303.03 West 103.0Now let’s merge joined_df_merge with grouped_df using the region column.",en,['Tony Yiu'],2020-01-24 14:53:13.214000+00:00,"{'Data Science', 'Python', 'Programming', 'Technology', 'Analytics'}","{'https://miro.medium.com/fit/c/160/160/2*CSDritfpmHLYxn63arD9sQ.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/0*Z1yJhbxQgvVDd6m-', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*Z1yJhbxQgvVDd6m-?q=20', 'https://miro.medium.com/max/14720/0*Z1yJhbxQgvVDd6m-', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/96/96/2*CSDritfpmHLYxn63arD9sQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:10:41.846511,7.466200113296509
https://towardsdatascience.com/optuna-vs-hyperopt-which-hyperparameter-optimization-library-should-you-choose-ed8564618151,Optuna vs Hyperopt,"Ease of use and API

In this section I want to see how to run a basic hyperparameter tuning script for both libraries, see how natural and easy-to-use it is and what is the API.

Optuna

You define your search space and objective in one function.

Moreover, you sample the hyperparameters from the trial object. Because of that, the parameter space is defined at execution. For those of you who like Pytorch because of this imperative approach, Optuna will feel natural.

Then, you create the study object and optimize it. What is great is that you can choose whether you want to maximize or minimize your objective. That is useful when optimizing a metric like AUC because you don’t have to change the sign of the objective before training and then convert best results after training to get a positive score.

That is it.

Everything you may want to know about the optimization is available in the study object.

What I love about Optuna is that I get to define how I want to sample my search space on-the-fly which gives me a lot of flexibility. Ability to choose a direction of optimization is also pretty nice.

If you want to see the full code example you can scroll down to the example script.

10 / 10","['sample', 'object', 'optimization', 'example', 'hyperopt', 'space', 'study', 'vs', 'search', 'objective', 'training', 'optuna']","OptunaYou define your search space and objective in one function.
For those of you who like Pytorch because of this imperative approach, Optuna will feel natural.
Then, you create the study object and optimize it.
Everything you may want to know about the optimization is available in the study object.
What I love about Optuna is that I get to define how I want to sample my search space on-the-fly which gives me a lot of flexibility.",en,['Jakub Czakon'],2020-01-16 15:27:33.984000+00:00,"{'Deep Learning', 'Data Science', 'Classification Models', 'Machine Learning', 'Hyperparameter Tuning'}","{'https://miro.medium.com/max/60/0*EXIfiqLJGZYHMZcr?q=20', 'https://miro.medium.com/max/1516/0*4-etB3QVQ9rV3Kix', 'https://miro.medium.com/max/60/0*-cxsR6Te90TUzb6g?q=20', 'https://miro.medium.com/max/60/0*YUKuhtb1K4imG6vX?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*bzmIZThEMLsYe6kSftMhrQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*76yVgFSE4AoNMqmT7VD9rw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2144/0*YUKuhtb1K4imG6vX', 'https://miro.medium.com/max/60/0*4-etB3QVQ9rV3Kix?q=20', 'https://miro.medium.com/fit/c/96/96/1*76yVgFSE4AoNMqmT7VD9rw.png', 'https://miro.medium.com/max/60/1*bzmIZThEMLsYe6kSftMhrQ.jpeg?q=20', 'https://miro.medium.com/max/60/0*GL2Dckl2sEZx8-PD?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/7086/1*bzmIZThEMLsYe6kSftMhrQ.jpeg', 'https://miro.medium.com/max/60/0*Tuv9CWLMEZ5K3wW3?q=20', 'https://miro.medium.com/max/1322/0*-cxsR6Te90TUzb6g', 'https://miro.medium.com/max/1728/0*GL2Dckl2sEZx8-PD', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1854/0*EXIfiqLJGZYHMZcr', 'https://miro.medium.com/max/1728/0*Tuv9CWLMEZ5K3wW3'}",2020-03-05 00:10:44.674735,2.828223943710327
https://medium.com/analytics-vidhya/introducing-trelawney-a-unified-python-api-for-interpretation-of-machine-learning-models-6fbc0a1fd6e7,Introducing Trelawney : a unified Python API for interpretation of Machine Learning Models,"Introducing Trelawney : a unified Python API for interpretation of Machine Learning Models Antoine Redier Follow Feb 6 · 9 min read

This article lays out the advantages of explaining black box machine learning models and explains how to do it with trelawney, a package that aims at simplifying the model interpretation workflow.

The importance of interpretation: why we built Trelawney

Interpretability : The final step of building Machine Learning Models

For a long time, models have focused on reaching high performances and not necessarily on explaining the causes of such results. For instance, computer vision problematics are more about being as precise as possible than about explaining what features led specifically to such outstanding precision… With the democratization of machine learning solutions, the need to understand the drivers of a specific model has become more and more apparent : many applications of machine learning algorithms cannot only rely on shiny figures but need the possibility to give easy access to their interpretation.

The interpretation first helps to assess the model relevance and to increase the confidence one may have in it. In that sense, it helps both technical and non-technical stakeholders. Indeed, most people will not blindly trust performance metrics, they will want that model and the product that is being built with it to “make sense”. This is the heart of explainability: making sense of black box models.

In addition, explaining how your model works enables non-technical stakeholders to derive more value from your model. Is the feature important ? If yes, in which direction does it drive the output? These are the type of questions we want to answer. For instance, in the original project for which we built Trelawney, the interpretation of the model was used to build a sales pitch for bankers.

Furthermore, being able to explain the predictions of your model will also enable you to build better products. For instance, if you are building a fraud detection system, being able to tell a user why their transaction was blocked might reduce friction and educate them.

Trelawney : Python Package for all interpretability methods in one place

This need for model interpretability led us to build Trelawney in collaboration with Quinten, a French consulting company specialised in Data Science and Data Translation. Indeed, Quinten works in applied machine learning, in particular in the healthcare and banking industries. For them, a good solution not only needs to meet analytical expectations, but should also be understandable and actionable on the field.

Our project during this collaboration was to give an appetence score for a banking product, for each given client, based on its profile.

In the context of selling a financial product, the banker would call a potential client to explain why the product would be relevant for them. An interpretation of the model therefore helps us build sales pitch (emphasising which characteristics in the client profile the banker should mention, and which one he/she should avoid).

We built several Machine Learning models, from the most interpretable ones (such as a single decision tree, or logistic regression) to less interpretable models (such as Random Forest, XGBoost and Neural Networks), and then applied several interpretability methods, model-dependent or model-agnostic, and global or local.

The whole philosophy behind the Trelawney package was to unify the different existing APIs of current explainability packages. Indeed, for the different methods we will present, there is an existing python implementation (usually provided by the researchers who introduced said methods). Our idea was to write a common API that would allow Data Scientists to learn how to use those methods once and focus on choosing the best one for their use case (instead of learning a new package each time). Furthermore, we wanted to create a unique go-to package for all-interpretability methods, with a unified syntax.

We named our package Trelawney in honour of the professor of divination of Hogwarts, known for her capacity to interpret blackbox tea-cups.

Interpretability methods, theory and implementation with Trelawney

Speaking of all the methods we are bringing under the same roof, let’s spend a bit of time explaining each of them and how to use them with Trelawney. This will be the most theoretical part of this article and you might want to skip if you already know it.

We will do some demonstrations by explaining a Random Forest trained on the Titanic Dataset. You can find the full notebook here .

We will first explain global interpretation methods with SHAP and surrogate models and then look at local explanation methods with LIME and SHAP again.

Global interpretation

The first type of explanation you will want to do is a general explanation of the model. The idea here is to tell which features are most influential on the model’s overall output. For this, there are two main methods that have been implemented in Trelawney: SHAP and using a Surrogate model.

SHAP

The first global interpretation method we will present is SHAP. SHAP (SHapley Additive exPlanations) uses game theory to explain the prediction by computing the contribution of each feature to the prediction. It computes a fair measure of each feature’s contribution to the final output (increasing or decreasing the prediction). This is divided between intrinsic contribution (if the feature was alone) and the marginal contribution (when the feature is combined with other features). Bear in mind that SHAP tries to approximate the prediction, and therefore boils down to building a simpler model on top of the original one.

If you sum the absolute value of all the SHAP values for each individual prediction, you get the feature importance.

Below is the application of SHAP for our Titanic Example :

>>> explainer = ShapExplainer() >>> explainer.fit(model, x_train, y_train)

We can then get the importance of each feature:

>>> importance_dict = explainer.feature_importance(x_train)

Or we can directly use Trelawney’s graphs:

>>> explainer.graph_feature_importance(x_train)

SHAP feature importance graph

In the above graph we can validate our intuition of what is important in order to determine if one survives the Titanic.

You might wonder what the ‘rest’ column is. Trelawney lets you limit the graph to the n most impactful features (the rest is the importance of all the other features summed).

Surrogate models

The second global interpretability method available in Trelawney is using a surrogate model. The main idea behind this method is to fit a fully explainable model (such as a decision tree or a linear regression) on the output of your black box model.

Here is the implementation with a single decision tree:

We fit our surrogate model:

>>> explainer = SurrogateExplainer(DecisionTreeClassifier(max_depth=3)) >>> explainer.fit(model, x_train, y_train)

And we see how well it explains our model:

>>> explainer.adequation_score() 0.968562874251497

This means a single decision Tree with a maximum depth of 3 explains 96.8% of our random forest’s predictions. The adequation score here defaults to accuracy but you can choose to use any sci-kit learn metric you wish (if you have an imbalanced dataset for instance).

The advantage here is that you can then visualize your model in a very intuitive way:

>>> explainer.plot_tree(out_path=’./tree_viz’)

surrogate tree visualisation

This approach has the advantage of not being an approximation of a value for each of your features (as for SHAP) since it is a truly global approach that takes into account the strong correlation that might occur between features. Furthermore, in our experience, this method is also fairly simple to explain to non technical stakeholders, as people understand the principle of decision trees.

Obviously a simple decision tree cannot be as accurate as a more complex model, but we can compute to what extent the decision tree is sufficient, and explain that, in the rest of the cases, the prediction is too complex to be approximated by a simple decision tree.

Local interpretation methods

The other type of explanation methods that you might want to produce are local explanations. Here you are not trying to get a feel of your model but you want to know how it came to a result for a given sample. If we take our fraud detection system example from earlier, your user will not really care that X or Y influences to block transactions globally, he or she wants to know why he or she got blocked right now. This type of explanation are also generally quite useful for us data-scientists as debugging tools. Indeed, being able to peek at how our model made outrageous predictions might help unveil some overfitting issues with our model as a whole or biases in our training data.

LIME

The first local method that is implemented in Trelawney is LIME (Local Interpretable Model-agnostic Explanations). Lime was one of the first model agnostic explainability methods introduced by Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin in this paper. As one of the oldest methods it is fairly well known by data scientists but we will recap quickly. LIME works by building local approximations of your model using an explainable model (generally linear regressions). Basically, if you want to explain a prediction, LIME will sample points around the input of your prediction, run your model for those artificial points and try to approximate the output using an explainable model. The resulting model will give you the local approximation around the prediction of interest.

Here is how you can use Lime with trelawney:

>>> explainer = LimeExplainer() >>> explainer.fit(model, x_train, y_train) >>> explainer.explain_local(x_test.loc[most_probable, :]) [{‘Age’: -0.029329040463817208, ‘Deck’: 0.15191169970680513, ‘Elderly’: 0.004273665522566747, ‘Embarked’: 0.05382052701464114, ‘Family_Size’: -0.020401593852417545, ‘Is_Married’: 0.01739687755247181, ‘Parch’: -0.033679921376068664, ‘Pclass’: 0.11590397989964185, ‘Sex’: 0.0012503083084444357, ‘SibSp’: -0.0029291606226194682, ‘rest’: 0.0}]

Here, the local explanation of our biggest false positive can be used to further improve our model as we’d think where a passenger embarked shouldn’t have influenced whether they survived the disaster or not that much.

SHAP for local interpretation

In SHAP for local interpretation, instead of summing the contribution for each individual sample to get the overall SHAP value for a particular feature (as we did above), we only look at the SHAP values at the observation level.

Here is the implementation:

explainer = ShapExplainer() explainer.fit(model, x_train, y_train) explainer.graph_local_explanation(x_test.loc[biggest_false_positive, :])

shap local explanation graph (biggest false positive)

Final Thoughts and further improvements:

All in all, explaining your model can boost dramatically the value you are able to get out of it and we hope Trelawney will simplify this process for you (by unifying several packages with the same syntax and workflow). Explaining your models might also become a necessity in a lot of fields as regulations might require to explain the outputs of your models. For instance, the article 22 of the European General Data Protection Regulation states that “data subject shall have the right not to be subject to a decision based solely on automated processing”. The data scientist should then be able to provide explanations on the decision process. Such an opportunity is highly valuable for experts like Quinten as it allows to use black-box models when an increase in performance is needed without suffering from the lack of interpretability. Not to mention all the fields of application these new methods open.

Although we are proud of the work we have done so far, we still have a long way to go and have plenty of ways to improve Trelawney. Here are a few ideas of things that might come along in future versions:

Support for regression problems (as Trelawney only supports classifiers as of writing)

More model-focused methods (for intrinsically explainable models)

Use more readable and faithful graphs

If any of those projects interests you or if you just see a bug/typo you would feel like fixing, we would love you to contribute !

co-authored by Antoine Redier and Amélie Meurer. Trelawney was built thanks to Ludmila Exbrayat, Inès VanAgt and Skander Kamoun and thanks to A. Grangetas for his support and advice.","['machine', 'models', 'introducing', 'unified', 'prediction', 'trelawney', 'python', 'learning', 'decision', 'methods', 'model', 'api', 'shap', 'local', 'interpretation']","Introducing Trelawney : a unified Python API for interpretation of Machine Learning Models Antoine Redier Follow Feb 6 · 9 min readThis article lays out the advantages of explaining black box machine learning models and explains how to do it with trelawney, a package that aims at simplifying the model interpretation workflow.
For instance, in the original project for which we built Trelawney, the interpretation of the model was used to build a sales pitch for bankers.
We will first explain global interpretation methods with SHAP and surrogate models and then look at local explanation methods with LIME and SHAP again.
For this, there are two main methods that have been implemented in Trelawney: SHAP and using a Surrogate model.
Local interpretation methodsThe other type of explanation methods that you might want to produce are local explanations.",en,['Antoine Redier'],2020-02-11 08:12:13.015000+00:00,"{'Visualization', 'Machine Learning', 'Python', 'Data Science'}","{'https://miro.medium.com/max/1892/1*YAzdf3aQJGsmqnTyg5a8Ig.png', 'https://miro.medium.com/fit/c/80/80/1*PFdJBI5MLv6iMemP3QtVlA.jpeg', 'https://miro.medium.com/max/1728/1*BZ-ZF57CAkTwOEEam5zZ3A.png', 'https://miro.medium.com/max/60/1*K7I3MRct97VogAtr1dboMA.png?q=20', 'https://miro.medium.com/max/1000/1*jIVNf-MM-AafL2LA-MUdEQ.gif', 'https://miro.medium.com/max/3200/1*niPNh31eohqwfRHLaz2-nQ.png', 'https://miro.medium.com/fit/c/80/80/2*-22Bm0tuymQyxNXw1WBxyQ.jpeg', 'https://miro.medium.com/max/60/1*niPNh31eohqwfRHLaz2-nQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*miCA9MEw8TjpXyR0xY1w-A.png', 'https://miro.medium.com/fit/c/96/96/0*-B1DHy_aO1wGbmZR.jpg', 'https://miro.medium.com/max/3200/1*K7I3MRct97VogAtr1dboMA.png', 'https://miro.medium.com/freeze/max/60/1*jIVNf-MM-AafL2LA-MUdEQ.gif?q=20', 'https://miro.medium.com/max/60/1*YAzdf3aQJGsmqnTyg5a8Ig.png?q=20', 'https://miro.medium.com/max/290/1*cK8jYS5H7rDYhb0vZkW4NA.png', 'https://miro.medium.com/fit/c/80/80/2*f0xB_2JvkHaF1zlsF-hsHA.jpeg', 'https://miro.medium.com/max/946/1*YAzdf3aQJGsmqnTyg5a8Ig.png', 'https://miro.medium.com/max/60/1*BZ-ZF57CAkTwOEEam5zZ3A.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*-B1DHy_aO1wGbmZR.jpg'}",2020-03-05 00:10:46.164536,1.4888026714324951
https://towardsdatascience.com/top-5-functions-for-exploratory-data-analysis-with-pandas-7b3cbe1a1566,Top 5 Functions for Exploratory Data Analysis with Pandas,"While Pandas by itself isn’t that difficult to learn, mainly due to the self-explanatory method names, having a cheat sheet is still worthy, especially if you want to code out something quickly. That’s why today I want to put the focus on how I use Pandas to do Exploratory Data Analysis by providing you with the list of my most used methods and also a detailed explanation of those.

I don’t want to dwell too much with the intro, so I’ll just quickly go over the dataset used and then we’ll jump into the good stuff.

Dataset used

There’s no need to use complex datasets to demonstrate simple ideas, so with that in mind, I decided to use the Iris dataset. If you decide to follow along with the code you can find the dataset on this link.

Here’s how to import the Pandas library and load in the dataset:

Okay, let’s not waste any more time and see how the EDA process typically looks like, Pandas-wise.","['today', 'exploratory', 'worthy', 'pandas', 'waste', 'used', 'dataset', 'usedtheres', 'typically', 'quickly', 'data', 'functions', 'analysis', 'code']","While Pandas by itself isn’t that difficult to learn, mainly due to the self-explanatory method names, having a cheat sheet is still worthy, especially if you want to code out something quickly.
That’s why today I want to put the focus on how I use Pandas to do Exploratory Data Analysis by providing you with the list of my most used methods and also a detailed explanation of those.
I don’t want to dwell too much with the intro, so I’ll just quickly go over the dataset used and then we’ll jump into the good stuff.
Dataset usedThere’s no need to use complex datasets to demonstrate simple ideas, so with that in mind, I decided to use the Iris dataset.
Here’s how to import the Pandas library and load in the dataset:Okay, let’s not waste any more time and see how the EDA process typically looks like, Pandas-wise.",en,['Dario Radečić'],2020-02-24 22:12:22.351000+00:00,"{'Data Science', 'Artificial Intelligence', 'Python', 'Machine Learning', 'Programming'}","{'https://miro.medium.com/max/722/1*8ClxWHxzPTE1vUxGucv9IA.png', 'https://miro.medium.com/max/60/1*-9P4gsXXcCSnAulfUTGdYw.png?q=20', 'https://miro.medium.com/max/9444/0*3-4pJHKAvTLXoWQu', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*5vZiiabXSkkP4NGk4GusCQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*rvzZwiObX_wTSwk1yWxEVg.png?q=20', 'https://miro.medium.com/max/814/1*VSV0f1bHA43u53-S0l1imw.png', 'https://miro.medium.com/max/844/1*rvzZwiObX_wTSwk1yWxEVg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*D51Ds_FF1TZkpx8MFHVGlQ.png?q=20', 'https://miro.medium.com/max/60/0*3-4pJHKAvTLXoWQu?q=20', 'https://miro.medium.com/max/754/1*EgnotPVs1pU80Ruklr8KEg.png', 'https://miro.medium.com/max/60/1*R8VqxE9GW5a44Uel6ZsyKA.png?q=20', 'https://miro.medium.com/max/60/1*iMnmVR7w0H22qQV2_Hnb6A.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*VmdbajrpX9nwOc9UtkV3Yg.png', 'https://miro.medium.com/max/858/1*-9P4gsXXcCSnAulfUTGdYw.png', 'https://miro.medium.com/max/60/1*8ClxWHxzPTE1vUxGucv9IA.png?q=20', 'https://miro.medium.com/max/60/1*vEq0kbb0m0MobUcCSPlNHg.png?q=20', 'https://miro.medium.com/max/766/1*D51Ds_FF1TZkpx8MFHVGlQ.png', 'https://miro.medium.com/max/862/1*iMnmVR7w0H22qQV2_Hnb6A.png', 'https://miro.medium.com/max/490/1*R8VqxE9GW5a44Uel6ZsyKA.png', 'https://miro.medium.com/max/1200/0*3-4pJHKAvTLXoWQu', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/326/1*IByXj27KomY2-dwzzlozCQ.png', 'https://miro.medium.com/max/364/1*vEq0kbb0m0MobUcCSPlNHg.png', 'https://miro.medium.com/max/848/1*5vZiiabXSkkP4NGk4GusCQ.png', 'https://miro.medium.com/max/848/1*JMrvcR5JB9hGSYuTJnIJ8A.png', 'https://miro.medium.com/fit/c/160/160/2*VmdbajrpX9nwOc9UtkV3Yg.png', 'https://miro.medium.com/max/602/1*V3EFG_pHZ27QG5n9D_JOdw.png', 'https://miro.medium.com/max/60/1*V3EFG_pHZ27QG5n9D_JOdw.png?q=20', 'https://miro.medium.com/max/60/1*JMrvcR5JB9hGSYuTJnIJ8A.png?q=20', 'https://miro.medium.com/max/60/1*VSV0f1bHA43u53-S0l1imw.png?q=20', 'https://miro.medium.com/max/60/1*keCJuZBKJbAF_NNnJVcXxQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*EgnotPVs1pU80Ruklr8KEg.png?q=20', 'https://miro.medium.com/max/60/1*IByXj27KomY2-dwzzlozCQ.png?q=20', 'https://miro.medium.com/max/562/1*keCJuZBKJbAF_NNnJVcXxQ.png'}",2020-03-05 00:10:53.774785,7.60924768447876
https://towardsdatascience.com/using-functiontransformer-and-pipeline-in-sklearn-to-predict-chardonnay-ratings-9b13fdd6c6fd,Using FunctionTransformer and Pipeline in SkLearn to Predict Chardonnay Ratings,"Discovering the Pipeline

One of the best things about learning to code is the endless number of rabbit holes to fall into as you discover new concepts. An excercise I worked through on DataCamp exposed me to Pipeline in Scikit-Learn, and it blew my mind! Pipelines allow you to sequentially transform your data and implement fit(). As soon as I was done with the excercise, I started applying what I had learned and crafted a pipeline to clean the wine review dataset and predict the rating of Chardonnay. This is an in-depth review of the process I went through while creating my pipeline and predictive model. I discuss creating features, applying category encoders, constructing a pipeline and generating predictions. The complete code and a link to the github repo can be found at the bottom of the article:

Importing Dependancies and Data

The data originates from the Wine Review dataset available on Kaggle. I’ve written several articles covering exploring the dataset and have already stored it in a SQLite database. The data is only partially cleaned, and it needs to be transformed so it can be used in the machine learning model.

import numpy as np

import pandas as pd

import sqlite3

import category_encoders as ce

import re from sklearn.feature_selection import chi2, SelectKBest

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import HashingVectorizer from sklearn.pipeline import Pipeline, FeatureUnion

from sklearn.impute import SimpleImputer from sklearn import ensemble from sklearn.preprocessing import MaxAbsScaler

from sklearn.preprocessing import FunctionTransformer conn = sqlite3.connect('db/wine_data.sqlite')

c = conn.cursor #create Dataframe

df = pd.read_sql(""select country \

,description \

,rating \

,price \

,province \

,title \

,winery from wine_data where variety = 'Chardonnay'"", conn)

df.head(2)

Notice how many columns contain text data.

In order to use them in the machine learning model, text will have to be transformed using encoding. There are several strategies for encoding the categorical data like winery, country, province, and title, and I’ll cover them in the Category Encoders section. Before I encode them, I need to create a few features that might help the predictive model.

Creating New Features

Feature engineering can be done to extract potentially predictive qualities within the data, and add it to the dataset as a column. I have created a set of funtions that take in a dataframe and output the transformed dataframe. Using Scikit Learn’s Function Transformer, I can use the functions in the pipeline to transform the dataframe. I tried to make the functions dynamic using global variables.

Extracting the Year from Title

One of the features I want to add is the wine’s year found within the title column. The function extract_year takes in a dataframe and returns a dataframe with a year column added.

def extract_year(dataframe): #set the column name containing the year using a global variable

global year_column

years = dataframe[year_column]

#years.reset_index(inplace=False)

#years.fillna("""", inplace=True)

l = []

i = 0 #use for loop to extract the year from each title row

for year in range(len(dataframe)):

temp = re.findall(r'\d+', years[i])

res = list(map(int, temp))

try:

if len(str(res[0])) == 4:

l.append(res[0])

elif len(str(res[0])) != 4:

l.append(0)

except:

l.append(0)

#print(res[0])

i+=1

dataframe['year'] = l return dataframe

Counting the Words in the Description

While exploring the dataset, I noticed wines with shorter reviews tended to have lower ratings. Because of that, I’ll add the word count of the description to the model to see if it is a predictor. I use a function that relies on a global variable named word_count_column to indicate which column of the data frame to use. It takes in a data frame and returns a data frame with a word_count column added.

def word_count(dataframe):

global word_count_column

dataframe['word_count'] = dataframe[text].apply(lambda word: len(str(word).split("" "")))

return dataframe

Data Frame with Added Features

We can see the two functions work and produce the desired data frame.

After adding the two numeric features, it is time to consider encoding the categorical data in columns country, province, title, and winery.

Category Encoders

Since the machine learning model takes numbers as input, the text must be transformed. The strategy used to encode the data can have a big impact on the accuracy of the model. To easily try a variety of coding strategies, I recommend using the Category Encoder package that works with Scikit Learn.

The category encoders available in the package are compatible with Pipeline since they are transformers.

pip install category_encoders

OR

conda install -c conda-forge category_encoders

There are 15 or so encoders included in the package. I recommend trying these with the model pipeline to see how the different strategies impact the accuracy of the model.

Although they are compatible with Pipeline, I created a function so I could pass in additional logic. I pass in a data frame and two global variables to control which category and target columns are used in the transformation.

# encoder = ce.JamesSteinEncoder(cols=[...]) --maybe

# encoder = ce.LeaveOneOutEncoder(cols=[...]) --maybe

# encoder = ce.MEstimateEncoder(cols=[...]) --maybe

# encoder = ce.OrdinalEncoder(cols=[...]) --maybe

# encoder = ce.TargetEncoder(cols=[...]) --maybe def category_encode(dataframe):

global category_columns

global category_target

x = dataframe[category_columns]

y = dataframe[target]

ce_ord = ce.OrdinalEncoder(cols=category_columns)

dataframe[category_columns] = ce_ord.fit_transform(x, y)

return dataframe

Example of Ordinal Encoder

Notice the categorical text columns have been transformed into numeric columns.

The description column can be used in the model, but it needs to go through a different transformation method. Instead, I’m going to use a function that selects the numeric columns from the data fram and ignores the description column.

get_numeric_data = FunctionTransformer(lambda x: x[numeric], validate=False)

Constructing the Pipeline

Pipelines are great because they enforce order during the transformation process making the workflow compact and easy to understand. This also can make the work easier to reproduce.

Remember that pipeline uses transformers, so we need to use the FunctionTransformer on our functions to make them compatible.

Creating Transformation Functions

Using FunctionTransformer, it is easy to make the functions used in the feature engineering and column selection process compatible with the pipeline.

get_year = FunctionTransformer(extract_year, validate=False)

get_word_count = FunctionTransformer(word_count, validate=False)

get_encoded_text = FunctionTransformer(category_encode, validate=False)

get_numeric_data = FunctionTransformer(lambda x: x[numeric], validate=False)

Because some of functions rely on an index value, I need to make a function that resets the index so the pipeline works properly after splitting the data into train and test sets.

def reset_index(dataframe):

dataframe = dataframe.reset_index(inplace = False)

return dataframe get_reset_index = FunctionTransformer(reset_index, validate=False)

Setting the Global Variables

The functions I created use global variables instead of hard-coded values so they are easier to reuse. Because of this, all of the global variables need to be set:

year_column = 'title'

word_count_column = 'description'

category_columns = ['country','province','title','winery']

target = 'price'

numeric= ['price', 'year', 'word_count', 'country', 'province', 'title', 'winery']

Selecting a Model

The pipeline can be used to select the model you want to use. To learn more about setting up a pipeline for model selection, I recommend checking out this Medium article by Rebecca Vickery. I am going to keep things simple and just use the Gradient Boost Regressor in Scikit Learn.

#create Gradient Boosting Regressor model

model = ensemble.GradientBoostingRegressor(

n_estimators = 100, #how many decision trees to build

learning_rate = 0.5, #controls rate at which additional decision trees influes overall prediction

max_depth = 6,

min_samples_split = 21,

min_samples_leaf = 19,

max_features = 0.9,

loss = 'huber'

)

Put it in the Pipeline

Once the model has been selected, the global variables have been set, and all the functions have been transformed and are compatible with the pipeline, it is time to put the pieces together:

pl = Pipeline(memory=None,

steps=[

('reset_index', get_reset_index),

('year', get_year),

('word_count', get_word_count),

('encode', get_encoded_text),

('selector', get_numeric_data),

('model', model)

], verbose=True)

Notice I set Verbose = True. Doing this prints the step has been complete. It makes the process more transparent for debugging.

Verbose = True output

Putting it all Together

To test the pipeline, use the Train Test Split to split the data and run it through the pipline:

features = df.drop(['rating'], axis=1) X = features

y = df['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y

, test_size = .3

#, stratify=y

)

pl.fit(X_train, y_train)

pl.score(X_test, y_test)

Score

The Complete Code

import numpy as np

import pandas as pd

import sqlite3

import category_encoders as ce

import re from sklearn.feature_selection import chi2, SelectKBest

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import HashingVectorizer from sklearn.pipeline import Pipeline, FeatureUnion

from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import MaxAbsScaler

from sklearn.preprocessing import FunctionTransformer conn = sqlite3.connect('db/wine_data.sqlite')

c = conn.cursor #create Dataframe

df = pd.read_sql(""select country \

,description \

,rating \

,price \

,province \

,title \

,winery from wine_data where variety = 'Chardonnay'"", conn)

#df.head(2) def extract_year(dataframe):

global year_column

years = dataframe[year_column]

#years.reset_index(inplace=False)

#years.fillna("""", inplace=True)

l = []

i = 0

for year in range(len(dataframe)):

temp = re.findall(r'\d+', years[i])

res = list(map(int, temp))

try:

if len(str(res[0])) == 4:

l.append(res[0])

elif len(str(res[0])) != 4:

l.append(0)

except:

l.append(0)

#print(res[0])

i+=1

dataframe['year'] = l return dataframe

#df = extract_year(df) def word_count(dataframe):

global word_count_column

dataframe['word_count'] = dataframe[word_count_column].apply(lambda word: len(str(word).split("" "")))

return dataframe

# df = word_count(df)

# df.head(3) # encoder = ce.JamesSteinEncoder(cols=[...]) --maybe (best score)

# encoder = ce.LeaveOneOutEncoder(cols=[...]) --maybe

# encoder = ce.MEstimateEncoder(cols=[...]) --maybe (good)

# encoder = ce.OrdinalEncoder(cols=[...]) --maybe

# encoder = ce.TargetEncoder(cols=[...]) --maybe year_column = 'title'

word_count_column = 'description'

category_columns = ['country','province','title','winery']

target = 'price'

combine_text = ['country','province','title','winery', 'description']

numeric= ['price', 'year', 'word_count','country','province','title','winery'] def category_encode(dataframe):

global category_columns

global category_target

x = dataframe[category_columns]

y = dataframe[target]

ce_ord = ce.OrdinalEncoder(cols=category_columns)

dataframe[category_columns] = ce_ord.fit_transform(x, y)

return dataframe # df = category_encode(df)

# df.head() get_year = FunctionTransformer(extract_year, validate=False) get_word_count = FunctionTransformer(word_count, validate=False) get_encoded_text = FunctionTransformer(category_encode, validate=False) get_numeric_data = FunctionTransformer(lambda x: x[numeric], validate=False) def reset_index(dataframe):

dataframe = dataframe.reset_index(inplace = False)

return dataframe get_reset_index = FunctionTransformer(reset_index, validate=False) from sklearn import ensemble

model = ensemble.GradientBoostingRegressor(

n_estimators = 100, #how many decision trees to build

learning_rate = 0.5, #controls rate at which additional decision trees influes overall prediction

max_depth = 6,

min_samples_split = 21,

min_samples_leaf = 19,

max_features = 0.9,

loss = 'huber'

) pl = Pipeline(memory=None,

steps=[

('reset_index', get_reset_index),

('year', get_year),

('word_count', get_word_count),

('encode', get_encoded_text),

('selector', get_numeric_data),

('model', model)

], verbose=False) features = df.drop(['rating'], axis=1) X = features

y = df['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y

, test_size = .3

#, stratify=y

)

pl.fit(X_train, y_train) pl.score(X_test, y_test)

Github","['import', 'predict', 'column', 'sklearn', 'maybe', 'functions', 'chardonnay', 'functiontransformer', 'ratings', 'data', 'model', 'encoder', 'global', 'dataframe', 'using', 'pipeline']","This is an in-depth review of the process I went through while creating my pipeline and predictive model.
Using Scikit Learn’s Function Transformer, I can use the functions in the pipeline to transform the dataframe.
I recommend trying these with the model pipeline to see how the different strategies impact the accuracy of the model.
# encoder = ce.JamesSteinEncoder(cols=[...]) --maybe# encoder = ce.LeaveOneOutEncoder(cols=[...]) --maybe# encoder = ce.MEstimateEncoder(cols=[...]) --maybe# encoder = ce.OrdinalEncoder(cols=[...]) --maybe# encoder = ce.TargetEncoder(cols=[...]) --maybe def category_encode(dataframe):global category_columnsglobal category_targetx = dataframe[category_columns]y = dataframe[target]ce_ord = ce.OrdinalEncoder(cols=category_columns)dataframe[category_columns] = ce_ord.fit_transform(x, y)return dataframeExample of Ordinal EncoderNotice the categorical text columns have been transformed into numeric columns.
Remember that pipeline uses transformers, so we need to use the FunctionTransformer on our functions to make them compatible.",en,['Eric Kleppen'],2019-11-29 15:00:09.308000+00:00,"{'Coding', 'Data Science', 'Python', 'Machine Learning', 'Programming'}","{'https://miro.medium.com/max/444/1*ZKJ_ShO0CAXTOt6w2rZI1w.png', 'https://miro.medium.com/max/1928/1*crxfZTS4n3Tid7wln8LE4w.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1982/1*uZV3hh3NgUiL3NiJJRmd_A.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1142/1*jD6g23QE3qHRA4MhwL4fFg.png', 'https://miro.medium.com/max/1478/1*nyBSbAyfQRbRajr7g_Sxaw.png', 'https://miro.medium.com/max/60/1*nyBSbAyfQRbRajr7g_Sxaw.png?q=20', 'https://miro.medium.com/max/1236/1*957CF4ztyE2eVId5IbEiow.png', 'https://miro.medium.com/max/60/1*jD6g23QE3qHRA4MhwL4fFg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*ZKJ_ShO0CAXTOt6w2rZI1w.png?q=20', 'https://miro.medium.com/max/618/1*957CF4ztyE2eVId5IbEiow.png', 'https://miro.medium.com/max/60/1*uZV3hh3NgUiL3NiJJRmd_A.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*AqUDXtSW0H3VnHzNe-m4Hw.png', 'https://miro.medium.com/fit/c/96/96/2*AqUDXtSW0H3VnHzNe-m4Hw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*crxfZTS4n3Tid7wln8LE4w.png?q=20', 'https://miro.medium.com/max/60/1*957CF4ztyE2eVId5IbEiow.png?q=20'}",2020-03-05 00:11:01.295457,7.520672082901001
https://towardsdatascience.com/this-will-make-you-a-command-line-ninja-93a51cdb16b1,This Will Make You a Command-Line Ninja,"8. Using the find command

There’s a very powerful command that can save you lots of headaches when used in the right places — it’s called find .

Find, in its base, walks the file tree starting from the provided path. It then prints each directory and file including its path relative to the current working directory.

Just try it, enter the following and see what output you get:

$ find .

I’m currently in a directory created for this article, it’s contents according to find are:

.

./images

./images/cover.jpg

./scripts

./scripts/loop.sh

./scripts/arguments.sh

./scripts/words.txt

The first dot is simply the current directory. If you only want to list files, use:

$ find . -type f

If you only want to list directories, use:

find . -type d

Expressions

These are just the basics. Find also takes expressions, which further narrows down what files it should return. A useful expression that I use often is -mtime . The following command returns all files that were modified more than 5 minutes ago:

$ find . -type f -mtime +5m

The m is a modifier, your options are:

s: second

second m: minute (60 seconds)

minute (60 seconds) h: hour (60 minutes)

hour (60 minutes) d: day (24 hours)

day (24 hours) w: week (7 days)

The plus sign is important as well:

a preceding plus sign means “more than n’’

a preceding minus sign means “less than n’’

neither means “exactly n’’

Another very useful expression is -name , which will filter the results based on the file name. For the name, you can use a shell pattern. So the command find . -name ""*.txt"" will only return files with the .txt extension.

Using the results

With our new ninja scripting powers, we can now use find to return a bunch of files, feed them to our for loop and do some awesome stuff with them, right?

True, but since it’s pretty obvious you want to perform some actions on the returned files, find has us covered already. No scripting necessary.

Say you want to delete the files that match your criteria, you can simply add -delete to your find-command. This is obviously a bit dangerous, so before using this flag, always check the output of the find command without it first.

What gives us much more flexibility, is the -exec flag. It allows us to execute any command. An example that you may have seen before is this one:

$ find /path/to/files* -mtime +5m -exec rm {} +

Let’s break it down:

find all files in the given path that are modified more than 5 minutes ago

execute the rm command (which deletes the files)

{} + is replaced with as many pathnames as possible for each invocation of the given utility (in our case ls)

It’s a very efficient way of removing lots of files, like thousands or even millions.

A slight alternative to this command is:

$ find /path/to/files* -mtime +5m -exec rm {} \;

The difference is that rm is now executed for every single file, instead of as many files per batch as your system allows. Which version you use depends on the command you’re executing on the results.

If you can use the one with the plus sign, it’s a lot faster! That’s because the command is executed on many files at once, saving your OS lots of CPU cycles that are needed to open, start, run, and exit a program. However, some commands take just one file at a time and for those, the second version comes to rescue.","['mtime', 'file', 'rm', '60', 'files', 'type', 'sign', 'ninja', 'command', 'return', 'plus', 'commandline']","It then prints each directory and file including its path relative to the current working directory.
-name ""*.txt"" will only return files with the .txt extension.
Using the resultsWith our new ninja scripting powers, we can now use find to return a bunch of files, feed them to our for loop and do some awesome stuff with them, right?
If you can use the one with the plus sign, it’s a lot faster!
However, some commands take just one file at a time and for those, the second version comes to rescue.",en,['Erik-Jan Van Baaren'],2020-02-27 15:36:50.080000+00:00,"{'Data Science', 'Command Line', 'Programming', 'Linux', 'Bash'}","{'https://miro.medium.com/max/60/0*zoqx_HaUhMH0tI_P?q=20', 'https://miro.medium.com/fit/c/96/96/0*cu4Pztlg9excQP7i.jpg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/10368/0*zoqx_HaUhMH0tI_P', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/0*zoqx_HaUhMH0tI_P', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*U-ay0n_D0a0x6inGovS16w.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/0*cu4Pztlg9excQP7i.jpg', 'https://miro.medium.com/max/844/1*U-ay0n_D0a0x6inGovS16w.png'}",2020-03-05 00:11:08.612754,7.316297769546509
https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db,CatBoost vs. Light GBM vs. XGBoost,"I recently participated in this Kaggle competition (WIDS Datathon by Stanford) where I was able to land up in Top 10 using various boosting algorithms. Since then, I have been very curious about the fine workings of each model including parameter tuning, pros and cons and hence decided to write this blog. Despite the recent re-emergence and popularity of neural networks, I am focusing on boosting algorithms because they are still more useful in the regime of limited training data, little training time and little expertise for parameter tuning.

Since XGBoost (often called GBM Killer) has been in the machine learning world for a longer time now with lots of articles dedicated to it, this post will focus more on CatBoost & LGBM. Below are the topics we will cover-

Structural Differences

Treatment of categorical variables by each algorithm

Understanding Parameters

Implementation on Dataset

Performance of each algorithm

Structural Differences in LightGBM & XGBoost

LightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value while XGBoost uses pre-sorted algorithm & Histogram-based algorithm for computing the best split. Here instances mean observations/samples.

First, let us understand how pre-sorting splitting works-

For each node, enumerate over all features

For each feature, sort the instances by feature value

Use a linear scan to decide the best split along that feature basis information gain

Take the best split solution along all the features

In simple terms, Histogram-based algorithm splits all the data points for a feature into discrete bins and uses these bins to find the split value of histogram. While, it is efficient than pre-sorted algorithm in training speed which enumerates all possible split points on the pre-sorted feature values, it is still behind GOSS in terms of speed.

So what makes this GOSS method efficient?

In AdaBoost, the sample weight serves as a good indicator for the importance of samples. However, in Gradient Boosting Decision Tree (GBDT), there are no native sample weights, and thus the sampling methods proposed for AdaBoost cannot be directly applied. Here comes gradient-based sampling.

Gradient represents the slope of the tangent of the loss function, so logically if gradient of data points are large in some sense, these points are important for finding the optimal split point as they have higher error

GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients. For example, let’s say I have 500K rows of data where 10k rows have higher gradients. So my algorithm will choose (10k rows of higher gradient+ x% of remaining 490k rows chosen randomly). Assuming x is 10%, total rows selected are 59k out of 500K on the basis of which split value if found.

The basic assumption taken here is that samples with training instances with small gradients have smaller training error and it is already well-trained.

In order to keep the same data distribution, when computing the information gain, GOSS introduces a constant multiplier for the data instances with small gradients. Thus, GOSS achieves a good balance between reducing the number of data instances and keeping the accuracy for learned decision trees.

Leaf with higher gradient/error is used for growing further in LGBM

How each model treats Categorical Variables?

CatBoost

CatBoost has the flexibility of giving indices of categorical columns so that it can be encoded as one-hot encoding using one_hot_max_size (Use one-hot encoding for all features with number of different values less than or equal to the given parameter value).

If you don’t pass any anything in cat_features argument, CatBoost will treat all the columns as numerical variables.

Note: If a column having string values is not provided in the cat_features, CatBoost throws an error. Also, a column having default int type will be treated as numeric by default, one has to specify it in cat_features to make the algorithm treat it as categorical.

For remaining categorical columns which have unique number of categories greater than one_hot_max_size, CatBoost uses an efficient method of encoding which is similar to mean encoding but reduces overfitting. The process goes like this —

1. Permuting the set of input observations in a random order. Multiple random permutations are generated

2. Converting the label value from a floating point or category to an integer

3. All categorical feature values are transformed to numeric values using the following formula:

Where, CountInClass is how many times the label value was equal to “1” for objects with the current categorical feature value

Prior is the preliminary value for the numerator. It is determined by the starting parameters. TotalCount is the total number of objects (up to the current one) that have a categorical feature value matching the current one.

Mathematically, this can be represented using below equation:

LightGBM

Similar to CatBoost, LightGBM can also handle categorical features by taking the input of feature names. It does not convert to one-hot coding, and is much faster than one-hot coding. LGBM uses a special algorithm to find the split value of categorical features [Link].

Note: You should convert your categorical features to int type before you construct Dataset for LGBM. It does not accept string values even if you passes it through categorical_feature parameter.

XGBoost

Unlike CatBoost or LGBM, XGBoost cannot handle categorical features by itself, it only accepts numerical values similar to Random Forest. Therefore one has to perform various encodings like label encoding, mean encoding or one-hot encoding before supplying categorical data to XGBoost.

Similarity in Hyperparameters

All these models have lots of parameters to tune but we will cover only the important ones. Below is the list of these parameters according to their function and their counterparts across different models.","['algorithm', 'catboost', 'feature', 'xgboost', 'encoding', 'gbm', 'vs', 'light', 'categorical', 'data', 'instances', 'values', 'value', 'split']","Assuming x is 10%, total rows selected are 59k out of 500K on the basis of which split value if found.
Thus, GOSS achieves a good balance between reducing the number of data instances and keeping the accuracy for learned decision trees.
TotalCount is the total number of objects (up to the current one) that have a categorical feature value matching the current one.
XGBoostUnlike CatBoost or LGBM, XGBoost cannot handle categorical features by itself, it only accepts numerical values similar to Random Forest.
Therefore one has to perform various encodings like label encoding, mean encoding or one-hot encoding before supplying categorical data to XGBoost.",en,['Alvira Swalin'],2019-06-11 22:08:27.695000+00:00,"{'Xgboost', 'Boosting', 'Data Science', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/3384/1*w05Hg2QZ5ioDi2OXdCCMiw.png', 'https://miro.medium.com/max/60/1*pDN6jzHnCYBqlW4_RoT0Gg.png?q=20', 'https://miro.medium.com/max/60/1*B9wDla365AI9HjZEcUFq6A.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*fR5nLi61SkS031Spb3qgLg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*i0CA9ho0WArOj-0UdpuKGQ.png?q=20', 'https://miro.medium.com/max/2580/1*fR5nLi61SkS031Spb3qgLg.png', 'https://miro.medium.com/max/60/1*gw_AZFeu-Q0C95W6QWFJ2g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*czLBAIH3EyO7s9kTPMWPDQ.jpeg', 'https://miro.medium.com/max/60/1*A0b_ahXOrrijazzJengwYw.png?q=20', 'https://miro.medium.com/max/2668/1*09uNKZvIG2rhSpjTTnrDvw.png', 'https://miro.medium.com/max/60/1*w05Hg2QZ5ioDi2OXdCCMiw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2548/1*B9wDla365AI9HjZEcUFq6A.jpeg', 'https://miro.medium.com/max/3076/1*i0CA9ho0WArOj-0UdpuKGQ.png', 'https://miro.medium.com/max/1538/1*Zo9K6RiHvBdjYxJKLpsyaA.png', 'https://miro.medium.com/max/2464/1*pDN6jzHnCYBqlW4_RoT0Gg.png', 'https://miro.medium.com/max/1200/1*B9wDla365AI9HjZEcUFq6A.jpeg', 'https://miro.medium.com/fit/c/96/96/1*czLBAIH3EyO7s9kTPMWPDQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*Zo9K6RiHvBdjYxJKLpsyaA.png?q=20', 'https://miro.medium.com/max/1256/1*gw_AZFeu-Q0C95W6QWFJ2g.png', 'https://miro.medium.com/max/3400/1*A0b_ahXOrrijazzJengwYw.png', 'https://miro.medium.com/max/60/1*09uNKZvIG2rhSpjTTnrDvw.png?q=20'}",2020-03-05 00:11:15.944237,7.331483602523804
https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621,Label Encoder vs. One Hot Encoder in Machine Learning,"Label Encoding

To begin with, you can find the SciKit Learn documentation for Label Encoder here. Now, let’s consider the following data:

Data from SuperDataScience

In this example, the first column is the country column, which is all text. As you might know by now, we can’t have text in our data if we’re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.

And to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class. So all we have to do, to label encode the first column, is import the LabelEncoder class from the sklearn library, fit and transform the first column of the data, and then replace the existing text data with the new encoded data. Let’s have a look at the code.

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()

x[:, 0] = labelencoder.fit_transform(x[:, 0])

We’ve assumed that the data is in a variable called ‘x’. After running this piece of code, if you check the value of x, you’ll see that the three countries in the first column have been replaced by the numbers 0, 1, and 2.

That’s all label encoding is about. But depending on the data, label encoding introduces a new problem. For example, we have encoded a set of country names into numerical data. This is actually categorical data and there is no relation, of any kind, between the rows.

The problem here is, since there are different numbers in the same column, the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isn’t the case at all. To overcome this problem, we use One Hot Encoder.","['machine', 'column', 'hot', 'learning', 'run', 'vs', 'data', 'encoder', 'model', 'x', 'problem', 'label', 'kind', 'text']","Label EncodingTo begin with, you can find the SciKit Learn documentation for Label Encoder here.
Now, let’s consider the following data:Data from SuperDataScienceIn this example, the first column is the country column, which is all text.
So before we can run a model, we need to make this data ready for the model.
And to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.
But depending on the data, label encoding introduces a new problem.",en,['Sunny Srinidhi'],2020-01-09 09:15:45.090000+00:00,"{'Data Science', 'Scikit Learn', 'Python', 'Machine Learning', 'Programming', 'Technology'}","{'https://miro.medium.com/max/60/1*zS-7qHEGhZ7tX6aamc3RpQ.png?q=20', 'https://miro.medium.com/max/1038/1*HMAPmcCtGwZSjSvgS4MKGw.png', 'https://miro.medium.com/max/60/1*eWtGdqsEXCnX_ZiVw2Ba0g.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*F5rFz8-JzlrEpFpP.jpg', 'https://miro.medium.com/fit/c/80/80/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*HMAPmcCtGwZSjSvgS4MKGw.png?q=20', 'https://miro.medium.com/max/634/1*zS-7qHEGhZ7tX6aamc3RpQ.png', 'https://miro.medium.com/fit/c/80/80/0*F5rFz8-JzlrEpFpP.jpg', 'https://miro.medium.com/fit/c/96/96/0*F5rFz8-JzlrEpFpP.jpg', 'https://miro.medium.com/max/800/1*eWtGdqsEXCnX_ZiVw2Ba0g.png', 'https://miro.medium.com/max/400/1*eWtGdqsEXCnX_ZiVw2Ba0g.png'}",2020-03-05 00:11:16.615574,0.6713371276855469
https://medium.com/datadriveninvestor/improve-your-classification-models-using-mean-target-encoding-a3d573df31e8,Improve your classification models using Mean /Target Encoding,"Encoding is a pre-processing step where you convert text, sequences or even images and audios into a machine understandable form (Typically using integers or matrices)

If you have been solving machine learning problems on HackerEarth or kaggle then you would have definitely come across problems of this type where some columns need to be encoded

City and State columns need to be encoded

The most common type of encoder is the label encoder, where each unique Label is assigned an integer and we can easily implement it once we know the number of unique labels. eg: In the above example, the number of unique labels is the number of cities.

>>> from sklearn import preprocessing

>>> le = preprocessing.LabelEncoder()

>>> le.fit([""Bristol"", ""Bristol"", ""GeorgeTown"", ""GeorgeTown""])

LabelEncoder()

>>> le.transform([""GeorgeTown"", ""GeorgeTown"", ""Bristol""])

array([2, 2, 1])

Now let’s talk about mean Encoding,

In Mean Encoding we take the number of labels into account along with the target variable to encode the labels into machine comprehensible values

Let us consider the above table (A simple binary classification) where we have two labels…Moscow and Tver. In normal label encoding we would assign Moscow as 1 and Tver as 2 but in mean encoding

Encoding for Moscow = [Number of true targets under the label Moscow/ Total Number of targets under the label Moscow ] which is 2/5 = 0.4 and similarly Encoding for Tver = 3/4 ~ 0.8(Approx)

Instead of finding the mean of the targets, we can also focus on median and other statistical correlations….These are broadly called target encodings

But why are these encodings Better ?

Mean encoding can embody the target in the label whereas label encoding has no correlation with the target

In case of large number of features, mean encoding could prove to be a much simpler alternative

A histogram of predictions using label & mean encoding show that mean encoding tend to group the classes together whereas the grouping is random in case of label encoding

Even though it looks like mean encoding is Superman…… it’s kryptonite is overfitting. The fact that we use target classes to encode for our training labels may leak data about the predictions causing the encoding to become biased. Well we can avoid this by Regularizing

Several Kaggle Competitors use mean encoding with regularization to predict much better and rise through ranks in the leaderboard.

Github link:

References:","['tver', 'models', 'improve', 'number', 'encoding', 'targets', 'target', 'mean', 'labels', 'moscow', 'unique', 'classification', 'label', 'using']","eg: In the above example, the number of unique labels is the number of cities.
>>> from sklearn import preprocessing>>> le = preprocessing.LabelEncoder()>>> le.fit([""Bristol"", ""Bristol"", ""GeorgeTown"", ""GeorgeTown""])LabelEncoder()>>> le.transform([""GeorgeTown"", ""GeorgeTown"", ""Bristol""])array([2, 2, 1])Now let’s talk about mean Encoding,In Mean Encoding we take the number of labels into account along with the target variable to encode the labels into machine comprehensible valuesLet us consider the above table (A simple binary classification) where we have two labels…Moscow and Tver.
Mean encoding can embody the target in the label whereas label encoding has no correlation with the targetIn case of large number of features, mean encoding could prove to be a much simpler alternativeA histogram of predictions using label & mean encoding show that mean encoding tend to group the classes together whereas the grouping is random in case of label encodingEven though it looks like mean encoding is Superman…… it’s kryptonite is overfitting.
The fact that we use target classes to encode for our training labels may leak data about the predictions causing the encoding to become biased.
Well we can avoid this by RegularizingSeveral Kaggle Competitors use mean encoding with regularization to predict much better and rise through ranks in the leaderboard.",en,[],2018-06-26 19:22:41.976000+00:00,"{'Data Science', 'Encoding', 'Machine Learning', 'Classification', 'Kaggle'}","{'https://miro.medium.com/max/1196/1*qwooYKx8rU6h1VDnUCgsNg.png', 'https://miro.medium.com/max/792/1*zJ-TwjLVZLYVXQb4pesebw.png', 'https://miro.medium.com/fit/c/80/80/1*-pV8MMqetli5oQhovupPrg.jpeg', 'https://miro.medium.com/max/60/1*qwooYKx8rU6h1VDnUCgsNg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*8v6dfENqVLsuHu9KtDCj6Q.jpeg', 'https://miro.medium.com/fit/c/80/80/0*ewcxmwIHC3RQwRqB.', 'https://miro.medium.com/max/1048/1*a8gQ-TBxuCWXAnDhxzIRiw.png', 'https://miro.medium.com/max/598/1*qwooYKx8rU6h1VDnUCgsNg.png', 'https://miro.medium.com/fit/c/96/96/1*8v6dfENqVLsuHu9KtDCj6Q.jpeg', 'https://miro.medium.com/max/60/1*zJ-TwjLVZLYVXQb4pesebw.png?q=20', 'https://miro.medium.com/max/60/1*a8gQ-TBxuCWXAnDhxzIRiw.png?q=20', 'https://miro.medium.com/max/432/1*fr3BFYQ7ZiodLBbY1emz9g.png', 'https://miro.medium.com/fit/c/160/160/1*2mBCfRUpdSYRuf9EKnhTDQ.png', 'https://miro.medium.com/fit/c/80/80/0*030HOaJEWhVa-6K_.'}",2020-03-05 00:11:18.152241,1.5366666316986084
https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd,The magic of LSTM neural networks,"https://www.captionbot.ai/How they work

LSTM networks manage to keep contextual information of inputs by integrating a loop that allows information to flow from one step to the next. These loops make recurrent neural networks seem magical. But if we think about it for a second, as you are reading this post, you are understanding each word based on your understanding of the previous words. You don’t throw everything away and start thinking from scratch at each word. Similarly, LSTM predictions are always conditioned by the past experience of the network’s inputs.

LSTM loop unrolled

On the other hand, the more time passes, the less likely it becomes that the next output depends on a very old input. This time dependency distance itself is as well a contextual information to be learned. LSTM networks manage this by learning when to remember and when to forget, through their forget gate weights. In a simple way, if the forget gate is just a multiplicative factor of 0.9, within 10 time steps this factor becomes: 0.9¹⁰=0.348 (or 65% of information forgotten), and within 30 steps -> 0.04 (96% forgotten).

LSTM internal wiring showing the forget gate

In this post, I won’t go more than this about the technical details of LSTM. This blog post instead, explains very nicely their mechanics. Instead I will present here a compilation of the different applications of LSTM.

Examples of LSTM usage

Text Generation

Generating a text, like this one, can be converted to an LSTM task where each letter is generated taking into account all the previously generated letters. In fact, teaching an LSTM network the alphabet can be the first programming example to learn how to make LSTM learn something. Andrej Karpathy, shows different examples of text generation by LSTM depending on the training set you feed them. These are some of the automatically generated text:

Shakespeare:

PANDARUS:

Alas, I think he shall be come approached and the day

When little srain would be attain’d into being never fed,

And who is but a chain and subjects of his death,

I should not sleep. Second Senator:

They are away this miseries, produced upon my soul,

Breaking and strongly should be buried, when I perish

The earth and thoughts of many states.

Wikipedia:

Naturalism and decision for the majority of Arab countries’ capitalide was grounded

by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated

with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal

in the [[Protestant Immineners]], which could be said to be directly in Cantonese

Communication, which followed a ceremony and set inspired prison, training. The

emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom

of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known

in western [[Scotland]], near Italy to the conquest of India with the conflict.

XML:

<revision>

<id>15900676</id>

<timestamp>2002–08–03T18:14:12Z</timestamp>

<contributor>

<username>Paris</username>

<id>23</id>

</contributor>

<minor />

<comment>Automated conversion</comment>

</revision>

Obama:

SEED: War on terror

Good everybody. Thank you very much. God bless the United States of America, and has already began with the world’s gathering their health insurance.

It’s about hard-earned for our efforts that are not continued.

We are all the assumptionion to the streets of the Americas that we are still for everybody and destruction.

We are doing a lot of this.

I know that someone would be prefered to their children to take a million insurance company. We’re watching their people and continued to find ourselves with Republicans — to give up on these challenges and despite the challenges of our country. In the last two years, we must recognise that our borders have access from the world. We’re continuing that this day of things that the United States will clean up it´s allies and prosperity to stand up enough to be a sanctions that we made their faith, and for the country of the Internet to Osama bin Laden.

Thank you. God bless you. Good morning, everybody. And May God loss man. Thank you very much. Thank you very much, everybody.

Handwriting recognition

This is an animation from Alex Graves, showing an LSTM network performing in live a handwriting recognition:

Row 1: Shows the letters that are recognised ( outputs of the network)

of the network) Row 2: Shows the states of the memory cells (Notice how they reset when a character is recognised)

Row 3: Shows the writing as it’s being analysed by the LSTM ( inputs of the network)

of the network) Row 4: Shows the gradient back-propagated to the inputs from the most active characters. This reflects the forget effect.

Handwriting generation

As an inverted experiment, here are some handwriting generated by LSTM.

For a live demo, and to automatically generate a LSTM-’hand’writing text yourself, visit this page.

Music generation

Since music, just like text, is a sequence of notes (instead of characters), it can be generated as well by LSTM by taking into account the previously played notes (or combinations of notes). Here you can find an interesting explanation of how to train LSTM on midi files. Otherwise, you can enjoy the following generated music (from a classical music training set):

Language Translation

Language translation can be seen as a sequence-to-sequence mapping. A group of researchers, in collaboration with Nvidia published details on how to tame LSTM for such task (part1, part2, part3).

In a nutshell, they created a neural net with an encoder to compress the text to a higher abstract vectorial representation and a decoder to decode it back to the target language.

Machine translation encoder/decoder architecture

English to french translation by NVidia

Image captioning

Finally, the most impressive use of LSTM networks is to generate from an input image, a text caption describing the contents of the image. Microsoft research is progressing a lot in this area. Here are some sample demos of their results:

You can try their online demo yourself here: https://www.captionbot.ai

Have fun!","['network', 'information', 'shows', 'magic', 'translation', 'lstm', 'neural', 'networks', 'generated', 'training', 'forget', 'text']","Similarly, LSTM predictions are always conditioned by the past experience of the network’s inputs.
LSTM networks manage this by learning when to remember and when to forget, through their forget gate weights.
LSTM internal wiring showing the forget gateIn this post, I won’t go more than this about the technical details of LSTM.
In fact, teaching an LSTM network the alphabet can be the first programming example to learn how to make LSTM learn something.
Machine translation encoder/decoder architectureEnglish to french translation by NVidiaImage captioningFinally, the most impressive use of LSTM networks is to generate from an input image, a text caption describing the contents of the image.",en,['Assaad Moawad'],2019-07-19 16:18:23.374000+00:00,"{'Neural Networks', 'Big Data Analytics', 'Machine Learning', 'Lstm', 'Magic'}","{'https://miro.medium.com/max/60/1*68FuTsfvDH-sOVTpBv5BKw.jpeg?q=20', 'https://miro.medium.com/max/1204/1*7-3Mb2E96ul52R3uDxZlXg.png', 'https://miro.medium.com/fit/c/160/160/1*pPoZZfAIf7o7nJ1MLrzeXA.jpeg', 'https://miro.medium.com/max/2098/1*68FuTsfvDH-sOVTpBv5BKw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*8z7-8-uSz7J7SEYWoI9ZOA.jpeg', 'https://miro.medium.com/max/5412/1*NKhwsOYNUT5xU7Pyf6Znhg.png', 'https://miro.medium.com/max/516/1*uNDIdhlXi1t0vWvROtslng.jpeg', 'https://miro.medium.com/max/600/1*MxcWRNyZLPBSgXQhieTyJg.png', 'https://miro.medium.com/max/1248/1*SiOIKWyN-D3viccI8RmaNg.png', 'https://miro.medium.com/max/60/1*7-3Mb2E96ul52R3uDxZlXg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*pPoZZfAIf7o7nJ1MLrzeXA.jpeg', 'https://miro.medium.com/max/714/1*hrbOCSHaCFGrQkc8A1i3Sg.jpeg', 'https://miro.medium.com/max/60/1*SiOIKWyN-D3viccI8RmaNg.png?q=20', 'https://miro.medium.com/max/60/1*JoG7WmQOSgjscLtOhB_H5A.png?q=20', 'https://miro.medium.com/max/60/1*NKhwsOYNUT5xU7Pyf6Znhg.png?q=20', 'https://miro.medium.com/max/60/1*r2fC_4ahhW-CEiCXXgHv4A.png?q=20', 'https://miro.medium.com/max/60/1*uNDIdhlXi1t0vWvROtslng.jpeg?q=20', 'https://miro.medium.com/max/1049/1*68FuTsfvDH-sOVTpBv5BKw.jpeg', 'https://miro.medium.com/max/72/1*Gj29FMC9JLL0rasr-HP4MQ.png', 'https://miro.medium.com/max/60/1*hrbOCSHaCFGrQkc8A1i3Sg.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*pPoZZfAIf7o7nJ1MLrzeXA.jpeg', 'https://miro.medium.com/max/60/1*6YwqrScyczEaG0l05G4-_A.jpeg?q=20', 'https://miro.medium.com/max/60/1*MxcWRNyZLPBSgXQhieTyJg.png?q=20', 'https://miro.medium.com/max/1476/1*6YwqrScyczEaG0l05G4-_A.jpeg', 'https://miro.medium.com/max/600/1*JoG7WmQOSgjscLtOhB_H5A.png', 'https://miro.medium.com/max/1182/1*r2fC_4ahhW-CEiCXXgHv4A.png'}",2020-03-05 00:11:20.179881,2.026235342025757
https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47,Understanding LSTM and its quick implementation in keras for sentiment analysis.,"Long Short Term Memory networks, usually called “LSTMs” , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN.

Recurrent Neural Networks (RNN)

Being human, when we watch a movie, we don’t think from scratch every time while understanding any event. We rely on the recent experiences happening in the movie and learn from them. But, a conventional neural network is unable to learn from the previous events because the information does not pass from one step to the next. On contrary, RNN learns information from immediate previous step.

For example, there is a scene in a movie where a person is in a basketball court. We will improvise the basketball activities in the future frames: an image of someone running and jumping probably be labeled as playing basketball, and an image of someone sitting and watching is probably a spectator watching the game.

A typical RNN (Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

A typical RNN looks like above-where X(t) is input, h(t) is output and A is the neural network which gains information from the previous step in a loop. The output of one unit goes into the next one and the information is passed.

But, sometimes we don’t need our network to learn only from immediate past information. Suppose we want to predict the blank word in the text ‘ David, a 36-year old man lives in San Francisco. He has a female friend Maria. Maria works as a cook in a famous restaurant in New York whom he met recently in a school alumni meet. Maria told him that she always had a passion for _________ . Here, we want our network to learn from dependency ‘cook’ to predict ‘cooking. There is a gap between the information what we want to predict and from where we want it to get predicted . This is called long-term dependency. We can say that anything larger than trigram as a long term dependency. Unfortunately, RNN does not work practically in this situation.

Why RNN does not work practically

During the training of RNN, as the information goes in loop again and again which results in very large updates to neural network model weights. This is due to the accumulation of error gradients during an update and hence, results in an unstable network. At an extreme, the values of weights can become so large as to overflow and result in NaN values.The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than 1 or vanishing occurs if the values are less than 1.

Long Short Term Memory

The above drawback of RNN pushed the scientists to develop and invent a new variant of the RNN model, called Long Short Term Memory. LSTM can solve this problem, because it uses gates to control the memorizing process.

Let’s understand the architecture of LSTM and compare it with that of RNN:

A LSTM unit (Source : http://colah.github.io/posts/2015-08-Understanding-LSTMs)

The symbols used here have following meaning:

a) X : Scaling of information

b)+ : Adding information

c) σ : Sigmoid layer

d) tanh: tanh layer

e) h(t-1) : Output of last LSTM unit

f) c(t-1) : Memory from last LSTM unit

g) X(t) : Current input

h) c(t) : New updated memory

i) h(t) : Current output

Why tanh?

To overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero. tanh is a suitable function with the above property.

Why Sigmoid?

As Sigmoid can output 0 or 1, it can be used to forget or remember the information.

Information passes through many such LSTM units.There are three main components of an LSTM unit which are labeled in the diagram:

LSTM has a special architecture which enables it to forget the unnecessary information .The sigmoid layer takes the input X(t) and h(t-1) and decides which parts from old output should be removed (by outputting a 0). In our example, when the input is ‘He has a female friend Maria’, the gender of ‘David’ can be forgotten because the subject has changed to ‘Maria’. This gate is called forget gate f(t). The output of this gate is f(t)*c(t-1). The next step is to decide and store information from the new input X(t) in the cell state. A Sigmoid layer decides which of the new information should be updated or ignored. A tanh layer creates a vector of all the possible values from the new input. These two are multiplied to update the new cell sate. This new memory is then added to old memory c(t-1) to give c(t). In our example, for the new input ‘ He has a female friend Maria’, the gender of Maria will be updated. When the input is ‘Maria works as a cook in a famous restaurant in New York whom he met recently in a school alumni meet’, the words like ‘famous’, ‘school alumni meet’ can be ignored and words like ‘cook, ‘restaurant’ and ‘New York’ will be updated. Finally, we need to decide what we’re going to output. A sigmoid layer decides which parts of the cell state we are going to output. Then, we put the cell state through a tanh generating all the possible values and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. In our example, we want to predict the blank word, our model knows that it is a noun related to ‘cook’ from its memory, it can easily answer it as ‘cooking’. Our model does not learn this answer from the immediate dependency, rather it learnt it from long term dependency.

We just saw that there is a big difference in the architecture of a typical RNN and a LSTM. In LSTM, our model learns what information to store in long term memory and what to get rid of.

Quick implementation of LSTM for Sentimental Analysis

Here, I used LSTM on the reviews data from Yelp open dataset for sentiment analysis using keras.

This is what my data looks like.

Dataset

I used Tokenizer to vectorize the text and convert it into sequence of integers after restricting the tokenizer to use only top most common 2500 words. I used pad_sequences to convert the sequences into 2-D numpy array.

Then, I built my LSTM network.There are a few hyper parameters:

embed_dim : The embedding layer encodes the input sequence

into a sequence of dense vectors of dimension embed_dim. lstm_out : The LSTM transforms the vector sequence into a single vector of size lstm_out, containing information about the entire sequence.

The other hyper parameters like dropout, batch_size are similar to that of CNN.

I used softmax as activation function.

LSTM network

Now, I fit my model on training set and check the accuracy on validation set.

I got a validation accuracy of 86% in just one epoch while running on a small dataset which includes all the businesses.

Future Work:

We can filter the specific businesses like restaurants and then use LSTM for sentiment analysis. We can use much larger dataset with more epochs to increase the accuracy. More hidden dense layers can be used to improve the accuracy. We can tune other hyper parameters as well.

Conclusion

LSTM outperforms the other models when we want our model to learn from long term dependencies. LSTM’s ability to forget, remember and update the information pushes it one step ahead of RNNs.

References and other useful resources:

4. Exploring LSTMs

5. Research paper on LSTM","['implementation', 'analysis', 'information', 'network', 'sentiment', 'quick', 'used', 'lstm', 'maria', 'keras', 'model', 'output', 'rnn', 'input', 'understanding', 'term']","Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN).
Long Short Term MemoryThe above drawback of RNN pushed the scientists to develop and invent a new variant of the RNN model, called Long Short Term Memory.
In LSTM, our model learns what information to store in long term memory and what to get rid of.
Quick implementation of LSTM for Sentimental AnalysisHere, I used LSTM on the reviews data from Yelp open dataset for sentiment analysis using keras.
Future Work:We can filter the specific businesses like restaurants and then use LSTM for sentiment analysis.",en,['Nimesh Sinha'],2018-03-03 04:41:05.571000+00:00,"{'Neural Networks', 'Deep Learning', 'Machine Learning', 'Towards Data Science', 'Lstm'}","{'https://miro.medium.com/max/38/1*DvlB9rtndUHwtri4E2P-bg.png?q=20', 'https://miro.medium.com/max/56/1*Niu_c_FhGtLuHjrStkB_4Q.png?q=20', 'https://miro.medium.com/max/634/1*GsT2fwE_39fgmtC735924Q.png', 'https://miro.medium.com/max/42/1*GsT2fwE_39fgmtC735924Q.png?q=20', 'https://miro.medium.com/max/2242/1*voNageVB1gI8Dfrmr-U3Ew.png', 'https://miro.medium.com/max/60/1*pOZ3Dcc27oPlbt6dE-FbSg.png?q=20', 'https://miro.medium.com/max/60/1*voNageVB1gI8Dfrmr-U3Ew.png?q=20', 'https://miro.medium.com/max/1488/1*xTKE0g6XNMLM8IQ4aFdP0w.png', 'https://miro.medium.com/fit/c/160/160/1*2lsrnSkXFL6celMtqlOlWA.jpeg', 'https://miro.medium.com/max/60/1*xTKE0g6XNMLM8IQ4aFdP0w.png?q=20', 'https://miro.medium.com/max/1576/1*Niu_c_FhGtLuHjrStkB_4Q.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1121/1*voNageVB1gI8Dfrmr-U3Ew.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*XfPXSNqVb3vc5_jTRl-Q3w.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1176/1*XfPXSNqVb3vc5_jTRl-Q3w.png', 'https://miro.medium.com/max/258/1*DvlB9rtndUHwtri4E2P-bg.png', 'https://miro.medium.com/max/60/1*PCpvPF7eBi07rRc0ayFqXw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2824/1*dmZz0dp17eqlcrAWFtk2AA.png', 'https://miro.medium.com/max/2488/1*PCpvPF7eBi07rRc0ayFqXw.png', 'https://miro.medium.com/max/3612/1*pOZ3Dcc27oPlbt6dE-FbSg.png', 'https://miro.medium.com/fit/c/96/96/1*2lsrnSkXFL6celMtqlOlWA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*dmZz0dp17eqlcrAWFtk2AA.png?q=20'}",2020-03-05 00:11:26.433200,6.253318786621094
https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-1-3b71f1b8ac4e,How to Hadoop at home with Raspberry Pi — Part 1,"Setting the Stage

I need to buy some hardware and get some free software. My aim here is to just buy some stuff in as simple and painless a process as possible. You can probably get cheaper and cooler elsewhere.

Amazon+BestBuy:

3x Raspberry Pi 2 Model B(starter kit) = CAD$210

1x 5 port switch = CAD$34

1x mini power bar (already own)

1x wireless mouse and keyboard (already own)

Kit contained: Raspberry Pi, 8GB SD Class 10, Wi-Fi dongle, and a few other things we won’t need for this project.

Update: I decided not to actually use a switch because I already have a wireless network setup and all 3 of my pi’s have wifi dongles

Software:

Hadoop v2.7.2 (just plain vanilla from Apache)

Raspbian (from the NOOB SD Card included in the hardware kit)

Update: Raspbian Wheezy was on the NOOB but I ran into some newbie like issues and then discovered Raspbian Jessie was available so I made the switch. https://www.raspberrypi.org/documentation/installation/installing-images/mac.md

Installing Raspbian…

is ridiculously easy. To start off with I just wanted to get one Raspberry up and running before jumping into anything with Hadoop. So, my setup is:

Raspberry Pi WiFi USB dongle HDMI display SD Card with NOOB (updated, reformatted card) Wireless mouse and keyboard with USB dongle Power supply

With all that done, I’ve plugged in Raspberry, it started up with OS install dialog, I selected Raspbian and now the long install process begins.

Update: With my reformatted Jessie SD Card, things are a bit different but you can follow along if you have the NOOB SD Card.

Raspi-config…

pops up once the install is completed. Here we would normally do some simple configuration stuff but because this will also be my nameNode server I’ll also do some more specific changes for that purpose.

Numbers don't map to the rasp-config, just the order

1. Expand Filesystem - No. Using NOOB

2. Enable Boot to Desktop/Scratch/CLI - Going with Desktop and terminal

4. Internationalization Options - You'll want to change timezone

5. Overclock - After reading the warning I'll go with the Pi2 setting

6. Advanced: Hostname - node1 (will be my NameNode)

7. Advanced: Memory split: 32MB (memory made available to the GPU)

8. Advanced: SSH (will be used later)

Update: this was the original setup with Wheezy but Jessie does away with the Raspi-config, and replaces it with the GUI after you login.

Once that’s all done. Reboot! Raspberry Pi is now up and running. The next two steps are network configurations and Java environment, which look pretty simple so I’ll do that now before calling it a day aka Part 1.

Network configuring and Java…

gave me 10mins of confusion but in the end my /etc/network/interfaces file looks like this:

Use this to find your raspberry's ip address

$hostname -I Use sudo or root to edit the interfaces file and save

$sudo nano /etc/network/interfaces #these values are specific to me, use your own from above

iface eth0 inet static

address 192.168.0.107

netmask 255.255.255.0

gateway 192.168.0.1

Some tutorials talked about checking my /etc/resolv.conf but this has something to do with a “nameserver / DNS” which I’m pretty sure doesn’t apply to my situation (Note-to-self: if something breaks come back here). Now, some tutorials also say reboot and then check for Java which should be pre-installed with NOOB. But why reboot first? So, last step, check for Java

From the terminal: $java -version

Thankfully, my system has Java installed (Java version 1.8.0 was printed on the screen). And I say thankfully because I have no idea what to do if it wasn’t — well, I’d have to install it, which isn’t a big deal but still.

Also, I’m going to update my hosts file to make things a little easier when looking up each machine (once we get the other two nodes up and running).

From the terminal: $sudo nano /etc/hosts

add: 192.168.0.107 node1 From the terminal: $sudo nano /etc/hostname

replace: raspberrypi with node1

Last step, reboot and take a break. I’m done with Raspberry. Pretty straight forward so far. Next step (Part 2) getting Hadoop installed.","['wifi', 'java', 'wireless', 'raspberry', 'hadoop', 'install', 'reboot', 'noob', 'sd', 'card', 'pi']","Amazon+BestBuy:3x Raspberry Pi 2 Model B(starter kit) = CAD$2101x 5 port switch = CAD$341x mini power bar (already own)1x wireless mouse and keyboard (already own)Kit contained: Raspberry Pi, 8GB SD Class 10, Wi-Fi dongle, and a few other things we won’t need for this project.
To start off with I just wanted to get one Raspberry up and running before jumping into anything with Hadoop.
Update: With my reformatted Jessie SD Card, things are a bit different but you can follow along if you have the NOOB SD Card.
Raspberry Pi is now up and running.
So, last step, check for JavaFrom the terminal: $java -versionThankfully, my system has Java installed (Java version 1.8.0 was printed on the screen).",en,['Jason I. Carter'],2016-03-28 02:09:21.958000+00:00,"{'Big Data', 'Raspberry Pi', 'Hadoop'}","{'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/80/80/0*jCSEj85ojuJZVikn.jpg', 'https://miro.medium.com/fit/c/80/80/1*aiV4IzhhhPYJg6_SkU53LQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*SwZ64Vy614Tk5tCouzd6sQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*SwZ64Vy614Tk5tCouzd6sQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*SwZ64Vy614Tk5tCouzd6sQ.jpeg'}",2020-03-05 00:11:27.171208,0.7380077838897705
https://towardsdatascience.com/docker-for-data-science-9c0ce73e8263,Docker for Data Science,"I’m going to show all steps from installing Anaconda and upgrading packages to building Docker image and publishing it on Docker hub.

Anaconda

If you just started own way in Data Science you can find a lot of books and courses (Coursera, Udemy…). And almost everywhere you should start by installing environment. You can use python with pip or install Anaconda.

Here I’ll show you how to install Anaconda and update all dependencies.

There is a good documentation how to install it on any platform (Mac OS, Linux, Windows). As I’m working mostly with mac I’ll show all steps for Mac OS. But for Linux and Windows it’s almost the same. Let’s open Installing on macOS page and download installer by clicking Anaconda installer for macOS link.

I’ll install Python 3.6 version 64-Bit Graphical Installer (569 MB). Just click link and download it. As always it’s easy to install software on mac.

After running installation you can see some steps. Click next:

You can see some installation information, click next:

Read license:

And click next:

You need to agree with license:

Choose where you should install it:

For all packages we need almost 2 Gb disk space. Click install:

And wait:

So it’s done. We can remove installator:

To test it just open terminal and run command conda:

Conda is a package, dependency and environment management for any language — Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN.

Next step is updating all dependencies. We need to update conda, anaconda and all packages:

conda update conda

conda update anaconda

conda update --all

Even if you just installed anaconda you may need to update some packages.

And finally we can see

That’s all. To test it we can run jupyter notebook:

jupyter notebook

And see it in browser:

For local development that’s all. But if you need to share your environment for other PC (home and work or share it for team), you should use Docker.

Docker installation

To work with Docker we need to install it. Open documentation page. We will use Community Edition (CE) version as it’s free and it’s ok for us.

As before I choose Docker for Mac (macOS).

I’ll use stable channel

Click link and download it. After opening just drag & drop it

After you have it in your programs. Run it:

And you’ll see icon

It’s working. Let’s run it in command line:

I created an article and described some useful cases: Making right things using Docker. I recommend to read it before we continue.

Docker image

Next step is creating an image and saving it on github. I hope you have gihub account. If no it’s easy to create it.

Next I’ll show how to create Dockerfile and build image, run it and stop it. but before we need to create working directory and empty Dockerfile:

mkdir docker-data-science

cd docker-data-science

touch Dockerfile

And open it in your favorite IDE. As we have jupyter notebook already installed we can use it:

And open it

I’m going to use Ubuntu version of linux for our image. It’s the most popular linux distributive. You can read how to install Anaconda for linux in documentation. It’s not so easy as for mac os.

Let’s open download page and copy link for Python 3.6 64-Bit (x86) Installer (525 MB).

Here is our Dockerfile:

# We will use Ubuntu for our image

FROM ubuntu # Updating Ubuntu packages

RUN apt-get update && yes|apt-get upgrade # Adding wget and bzip2

RUN apt-get install -y wget bzip2

RUN wget

RUN bash Anaconda3-5.0.1-Linux-x86_64.sh -b

RUN rm Anaconda3-5.0.1-Linux-x86_64.sh # Anaconda installingRUN wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh RUN bash Anaconda3-5.0.1-Linux-x86_64.sh -bRUN rm Anaconda3-5.0.1-Linux-x86_64.sh # Set path to conda

ENV PATH /root/anaconda3/bin:$PATH # Updating Anaconda packages

RUN conda update conda

RUN conda update anaconda

RUN conda update --all # Configuring access to Jupyter

RUN mkdir /opt/notebooks

RUN jupyter notebook --generate-config --allow-root

RUN echo ""c.NotebookApp.password = u'sha1:6a3f528eec40:6e896b6e4828f525a6e20e5411cd1c8075d68619'"" >> /root/.jupyter/jupyter_notebook_config.py # Jupyter listens port: 8888

EXPOSE 8888 # Run Jupytewr notebook as Docker main process

CMD [""jupyter"", ""notebook"", ""--allow-root"", ""--notebook-dir=/opt/notebooks"", ""--ip='*'"", ""--port=8888"", ""--no-browser""]

I described each step and put link to Anaconda file in Anaconda installing part. So we need to see:

And to build image we need to run command:

docker build -t docker-data-science .

You can see each step, starting from downloading Ubuntu image:

And updating:

Downloading Anaconda installer:

Installing Anaconda:

And updating packages:

You can stop building and re-run it again in any time. Docker saves each step so you will continue from the last point. It helps if you need to add some new packages or add other dependencies to Dockerfile and continue build an image.

Finally it’s done. We can see images that we downloaded and created:

docker images

Don’t forget to stop jupyter notebook locally after saving Dockerfile as we will use the same port from Docker container:

Now we can run container based on our new image:

docker run --name docker-data-science -p 8888:8888 -v ""$PWD/notebooks:/opt/notebooks"" -d docker-data-science

And open http://localhost:8888/

Enter root

If we create a new file

We can see it in subfolder:

So our mapping is working. We can run container, create file, test it and it will be saved on PC.

To stop container run:

docker rm -f docker-data-science

And we can see it in browser:

Github

Now there is a time to save our Dockerfile on github. After that we start working with Docker hub.

After log in to github Click Create new repository button or just open https://github.com/new.

After we see steps to add and save our files:

So let’s make those commands:

git init

echo ""notebooks"" > .gitignore

git add .

git commit -m ""first commit""

Command echo “notebooks” > .gitignore creates .gitignore file where we can put directories or files which we don’t want to save in git. In our case it’s notebooks. You need to store only source code.

Next:

git remote add origin https://github.com/evheniy/docker-data-science.git

git push -u origin master

We just saved our Dockerfile on github. If we reload page we can see it there:

But we see message that we need to create README.md file to describe our repository. Let’s make it:

Save it and refresh page:

Now you can make command:

And build image on any PC with Docker. But if you don’t want to wait half of hour while it’s building you can build it once and save it on Docker hub.

Docker hub

Docker hub is a storage like github for Docker images. You need to create account and after you can store there own images.

To store image you need to click Create Repository link and make next steps:

Choose a namespace (Required) Add a repository name (Required) Add a short description Add markdown to the full description field Set it to be a private or public repository

After saving we have it on docker hub:

After that we need to push our image. But before we need to login:

docker login

Next we need set docker user

export DOCKER_ID_USER=""evheniy""

Make a new tag:

docker tag docker-data-science evheniy/docker-data-science

And push image:

docker push evheniy/docker-data-science

Our image is a really huge.

We can see just created tag:

Now we can run command

docker pull evheniy/docker-data-science

To get image on nay PC

And we can see updated docker hub profile:

Automatic Build

It’s ok to make image manually. But Docker hub can help with building it automatically if you change code on github. For this we need to create automated build:","['create', 'update', 'install', 'docker', 'need', 'image', 'anaconda', 'data', 'open', 'run', 'add', 'science']","I’m going to show all steps from installing Anaconda and upgrading packages to building Docker image and publishing it on Docker hub.
AnacondaIf you just started own way in Data Science you can find a lot of books and courses (Coursera, Udemy…).
We need to update conda, anaconda and all packages:conda update condaconda update anacondaconda update --allEven if you just installed anaconda you may need to update some packages.
Docker installationTo work with Docker we need to install it.
Docker hubDocker hub is a storage like github for Docker images.",en,['Evheniy Bystrov'],2018-05-29 18:32:51.745000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Anaconda', 'Docker'}","{'https://miro.medium.com/max/5316/1*yCX2bqAeNEhrrMRa7R2E3Q.png', 'https://miro.medium.com/max/60/1*lB_xIsw0ukF3DqRob3weqw.png?q=20', 'https://miro.medium.com/max/3428/1*lB_xIsw0ukF3DqRob3weqw.png', 'https://miro.medium.com/max/60/1*yCX2bqAeNEhrrMRa7R2E3Q.png?q=20', 'https://miro.medium.com/max/60/1*RMgja_GZsaf4h6w5eIdt3w.png?q=20', 'https://miro.medium.com/max/60/1*p-E2F9g11KLY2XsLNThthA.png?q=20', 'https://miro.medium.com/max/3540/1*c8oeWVyGxsyGHI_BmCdCxg.png', 'https://miro.medium.com/max/4516/1*TT3Xhp0hUME6ADUeyY7XHg.png', 'https://miro.medium.com/max/5356/1*saV_LDNe5g0qfK8D6oX6MQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/5356/1*l-oF0fb0iLsuCTKM1TfkIw.png', 'https://miro.medium.com/max/5316/1*j7dhTbNEIaWflfrakaCmMA.png', 'https://miro.medium.com/fit/c/96/96/0*8TaoR2_HuuhD4I7s.', 'https://miro.medium.com/max/5356/1*1TN5-J29dIbTfht23awntg.png', 'https://miro.medium.com/max/3596/1*p-E2F9g11KLY2XsLNThthA.png', 'https://miro.medium.com/max/3328/1*SRhcVHkPCRgNxw796B2eIw.png', 'https://miro.medium.com/max/6720/1*Db1R6HBDZEbaRzPRJ9WSGw.png', 'https://miro.medium.com/max/5344/1*3wT6Sz5flUsrk3Qg33YJbA.png', 'https://miro.medium.com/max/5148/1*RWK3MG-V4t1YI891HnPuHA.png', 'https://miro.medium.com/max/60/1*I8R5BwxNUJMmSzXuBI9Fww.png?q=20', 'https://miro.medium.com/max/4436/1*wRetmLuHNxDDWagwW0f-mQ.png', 'https://miro.medium.com/max/5148/1*s0ZEzsnd6p9_ggsOZGd_0A.png', 'https://miro.medium.com/max/4588/1*DtHXGJ-ZWF2PICeDQtO98g.png', 'https://miro.medium.com/max/5316/1*bo3Cwzn9pCiaVLaZqevxeg.png', 'https://miro.medium.com/max/60/1*FiMTkrA5g0_m4VBZNcOiCg.png?q=20', 'https://miro.medium.com/max/60/1*2Vb340diR1fqtEZ94OlFZA.png?q=20', 'https://miro.medium.com/max/60/1*9qjHdLIWjabCX8P0wfHoiA.png?q=20', 'https://miro.medium.com/max/60/1*Qi1dUQZEhpjh3MlwWr-cmw.png?q=20', 'https://miro.medium.com/max/60/1*zF4KRR9wVgapGGyyunnoUA.png?q=20', 'https://miro.medium.com/max/60/1*YQV3VLA8C2Ei8UB4Hn-Jsw.png?q=20', 'https://miro.medium.com/max/60/1*k9zkF5-Qs3JM_vJjIq_V-A.png?q=20', 'https://miro.medium.com/max/60/1*bo3Cwzn9pCiaVLaZqevxeg.png?q=20', 'https://miro.medium.com/max/60/1*tQyJ60w9144ieAcoTWourQ.png?q=20', 'https://miro.medium.com/max/60/1*akFTfpFTYzkTAqTO0o4KXg.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*8TaoR2_HuuhD4I7s.', 'https://miro.medium.com/max/60/1*5MQnpXjJltqL2pWLYunEgA.png?q=20', 'https://miro.medium.com/max/5356/1*I8R5BwxNUJMmSzXuBI9Fww.png', 'https://miro.medium.com/max/60/1*eY2X86n_wm4aMULExT9KNQ.png?q=20', 'https://miro.medium.com/max/4200/1*oel9n0VaDp3NDE4Kfa2JIA.png', 'https://miro.medium.com/max/3316/1*7BWJHdMEnaczwnLBZbHCvw.png', 'https://miro.medium.com/max/5356/1*sbnzc2nF5ORAJdfmCcJLBw.png', 'https://miro.medium.com/max/60/1*lKwmLHL1Wj1gGkzQk_lIIw.png?q=20', 'https://miro.medium.com/max/60/1*DtHXGJ-ZWF2PICeDQtO98g.png?q=20', 'https://miro.medium.com/max/2928/1*_I1XxKoCFtl74P8ULuJVmw.png', 'https://miro.medium.com/max/60/1*iJ0LyEPeW29WwBWGwq-wTg.png?q=20', 'https://miro.medium.com/max/3316/1*Phsg9cArMtwwKUypVLEMjw.png', 'https://miro.medium.com/max/3540/1*sGIZsl8dxze4lOBFOzLYOA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*UcNMAJEytkRmKiWojImo3w.png?q=20', 'https://miro.medium.com/max/3540/1*W1iN7DTbrf6J5Y4XW0ifFA.png', 'https://miro.medium.com/max/3316/1*GUQA-k06wvbG0FDF8UcBbg.png', 'https://miro.medium.com/max/4588/1*DhWVNmhNraB00bigdsatvg.png', 'https://miro.medium.com/max/60/1*TT3Xhp0hUME6ADUeyY7XHg.png?q=20', 'https://miro.medium.com/max/60/1*c8oeWVyGxsyGHI_BmCdCxg.png?q=20', 'https://miro.medium.com/max/60/1*wRetmLuHNxDDWagwW0f-mQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/5356/1*t699HIoCsXU-UL4x_iOfOQ.png', 'https://miro.medium.com/max/60/1*C8A4upMgdvuR3Vm8EiICoA.png?q=20', 'https://miro.medium.com/max/60/1*LRM6l2_ZyD-fgnsGbu6uiQ.png?q=20', 'https://miro.medium.com/max/60/1*BIyjnAX0male49i98G9n7Q.png?q=20', 'https://miro.medium.com/max/5356/1*9El878jNEKeCqCcqSEH7Xg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*oel9n0VaDp3NDE4Kfa2JIA.png?q=20', 'https://miro.medium.com/max/4436/1*2Vb340diR1fqtEZ94OlFZA.png', 'https://miro.medium.com/max/60/1*A7uJEfwZFyNLXTnQWA0_vg.png?q=20', 'https://miro.medium.com/max/5356/1*rwiADydsMSVXDBahnUV3mg.png', 'https://miro.medium.com/max/3540/1*Y5VqGn8-jFzhNtALvN-4Pw.png', 'https://miro.medium.com/max/3596/1*Gj6Gc5mXFFiyDl9Kf5FbGw.png', 'https://miro.medium.com/max/60/1*G_urprJIluq_u15hjbElsg.png?q=20', 'https://miro.medium.com/max/2928/1*xOIamkmq0BmiLypZiKGCaA.png', 'https://miro.medium.com/max/60/1*Sh756XhLJVgWJd91mTzvZA.png?q=20', 'https://miro.medium.com/max/5328/1*06CjyLtb3mU1uSs0DhoOIg.png', 'https://miro.medium.com/max/5328/1*iEmGJWUOMOTmYxHishZvJg.png', 'https://miro.medium.com/max/5356/1*chMY2t8FTHNzFcZDJBhv5A.png', 'https://miro.medium.com/max/5356/1*G_urprJIluq_u15hjbElsg.png', 'https://miro.medium.com/max/5356/1*EBVaplNh7qm2ZeNHmw-QSg.png', 'https://miro.medium.com/max/3316/1*RMgja_GZsaf4h6w5eIdt3w.png', 'https://miro.medium.com/max/2928/1*ZY0kZF4CWpgWo7p2ez798A.png', 'https://miro.medium.com/max/5356/1*3n-hLHegjrxkE8PlxDAPjg.png', 'https://miro.medium.com/max/4436/1*wT6l3NPGNPd2yGk3u0s1xg.png', 'https://miro.medium.com/max/60/1*lX2t4sNcNFatJjkCHRtuNw.png?q=20', 'https://miro.medium.com/max/60/1*wT6l3NPGNPd2yGk3u0s1xg.png?q=20', 'https://miro.medium.com/max/3316/1*k9zkF5-Qs3JM_vJjIq_V-A.png', 'https://miro.medium.com/max/3316/1*Sh756XhLJVgWJd91mTzvZA.png', 'https://miro.medium.com/max/5328/1*GnmclzyCI79tPC-c5UEgQg.png', 'https://miro.medium.com/max/60/1*3n-hLHegjrxkE8PlxDAPjg.png?q=20', 'https://miro.medium.com/max/60/1*9El878jNEKeCqCcqSEH7Xg.png?q=20', 'https://miro.medium.com/max/60/1*fISwBW4EoI9__W1n7rJfsg.png?q=20', 'https://miro.medium.com/max/60/1*saV_LDNe5g0qfK8D6oX6MQ.png?q=20', 'https://miro.medium.com/max/60/1*sGIZsl8dxze4lOBFOzLYOA.png?q=20', 'https://miro.medium.com/max/60/1*rwiADydsMSVXDBahnUV3mg.png?q=20', 'https://miro.medium.com/max/2932/1*UcNMAJEytkRmKiWojImo3w.png', 'https://miro.medium.com/max/60/1*iEmGJWUOMOTmYxHishZvJg.png?q=20', 'https://miro.medium.com/max/3540/1*FiMTkrA5g0_m4VBZNcOiCg.png', 'https://miro.medium.com/max/60/1*7BWJHdMEnaczwnLBZbHCvw.png?q=20', 'https://miro.medium.com/max/5356/1*akFTfpFTYzkTAqTO0o4KXg.png', 'https://miro.medium.com/max/60/1*1TN5-J29dIbTfht23awntg.png?q=20', 'https://miro.medium.com/max/2928/1*5MQnpXjJltqL2pWLYunEgA.png', 'https://miro.medium.com/max/4200/1*4944uPri8kLQzLNdwBwaRw.png', 'https://miro.medium.com/max/3596/1*L_3wuCtrEMO7W-ZO7ZZQ-w.png', 'https://miro.medium.com/max/60/1*238yiu5J9n2SU-rwfr1u0w.png?q=20', 'https://miro.medium.com/max/60/1*_I1XxKoCFtl74P8ULuJVmw.png?q=20', 'https://miro.medium.com/max/5356/1*NwP7g_WWpt8g4rKI73cgQw.png', 'https://miro.medium.com/max/2952/1*aCDFZLC4nUT74eEpfqboFw.png', 'https://miro.medium.com/max/3400/1*4cZuCGYsRQxc6dwVJ2u7Vw.png', 'https://miro.medium.com/max/60/1*Phsg9cArMtwwKUypVLEMjw.png?q=20', 'https://miro.medium.com/max/60/1*Opd22T38yWPeLUxUt8P-Bw.png?q=20', 'https://miro.medium.com/max/5344/1*h58ChRQLU85ugvG0-Oq6WQ.png', 'https://miro.medium.com/max/60/1*g-2JZXEVw9iLmosir6eqwg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*lK1L_dumEl3WULTgqhtXEg.png?q=20', 'https://miro.medium.com/max/60/1*oa4P8fZK-BcIeKyuiCi4-g.png?q=20', 'https://miro.medium.com/max/60/1*ZY0kZF4CWpgWo7p2ez798A.png?q=20', 'https://miro.medium.com/max/60/1*GnmclzyCI79tPC-c5UEgQg.png?q=20', 'https://miro.medium.com/max/3932/1*e7pdP9UoHDqa5Hqno-TczA.png', 'https://miro.medium.com/max/60/1*06CjyLtb3mU1uSs0DhoOIg.png?q=20', 'https://miro.medium.com/max/3540/1*lKwmLHL1Wj1gGkzQk_lIIw.png', 'https://miro.medium.com/max/60/1*chMY2t8FTHNzFcZDJBhv5A.png?q=20', 'https://miro.medium.com/max/60/1*4cZuCGYsRQxc6dwVJ2u7Vw.png?q=20', 'https://miro.medium.com/max/5148/1*238yiu5J9n2SU-rwfr1u0w.png', 'https://miro.medium.com/max/60/1*L_3wuCtrEMO7W-ZO7ZZQ-w.png?q=20', 'https://miro.medium.com/max/5356/1*9qjHdLIWjabCX8P0wfHoiA.png', 'https://miro.medium.com/max/5356/1*iJ0LyEPeW29WwBWGwq-wTg.png', 'https://miro.medium.com/max/60/1*Y5VqGn8-jFzhNtALvN-4Pw.png?q=20', 'https://miro.medium.com/max/3540/1*-1rbmMqJ1pYUUIHo7Bn0Dw.png', 'https://miro.medium.com/max/60/1*SRhcVHkPCRgNxw796B2eIw.png?q=20', 'https://miro.medium.com/max/4200/1*Qi1dUQZEhpjh3MlwWr-cmw.png', 'https://miro.medium.com/max/3120/1*Opd22T38yWPeLUxUt8P-Bw.png', 'https://miro.medium.com/max/60/1*DhWVNmhNraB00bigdsatvg.png?q=20', 'https://miro.medium.com/max/3316/1*fISwBW4EoI9__W1n7rJfsg.png', 'https://miro.medium.com/max/60/1*3wT6Sz5flUsrk3Qg33YJbA.png?q=20', 'https://miro.medium.com/max/2932/1*YQV3VLA8C2Ei8UB4Hn-Jsw.png', 'https://miro.medium.com/max/60/1*h58ChRQLU85ugvG0-Oq6WQ.png?q=20', 'https://miro.medium.com/max/5356/1*g-2JZXEVw9iLmosir6eqwg.png', 'https://miro.medium.com/max/60/1*8bjA1BaeH0SbzeE2CJHFOg.png?q=20', 'https://miro.medium.com/max/60/1*sbnzc2nF5ORAJdfmCcJLBw.png?q=20', 'https://miro.medium.com/max/3708/1*lX2t4sNcNFatJjkCHRtuNw.png', 'https://miro.medium.com/max/60/1*-1rbmMqJ1pYUUIHo7Bn0Dw.png?q=20', 'https://miro.medium.com/max/60/1*GUQA-k06wvbG0FDF8UcBbg.png?q=20', 'https://miro.medium.com/max/60/1*Db1R6HBDZEbaRzPRJ9WSGw.png?q=20', 'https://miro.medium.com/max/60/1*RWK3MG-V4t1YI891HnPuHA.png?q=20', 'https://miro.medium.com/max/60/1*Gj6Gc5mXFFiyDl9Kf5FbGw.png?q=20', 'https://miro.medium.com/max/60/1*EBVaplNh7qm2ZeNHmw-QSg.png?q=20', 'https://miro.medium.com/max/5356/1*Jna7JP69PqQMYYPcywh_NA.png', 'https://miro.medium.com/max/60/1*NwP7g_WWpt8g4rKI73cgQw.png?q=20', 'https://miro.medium.com/max/3596/1*eY2X86n_wm4aMULExT9KNQ.png', 'https://miro.medium.com/max/5356/1*lK1L_dumEl3WULTgqhtXEg.png', 'https://miro.medium.com/max/5356/1*8bjA1BaeH0SbzeE2CJHFOg.png', 'https://miro.medium.com/max/60/1*j7dhTbNEIaWflfrakaCmMA.png?q=20', 'https://miro.medium.com/max/60/1*t699HIoCsXU-UL4x_iOfOQ.png?q=20', 'https://miro.medium.com/max/60/1*s0ZEzsnd6p9_ggsOZGd_0A.png?q=20', 'https://miro.medium.com/max/60/1*Jna7JP69PqQMYYPcywh_NA.png?q=20', 'https://miro.medium.com/max/5316/1*C8A4upMgdvuR3Vm8EiICoA.png', 'https://miro.medium.com/max/60/1*4944uPri8kLQzLNdwBwaRw.png?q=20', 'https://miro.medium.com/max/2928/1*tQyJ60w9144ieAcoTWourQ.png', 'https://miro.medium.com/max/60/1*xOIamkmq0BmiLypZiKGCaA.png?q=20', 'https://miro.medium.com/max/3932/1*LRM6l2_ZyD-fgnsGbu6uiQ.png', 'https://miro.medium.com/max/2588/1*oa4P8fZK-BcIeKyuiCi4-g.png', 'https://miro.medium.com/max/60/1*aCDFZLC4nUT74eEpfqboFw.png?q=20', 'https://miro.medium.com/max/964/1*BIyjnAX0male49i98G9n7Q.png', 'https://miro.medium.com/max/60/1*e7pdP9UoHDqa5Hqno-TczA.png?q=20', 'https://miro.medium.com/max/1200/1*SRhcVHkPCRgNxw796B2eIw.png', 'https://miro.medium.com/max/5420/1*zF4KRR9wVgapGGyyunnoUA.png', 'https://miro.medium.com/max/2932/1*A7uJEfwZFyNLXTnQWA0_vg.png', 'https://miro.medium.com/max/60/1*W1iN7DTbrf6J5Y4XW0ifFA.png?q=20', 'https://miro.medium.com/max/60/1*l-oF0fb0iLsuCTKM1TfkIw.png?q=20'}",2020-03-05 00:11:31.010432,3.8392245769500732
https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877,A simple deep learning model for stock price prediction using TensorFlow,"For a recent hackathon that we did at STATWORX, some of our team members scraped minutely S&P 500 data from the Google Finance API. The data consisted of index as well as stock prices of the S&P’s 500 constituents. Having this data at hand, the idea of developing a deep learning model for predicting the S&P 500 index based on the 500 constituents prices one minute ago came immediately on my mind.

Playing around with the data and building the deep learning model with TensorFlow was fun and so I decided to write my first Medium.com story: a little TensorFlow tutorial on predicting S&P 500 stock prices. What you will read is not an in-depth tutorial, but more a high-level introduction to the important building blocks and concepts of TensorFlow models. The Python code I’ve created is not optimized for efficiency but understandability. The dataset I’ve used can be downloaded from here (40MB).

Note, that this story is a hands-on tutorial on TensorFlow. Actual prediction of stock prices is a really challenging and complex task that requires tremendous efforts, especially at higher frequencies, such as minutes used here.

Importing and preparing the data

Our team exported the scraped stock data from our scraping server as a csv file. The dataset contains n = 41266 minutes of data ranging from April to August 2017 on 500 stocks as well as the total S&P 500 index price. Index and stocks are arranged in wide format.

# Import data

data = pd.read_csv('data_stocks.csv') # Drop date variable

data = data.drop(['DATE'], 1) # Dimensions of dataset

n = data.shape[0]

p = data.shape[1] # Make data a numpy array

data = data.values

The data was already cleaned and prepared, meaning missing stock and index prices were LOCF’ed (last observation carried forward), so that the file did not contain any missing values.

A quick look at the S&P time series using pyplot.plot(data['SP500']) :

Time series plot of the S&P 500 index.

Note: This is actually the lead of the S&P 500 index, meaning, its value is shifted 1 minute into the future (this has already been done in the dataset). This operation is necessary since we want to predict the next minute of the index and not the current minute. Technically speaking, each row in the dataset contains the price of the S&P500 at t+1 and the constituent’s prices at T=t.

Preparing training and test data

The dataset was split into training and test data. The training data contained 80% of the total dataset. The data was not shuffled but sequentially sliced. The training data ranges from April to approx. end of July 2017, the test data ends end of August 2017.

# Training and test data

train_start = 0

train_end = int(np.floor(0.8*n))

test_start = train_end

test_end = n

data_train = data[np.arange(train_start, train_end), :]

data_test = data[np.arange(test_start, test_end), :]

There are a lot of different approaches to time series cross validation, such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling. The latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values.

Data scaling

Most neural network architectures benefit from scaling the inputs (sometimes also the output). Why? Because most common activation functions of the network’s neurons such as tanh or sigmoid are defined on the [-1, 1] or [0, 1] interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets anyway. Scaling can be easily accomplished in Python using sklearn’s MinMaxScaler .

# Scale data

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

data_train = scaler.fit_transform(data_train)

data_test = scaler.transform(data_test) # Build X and y

X_train = data_train[:, 1:]

y_train = data_train[:, 0]

X_test = data_test[:, 1:]

y_test = data_test[:, 0]

Remark: Caution must be undertaken regarding what part of the data is scaled and when. A common mistake is to scale the whole dataset before training and test split are being applied. Why is this a mistake? Because scaling invokes the calculation of statistics e.g. the min/max of a variable. When performing time series forecasting in real life, you do not have information from future observations at the time of forecasting. Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data. Otherwise, you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction.

Introduction to TensorFlow

TensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework. It is based on a C++ low level backend but is usually controlled via Python (there is also a neat TensorFlow library for R, maintained by RStudio). TensorFlow operates on a graph representation of the underlying computational task. This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. Check out this simple example (stolen from our deep learning introduction from our blog):

A very simple graph that adds two numbers together.

In the figure above, two numbers are supposed to be added. Those numbers are stored in two variables, a and b . The two values are flowing through the graph and arrive at the square node, where they are being added. The result of the addition is stored into another variable, c . Actually, a , b and c can be considered as placeholders. Any numbers that are fed into a and b get added and are stored into c . This is exactly how TensorFlow works. The user defines an abstract representation of the model (neural network) through placeholders and variables. Afterwards, the placeholders get ""filled"" with real data and the actual computations take place. The following code implements the toy example from above in TensorFlow:

# Import TensorFlow

import tensorflow as tf



# Define a and b as placeholders

a = tf.placeholder(dtype=tf.int8)

b = tf.placeholder(dtype=tf.int8)



# Define the addition

c = tf.add(a, b)



# Initialize the graph

graph = tf.Session()



# Run the graph

graph.run(c, feed_dict={a: 5, b: 4})

After having imported the TensorFlow library, two placeholders are defined using tf.placeholder() . They correspond to the two blue circles on the left of the image above. Afterwards, the mathematical addition is defined via tf.add() . The result of the computation is c = 9 . With placeholders set up, the graph can be executed with any integer value for a and b . Of course, the former problem is just a toy example. The required graphs and computations in a neural network are much more complex.

Placeholders

As mentioned before, it all starts with placeholders. We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t ) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1 ).

The shape of the placeholders correspond to [None, n_stocks] with [None] meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly.

# Placeholder

X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])

Y = tf.placeholder(dtype=tf.float32, shape=[None])

The None argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch, so we keep if flexible. We will later define the variable batch_size that controls the number of observations per training batch.

Variables

Besides placeholders, variables are another cornerstone of the TensorFlow universe. While placeholders are used to store input and target data in the graph, variables are used as flexible containers within the graph that are allowed to change during graph execution. Weights and biases are represented as variables in order to adapt during training. Variables need to be initialized, prior to model training. We will get into that a litte later in more detail.

The model consists of four hidden layers. The first layer contains 1024 neurons, slightly more than double the size of the inputs. Subsequent hidden layers are always half the size of the previous layer, which means 512, 256 and finally 128 neurons. A reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers. Of course, other network architectures and neuron configurations are possible but are out of scope for this introduction level article.

# Model architecture parameters

n_stocks = 500

n_neurons_1 = 1024

n_neurons_2 = 512

n_neurons_3 = 256

n_neurons_4 = 128

n_target = 1 # Layer 1: Variables for hidden weights and biases

W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))

bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1])) # Layer 2: Variables for hidden weights and biases

W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))

bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2])) # Layer 3: Variables for hidden weights and biases

W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))

bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3])) # Layer 4: Variables for hidden weights and biases

W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))

bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))



# Output layer: Variables for output weights and biases

W_out = tf.Variable(weight_initializer([n_neurons_4, n_target]))

bias_out = tf.Variable(bias_initializer([n_target]))

It is important to understand the required variable dimensions between input, hidden and output layers. As a rule of thumb in multilayer perceptrons (MLPs, the type of networks used here), the second dimension of the previous layer is the first dimension in the current layer for weight matrices. This might sound complicated but is essentially just each layer passing its output as input to the next layer. The biases dimension equals the second dimension of the current layer’s weight matrix, which corresponds the number of neurons in this layer.

Designing the network architecture

After definition of the required weight and bias variables, the network topology, the architecture of the network, needs to be specified. Hereby, placeholders (data) and variables (weighs and biases) need to be combined into a system of sequential matrix multiplications.

Furthermore, the hidden layers of the network are transformed by activation functions. Activation functions are important elements of the network architecture since they introduce non-linearity to the system. There are dozens of possible activation functions out there, one of the most common is the rectified linear unit (ReLU) which will also be used in this model.

# Hidden layer

hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))

hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))

hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))

hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))



# Output layer (must be transposed)

out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))

The image below illustrates the network architecture. The model consists of three major building blocks. The input layer, the hidden layers and the output layer. This architecture is called a feedforward network. Feedforward indicates that the batch of data solely flows from left to right. Other network architectures, such as recurrent neural networks, also allow data flowing “backwards” in the network.

Cool technical illustration of our feedforward network architecture.

Cost function

The cost function of the network is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets. Basically, any differentiable function can be implemented in order to compute a deviation measure between predictions and targets.

# Cost function

mse = tf.reduce_mean(tf.squared_difference(out, Y))

However, the MSE exhibits certain properties that are advantageous for the general optimization problem to be solved.

Optimizer

The optimizer takes care of the necessary computations that are used to adapt the network’s weight and bias variables during training. Those computations invoke the calculation of so called gradients, that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network’s cost function. The development of stable and speedy optimizers is a major field in neural network an deep learning research.

# Optimizer

opt = tf.train.AdamOptimizer().minimize(mse)

Here the Adam Optimizer is used, which is one of the current default optimizers in deep learning development. Adam stands for “Adaptive Moment Estimation” and can be considered as a combination between two other popular optimizers AdaGrad and RMSProp.

Initializers

Initializers are used to initialize the network’s variables before training. Since neural networks are trained using numerical optimization techniques, the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem. There are different initializers available in TensorFlow, each with different initialization approaches. Here, I use the tf.variance_scaling_initializer() , which is one of the default initialization strategies.

# Initializers

sigma = 1

weight_initializer = tf.variance_scaling_initializer(mode=""fan_avg"", distribution=""uniform"", scale=sigma)

bias_initializer = tf.zeros_initializer()

Note, that with TensorFlow it is possible to define multiple initialization functions for different variables within the graph. However, in most cases, a unified initialization is sufficient.

Fitting the neural network

After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by minibatch training. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The training dataset gets divided into n / batch_size batches that are sequentially fed into the network. At this point the placeholders X and Y come into play. They store the input and target data and present them to the network as inputs and targets.

A sampled data batch of X flows through the network until it reaches the output layer. There, TensorFlow compares the models predictions against the actual observed targets Y in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the networks parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself. The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.

The training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies.

# Make Session

net = tf.Session() # Run initializer

net.run(tf.global_variables_initializer())



# Setup interactive plot

plt.ion()

fig = plt.figure()

ax1 = fig.add_subplot(111)

line1, = ax1.plot(y_test)

line2, = ax1.plot(y_test*0.5)

plt.show()



# Number of epochs and batch size

epochs = 10

batch_size = 256



for e in range(epochs):



# Shuffle training data

shuffle_indices = np.random.permutation(np.arange(len(y_train)))

X_train = X_train[shuffle_indices]

y_train = y_train[shuffle_indices]



# Minibatch training

for i in range(0, len(y_train) // batch_size):

start = i * batch_size

batch_x = X_train[start:start + batch_size]

batch_y = y_train[start:start + batch_size]

# Run optimizer with batch

net.run(opt, feed_dict={X: batch_x, Y: batch_y})



# Show progress

if np.mod(i, 5) == 0:

# Prediction

pred = net.run(out, feed_dict={X: X_test})

line2.set_ydata(pred)

plt.title('Epoch ' + str(e) + ', Batch ' + str(i))

file_name = 'img/epoch_' + str(e) + '_batch_' + str(i) + '.jpg'

plt.savefig(file_name)

plt.pause(0.01) # Print final MSE after Training

mse_final = net.run(mse, feed_dict={X: X_test, Y: y_test})

print(mse_final)

During the training, we evaluate the networks predictions on the test set — the data which is not learned, but set aside — for every 5th batch and visualize it. Additionally, the images are exported to disk and later combined into a video animation of the training process (see below). The model quickly learns the shape and location of the time series in the test data and is able to produce an accurate prediction after some epochs. Nice!

Video animation of the network’s test data prediction (orange) during training.

One can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data. This also corresponds to the Adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum. After 10 epochs, we have a pretty close fit to the test data! The final test MSE equals 0.00078 (it is very low, because the target is scaled). The mean absolute percentage error of the forecast on the test set is equal to 5.31% which is pretty good. Note, that this is just a fit to the test data, no actual out of sample metrics in a real world scenario.

Scatter plot between predicted and actual S&P prices (scaled).

Please note that there are tons of ways of further improving this result: design of layers and neurons, choosing different initialization and activation schemes, introduction of dropout layers of neurons, early stopping and so on. Furthermore, different types of deep learning models, such as recurrent neural networks might achieve better performance on this task. However, this is not the scope of this introductory post.

Conclusion and outlook

The release of TensorFlow was a landmark event in deep learning research. Its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ML algorithms. However, flexibility comes at the cost of longer time-to-model cycles compared to higher level APIs such as Keras or MxNet. Nonetheless, I am sure that TensorFlow will make its way to the de-facto standard in neural network and deep learning development in research and practical applications. Many of our customers are already using TensorFlow or start developing projects that employ TensorFlow models. Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. Let’s see what Google has planned for the future of TensorFlow. One thing that is missing, at least in my opinion, is a neat graphical user interface for designing and developing neural net architectures with TensorFlow backend. Maybe, this is something Google is already working on ;)

Update: I’ve added both the Python script as well as a (zipped) dataset to a Github repository. Feel free to clone and fork.

Final remarks

If you have any comments or questions on my story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice. Follow me on LinkedIn or Twitter, if you want to stay in touch.

Make sure, you also check the awesome STATWORX Blog for more interesting data science, ML and AI content straight from the our office in Frankfurt, Germany!

If you’re interested in more quality content like this, join my mailing list, constantly bringing you new data science, machine learning and AI reads and treats from me and my team right into your inbox!

I hope you liked my story, I really enjoyed writing it. Thank you for your time!","['network', 'tensorflow', 'stock', 'prediction', 'variables', 'simple', 'price', 'learning', 'neural', 'deep', 'networks', 'model', 'data', 'training', 'test', 'layer', 'using']","Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data.
Introduction to TensorFlowTensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework.
This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators.
Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning.
Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development.",en,['Sebastian Heinz'],2018-10-08 19:10:12.449000+00:00,"{'TensorFlow', 'Deep Learning', 'Stock Market', 'Machine Learning', 'Tutorial'}","{'https://miro.medium.com/max/3258/1*SHAol6IyUGn5yoXt7mC4Uw.png', 'https://miro.medium.com/fit/c/160/160/1*3yjjoZ--eh8E84rH1xRIhQ.jpeg', 'https://miro.medium.com/proxy/1*aCqfu5QVufgPuJNWg4Vn7g.png', 'https://miro.medium.com/fit/c/160/160/1*8lHWKHMXnJJDjMmFaLSFTA.jpeg', 'https://miro.medium.com/proxy/1*OK6YP4-xG5v8oakDXk2oDw.png', 'https://miro.medium.com/max/60/1*SHAol6IyUGn5yoXt7mC4Uw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*R6303tLavDAf6jJAsMlaJQ.jpeg', 'https://miro.medium.com/max/9216/1*NG0bzk0wtQcBdMYAnXKeBQ.jpeg', 'https://miro.medium.com/max/1200/1*NG0bzk0wtQcBdMYAnXKeBQ.jpeg', 'https://miro.medium.com/fit/c/80/80/0*JAI6AWapx2N6wRi_', 'https://miro.medium.com/max/60/1*NG0bzk0wtQcBdMYAnXKeBQ.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*8lHWKHMXnJJDjMmFaLSFTA.jpeg', 'https://miro.medium.com/max/160/1*0ykYC2ubehxNL5XrIIbscg.png', 'https://miro.medium.com/max/60/1*tnbyLo91bfJUexf7TnJamA.png?q=20', 'https://miro.medium.com/max/1448/1*tnbyLo91bfJUexf7TnJamA.png', 'https://miro.medium.com/fit/c/80/80/1*ZTI9m1Cgre4LvFqzbQSD2w.jpeg'}",2020-03-05 00:11:32.665187,1.6537542343139648
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6,Activation Functions in Neural Networks,"Linear or Identity Activation Function

As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.

Fig: Linear Activation Function

Equation : f(x) = x

Range : (-infinity to infinity)

It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks.

Non-linear Activation Function

The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this

Fig: Non-linear Activation Function

It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.

The main terminologies needed to understand for nonlinear functions are:

Derivative or Differential: Change in y-axis w.r.t. change in x-axis.It is also known as slope. Monotonic function: A function which is either entirely non-increasing or non-decreasing.

The Nonlinear Activation Functions are mainly divided on the basis of their range or curves-

1. Sigmoid or Logistic Activation Function

The Sigmoid Function curve looks like a S-shape.

Fig: Sigmoid Function

The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.

The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.

The function is monotonic but function’s derivative is not.

The logistic sigmoid function can cause a neural network to get stuck at the training time.

The softmax function is a more generalized logistic activation function which is used for multiclass classification.

2. Tanh or hyperbolic tangent Activation Function

tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).

Fig: tanh v/s Logistic Sigmoid

The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.

The function is differentiable.

The function is monotonic while its derivative is not monotonic.

The tanh function is mainly used classification between two classes.

Both tanh and logistic sigmoid activation functions are used in feed-forward nets.

3. ReLU (Rectified Linear Unit) Activation Function

The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.

Fig: ReLU v/s Logistic Sigmoid

As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.

Range: [ 0 to infinity)

The function and its derivative both are monotonic.

But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.

4. Leaky ReLU

It is an attempt to solve the dying ReLU problem

Fig : ReLU v/s Leaky ReLU

Can you see the Leak? 😆

The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.

When a is not 0.01 then it is called Randomized ReLU.

Therefore the range of the Leaky ReLU is (-infinity to infinity).

Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.

Why derivative/differentiation is used ?

When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.

Fig: Activation Function Cheetsheet","['relu', 'tanh', 'zero', 'function', 'used', 'neural', 'sigmoid', 'activation', 'logistic', 'networks', 'functions', 'range']","Non-linear Activation FunctionThe Nonlinear Activation Functions are the most used activation functions.
Sigmoid or Logistic Activation FunctionThe Sigmoid Function curve looks like a S-shape.
The logistic sigmoid function can cause a neural network to get stuck at the training time.
Both tanh and logistic sigmoid activation functions are used in feed-forward nets.
ReLU (Rectified Linear Unit) Activation FunctionThe ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.",en,['Sagar Sharma'],2019-02-14 12:44:08.804000+00:00,"{'Neural Networks', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Activation Function'}","{'https://miro.medium.com/max/60/1*cxNqE_CMez7vUIkcLUH8PA.png?q=20', 'https://miro.medium.com/max/60/1*Xu7B5y9gp0iL5ooBj7LtWw.png?q=20', 'https://miro.medium.com/max/60/1*A_Bzn0CjUgOXtPCJKnKLqA.jpeg?q=20', 'https://miro.medium.com/freeze/max/60/1*B_EOC2l6EIKmgQRKJ4g_lg.gif?q=20', 'https://miro.medium.com/max/60/1*tldIgyDQWqm-sMwP7m3Bww.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*Yfq1AFxdV07S9TMoogUx2g.png', 'https://miro.medium.com/freeze/max/1200/1*GIPiAdQyOa8wUOkHaL-MJg.gif', 'https://miro.medium.com/max/1408/1*A_Bzn0CjUgOXtPCJKnKLqA.jpeg', 'https://miro.medium.com/freeze/max/60/1*GIPiAdQyOa8wUOkHaL-MJg.gif?q=20', 'https://miro.medium.com/max/3200/1*GIPiAdQyOa8wUOkHaL-MJg.gif', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1452/1*XxxiA0jJvPrHEJHD4z893g.png', 'https://miro.medium.com/max/60/1*f9erByySVjTjohfFdNkJYQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*p_hyqAtyI8pbt2kEl6siOQ.png?q=20', 'https://miro.medium.com/max/60/1*n1HFBpwv21FCAzGjmWt1sg.png?q=20', 'https://miro.medium.com/max/2010/1*p_hyqAtyI8pbt2kEl6siOQ.png', 'https://miro.medium.com/max/1190/1*f9erByySVjTjohfFdNkJYQ.jpeg', 'https://miro.medium.com/max/812/1*B_EOC2l6EIKmgQRKJ4g_lg.gif', 'https://miro.medium.com/max/60/1*XxxiA0jJvPrHEJHD4z893g.png?q=20', 'https://miro.medium.com/max/970/1*Xu7B5y9gp0iL5ooBj7LtWw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*Yfq1AFxdV07S9TMoogUx2g.png', 'https://miro.medium.com/max/1200/1*cxNqE_CMez7vUIkcLUH8PA.png', 'https://miro.medium.com/max/1600/1*n1HFBpwv21FCAzGjmWt1sg.png', 'https://miro.medium.com/max/1734/1*tldIgyDQWqm-sMwP7m3Bww.png'}",2020-03-05 00:11:38.814409,6.149221897125244
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d,Gradient Boosting from scratch,"Simplifying a complex algorithm

Motivation

Although most of the Kaggle competition winners use stack/ensemble of various models, one particular model that is part of most of the ensembles is some variant of Gradient Boosting (GBM) algorithm. Take for an example the winner of latest Kaggle competition: Michael Jahrer’s solution with representation learning in Safe Driver Prediction. His solution was a blend of 6 models. 1 LightGBM (a variant of GBM) and 5 Neural Nets. Although his success is attributed to the semi-supervised learning that he used for the structured data, but gradient boosting model has done the useful part too.

Even though GBM is being used widely, many practitioners still treat it as complex black-box algorithm and just run the models using pre-built libraries. The purpose of this post is to simplify a supposedly complex algorithm and to help the reader to understand the algorithm intuitively. I am going to explain the pure vanilla version of the gradient boosting algorithm and will share links for its different variants at the end. I have taken base DecisionTree code from fast.ai library (fastai/courses/ml1/lesson3-rf_foundations.ipynb) and on top of that, I have built my own simple version of basic gradient boosting model.

Brief description for Ensemble, Bagging and Boosting

When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error)

An ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction. The reason we use ensembles is that many different predictors trying to predict same target variable will perform a better job than any single predictor alone. Ensembling techniques are further classified into Bagging and Boosting.

Bagging is a simple ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)

We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process. Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is Random Forest models.

Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.

This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error). The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of boosting algorithm.

Fig 1. Ensembling

Fig 2. Bagging (independent models) & Boosting (sequential models). Reference: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/

Gradient Boosting algorithm

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. (Wikipedia definition)

The objective of any supervised learning algorithm is to define a loss function and minimize it. Let’s see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:

We want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.

So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values.

Intuition behind Gradient Boosting

The logic behind gradient boosting is simple, (can be understood intuitively, without using mathematical notation). I expect that whoever is reading this post might be familiar with simple linear regression modeling.

A basic assumption of linear regression is that sum of its residuals is 0, i.e. the residuals should be spread randomly around zero.

Fig 3. Sample random normally distributed residuals with mean around 0

Now think of these residuals as mistakes committed by our predictor model. Although, tree-based models (considering decision tree as base models for our gradient boosting here) are not based on such assumptions, but if we think logically (not statistically) about this assumption, we might argue that, if we are able to see some pattern of residuals around 0, we can leverage that pattern to fit a model.

So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). Algorithmically, we are minimizing our loss function, such that test loss reach its minima.

In summary,

• We first model data with simple models and analyze data for errors.

• These errors signify data points that are difficult to fit by a simple model.

• Then for later models, we particularly focus on those hard to fit data to get them right.

• In the end, we combine all the predictors by giving some weights to each predictor.

A more technical quotation of the same logic is written in Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World,

“The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. … Note, however, it is not obvious at all how this can be done”

Steps to fit a Gradient Boosting model

Let’s consider simulated data as shown in scatter plot below with 1 input (x) and 1 output (y) variables.

Fig 4. Simulated data (x: input, y: output)

Data for above shown plot is generated using below python code:

Code Chunk 1. Data simulation

1. Fit a simple linear regressor or decision tree on data (I have chosen decision tree in my code) [call x as input and y as output]

Code Chunk 2. (Step 1)Using decision tree to find best split (here depth of our tree is 1)

2. Calculate error residuals. Actual target value, minus predicted target value [e1= y - y_predicted1 ] 3. Fit a new model on error residuals as target variable with same input variables [call it e1_predicted] 4. Add the predicted residuals to the previous predictions

[y_predicted2 = y_predicted1 + e1_predicted] 5. Fit another model on residuals that is still left. i.e. [e2 = y - y_predicted2] and repeat steps 2 to 5 until it starts overfitting or the sum of residuals become constant. Overfitting can be controlled by consistently checking accuracy on validation data.

Code Chunk 3. (Steps 2 to 5) Calculate residuals and update new target variable and new predictions

To aid the understanding of the underlying concepts, here is the link with complete implementation of a simple gradient boosting model from scratch. [Link: Gradient Boosting from scratch]

Shared code is a non-optimized vanilla implementation of gradient boosting. Most of the gradient boosting models available in libraries are well optimized and have many hyper-parameters.

Visualization of working Gradient Boosting Tree

Blue dots (left) plots are input (x) vs. output (y) • Red line (left) shows values predicted by decision tree • Green dots (right) shows residuals vs. input (x) for ith iteration • Iteration represent sequential order of fitting gradient boosting tree

Fig 5. Visualization of gradient boosting predictions (First 4 iterations)

Fig 6. Visualization of gradient boosting predictions (18th to 20th iterations)

We observe that after 20th iteration , residuals are randomly distributed (I am not saying random normal here) around 0 and our predictions are very close to true values. (iterations are called n_estimators in sklearn implementation). This would be a good point to stop or our model will start overfitting.

Let’s see how our model look like for 50th iteration.

Fig 7. Visualization of gradient boosting prediction (iteration 50th)

We see that even after 50th iteration, residuals vs. x plot look similar to what we see at 20th iteration. But the model is becoming more complex and predictions are overfitting on the training data and are trying to learn each training data. So, it would have been better to stop at 20th iteration.

Python code snippet used for plotting all the above figures.","['models', 'predictors', 'simple', 'learning', 'model', 'data', 'predictions', 'residuals', 'scratch', 'boosting', 'gradient']","Gradient Boosting is an example of boosting algorithm.
Intuition behind Gradient BoostingThe logic behind gradient boosting is simple, (can be understood intuitively, without using mathematical notation).
[Link: Gradient Boosting from scratch]Shared code is a non-optimized vanilla implementation of gradient boosting.
Most of the gradient boosting models available in libraries are well optimized and have many hyper-parameters.
Visualization of working Gradient Boosting TreeBlue dots (left) plots are input (x) vs. output (y) • Red line (left) shows values predicted by decision tree • Green dots (right) shows residuals vs. input (x) for ith iteration • Iteration represent sequential order of fitting gradient boosting treeFig 5.",en,['Prince Grover'],2019-08-01 07:20:00.917000+00:00,"{'Artificial Intelligence', 'Machine Learning', 'Gradient Boosting'}","{'https://miro.medium.com/fit/c/160/160/1*3yjjoZ--eh8E84rH1xRIhQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*iBPQWjoV0vshKxI2BckyEg.png', 'https://miro.medium.com/fit/c/80/80/2*R6303tLavDAf6jJAsMlaJQ.jpeg', 'https://miro.medium.com/max/60/1*LLbC4TstqzXQ3hzA8wCmeg.png?q=20', 'https://miro.medium.com/max/160/1*0ykYC2ubehxNL5XrIIbscg.png', 'https://miro.medium.com/max/1200/1*8T4HEjzHto_V8PrEFLkd9A.png', 'https://miro.medium.com/fit/c/80/80/2*i9nJ2AW_szQp4jOe4qntjw.jpeg', 'https://miro.medium.com/max/60/1*8T4HEjzHto_V8PrEFLkd9A.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*iBPQWjoV0vshKxI2BckyEg.png', 'https://miro.medium.com/max/1820/1*mBStjWVK-yLvPvGYjw-1dA.png', 'https://miro.medium.com/max/60/1*7EPVSm_80fyh-29g8q6yRQ.png?q=20', 'https://miro.medium.com/max/1172/1*7EPVSm_80fyh-29g8q6yRQ.png', 'https://miro.medium.com/max/1284/1*Ram0yHpCwXWZ23HZUN1QwA.png', 'https://miro.medium.com/max/1788/1*LLbC4TstqzXQ3hzA8wCmeg.png', 'https://miro.medium.com/max/60/1*mBStjWVK-yLvPvGYjw-1dA.png?q=20', 'https://miro.medium.com/max/1888/1*2fGb3jTF85XyHtnpJYA8ug.png', 'https://miro.medium.com/max/2100/1*fHenn7NVqcWvw25D3-zRiQ.png', 'https://miro.medium.com/max/1868/1*tNYXUUU23kcoiww26Uh6jw.png', 'https://miro.medium.com/max/2552/1*8T4HEjzHto_V8PrEFLkd9A.png', 'https://miro.medium.com/max/60/1*PaXJ8HCYE9r2MgiZ32TQ2A.png?q=20', 'https://miro.medium.com/max/60/1*tNYXUUU23kcoiww26Uh6jw.png?q=20', 'https://miro.medium.com/max/54/1*Ram0yHpCwXWZ23HZUN1QwA.png?q=20', 'https://miro.medium.com/max/2848/1*PaXJ8HCYE9r2MgiZ32TQ2A.png', 'https://miro.medium.com/fit/c/80/80/0*JAI6AWapx2N6wRi_', 'https://miro.medium.com/max/60/1*fHenn7NVqcWvw25D3-zRiQ.png?q=20', 'https://miro.medium.com/max/52/1*2fGb3jTF85XyHtnpJYA8ug.png?q=20'}",2020-03-05 00:11:40.639299,1.8248896598815918
https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730,Topic Modeling with Scikit Learn,"Latent Dirichlet Allocation (LDA) is a algorithms used to discover the topics that are present in a corpus. A few open source libraries exist, but if you are using Python then the main contender is Gensim. Gensim is an awesome library and scales really well to large text corpuses. Gensim, however does not include Non-negative Matrix Factorization (NMF), which can also be used to find topics in text. The mathematical basis underpinning NMF is quite different from LDA. I have found it interesting to compare the results of both of the algorithms and have found that NMF sometimes produces more meaningful topics for smaller datasets. NMF has been included in Scikit Learn for quite a while but LDA has only recently (late 2015) been included. The great thing about using Scikit Learn is that it brings API consistency which makes it almost trivial to perform Topic Modeling using both LDA and NMF. Scikit Learn also includes seeding options for NMF which greatly helps with algorithm convergence and offers both online and batch variants of LDA.

How do LDA and NMF work?

I won’t go into any lengthy mathematical detail — there are many blogs posts and academic journal articles that do. While LDA and NMF have differing mathematical underpinning, both algorithms are able to return the documents that belong to a topic in a corpus and the words that belong to a topic. LDA is based on probabilistic graphical modeling while NMF relies on linear algebra. Both algorithms take as input a bag of words matrix (i.e., each document represented as a row, with each columns containing the count of words in the corpus). The aim of each algorithm is then to produce 2 smaller matrices; a document to topic matrix and a word to topic matrix that when multiplied together reproduce the bag of words matrix with the lowest error.

The resulting matrices from the NMF algorithm

No magic here — you need to specify the number of topics!

How many topics? Well that is the question! Both NMF and LDA are not able to automatically determine the number of topics and this must be specified.

Dataset Preprocessing

I searched far and wide for an exciting dataset and finally selected the 20 Newsgoups dataset. I’m just being sarcastic — I selected a dataset that is both easy to interpret and load in Scikit Learn. The dataset is easy to interpret because the 20 Newsgroups are known and the generated topics can be compared to the known topics being discussed. Headers, footers and quotes are excluded from the dataset.

The creation of the bag of words matrix is very easy in Scikit Learn — all the heavy lifting is done by the feature extraction functionality provided for text datasets. A tf-idf transformer is applied to the bag of words matrix that NMF must process with the TfidfVectorizer. LDA on the other hand, being a probabilistic graphical model (i.e. dealing with probabilities) only requires raw counts, so a CountVectorizer is used. Stop words are removed and the number of terms included in the bag of words matrix is restricted to the top 1000.

NMF and LDA with Scikit Learn

As mentioned previously the algorithms are not able to automatically determine the number of topics and this value must be set when running the algorithm. Comprehensive documentation on available parameters is available for both NMF and LDA. Initialising the W and H matrices in NMF with ‘nndsvd’ rather than random initialisation improves the time it takes for NMF to converge. LDA can also be set to run in either batch or online mode.

Displaying and Evaluating Topics

The structure of the resulting matrices returned by both NMF and LDA is the same and the Scikit Learn interface to access the returned matrices is also the same. This is great and allows for a common Python method that is able to display the top words in a topic. Topics are not labeled by the algorithm — a numeric index is assigned.

The derived topics from NMF and LDA are displayed below. From the NMF derived topics, Topic 0 and 8 don’t seem to be about anything in particular but the other topics can be interpreted based upon there top words. LDA for the 20 Newsgroups dataset produces 2 topics with noisy data (i.e., Topic 4 and 7) and also some topics that are hard to interpret (i.e., Topic 3 and Topic 9). I’d say the NMF was able to find more meaningful topics in the 20 Newsgroups dataset.

NMF Topics:

Topic 0: people don think like know time right good did say

Topic 1: windows file use dos files window using program problem card

Topic 2: god jesus bible christ faith believe christian christians church sin

Topic 3: drive scsi drives hard disk ide controller floppy cd mac

Topic 4: game team year games season players play hockey win player

Topic 5: key chip encryption clipper keys government escrow public use algorithm

Topic 6: thanks does know mail advance hi anybody info looking help

Topic 7: car new 00 sale price 10 offer condition shipping 20

Topic 8: just like don thought ll got oh tell mean fine

Topic 9: edu soon cs university com email internet article ftp send

LDA Topics:

Topic 0: government people mr law gun state president states public use

Topic 1: drive card disk bit scsi use mac memory thanks pc

Topic 2: said people armenian armenians turkish did saw went came women

Topic 3: year good just time game car team years like think

Topic 4: 10 00 15 25 12 11 20 14 17 16

Topic 5: windows window program version file dos use files available display

Topic 6: edu file space com information mail data send available program

Topic 7: ax max b8f g9v a86 pl 145 1d9 0t 34u

Topic 8: god people jesus believe does say think israel christian true

Topic 9: don know like just think ve want does use good

In my next blog post, I’ll discuss topic interpretation and show how top documents within a theme can also be displayed.

Full Code Listing

It’s amazing how much can be achieved with just 36 lines of Python code and some Scikit Learn magic. The full code listing is provided below:","['scikit', 'topics', 'modeling', 'lda', 'matrix', 'dataset', 'learn', 'topic', 'nmf', 'words', 'matrices']","NMF has been included in Scikit Learn for quite a while but LDA has only recently (late 2015) been included.
The great thing about using Scikit Learn is that it brings API consistency which makes it almost trivial to perform Topic Modeling using both LDA and NMF.
Scikit Learn also includes seeding options for NMF which greatly helps with algorithm convergence and offers both online and batch variants of LDA.
I’m just being sarcastic — I selected a dataset that is both easy to interpret and load in Scikit Learn.
Full Code ListingIt’s amazing how much can be achieved with just 36 lines of Python code and some Scikit Learn magic.",en,['Aneesha Bakharia'],2017-12-19 11:14:27.426000+00:00,"{'Lda', 'Data Science', 'Scikit Learn', 'Topic Modeling', 'Machine Learning'}","{'https://miro.medium.com/fit/c/160/160/1*3yjjoZ--eh8E84rH1xRIhQ.jpeg', 'https://miro.medium.com/fit/c/96/96/0*buwULpatMyE7WR74.png', 'https://miro.medium.com/fit/c/80/80/2*R6303tLavDAf6jJAsMlaJQ.jpeg', 'https://miro.medium.com/fit/c/160/160/0*buwULpatMyE7WR74.png', 'https://miro.medium.com/max/940/1*MLJVWz4EdOFsqhvBxEi9iA.png', 'https://miro.medium.com/fit/c/80/80/0*JAI6AWapx2N6wRi_', 'https://miro.medium.com/max/1880/1*MLJVWz4EdOFsqhvBxEi9iA.png', 'https://miro.medium.com/max/160/1*0ykYC2ubehxNL5XrIIbscg.png', 'https://miro.medium.com/max/60/1*MLJVWz4EdOFsqhvBxEi9iA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*F_zsqP5R4QRex-r2ajh09A@2x.jpeg'}",2020-03-05 00:11:41.603444,0.9631454944610596
https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2,"Why, How and When to apply Feature Selection","Modern day datasets are very rich in information with data collected from millions of IoT devices and sensors. This makes the data high dimensional and it is quite common to see datasets with hundreds of features and is not unusual to see it go to tens of thousands.

Feature Selection is a very critical component in a Data Scientist’s workflow. When presented data with very high dimensionality, models usually choke because

Training time increases exponentially with number of features. Models have increasing risk of overfitting with increasing number of features.

Feature Selection methods helps with these problems by reducing the dimensions without much loss of the total information. It also helps to make sense of the features and its importance.

In this article, I discuss following feature selection techniques and their traits.

Filter Methods Wrapper Methods and Embedded Methods.

Filter Methods

Filter Methods considers the relationship between features and the target variable to compute the importance of features.

F Test

F Test is a statistical test used to compare between models and check if the difference is significant between the model.

F-Test does a hypothesis testing model X and Y where X is a model created by just a constant and Y is the model created by a constant and a feature.

The least square errors in both the models are compared and checks if the difference in errors between model X and Y are significant or introduced by chance.

F-Test is useful in feature selection as we get to know the significance of each feature in improving the model.

Scikit learn provides the Selecting K best features using F-Test.

sklearn.feature_selection.f_regression

For Classification tasks

sklearn.feature_selection.f_classif

There are some drawbacks of using F-Test to select your features. F-Test checks for and only captures linear relationships between features and labels. A highly correlated feature is given higher score and less correlated features are given lower score.

Correlation is highly deceptive as it doesn’t capture strong non-linear relationships.

2. Using summary statistics like correlation may be a bad idea, as illustrated by Anscombe’s quartet.

Francis Anscombe illustrates how four distinct datasets have same mean, variance and correlation to emphasize ‘summary statistics’ does not completely describe the datasets and can be quite deceptive.

Mutual Information

Mutual Information between two variables measures the dependence of one variable to another. If X and Y are two variables, and

If X and Y are independent, then no information about Y can be obtained by knowing X or vice versa. Hence their mutual information is 0. If X is a deterministic function of Y, then we can determine X from Y and Y from X with mutual information 1. When we have Y = f(X,Z,M,N), 0 < mutual information < 1

We can select our features from feature space by ranking their mutual information with the target variable.

Advantage of using mutual information over F-Test is, it does well with the non-linear relationship between feature and target variable.

Sklearn offers feature selection with Mutual Information for regression and classification tasks.","['information', 'models', 'feature', 'y', 'apply', 'selection', 'mutual', 'model', 'x', 'features', 'using']","Feature Selection is a very critical component in a Data Scientist’s workflow.
Feature Selection methods helps with these problems by reducing the dimensions without much loss of the total information.
In this article, I discuss following feature selection techniques and their traits.
F-Test is useful in feature selection as we get to know the significance of each feature in improving the model.
Sklearn offers feature selection with Mutual Information for regression and classification tasks.",en,['Sudharsan Asaithambi'],2018-02-01 04:52:57.578000+00:00,"{'Data Science', 'Artificial Intelligence', 'Data Visualization', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/1980/1*oMbcPjuDprAu_QAGizFf7g.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*YnXZS86uR2HibB3jlJB10A.png?q=20', 'https://miro.medium.com/max/1604/1*jb_Zlb85QsArbwC6PEjrHg.png', 'https://miro.medium.com/max/60/1*Kr8bbyVaQNLCZBPUXs8b1g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*qXqx7_hDtsO9ez7_nxSXOw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*m15OB5BljiCFlDDXBqflYA.png?q=20', 'https://miro.medium.com/max/60/1*2236JElUGuc6eLA3Miidlw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ntNJGMVqNNnESx9k3HBZyA.jpeg', 'https://miro.medium.com/max/60/1*SGai7lOKRn9YhM2p-8SChQ.jpeg?q=20', 'https://miro.medium.com/max/2464/1*m15OB5BljiCFlDDXBqflYA.png', 'https://miro.medium.com/max/2980/1*Kr8bbyVaQNLCZBPUXs8b1g.png', 'https://miro.medium.com/max/1538/1*2236JElUGuc6eLA3Miidlw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/3832/1*qXqx7_hDtsO9ez7_nxSXOw.png', 'https://miro.medium.com/max/1240/1*SGai7lOKRn9YhM2p-8SChQ.jpeg', 'https://miro.medium.com/max/58/1*jb_Zlb85QsArbwC6PEjrHg.png?q=20', 'https://miro.medium.com/max/620/1*SGai7lOKRn9YhM2p-8SChQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*ntNJGMVqNNnESx9k3HBZyA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3220/1*YnXZS86uR2HibB3jlJB10A.png', 'https://miro.medium.com/max/60/1*oMbcPjuDprAu_QAGizFf7g.png?q=20'}",2020-03-05 00:11:47.870313,6.266869068145752
https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637,The use of KNN for missing values,"Problem:

When working with real world data, you will often encounter missing values in your data-set. How you deal with them can be crucial for your analysis and the conclusion you will draw.

Missing values can be of three general types:

Missing Completely At Random (MCAR): When missing data are MCAR, the presence/absence of data is completely independent of observable variables and parameters of interest. In this case, the analysis performed on the data are unbiased. In practice, it is highly unlikely. Missing At Random (MAR): When missing data is not random but can be totally related to a variable where there is complete information. An example is that males are less likely to fill in a depression survey but this has nothing to do with their level of depression, after accounting for maleness. This kind of missing data can induce a bias in your analysis especially if it unbalances your data because of many missing values in a certain category. Missing Not At Random (MNAR): When the missing values are neither MCAR nor MAR. In the previous example that would be the case if people tended not to answer the survey depending on their depression level.

In this tutorial we will use a non-parametric algorithm called k-nearest-neighbors (KNN) to replace missing values. This algorithm is applicable in any of the three previous situation, as long as there is a relationship between the variable with the missing value and the other variables.

Why using KNN ?

KNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data.

The assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables.

Let’s keep the previous example and add another variable, the income of the person. Now we have three variables, the gender, the income and the level of depression that has missing values. We then assume that people of similar income and of same gender tend to have the same level of depression. For a given missing value, we will look at the gender of the person, its income, look for its k nearest neighbors and get their level of depression. We can then approximate the depression level of the person we wanted.

KNN parameters calibration

When using KNN, you have to take many parameters into consideration. You will find below what is allowed by the function provided with this article:

The number of neighbors to look for. Taking a low k will increase the influence of noise and the results are going to be less generalizable. On the other hand, taking a high k will tend to blur local effects which are exactly what we are looking for. It is also recommended to take an odd k for binary classes to avoid ties.

to look for. Taking a will increase the influence of noise and the results are going to be less generalizable. On the other hand, taking a will tend to blur local effects which are exactly what we are looking for. It is also recommended to take an for binary classes to avoid ties. The aggregation method to use. Here we allow for arithmetic mean, median and mode for numeric variables and mode for categorical ones.

to use. Here we allow for arithmetic mean, median and mode for numeric variables and mode for categorical ones. Normalizing the data is a method that allows to give every attribute the same influence in identifying neighbors when computing certain type of distances like the Euclidean one. You should normalize your data when the scales have no meaning and/or you have inconsistent scales like centimeters and meters. It implies prior knowledge on the data to know which one are more important. The algorithm automatically normalize the data when both numeric and categorical variable are provided.

is a method that allows to give every attribute the same influence in identifying neighbors when computing certain type of distances like the one. You should normalize your data when the scales have no meaning and/or you have inconsistent scales like centimeters and meters. It implies prior knowledge on the data to know which one are more important. The algorithm automatically normalize the data when both numeric and categorical variable are provided. Numeric attribute distances: among the various distance metrics available, we will focus on the main ones, Euclidean and Manhattan. Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, height, etc…).

among the various distance metrics available, we will focus on the main ones, Euclidean and Manhattan. Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, height, etc…). Categorical attribute distances: without prior transformation, applicable distances are related to frequency and similarity. Here we allow the use of two distances: Hamming distance and the Weighted Hamming distance.



- Hamming distance: take all the categorical attributes and for each, count one if the value is not the same between two points. The Hamming distance is then the number of attributes for which the value was different.



- Weighted Hamming distance: also return one if the value is different, but returns the frequency of the value in the attribute if they are matching, increasing the distance when the value is more frequent. When more than one attribute is categorical, the harmonic mean is applied. The result remain between zero and one but the mean value is shifted toward the lower values compared to the arithmetic mean .

without prior transformation, applicable distances are related to frequency and similarity. Here we allow the use of two distances: Hamming distance and the Weighted Hamming distance. - take all the categorical attributes and for each, count one if the value is not the same between two points. The Hamming distance is then the number of attributes for which the value was different. - also return one if the value is different, but returns the frequency of the value in the attribute if they are matching, increasing the distance when the value is more frequent. When more than one attribute is categorical, the is applied. The result remain between zero and one but the mean value is shifted toward the lower values compared to the . Binary attribute distances: those attributes are generally obtained via categorical variables transformed into dummies. As already mentioned for the continuous variables, the Euclidean distance can also be applied here. However there is also another metric based on dissimilarity that can be used, the Jaccard distance.

In order to identify the best distance metric to use for your data, you can use the tips given above but above all, you will have to experiment to find what best improves your model.","['variables', 'hamming', 'distance', 'data', 'categorical', 'distances', 'attribute', 'missing', 'values', 'knn', 'value']","Missing At Random (MAR): When missing data is not random but can be totally related to a variable where there is complete information.
This kind of missing data can induce a bias in your analysis especially if it unbalances your data because of many missing values in a certain category.
Missing Not At Random (MNAR): When the missing values are neither MCAR nor MAR.
In this tutorial we will use a non-parametric algorithm called k-nearest-neighbors (KNN) to replace missing values.
The assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables.",en,['Yohan Obadia'],2018-02-21 13:33:54.763000+00:00,"{'Knn', 'Data Science', 'Towards Data Science', 'Programming', 'Missing Values'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/8064/1*L_kzed3FfjLJV0oBjIF-eA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*L_kzed3FfjLJV0oBjIF-eA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*xaJeuRItZGi3rUXpKSZ5Vw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*xaJeuRItZGi3rUXpKSZ5Vw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*L_kzed3FfjLJV0oBjIF-eA.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:11:54.739418,6.867586135864258
https://towardsdatascience.com/a-flask-api-for-serving-scikit-learn-models-c8bcdaa41daa,A Flask API for serving scikit-learn models,"Scikit-learn is an intuitive and powerful Python machine learning library that makes training and validating many models fairly easy. Scikit-learn models can be persisted (pickled) to avoid retraining the model every time they are used. You can use Flask to create an API that can provide predictions based on a set of input variables using a pickled model.

Before we get into Flask it’s important to point out that scikit-learn does not handle categorical variables and missing values. Categorical variables need to be encoded as numeric values. Typically categorical variables are transformed using OneHotEncoder (OHE) or LabelEncoder. LabelEncoder assigns an integer to each categorical value and transforms the original variable to a new variable with corresponding integers replaced for categorical variables. The problem with this approach is that a nominal variable is effectively transformed to an ordinal variable which may fool a model into thinking that the order is meaningful. OHE, on the other hand, does not suffer from this issue, however it tends to explode the number of transformed variables since a new variable is created for every value of a categorical variables.

One thing to know about LabelEncoder is that the transformation will change based on the number of categorical values in a variable. Let’s say you have a “subscription” variable with “gold” and “platinum” values. LabelEncoder will map these to 0 and 1 respectively. Now if you add the value “free” to the mix the assignment is changed (free is encoded as 0, gold to 1, and platinum to 2). For this reason it’s important to keep your original LabelEncoder around for transformation at the prediction time.","['serving', 'models', 'transformation', 'variables', 'variable', 'scikitlearn', 'flask', 'labelencoder', 'transformed', 'api', 'categorical', 'values', 'value', 'using']","Scikit-learn is an intuitive and powerful Python machine learning library that makes training and validating many models fairly easy.
Scikit-learn models can be persisted (pickled) to avoid retraining the model every time they are used.
You can use Flask to create an API that can provide predictions based on a set of input variables using a pickled model.
Before we get into Flask it’s important to point out that scikit-learn does not handle categorical variables and missing values.
LabelEncoder assigns an integer to each categorical value and transforms the original variable to a new variable with corresponding integers replaced for categorical variables.",en,['Amir Ziai'],2016-03-14 06:07:34.333000+00:00,"{'API', 'Machine Learning', 'Python'}","{'https://miro.medium.com/max/878/1*TN3K15kcQUDYPsl8SM5DcA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*Hjp43pf5PzJwldRys-qHCw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*Hjp43pf5PzJwldRys-qHCw.jpeg', 'https://miro.medium.com/max/60/1*TN3K15kcQUDYPsl8SM5DcA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/439/1*TN3K15kcQUDYPsl8SM5DcA.png'}",2020-03-05 00:11:57.944115,3.2046968936920166
https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359,Curiosity-Driven Learning made easy Part I,"In the recent years, we’ve seen a lot of innovations in Deep Reinforcement Learning. From DeepMind and the Deep Q learning architecture in 2014 to OpenAI playing Dota2 with OpenAI five in 2018, we live in an exciting and promising moment.

And today we’ll learn about Curiosity-Driven Learning, one of the most exciting and promising strategy in Deep Reinforcement Learning.

Reinforcement Learning is based on the reward hypothesis, which is the idea that each goal can be described as the maximization of the rewards. However, the current problem of extrinsic rewards (aka rewards given by the environment) is that this function is hard coded by a human, which is not scalable.

The idea of Curiosity-Driven learning, is to build a reward function that is intrinsic to the agent (generated by the agent itself). It means that the agent will be a self-learner since he will be the student but also the feedback master.

Sounds crazy? Yes but that’s a genius idea that was re-introduced in the 2017 paper Curiosity-driven Exploration by Self-supervised Prediction. The results were then improved with the second paper Large-Scale Study of Curiosity-Driven Learning.

They discovered that curiosity driven learning agents perform as good as if they had extrinsic rewards, and were able to generalize better with unexplored environments.

Edit: Curiosity has been a subject of research in Reinforcement Learning since the 90’s with the awesome work of Mr. J. Schmidhuber that you can read here.

In this first article we’ll talk about the theory and explain how works Curiosity Driven Learning in theory.

Then, in a second article, we’ll implement a Curiosity driven PPO agent playing Super Mario Bros.

Sounds fun? Let’s dive on in !

Two main problems in Reinforcement Learning

Modern RL suffers from two problems:

First, the sparse rewards or non-existing rewards problem: that is, most rewards do not contain information, and hence are set to zero. However, as rewards act as feedback for RL agents, if they don’t receive any, their knowledge of which action is appropriate (or not) cannot change.

Thanks to the reward, our agent knows that this action at that state was good

For instance, in Vizdoom “DoomMyWayHome,” your agent is only rewarded if it finds the vest. However, the vest is far away from your starting point, so most of your rewards will be zero.

Therefore, if our agent does not receive useful feedback (dense rewards), it will take much longer to learn an optimal policy.

The second big problem is that the extrinsic reward function is handmade — that is, in each environment, a human has to implement a reward function. But how we can scale that in big and complex environments?

A new reward function: curiosity

Curiosity is an intrinsic reward that is equal to the error of our agent to predict the consequence of its own actions given its current state (aka to predict the next state given current state and action taken).

Why? Because the idea of curiosity is to encourage our agent to perform actions that reduce the uncertainty in the agent’s ability to predict the consequence of its own action (uncertainty will be higher in areas where the agent has spent less time, or in areas with complex dynamics).

Consequently measuring error requires building a model of environmental dynamics that predicts the next state given the current state and the action a.

The question that we can ask here is how we can calculate this error?

To calculate curiosity, we will use a module introduced in the first paper called Intrinsic Curiosity module.

Introducing the Intrinsic Curiosity Module

The need of a good feature space

Before diving into the description of the module, we must ask ourselves how our agent can predict the next state given our current state and our action?

We know that we can define the curiosity as the error between the predicted new state (st+1) given our state st and action at and the real new state.

But, remember that most of the time, our state is a stack of 4 frames (pixels). It means that we need to find a way to predict the next stack of frames which is really hard for two reasons:

First of all, it’s hard to predict the pixels directly, imagine you’re in Doom you move left, you need to predict 248*248 = 61504 pixels!

Second, the researchers think that’s not the right thing to do and take a good example to prove it.

Imagine you need to study the movement of the tree leaves in a breeze. First of all, it’s already hard to model breeze, consequently it is much harder to predict the pixel location of each leaves at each time step.

The problem, is that because you’ll always have a big pixel prediction error, the agent will always be curious even if the movement of the leaves is not the consequence of the agent actions therefore its continued curiosity is undesirable.

Trying to predict the movement of each pixel at each timeframe is really hard

So instead of making prediction in the raw sensory space (pixels), we need to transform the raw sensory input (array of pixels) into a feature space with only relevant information.

We need to define what rules must respect a good feature space, there are 3:

Needs to model things that can be controlled by the agent.

Needs also to model things that can’t be controlled by the agent but that can affect an agent.

Needs to not model (and consequently be unaffected) by things that are not in agent’s control and have no effect on him.

Let’s take this example, your agent is a car, if we want to create a good feature representation we need to model:

The yellow boxes are the important elements

Our car (controlled by our agent), the other cars (we can’t control it but that can affect the agent) but we don’t need to model the leaves (not affect the agent and we can’t control it). This way we will have a feature representation with less noise.

The desired embedding space should:

Be compact in terms of dimensional (remove irrelevant parts of the observation space).

Preserve sufficient information about the observation.

Stable: because non-stationary rewards make it difficult for reinforcement agents to learn.

Intrinsic Curiosity Module (ICM)

ICM Taken from the Paper

The Intrinsic Curiosity Module is the system that helps us to generate curiosity. It is composed of two neural networks.

Remember, we want to only predict changes in the environment that could possibly be due to the actions of our agent or affect the agent and ignore the rest. It means, we need instead of making predictions from a raw sensory space (pixels), transform the sensory input into a feature vector where only the information relevant to the action performed by the agent is represented.

To learn this feature space: we use self-supervision, training a neural network on a proxy inverse dynamics task of predicting the agent action (ât) given its current and next states (st and st+1).

Inverse Model Part

Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space, the factors of variation in the environment that does not affect the agent itself.

Forward Model Part

Then we use this feature space to train a forward dynamics model that predicts the future representation of the next state phi(st+1), given the feature representation of the current state phi(st) and the action at.

And we provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.

Curiosity = predicted_phi(st+1) — phi(st+1)

So, we have two models in ICM:

Inverse Model (Blue): Encode the states st and st+1 into the feature vectors phi(st) and phi(st+1) that are trained to predict action ât.

Inverse Loss function that measures the difference between the real action and our predicted action

Forward Model (Red): Takes as input phi(st) and at and predict the feature representation phi(st+1) of st+1.

Forward Model Loss function

Then mathematically speaking, curiosity will be the difference between our predicted feature vector of the next state and the real feature vector of the next state.

Finally the overall optimization problem of this module is a composition of Inverse Loss, Forward Loss.","['feature', 'predict', 'easy', 'action', 'curiosity', 'rewards', 'need', 'learning', 'space', 'model', 'state', 'curiositydriven', 'agent']","And today we’ll learn about Curiosity-Driven Learning, one of the most exciting and promising strategy in Deep Reinforcement Learning.
The idea of Curiosity-Driven learning, is to build a reward function that is intrinsic to the agent (generated by the agent itself).
Yes but that’s a genius idea that was re-introduced in the 2017 paper Curiosity-driven Exploration by Self-supervised Prediction.
The results were then improved with the second paper Large-Scale Study of Curiosity-Driven Learning.
In this first article we’ll talk about the theory and explain how works Curiosity Driven Learning in theory.",en,['Thomas Simonini'],2019-10-17 08:05:21.980000+00:00,"{'Deep Learning', 'Artificial Intelligence', 'Reinforcement Learning', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/1700/1*pLDg3MIz5Q6TRsGesVmRwA.png', 'https://miro.medium.com/max/2666/1*4BiRJ-_jGRF8N1HRFInBtQ.png', 'https://miro.medium.com/max/60/1*hw9WW9_DqI2DLiK5GjOSig.png?q=20', 'https://miro.medium.com/max/1700/1*xJehwVNbkI6SdrShiSyCbQ.png', 'https://miro.medium.com/fit/c/96/96/2*NgincxJvCzm9QiunldBRmQ.png', 'https://miro.medium.com/max/60/1*GnFjuCAHDy1Hkvkmmp_SOw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2514/1*cmKEatcnl83GRZ8kJriBiQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2712/1*GnFjuCAHDy1Hkvkmmp_SOw.png', 'https://miro.medium.com/max/60/0*qvMOs9XAWBAGfCgU.jpg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*4BiRJ-_jGRF8N1HRFInBtQ.png?q=20', 'https://miro.medium.com/max/60/1*cmKEatcnl83GRZ8kJriBiQ.png?q=20', 'https://miro.medium.com/max/60/1*JHhacgi6jzpzKtReLgNE2w.png?q=20', 'https://miro.medium.com/max/60/1*_yN1FzvEFDmlObiYsstIzg.png?q=20', 'https://miro.medium.com/max/60/1*pLDg3MIz5Q6TRsGesVmRwA.png?q=20', 'https://miro.medium.com/max/4594/1*4KHYW0Xapq_hOKhFIgiRDQ.png', 'https://miro.medium.com/max/60/1*4KHYW0Xapq_hOKhFIgiRDQ.png?q=20', 'https://miro.medium.com/max/60/1*G7O492AyEu-jlOHHQvTRug.png?q=20', 'https://miro.medium.com/max/1200/0*qvMOs9XAWBAGfCgU.jpg', 'https://miro.medium.com/max/60/1*PqiptT-Cdi8uwosxuFn2DQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2400/0*qvMOs9XAWBAGfCgU.jpg', 'https://miro.medium.com/max/60/1*xJehwVNbkI6SdrShiSyCbQ.png?q=20', 'https://miro.medium.com/max/806/1*hw9WW9_DqI2DLiK5GjOSig.png', 'https://miro.medium.com/max/60/1*EsMzj_wLYR_kx1UZ2fC1dQ.png?q=20', 'https://miro.medium.com/max/60/1*mD-f5VN1SWYvhrZAbvSu_w.png?q=20', 'https://miro.medium.com/max/60/1*r5i0ZxqEWNE5nvY5thMdYg.png?q=20', 'https://miro.medium.com/max/60/1*-hRqX-e4OEcJlc6jp8rgRw.png?q=20', 'https://miro.medium.com/max/60/1*SI8itmr1PZPkXCBtgIh2Sw.png?q=20', 'https://miro.medium.com/max/2400/1*r5i0ZxqEWNE5nvY5thMdYg.png', 'https://miro.medium.com/max/1550/1*EsMzj_wLYR_kx1UZ2fC1dQ.png', 'https://miro.medium.com/max/1330/1*G7O492AyEu-jlOHHQvTRug.png', 'https://miro.medium.com/max/1620/1*-hRqX-e4OEcJlc6jp8rgRw.png', 'https://miro.medium.com/max/1000/1*_yN1FzvEFDmlObiYsstIzg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*k8gMwh8_ZVgE2bVCKZo_gA.png?q=20', 'https://miro.medium.com/max/1998/1*SI8itmr1PZPkXCBtgIh2Sw.png', 'https://miro.medium.com/max/1700/1*JHhacgi6jzpzKtReLgNE2w.png', 'https://miro.medium.com/max/1000/1*mD-f5VN1SWYvhrZAbvSu_w.png', 'https://miro.medium.com/max/1000/1*PqiptT-Cdi8uwosxuFn2DQ.png', 'https://miro.medium.com/max/1700/1*k8gMwh8_ZVgE2bVCKZo_gA.png', 'https://miro.medium.com/fit/c/160/160/2*NgincxJvCzm9QiunldBRmQ.png'}",2020-03-05 00:12:05.477379,7.53326416015625
https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40,The Dummy’s Guide to Creating Dummy Variables,"As a math person, I try to quantify everything in my daily life, so when I see a data set with lots of qualitative variables, my mind naturally tries to quantify them. Luckily, there’s a nice, neat function that can help us do that!

As someone who is new to the data science world, the discovery of pandas was pretty life-changing. Between pandas and scikit learn I think everyone could conquer the world (or at least the data science world). Pandas has a function which can turn a categorical variable into a series of zeros and ones, which makes them a lot easier to quantify and compare.

I started with loading in my data which I got from the website “http://data.princeton.edu/wws509/datasets/#salary”. This is a very small data set consisting of salary data for 52 professors at a small college, categorized by gender, professor rank, highest degree, and years of service paired with salary. I used this data set for this example because it’s short and has a few categorical variables.

sx= sex, rk = rank, yr = year in current rank, dg= degree, yd = years since earning highest degree, sl = salary

Since I loaded the data in using pandas, I used the pandas function pd.get_dummies for my first categorical variable sex. Since this variable has only two answer choices: male and female (not the most progressive data set but it is from 1985). pd.get_dummies creates a new dataframe which consists of zeros and ones. The dataframe will have a one depending on the sex of the professor in this case.

Since we’ve created a whole new dataframe, in order to compare it to our original dataframe, we’re going to need to either merge or concatenate them to work with them properly. In creating dummy variables, we essentially created new columns for our original dataset. The old and new dataset don’t have any columns in common, so it would make most sense to concatenate them (although I’m going to go through both ways).

I chose to put my dummy variable on the right side of my dataframe so when I use pd.concat (the concatenation function) and put my dataframe first, and then the dummy variable I declared. As they are columns, I concatenate them on axis=1.

Merging these dataframes is slightly more difficult as there are no overlapping columns. However, it can be done!

To merge on an index (our left-most column), all we have to do is set our left_index=True and right_index=True!

With just two lines of code, we can now compare our sex variable to our other numerical columns!","['set', 'function', 'sex', 'pandas', 'variables', 'columns', 'variable', 'world', 'data', 'rank', 'dummy', 'dataframe', 'creating', 'dummys', 'guide']","As a math person, I try to quantify everything in my daily life, so when I see a data set with lots of qualitative variables, my mind naturally tries to quantify them.
I used this data set for this example because it’s short and has a few categorical variables.
Since this variable has only two answer choices: male and female (not the most progressive data set but it is from 1985).
In creating dummy variables, we essentially created new columns for our original dataset.
With just two lines of code, we can now compare our sex variable to our other numerical columns!",en,['Rowan Langford'],2017-03-29 23:34:43.489000+00:00,"{'Data', 'Python', 'Pandas', 'Data Science'}","{'https://miro.medium.com/max/60/1*HfhgywtwXtxVcUmQuyu-_w.png?q=20', 'https://miro.medium.com/max/2704/1*HfhgywtwXtxVcUmQuyu-_w.png', 'https://miro.medium.com/max/1924/1*ZBuFyyTCcamPdQBCsK12xg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/160/160/1*bJhYONoIv-wqU1Pf6HWuaA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1464/1*Pm1PEWTTgrPPhvH0U5-0LQ.png', 'https://miro.medium.com/max/3352/1*psCS6W7FNKJ_auc9fdnA1g.png', 'https://miro.medium.com/max/60/1*psCS6W7FNKJ_auc9fdnA1g.png?q=20', 'https://miro.medium.com/max/60/1*ZBuFyyTCcamPdQBCsK12xg.png?q=20', 'https://miro.medium.com/max/1200/1*psCS6W7FNKJ_auc9fdnA1g.png', 'https://miro.medium.com/fit/c/96/96/1*bJhYONoIv-wqU1Pf6HWuaA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*Pm1PEWTTgrPPhvH0U5-0LQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:12:07.223466,1.746086835861206
https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c,How to Visualize a Decision Tree from a Random Forest in Python using Scikit-Learn,"Explanation of code

Create a model train and extract: we could use a single decision tree, but since I often employ the random forest for modeling it’s used in this example. (The trees will be slightly different from one another!).

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=10) # Train

model.fit(iris.data, iris.target)

# Extract single tree

estimator = model.estimators_[5]

2. Export Tree as .dot File: This makes use of the export_graphviz function in Scikit-Learn. There are many parameters here that control the look and information displayed. Take a look at the documentation for specifics.

from sklearn.tree import export_graphviz # Export as dot file

export_graphviz(estimator_limited,

out_file='tree.dot',

feature_names = iris.feature_names,

class_names = iris.target_names,

rounded = True, proportion = False,

precision = 2, filled = True)

3. Convert dot to png using a system command: running system commands in Python can be handy for carrying out simple tasks. This requires installation of graphviz which includes the dot utility. For the complete options for conversion, take a look at the documentation.

# Convert to png

from subprocess import call

call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

4. Visualize: the best visualizations appear in the Jupyter Notebook. (Equivalently you can use matplotlib to show images).

# Display in jupyter notebook

from IPython.display import Image

Image(filename = 'tree.png')

Considerations

With a random forest, every tree will be built differently. I use these images to display the reasoning behind a decision tree (and subsequently a random forest) rather than for specific details.

It’s helpful to limit maximum depth in your trees when you have a lot of features. Otherwise, you end up with massive trees, which look impressive, but cannot be interpreted at all! Here’s a full example with 50 features.","['system', 'forest', 'import', 'single', 'dot', 'python', 'decision', 'visualize', 'scikitlearn', 'trees', 'tree', 'random', 'look', 'jupyter', 'using']","Explanation of codeCreate a model train and extract: we could use a single decision tree, but since I often employ the random forest for modeling it’s used in this example.
Export Tree as .dot File: This makes use of the export_graphviz function in Scikit-Learn.
Convert dot to png using a system command: running system commands in Python can be handy for carrying out simple tasks.
# Display in jupyter notebookfrom IPython.display import ImageImage(filename = 'tree.png')ConsiderationsWith a random forest, every tree will be built differently.
I use these images to display the reasoning behind a decision tree (and subsequently a random forest) rather than for specific details.",en,['Will Koehrsen'],2018-08-19 12:28:32.564000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3674/1*hW67kyPZZJ6I_7Z8huwDkg.png', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*lAhJt7bvEDxT4DEdd29yCA.jpeg?q=20', 'https://miro.medium.com/max/11566/1*IPLwmH-TJRhEWXW7uaetMw.png', 'https://miro.medium.com/max/60/1*hW67kyPZZJ6I_7Z8huwDkg.png?q=20', 'https://miro.medium.com/max/4500/1*lAhJt7bvEDxT4DEdd29yCA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1200/1*lAhJt7bvEDxT4DEdd29yCA.jpeg', 'https://miro.medium.com/max/60/1*IPLwmH-TJRhEWXW7uaetMw.png?q=20'}",2020-03-05 00:12:14.462537,7.239071369171143
https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e,Outlier Detection with Isolation Forest,"Outlier Detection with Isolation Forest

Learn how to efficiently detect outliers!

Update: Part 2 describing the Extended Isolation Forest is available here.

During a recent project, I was working on a clustering problem with data collected from users of a mobile app. The goal was to classify the users in terms of their behavior, potentially with the use of K-means clustering. However, after inspecting the data it turned out that some users represented abnormal behavior — they were outliers.

A lot of machine learning algorithms suffer in terms of their performance when outliers are not taken care of. In order to avoid this kind of problem you could, for example, drop them from your sample, cap the values at some reasonable point (based on domain knowledge) or transform the data. However, in this article, I would like to focus on identifying them and leave the possible solutions for another time.

As in my case, I took a lot of features into consideration, I ideally wanted to have an algorithm that would identify the outliers in a multidimensional space. That is when I came across Isolation Forest, a method which in principle is similar to the well-known and popular Random Forest. In this article, I will focus on the Isolation Forest, without describing in detail the ideas behind decision trees and ensembles, as there is already a plethora of good sources available.

Some theory first

The main idea, which is different from other popular outlier detection methods, is that Isolation Forest explicitly identifies anomalies instead of profiling normal data points. Isolation Forest, like any tree ensemble method, is built on the basis of decision trees. In these trees, partitions are created by first randomly selecting a feature and then selecting a random split value between the minimum and maximum value of the selected feature.

In principle, outliers are less frequent than regular observations and are different from them in terms of values (they lie further away from the regular observations in the feature space). That is why by using such random partitioning they should be identified closer to the root of the tree (shorter average path length, i.e., the number of edges an observation must pass in the tree going from the root to the terminal node), with fewer splits necessary.

The idea of identifying a normal vs. abnormal observation can be observed in Figure 1 from [1]. A normal point (on the left) requires more partitions to be identified than an abnormal point (right).

Figure 1 Identifying normal vs. abnormal observations

As with other outlier detection methods, an anomaly score is required for decision making. In the case of Isolation Forest, it is defined as:

where h(x) is the path length of observation x, c(n) is the average path length of unsuccessful search in a Binary Search Tree and n is the number of external nodes. More on the anomaly score and its components can be read in [1].

Each observation is given an anomaly score and the following decision can be made on its basis:

A score close to 1 indicates anomalies

Score much smaller than 0.5 indicates normal observations

If all scores are close to 0.5 then the entire sample does not seem to have clearly distinct anomalies

Python example

Okay, so now let’s see a hand-on example. For simplicity, I will work on an artificial, 2-dimensional dataset. This way we can monitor the outlier identification process on a plot.

First, I need to generate observations. I will start with observations that will be considered normal and will be used to train the model (training and scoring in Python’s scikit-learn implementation of Isolation Forest are analogous to all other machine learning algorithms). The second group is new observations, coming from the same distribution as the training ones. Lastly, I generate outliers.

# importing libaries ----

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

from pylab import savefig

from sklearn.ensemble import IsolationForest # Generating data ----



rng = np.random.RandomState(42)



# Generating training data

X_train = 0.2 * rng.randn(1000, 2)

X_train = np.r_[X_train + 3, X_train]

X_train = pd.DataFrame(X_train, columns = ['x1', 'x2'])



# Generating new, 'normal' observation

X_test = 0.2 * rng.randn(200, 2)

X_test = np.r_[X_test + 3, X_test]

X_test = pd.DataFrame(X_test, columns = ['x1', 'x2'])



# Generating outliers

X_outliers = rng.uniform(low=-1, high=5, size=(50, 2))

X_outliers = pd.DataFrame(X_outliers, columns = ['x1', 'x2'])

Figure 2 presents the generated dataset. As desired, training and ‘normal’ observations are basically stacked on each other, while outliers are spread over. Due to the random nature of the outliers, some of them are overlapping with the training/normal observations, but I will account for that later.

Figure 2 Generated Dataset

Now I need to train the Isolation Forest on the training set. I am using the default settings here. One thing worth noting is the contamination parameter, which specifies the percentage of observations we believe to be outliers ( scikit-learn ’s default value is 0.1).

# Isolation Forest ----



# training the model

clf = IsolationForest(max_samples=100, random_state=rng)

clf.fit(X_train)



# predictions

y_pred_train = clf.predict(X_train)

y_pred_test = clf.predict(X_test)

y_pred_outliers = clf.predict(X_outliers)

Okay, so now we have the predictions. How to assess the performance? We know that the test set contains only observations from the same distribution as the normal observations. So, all of the test set observations should be classified as normal. And vice versa for the outlier set. Let’s look at the accuracy.

# new, 'normal' observations ----

print(""Accuracy:"", list(y_pred_test).count(1)/y_pred_test.shape[0])

# Accuracy: 0.93 # outliers ----

print(""Accuracy:"", list(y_pred_outliers).count(-1)/y_pred_outliers.shape[0])

# Accuracy: 0.96

At first, this looks pretty good, especially considering the default settings, however, there is one issue still to consider. As the outlier data was generated randomly, some of the outliers are actually located within the normal observations. To inspect it more carefully, I will plot the normal observation dataset together with a labeled outlier set. We can see that some of the outliers lying within the normal observation sets were correctly classified as regular observations, with a few of them being misclassified. What we could do is to try different parameter specifications (contamination, number of estimators, number of samples to draw for trining the base estimators, etc.) to get a better fit. But for now, these results are satisfactory.

Figure 3 Inspecting outlier classification

Summing up:

Isolation Forest is an outlier detection technique that identifies anomalies instead of normal observations

Similarly to Random Forest, it is built on an ensemble of binary (isolation) trees

It can be scaled up to handle large, high-dimensional datasets

This was my first article here and in case I write some more I will try to improve the level of both writing and editing. As always, any constructive feedback is welcome. You can reach out to me on Twitter or in the comments.

The code used in this article can be found on my GitHub.

You might also be interested in a simpler outlier detection algorithm — the Hampel filter. I wrote a short piece on it here.

References:

[1] Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008, December). Isolation forest. In Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on (pp. 413–422). IEEE.

[2] http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html","['observations', 'observation', 'forest', 'set', 'isolation', 'normal', 'outliers', 'data', 'training', 'detection', 'outlier']","That is when I came across Isolation Forest, a method which in principle is similar to the well-known and popular Random Forest.
Some theory firstThe main idea, which is different from other popular outlier detection methods, is that Isolation Forest explicitly identifies anomalies instead of profiling normal data points.
Isolation Forest, like any tree ensemble method, is built on the basis of decision trees.
# Isolation Forest ----# training the modelclf = IsolationForest(max_samples=100, random_state=rng)clf.fit(X_train)# predictionsy_pred_train = clf.predict(X_train)y_pred_test = clf.predict(X_test)y_pred_outliers = clf.predict(X_outliers)Okay, so now we have the predictions.
Isolation forest.",en,['Eryk Lewinson'],2019-09-26 12:17:34.725000+00:00,"{'Data Science', 'Outliers', 'Python', 'Machine Learning', 'Anomaly Detection'}","{'https://miro.medium.com/max/60/1*ylTlOUb6O4yWfmPQpd5hYg.png?q=20', 'https://miro.medium.com/max/1178/1*5TLIbEb96Wl8-b8AvByRww.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/3860/1*ujfv7WJH-tL1cRLxYEoicg.png', 'https://miro.medium.com/max/9000/1*ylTlOUb6O4yWfmPQpd5hYg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/589/1*5TLIbEb96Wl8-b8AvByRww.jpeg', 'https://miro.medium.com/max/60/1*WWidkWeV_IjUTyJs-B6KIw.png?q=20', 'https://miro.medium.com/max/60/1*Zha5PJSauUmig8gstAjflg.png?q=20', 'https://miro.medium.com/max/872/1*Zha5PJSauUmig8gstAjflg.png', 'https://miro.medium.com/max/60/1*5TLIbEb96Wl8-b8AvByRww.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/9000/1*WWidkWeV_IjUTyJs-B6KIw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*ujfv7WJH-tL1cRLxYEoicg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*KUipyYqi56L5EZiI9qe6JQ.png', 'https://miro.medium.com/fit/c/96/96/1*KUipyYqi56L5EZiI9qe6JQ.png'}",2020-03-05 00:12:21.416322,6.953784465789795
https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c,Selecting Subsets of Data in Pandas: Part 1,"Part 1: Selection with [ ] , .loc and .iloc

This is the beginning of a four-part series on how to select subsets of data from a pandas DataFrame or Series. Pandas offers a wide variety of options for subset selection which necessitates multiple articles. This series is broken down into the following four topics.

Become an Expert

If you want to be trusted to make decisions using pandas, you must become an expert. I have completely mastered pandas and have developed courses and exercises that will massively improve your knowledge and efficiency to do data analysis.

Assumptions before we begin

These series of articles assume you have no knowledge of pandas, but that you understand the fundamentals of the Python programming language. It also assumes that you have installed pandas on your machine.

The easiest way to get pandas along with Python and the rest of the main scientific computing libraries is to install the Miniconda distribution (follow the link for a comprehensive tutorial).

If you have no knowledge of Python then I suggest completing an introductory book like Exercise Python cover to cover.

The importance of making subset selections

You might be wondering why there need to be so many articles on selecting subsets of data. This topic is extremely important to pandas and it’s unfortunate that it is fairly complicated because subset selection happens frequently during an actual analysis. Because you are frequently making subset selections, you need to master it in order to make your life with pandas easier.

Always reference the documentation

The material in this article is also covered in the official pandas documentation on Indexing and Selecting Data. I highly recommend that you read that part of the documentation along with this tutorial. In fact, the documentation is one of the primary means for mastering pandas. I wrote a step-by-step article, How to Learn Pandas, which gives suggestions on how to use the documentation as you master pandas.

The anatomy of a DataFrame and a Series

The pandas library has two primary containers of data, the DataFrame and the Series. You will spend nearly all your time working with both of the objects when you use pandas. The DataFrame is used more than the Series, so let’s take a look at an image of it first.

Anatomy of a DataFrame

This image comes with some added illustrations to highlight its components. At first glance, the DataFrame looks like any other two-dimensional table of data that you have seen. It has rows and it has columns. Technically, there are three main components of the DataFrame.

The three components of a DataFrame

A DataFrame is composed of three different components, the index, columns, and the data. The data is also known as the values.

The index represents the sequence of values on the far left-hand side of the DataFrame. All the values in the index are in bold font. Each individual value of the index is called a label. Sometimes the index is referred to as the row labels. In the example above, the row labels are not very interesting and are just the integers beginning from 0 up to n-1, where n is the number of rows in the table. Pandas defaults DataFrames with this simple index.

The columns are the sequence of values at the very top of the DataFrame. They are also in bold font. Each individual value of the columns is called a column, but can also be referred to as column name or column label.

Everything else not in bold font is the data or values. You will sometimes hear DataFrames referred to as tabular data. This is just another name for a rectangular table data with rows and columns.

Axis and axes

It is also common terminology to refer to the rows or columns as an axis. Collectively, we call them axes. So, a row is an axis and a column is another axis.

The word axis appears as a parameter in many DataFrame methods. Pandas allows you to choose the direction of how the method will work with this parameter. This has nothing to do with subset selection so you can just ignore it for now.

Each row has a label and each column has a label

The main takeaway from the DataFrame anatomy is that each row has a label and each column has a label. These labels are used to refer to specific rows or columns in the DataFrame. It’s the same as how humans use names to refer to specific people.

What is subset selection?

Before we start doing subset selection, it might be good to define what it is. Subset selection is simply selecting particular rows and columns of data from a DataFrame (or Series). This could mean selecting all the rows and some of the columns, some of the rows and all of the columns, or some of each of the rows and columns.

Master Data Analysis with Python

Master Data Analysis with Python is an extremely comprehensive course that will help you learn pandas to do data analysis.

I believe that it is the best possible resource available for learning how to data analysis with pandas and provide a 30-day 100% money back guarantee if you are not satisfied.

Example selecting some columns and all rows

Let’s see some images of subset selection. We will first look at a sample DataFrame with fake data.

Sample DataFrame

Let’s say we want to select just the columns color , age , and height but keep all the rows.

Our final DataFrame would look like this:

Example selecting some rows and all columns

We can also make selections that select just some of the rows. Let’s select the rows with labels Aaron and Dean along with all of the columns:

Our final DataFrame would like:

Example selecting some rows and some columns

Let’s combine the selections from above and select the columns color , age , and height for only the rows with labels Aaron and Dean .

Our final DataFrame would look like this:

Pandas dual references: by label and by integer location

We already mentioned that each row and each column have a specific label that can be used to reference them. This is displayed in bold font in the DataFrame.

But, what hasn’t been mentioned, is that each row and column may be referenced by an integer as well. I call this integer location. The integer location begins at 0 and ends at n-1 for each row and column. Take a look above at our sample DataFrame one more time.

The rows with labels Aaron and Dean can also be referenced by their respective integer locations 2 and 4. Similarly, the columns color , age and height can be referenced by their integer locations 1, 3, and 4.

The documentation refers to integer location as position. I don’t particularly like this terminology as its not as explicit as integer location. The key thing term here is INTEGER.

What’s the difference between indexing and selecting subsets of data?

The documentation uses the term indexing frequently. This term is essentially just a one-word phrase to say ‘subset selection’. I prefer the term subset selection as, again, it is more descriptive of what is actually happening. Indexing is also the term used in the official Python documentation.

Focusing only on [] , .loc , and .iloc

There are many ways to select subsets of data, but in this article we will only cover the usage of the square brackets ( [] ), .loc and .iloc . Collectively, they are called the indexers. These are by far the most common ways to select data. A different part of this Series will discuss a few methods that can be used to make subset selections.

If you have a DataFrame, df , your subset selection will look something like the following:

df[ ]

df.loc[ ]

df.iloc[ ]

A real subset selection will have something inside of the square brackets. All selections in this article will take place inside of those square brackets.

Notice that the square brackets also follow .loc and .iloc . All indexing in Python happens inside of these square brackets.

A term for just those square brackets

The term indexing operator is used to refer to the square brackets following an object. The .loc and .iloc indexers also use the indexing operator to make selections. I will use the term just the indexing operator to refer to df[] . This will distinguish it from df.loc[] and df.iloc[] .

Read in data into a DataFrame with read_csv

Let’s begin using pandas to read in a DataFrame, and from there, use the indexing operator by itself to select subsets of data. All the data for these tutorials are in the data directory.

We will use the read_csv function to read in data into a DataFrame. We pass the path to the file as the first argument to the function. We will also use the index_col parameter to select the first column of data as the index (more on this later).

>>> import pandas as pd

>>> import numpy as np >>> df = pd.read_csv('data/sample_data.csv', index_col=0)

>>> df

Extracting the individual DataFrame components

Earlier, we mentioned the three components of the DataFrame. The index, columns and data (values). We can extract each of these components into their own variables. Let’s do that and then inspect them:

>>> index = df.index

>>> columns = df.columns

>>> values = df.values >>> index

Index(['Jane', 'Niko', 'Aaron', 'Penelope', 'Dean', 'Christina',

'Cornelia'], dtype='object') >>> columns

Index(['state', 'color', 'food', 'age', 'height', 'score'],

dtype='object') >>> values

array([['NY', 'blue', 'Steak', 30, 165, 4.6],

['TX', 'green', 'Lamb', 2, 70, 8.3],

['FL', 'red', 'Mango', 12, 120, 9.0],

['AL', 'white', 'Apple', 4, 80, 3.3],

['AK', 'gray', 'Cheese', 32, 180, 1.8],

['TX', 'black', 'Melon', 33, 172, 9.5],

['TX', 'red', 'Beans', 69, 150, 2.2]], dtype=object)

Data types of the components

Let’s output the type of each component to understand exactly what kind of object they are.

>>> type(index)

pandas.core.indexes.base.Index >>> type(columns)

pandas.core.indexes.base.Index >>> type(values)

numpy.ndarray

Understanding these types

Interestingly, both the index and the columns are the same type. They are both a pandas Index object. This object is quite powerful in itself, but for now you can just think of it as a sequence of labels for either the rows or the columns.

The values are a NumPy ndarray , which stands for n-dimensional array, and is the primary container of data in the NumPy library. Pandas is built directly on top of NumPy and it's this array that is responsible for the bulk of the workload.

Beginning with just the indexing operator on DataFrames

We will begin our journey of selecting subsets by using just the indexing operator on a DataFrame. Its main purpose is to select a single column or multiple columns of data.

Selecting a single column as a Series

To select a single column of data, simply put the name of the column in-between the brackets. Let’s select the food column:

>>> df['food']

Jane Steak

Niko Lamb

Aaron Mango

Penelope Apple

Dean Cheese

Christina Melon

Cornelia Beans

Name: food, dtype: object

Anatomy of a Series

Selecting a single column of data returns the other pandas data container, the Series. A Series is a one-dimensional sequence of labeled data. There are two main components of a Series, the index and the data(or values). There are NO columns in a Series.

The visual display of a Series is just plain text, as opposed to the nicely styled table for DataFrames. The sequence of person names on the left is the index. The sequence of food items on the right is the values.

You will also notice two extra pieces of data on the bottom of the Series. The name of the Series becomes the old-column name. You will also see the data type or dtype of the Series. You can ignore both these items for now.

Selecting multiple columns with just the indexing operator

It’s possible to select multiple columns with just the indexing operator by passing it a list of column names. Let’s select color , food , and score :

>>> df[['color', 'food', 'score']]

Selecting multiple columns returns a DataFrame

Selecting multiple columns returns a DataFrame. You can actually select a single column as a DataFrame with a one-item list:

df[['food']]

Although, this resembles the Series from above, it is technically a DataFrame, a different object.

Column order doesn’t matter

When selecting multiple columns, you can select them in any order that you choose. It doesn’t have to be the same order as the original DataFrame. For instance, let’s select height and color .

df[['height', 'color']]

Exceptions

There are a couple common exceptions that arise when doing selections with just the indexing operator.

If you misspell a word, you will get a KeyError

If you forgot to use a list to contain multiple columns you will also get a KeyError

>>> df['hight']

KeyError: 'hight' >>> df['color', 'age'] # should be: df[['color', 'age']]

KeyError: ('color', 'age')

Summary of just the indexing operator

Its primary purpose is to select columns by the column names

Select a single column as a Series by passing the column name directly to it: df['col_name']

Select multiple columns as a DataFrame by passing a list to it: df[['col_name1', 'col_name2']]

to it: You actually can select rows with it, but this will not be shown here as it is confusing and not used often.

Getting started with .loc

The .loc indexer selects data in a different way than just the indexing operator. It can select subsets of rows or columns. It can also simultaneously select subsets of rows and columns. Most importantly, it only selects data by the LABEL of the rows and columns.

Select a single row as a Series with .loc

The .loc indexer will return a single row as a Series when given a single row label. Let's select the row for Niko .

>>> df.loc['Niko']

state TX

color green

food Lamb

age 2

height 70

score 8.3

Name: Niko, dtype: object

We now have a Series, where the old column names are now the index labels. The name of the Series has become the old index label, Niko in this case.

Select multiple rows as a DataFrame with .loc

To select multiple rows, put all the row labels you want to select in a list and pass that to .loc . Let's select Niko and Penelope .

>>> df.loc[['Niko', 'Penelope']]

Use slice notation to select a range of rows with .loc

It is possible to ‘slice’ the rows of a DataFrame with .loc by using slice notation. Slice notation uses a colon to separate start, stop and step values. For instance we can select all the rows from Niko through Dean like this:

>>> df.loc['Niko':'Dean']

.loc includes the last value with slice notation

Notice that the row labeled with Dean was kept. In other data containers such as Python lists, the last value is excluded.

Other slices

You can use slice notation similarly to how you use it with lists. Let’s slice from the beginning through Aaron :

>>> df.loc[:'Aaron']

Slice from Niko to Christina stepping by 2:

>>> df.loc['Niko':'Christina':2]

Slice from Dean to the end:

>>> df.loc['Dean':]

Selecting rows and columns simultaneously with .loc

Unlike just the indexing operator, it is possible to select rows and columns simultaneously with .loc . You do it by separating your row and column selections by a comma. It will look something like this:

>>> df.loc[row_selection, column_selection]

Select two rows and three columns

For instance, if we wanted to select the rows Dean and Cornelia along with the columns age , state and score we would do this:

>>> df.loc[['Dean', 'Cornelia'], ['age', 'state', 'score']]

Use any combination of selections for either row or columns for .loc

Row or column selections can be any of the following as we have already seen:

A single label

A list of labels

A slice with labels

We can use any of these three for either row or column selections with .loc . Let's see some examples.

Let’s select two rows and a single column:

>>> df.loc[['Dean', 'Aaron'], 'food']

Dean Cheese

Aaron Mango

Name: food, dtype: object

Select a slice of rows and a list of columns:

>>> df.loc['Jane':'Penelope', ['state', 'color']]

Select a single row and a single column. This returns a scalar value.

>>> df.loc['Jane', 'age']

30

Select a slice of rows and columns

>>> df.loc[:'Dean', 'height':]

Selecting all of the rows and some columns

It is possible to select all of the rows by using a single colon. You can then select columns as normal:

>>> df.loc[:, ['food', 'color']]

You can also use this notation to select all of the columns:

>>> df.loc[['Penelope','Cornelia'], :]

But, it isn’t necessary as we have seen, so you can leave out that last colon:

>>> df.loc[['Penelope','Cornelia']]

Assign row and column selections to variables

It might be easier to assign row and column selections to variables before you use .loc . This is useful if you are selecting many rows or columns:

>>> rows = ['Jane', 'Niko', 'Dean', 'Penelope', 'Christina']

>>> cols = ['state', 'age', 'height', 'score']

>>> df.loc[rows, cols]

Summary of .loc

Only uses labels

Can select rows and columns simultaneously

Selection can be a single label, a list of labels or a slice of labels

Put a comma between row and column selections

If you are enjoying this article, consider purchasing the All Access Pass! which includes all my current and future material for one low price.

Getting started with .iloc

The .iloc indexer is very similar to .loc but only uses integer locations to make its selections. The word .iloc itself stands for integer location so that should help with remember what it does.

Selecting a single row with .iloc

By passing a single integer to .iloc , it will select one row as a Series:

>>> df.iloc[3]

state AL

color white

food Apple

age 4

height 80

score 3.3

Name: Penelope, dtype: object

Selecting multiple rows with .iloc

Use a list of integers to select multiple rows:

>>> df.iloc[[5, 2, 4]] # remember, don't do df.iloc[5, 2, 4]

Use slice notation to select a range of rows with .iloc

Slice notation works just like a list in this instance and is exclusive of the last element

>>> df.iloc[3:5]

Select 3rd position until end:

>>> df.iloc[3:]

Select 3rd position to end by 2:

>>> df.iloc[3::2]

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!

Selecting rows and columns simultaneously with .iloc

Just like with .iloc any combination of a single integer, lists of integers or slices can be used to select rows and columns simultaneously. Just remember to separate the selections with a comma.

Select two rows and two columns:

>>> df.iloc[[2,3], [0, 4]]

Select a slice of the rows and two columns:

>>> df.iloc[3:6, [1, 4]]

Select slices for both

>>> df.iloc[2:5, 2:5]

Select a single row and column

>>> df.iloc[0, 2]

'Steak'

Select all the rows and a single column

>>> df.iloc[:, 5]

Jane 4.6

Niko 8.3

Aaron 9.0

Penelope 3.3

Dean 1.8

Christina 9.5

Cornelia 2.2

Name: score, dtype: float64

Deprecation of .ix

Early in the development of pandas, there existed another indexer, ix . This indexer was capable of selecting both by label and by integer location. While it was versatile, it caused lots of confusion because it's not explicit. Sometimes integers can also be labels for rows or columns. Thus there were instances where it was ambiguous.

You can still call .ix , but it has been deprecated, so please never use it.

Selecting subsets of Series

We can also, of course, do subset selection with a Series. Earlier I recommended using just the indexing operator for column selection on a DataFrame. Since Series do not have columns, I suggest using only .loc and .iloc . You can use just the indexing operator, but its ambiguous as it can take both labels and integers. I will come back to this at the end of the tutorial.

Typically, you will create a Series by selecting a single column from a DataFrame. Let’s select the food column:

>>> food = df['food']

>>> food

Jane Steak

Niko Lamb

Aaron Mango

Penelope Apple

Dean Cheese

Christina Melon

Cornelia Beans

Name: food, dtype: object

Series selection with .loc

Series selection with .loc is quite simple, since we are only dealing with a single dimension. You can again use a single row label, a list of row labels or a slice of row labels to make your selection. Let's see several examples.

Let’s select a single value:

>>> food.loc['Aaron']

'Mango'

Select three different values. This returns a Series:

>>> food.loc[['Dean', 'Niko', 'Cornelia']]

Dean Cheese

Niko Lamb

Cornelia Beans

Name: food, dtype: object

Slice from Niko to Christina - is inclusive of last index

>>> food.loc['Niko':'Christina']

Niko Lamb

Aaron Mango

Penelope Apple

Dean Cheese

Christina Melon

Name: food, dtype: object

Slice from Penelope to the end:

>>> food.loc['Penelope':]

Penelope Apple

Dean Cheese

Christina Melon

Cornelia Beans

Name: food, dtype: object

Select a single value in a list which returns a Series

>>> food.loc[['Aaron']]

Aaron Mango

Name: food, dtype: object

Series selection with .iloc

Series subset selection with .iloc happens similarly to .loc except it uses integer location. You can use a single integer, a list of integers or a slice of integers. Let's see some examples.

Select a single value:

>>> food.iloc[0]

'Steak'

Use a list of integers to select multiple values:

>>> food.iloc[[4, 1, 3]]

Dean Cheese

Niko Lamb

Penelope Apple

Name: food, dtype: object

Use a slice — is exclusive of last integer

>>> food.iloc[4:6]

Dean Cheese

Christina Melon

Name: food, dtype: object

Comparison to Python lists and dictionaries

It may be helpful to compare pandas ability to make selections by label and integer location to that of Python lists and dictionaries.

Python lists allow for selection of data only through integer location. You can use a single integer or slice notation to make the selection but NOT a list of integers.

Let’s see examples of subset selection of lists using integers:

>>> some_list = ['a', 'two', 10, 4, 0, 'asdf', 'mgmt', 434, 99] >>> some_list[5]

'asdf' >>> some_list[-1]

99 >>> some_list[:4]

['a', 'two', 10, 4] >>> some_list[3:]

[4, 0, 'asdf', 'mgmt', 434, 99] >>> some_list[2:6:3]

[10, 'asdf']

Selection by label with Python dictionaries

All values in each dictionary are labeled by a key. We use this key to make single selections. Dictionaries only allow selection with a single label. Slices and lists of labels are not allowed.

>>> d = {'a':1, 'b':2, 't':20, 'z':26, 'A':27}

>>> d['a']

1 >>> d['A']

27

Pandas has power of lists and dictionaries

DataFrames and Series are able to make selections with integers like a list and with labels like a dictionary.

Extra Topics

There are a few more items that are important and belong in this tutorial and will be mentioned now.

Using just the indexing operator to select rows from a DataFrame — Confusing!

Above, I used just the indexing operator to select a column or columns from a DataFrame. But, it can also be used to select rows using a slice. This behavior is very confusing in my opinion. The entire operation changes completely when a slice is passed.

Let’s use an integer slice as our first example:

>>> df[3:6]

To add to this confusion, you can slice by labels as well.

>>> df['Aaron':'Christina']

I recommend not doing this!

This feature is not deprecated and completely up to you whether you wish to use it. But, I highly prefer not to select rows in this manner as can be ambiguous, especially if you have integers in your index.

Using .iloc and .loc is explicit and clearly tells the person reading the code what is going to happen. Let's rewrite the above using .iloc and .loc .

>>> df.iloc[3:6] # More explicit that df[3:6]

>>> df.loc['Aaron':'Christina']

Cannot simultaneously select rows and columns with []

An exception will be raised if you try and select rows and columns simultaneously with just the indexing operator. You must use .loc or .iloc to do so.

>>> df[3:6, 'Aaron':'Christina']

TypeError: unhashable type: 'slice'

Using just the indexing operator to select rows from a Series — Confusing!

You can also use just the indexing operator with a Series. Again, this is confusing because it can accept integers or labels. Let’s see some examples

>>> food

Jane Steak

Niko Lamb

Aaron Mango

Penelope Apple

Dean Cheese

Christina Melon

Cornelia Beans

Name: food, dtype: object >>> food[2:4]

Aaron Mango

Penelope Apple

Name: food, dtype: object >>> food['Niko':'Dean']

Niko Lamb

Aaron Mango

Penelope Apple

Dean Cheese

Name: food, dtype: object

Since Series don’t have columns you can use a single label and list of labels to make selections as well

>>> food['Dean']

'Cheese' >>> food[['Dean', 'Christina', 'Aaron']]

Dean Cheese

Christina Melon

Aaron Mango

Name: food, dtype: object

Again, I recommend against doing this and always use .iloc or .loc

Importing data without choosing an index column

We imported data by choosing the first column to be the index with the index_col parameter of the read_csv function. This is not typically how most DataFrames are read into pandas.

Usually, all the columns in the csv file become DataFrame columns. Pandas will use the integers 0 to n-1 as the labels. See the example data below with a slightly different dataset:

>>> df2 = pd.read_csv('data/sample_data2.csv')

>>> df2

The default RangeIndex

If you don’t specify a column to be the index when first reading in the data, pandas will use the integers 0 to n-1 as the index. This technically creates a RangeIndex object. Let's take a look at it.

>>> df2.index

RangeIndex(start=0, stop=7, step=1)

This object is similar to Python range objects. Let's create one:

>>> range(7)

range(0, 7)

Converting both of these objects to a list produces the exact same thing:

>>> list(df2.index)

[0, 1, 2, 3, 4, 5, 6] >>> list(range(7))

[0, 1, 2, 3, 4, 5, 6]

For now, it’s not at all important that you have a RangeIndex . Selections from it happen just the same with .loc and .iloc . Let's look at some examples.

>>> df2.loc[[2, 4, 5], ['food', 'color']]

>>> df2.iloc[[2, 4, 5], [3,2]]

There is a subtle difference when using a slice. .iloc excludes the last value, while .loc includes it:

>>> df2.iloc[:3]

>>> df2.loc[:3]

Setting an index from a column after reading in data

It is common to see pandas code that reads in a DataFrame with a RangeIndex and then sets the index to be one of the columns. This is typically done with the set_index method:

>>> df2_idx = df2.set_index('Names')

>>> df2_idx

The index has a name

Notice that this DataFrame does not look exactly like our first one from the very top of this tutorial. Directly above the index is the bold-faced word Names . This is technically the name of the index. Our original DataFrame had no name for its index. You can ignore this small detail for now. Subset selections will happen in the same fashion.

DataFrame column selection with dot notation

Pandas allows you to select a single column as a Series by using dot notation. This is also referred to as attribute access. You simply place the name of the column without quotes following a dot and the DataFrame like this:

>>> df.state

Jane NY

Niko TX

Aaron FL

Penelope AL

Dean AK

Christina TX

Cornelia TX

Name: state, dtype: object >>> df.age

Jane 30

Niko 2

Aaron 12

Penelope 4

Dean 32

Christina 33

Cornelia 69

Name: age, dtype: int64

Pros and cons when selecting columns by attribute access

The best benefit of selecting columns like this is that you get help when chaining methods after selection. For instance, if you place another dot after the column name and press tab, a list of all the Series methods will appear in a pop-up menu. It will look like this:

This help disappears when you use just the indexing operator:

The biggest drawback is that you cannot select columns that have spaces or other characters that are not valid as Python identifiers (variable names).

Selecting the same column twice?

This is rather peculiar, but you can actually select the same column more than once:

df[['age', 'age', 'age']]

Summary of Part 1

We covered an incredible amount of ground. Let’s summarize all the main points:

Before learning pandas, ensure you have the fundamentals of Python

Always refer to the documentation when learning new pandas operations

The DataFrame and the Series are the containers of data

A DataFrame is two-dimensional, tabular data

A Series is a single dimension of data

The three components of a DataFrame are the index , the columns and the data (or values )

, the and the (or ) Each row and column of the DataFrame is referenced by both a label and an integer location

and an There are three primary ways to select subsets from a DataFrame — [] , .loc and .iloc

, and I use the term just the indexing operator to refer to [] immediately following a DataFrame/Series

to refer to immediately following a DataFrame/Series Just the indexing operator’s primary purpose is to select a column or columns from a DataFrame

Using a single column name to just the indexing operator returns a single column of data as a Series

Passing multiple columns in a list to just the indexing operator returns a DataFrame

A Series has two components, the index and the data ( values ). It has no columns

and the ( ). It has no columns .loc makes selections only by label

makes selections .loc can simultaneously select rows and columns

can simultaneously select rows and columns .loc can make selections with either a single label, a list of labels, or a slice of labels

can make selections with either a single label, a list of labels, or a slice of labels .loc makes row selections first followed by column selections: df.loc[row_selection, col_selection]

makes row selections first followed by column selections: .iloc is analogous to . loc but uses only integer location to refer to rows or columns.

is analogous to but uses only to refer to rows or columns. .ix is deprecated and should never be used

is deprecated and should never be used .loc and .iloc work the same for Series except they only select based on the index as there are no columns

and work the same for Series except they only select based on the index as there are no columns Pandas combines the power of python lists (selection via integer location) and dictionaries (selection by label)

You can use just the indexing operator to select rows from a DataFrame, but I recommend against this and instead sticking with the explicit .loc and .iloc

and Normally data is imported without setting an index. Use the set_index method to use a column as an index.

method to use a column as an index. You can select a single column as a Series from a DataFrame with dot notation

Way more to the story

This is only part 1 of the series, so there is much more to cover on how to select subsets of data in pandas. Some of the explanations in this part will be expanded to include other possibilities.

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!","['rows', 'subsets', 'column', 'pandas', 'single', 'index', 'series', 'columns', 'data', 'select', 'dataframe', 'selecting']","Part 1: Selection with [ ] , .loc and .ilocThis is the beginning of a four-part series on how to select subsets of data from a pandas DataFrame or Series.
Always reference the documentationThe material in this article is also covered in the official pandas documentation on Indexing and Selecting Data.
Master Data Analysis with PythonMaster Data Analysis with Python is an extremely comprehensive course that will help you learn pandas to do data analysis.
Read in data into a DataFrame with read_csvLet’s begin using pandas to read in a DataFrame, and from there, use the indexing operator by itself to select subsets of data.
Selecting a single column as a SeriesTo select a single column of data, simply put the name of the column in-between the brackets.",en,['Ted Petrou'],2020-01-22 21:51:37.069000+00:00,"{'Python Pandas', 'Data Science', 'Python', 'Jupyter', 'Pydata'}","{'https://miro.medium.com/max/56/1*ctcUH__YabtZmT5SjzZLvA.png?q=20', 'https://miro.medium.com/max/800/1*LjZswKJUDlUFHnLRIs-tag.png', 'https://miro.medium.com/max/48/1*oRPKw_82d0_PFNB5Cho1zw.png?q=20', 'https://miro.medium.com/max/60/1*_MGwbnVitEnnH_RoOAxLxw.png?q=20', 'https://miro.medium.com/max/796/1*ckgKLl2gSA39EB2I1GH2hw.png', 'https://miro.medium.com/max/814/1*8F3TgEXSmJ3G4cRkiY56-Q.png', 'https://miro.medium.com/max/396/1*xdBEnpt4APUgtFnmo9uoAw.png', 'https://miro.medium.com/max/886/1*eV73GAR5_rkckmt_pGXAVQ.png', 'https://miro.medium.com/max/512/1*ctcUH__YabtZmT5SjzZLvA.png', 'https://miro.medium.com/max/60/1*4Ff8OMPaU6u1GEG88CrBEQ.png?q=20', 'https://miro.medium.com/max/60/1*VLlVzvrycrlMeQw_OPnlmg.png?q=20', 'https://miro.medium.com/max/850/1*WIHN9IKCZ2b9AJMVybtkqg.png', 'https://miro.medium.com/max/407/1*SMzyeMGukFU7y9lSwDim5A.png', 'https://miro.medium.com/max/804/1*WpJZ25LMTESDrozrs7Y5qA.png', 'https://miro.medium.com/max/794/1*r-KQCeJEsK2Ba9zdKOrFyw.png', 'https://miro.medium.com/max/60/1*WG_BZ7r77RZtRIiSxZVzqQ.png?q=20', 'https://miro.medium.com/max/60/1*wXdCm9RlUbMiPS9Q-cFa9g.png?q=20', 'https://miro.medium.com/max/808/1*LfdbzoBocHPGMXT5ZkpKRA.png', 'https://miro.medium.com/max/418/1*fEtloDbpCd9Z3qv_O1MCuQ.png', 'https://miro.medium.com/max/380/1*NA-MxX9soaLLhnq34cp7Rg.png', 'https://miro.medium.com/max/60/1*-_lSxZREXisX6Hc7vanI8w.png?q=20', 'https://miro.medium.com/max/48/1*fEtloDbpCd9Z3qv_O1MCuQ.png?q=20', 'https://miro.medium.com/max/60/1*5kyMLYDbLXCGvHCU17OEYQ.png?q=20', 'https://miro.medium.com/max/46/1*JJ5KTFSgyFEGTWvWsN_GSA.png?q=20', 'https://miro.medium.com/max/524/1*jFvBH1eyLSs09-SQIWa2Eg.png', 'https://miro.medium.com/max/482/1*lvkfp1UmWVp-ugCilEn7iw.png', 'https://miro.medium.com/max/60/1*r-KQCeJEsK2Ba9zdKOrFyw.png?q=20', 'https://miro.medium.com/max/820/1*5kyMLYDbLXCGvHCU17OEYQ.png', 'https://miro.medium.com/max/54/1*lvkfp1UmWVp-ugCilEn7iw.png?q=20', 'https://miro.medium.com/max/60/1*RVd_oBYyVDjY_yDCGQ5fZg.png?q=20', 'https://miro.medium.com/max/60/1*WpJZ25LMTESDrozrs7Y5qA.png?q=20', 'https://miro.medium.com/max/60/1*vDDHDznwNeGHYrM18V3ABA.png?q=20', 'https://miro.medium.com/max/60/1*xdBEnpt4APUgtFnmo9uoAw.png?q=20', 'https://miro.medium.com/max/800/1*3x34RRUQyGKE9lkI-Za6yw.png', 'https://miro.medium.com/max/432/1*wXdCm9RlUbMiPS9Q-cFa9g.png', 'https://miro.medium.com/max/60/1*aB19DS5gDeAoIZ0Qkgm1pQ.png?q=20', 'https://miro.medium.com/max/60/1*jFvBH1eyLSs09-SQIWa2Eg.png?q=20', 'https://miro.medium.com/max/286/1*CM8GwO6O4DjNlNB43AoKZQ.png', 'https://miro.medium.com/max/794/1*-_lSxZREXisX6Hc7vanI8w.png', 'https://miro.medium.com/fit/c/96/96/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/800/1*lbGk9hPAvv4CEOCTjGSGSA.png', 'https://miro.medium.com/max/60/1*ZSehcrMtBWN7_qCWq_HiSg.png?q=20', 'https://miro.medium.com/max/816/1*VLlVzvrycrlMeQw_OPnlmg.png', 'https://miro.medium.com/fit/c/160/160/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/792/1*_MGwbnVitEnnH_RoOAxLxw.png', 'https://miro.medium.com/fit/c/80/80/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/2516/1*ZSehcrMtBWN7_qCWq_HiSg.png', 'https://miro.medium.com/max/60/1*8F3TgEXSmJ3G4cRkiY56-Q.png?q=20', 'https://miro.medium.com/max/300/1*UaTEMv6chUaFv5mXIunb0g.png', 'https://miro.medium.com/max/60/1*eV73GAR5_rkckmt_pGXAVQ.png?q=20', 'https://miro.medium.com/max/796/1*vDDHDznwNeGHYrM18V3ABA.png', 'https://miro.medium.com/max/820/1*aB19DS5gDeAoIZ0Qkgm1pQ.png', 'https://miro.medium.com/max/60/1*NA-MxX9soaLLhnq34cp7Rg.png?q=20', 'https://miro.medium.com/max/60/1*ckgKLl2gSA39EB2I1GH2hw.png?q=20', 'https://miro.medium.com/max/60/1*pTp1BxEPoLERdhE6X-FVdg.png?q=20', 'https://miro.medium.com/max/60/1*RgXHsZiNzAd8Z_0BK7Olqg.png?q=20', 'https://miro.medium.com/max/452/1*03DYVgodNXVjcx_V9Hj6PQ.png', 'https://miro.medium.com/max/60/1*LjZswKJUDlUFHnLRIs-tag.png?q=20', 'https://miro.medium.com/max/406/1*JJ5KTFSgyFEGTWvWsN_GSA.png', 'https://miro.medium.com/max/416/1*hguNxXIZc878r11XzzAW_Q.png', 'https://miro.medium.com/max/60/1*LfdbzoBocHPGMXT5ZkpKRA.png?q=20', 'https://miro.medium.com/max/60/1*T02_TrcvbfKkU7U7J2-ugw.png?q=20', 'https://miro.medium.com/max/936/1*-LvX7f1u0S8xRPXp_YLJfw.png', 'https://miro.medium.com/max/60/1*hguNxXIZc878r11XzzAW_Q.png?q=20', 'https://miro.medium.com/max/60/1*CM8GwO6O4DjNlNB43AoKZQ.png?q=20', 'https://miro.medium.com/max/584/1*WG_BZ7r77RZtRIiSxZVzqQ.png', 'https://miro.medium.com/max/408/1*oRPKw_82d0_PFNB5Cho1zw.png', 'https://miro.medium.com/max/60/1*UaTEMv6chUaFv5mXIunb0g.png?q=20', 'https://miro.medium.com/max/60/1*WIHN9IKCZ2b9AJMVybtkqg.png?q=20', 'https://miro.medium.com/max/36/1*rIbPU6Q7wlNwuudMzFfZKA.png?q=20', 'https://miro.medium.com/max/404/1*4Ff8OMPaU6u1GEG88CrBEQ.png', 'https://miro.medium.com/max/60/1*SMzyeMGukFU7y9lSwDim5A.png?q=20', 'https://miro.medium.com/max/316/1*rIbPU6Q7wlNwuudMzFfZKA.png', 'https://miro.medium.com/max/60/1*lbGk9hPAvv4CEOCTjGSGSA.png?q=20', 'https://miro.medium.com/max/60/1*jEJWfGHzkyE24Wv7oy64Cw.png?q=20', 'https://miro.medium.com/max/444/1*tP8Fb4SL816YosGlt1Xvdg.png', 'https://miro.medium.com/max/814/1*SMzyeMGukFU7y9lSwDim5A.png', 'https://miro.medium.com/max/780/1*T02_TrcvbfKkU7U7J2-ugw.png', 'https://miro.medium.com/max/816/1*pTp1BxEPoLERdhE6X-FVdg.png', 'https://miro.medium.com/max/60/1*03DYVgodNXVjcx_V9Hj6PQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*VwblGkKV5WaS9NlQeoMUng.png', 'https://miro.medium.com/max/820/1*RgXHsZiNzAd8Z_0BK7Olqg.png', 'https://miro.medium.com/max/806/1*RVd_oBYyVDjY_yDCGQ5fZg.png', 'https://miro.medium.com/max/60/1*3x34RRUQyGKE9lkI-Za6yw.png?q=20', 'https://miro.medium.com/max/840/1*jEJWfGHzkyE24Wv7oy64Cw.png', 'https://miro.medium.com/max/60/1*-LvX7f1u0S8xRPXp_YLJfw.png?q=20'}",2020-03-05 00:12:24.055066,2.638744592666626
https://medium.com/hugo-ferreiras-blog/dealing-with-categorical-features-in-machine-learning-1bb70f07262d,Dealing with categorical features in machine learning,"Categorical data are commonplace in many Data Science and Machine Learning problems but are usually more challenging to deal with than numerical data. In particular, many machine learning algorithms require that their input is numerical and therefore categorical features must be transformed into numerical features before we can use any of these algorithms.

One of the most common ways to make this transformation is to one-hot encode the categorical features, especially when there does not exist a natural ordering between the categories (e.g. a feature ‘City’ with names of cities such as ‘London’, ‘Lisbon’, ‘Berlin’, etc.). For each unique value of a feature (say, ‘London’) one column is created (say, ‘City_London’) where the value is 1 if for that instance the original feature takes that value and 0 otherwise.

Even though this type of encoding is used very frequently, it can be frustrating to try to implement it using scikit-learn in Python, as there isn’t currently a simple transformer to apply, especially if you want to use it as a step of your machine learning pipeline. In this post, I’m going to describe how you can still implement it using only scikit-learn and pandas (but with a bit of effort). But, after that, I’ll also show you how you can use the category encoders library to achieve the same thing in a much easier fashion.

To illustrate the whole process, I’m going to use the Breast Cancer Data Set from the UCI Machine Learning Repository, which has many categorical features on which to implement the one-hot encoding.

Load the data

The data we’re going to use is the Breast Cancer Data Set from the UCI Machine Learning Repository. This data set is small and contains several categorical features, which will allow us to quickly explore a few ways to implement the one-hot encoding using Python, pandas and scikit-learn.

After downloading the data from the repository, we read it into a pandas dataframe df .

import pandas as pd # names of columns, as per description

cols_names = ['Class', 'age', 'menopause', 'tumor-size',

'inv-nodes', 'node-caps', 'deg-malig', 'breast',

'breast-quad', 'irradiat'] # read the data

df = (pd.read_csv('breast-cancer.data',

header=None, names=cols_names)

.replace({'?': 'unknown'})) # NaN are represented by '?'

This data set has 286 instances with 9 features and one target (‘Class’). The target and features are described in the data set description as follows:

1. Class: no-recurrence-events, recurrence-events.

2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.

3. menopause: lt40, ge40, premeno.

4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,45-49, 50-54, 55-59.

5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.

6. node-caps: yes, no.

7. deg-malig: 1, 2, 3.

8. breast: left, right.

9. breast-quad: left-up, left-low, right-up, right-low, central.

10. irradiat: yes, no.

We’ll take all columns to be of ‘object’ type and split the training and test sets using the train_test_split of scikit-learn.

from sklearn.model_selection import train_test_split X = df.drop(columns='Class')

y = df['Class'].copy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

One-hot encoding

There are several ways to encode categorical features (see, for example, here). In this post, we will focus on one of the most common and useful ones, one-hot encoding. After the transformation, each column of the resulting data set corresponds to one unique value of each original feature.

For example, suppose we have the following categorical feature with three different unique values.

+---------+

| Feature |

+---------+

| value_1 |

| value_2 |

| value_3 |

+---------+

After one-hot encoding, the data set looks like:

+-----------------+-----------------+-----------------+

| Feature_value_1 | Feature_value_2 | Feature_value_3 |

+-----------------+-----------------+-----------------+

| 1 | 0 | 0 |

| 0 | 1 | 0 |

| 0 | 0 | 1 |

+-----------------+-----------------+-----------------+

We want to implement the one-hot encoding to the breast cancer data set, in such a way that the resulting sets are suitable to use in machine learning algorithms. Note that for many of the features of this data set there is a natural ordering between the categories (e.g. the tumour size) and, therefore, other types of encoding might be more appropriate, but for concreteness we will focus only on one-hot encoding in this post.

Using scikit-learn

Let’s see how we would implement one-hot encoding using scikit-learn. There is a transformer conveniently named OneHotEncoder which, at first glance, seems to be exactly what we’re looking for.

from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder(sparse=False)

X_train_ohe = ohe.fit_transform(X_train)

If we try to apply the above code, we obtain an ValueError , as OneHotEncoder requires that all values are integers, and not strings as we have. This means we first have to encode all the possible values as integers: for a given feature, if it has n possible values (given by n different strings), we encode them with integers between 0 and n-1. Thankfully, there is another transformer in scikit-learn, called LabelEncoder , which does just that!

from sklearn.preprocessing import LabelEncoder le = LabelEncoder()

X_train_le = le.fit_transform(X_train)

And… we obtain another ValueError ! In reality, LabelEncoder is only intended to be used for the target vector, and as such it doesn’t work with more than one column. Unfortunately, in version 0.19 of scikit-learn, there is no transformer which can deal with several columns (there is some hope for version 0.20).

One solution is to make our own transformer, which we creatively call MultiColumnLabelEncoder , which applies the LabelEncoder in each of the features.

class MultiColumnLabelEncoder:



def __init__(self, columns = None):

self.columns = columns # list of column to encode def fit(self, X, y=None):

return self def transform(self, X):

'''

Transforms columns of X specified in self.columns using

LabelEncoder(). If no columns specified, transforms all

columns in X.

'''



output = X.copy()



if self.columns is not None:

for col in self.columns:

output[col] = LabelEncoder().fit_transform(output[col])

else:

for colname, col in output.iteritems():

output[colname] = LabelEncoder().fit_transform(col)



return output def fit_transform(self, X, y=None):

return self.fit(X, y).transform(X)

Will this work this time?

It worked! To better understand what happened, let’s check the original training set.

We see, for instance, that the age group 30–39 was given the label 0, 40–49 was given the label 1, etc, and analogously for the other features.

After applying the MultiColumnLabelEncoder , we can (finally!) use the OneHotEncoder to implement the one-hot encoding to both the training and test sets.

Some comments:

The OneHotEncoder is fitted to the training set, which means that for each unique value present in the training set, for each feature, a new column is created. We have 39 columns after the encoding.

is fitted to the training set, which means that for each unique value present in the training set, for each feature, a new column is created. We have 39 columns after the encoding. The output is a numpy array (when the option sparse=False is used), which has the disadvantage of losing all the information about the original column names and values.

is used), which has the disadvantage of losing all the information about the original column names and values. When we try to transform the test set, after having fitted the encoder to the training set, we obtain (again!) a ValueError . This is because the there are new, previously unseen unique values in the test set and the encoder doesn’t know how to handle these values. In order to use both the transformed training and test sets in machine learning algorithms, we need them to have the same number of columns.

This last problem can be solved by using the option handle_unknown='ignore' of the OneHotEncoder , which, as the name suggests, will ignore previously unseen values when transforming the test set.

And that’s it! Both the training and test sets have 39 columns and are now in a suitable form to be used in machine learning algorithms which require numerical data.

However, the procedure shown above is quite inelegant and we lose the dataframe format for the data. Is there an easier way to implement all of this?

There is!

Using category encoders

Category Encoders is a library of scikit-learn-compatible categorical variable encoders, such as:

Ordinal

One-Hot

Binary

Helmert Contrast

Sum Contrast

Polynomial Contrast

Backward Difference Contrast

Hashing

BaseN

LeaveOneOut

Target Encoding

The Ordinal, One-Hot and Hashing encoders are improved versions of the ones present in scikit-learn with the following advantages:

Support for pandas dataframes as an input and, optionally, as output;

Can explicitly configure which columns in the data are encoded by name or index, or infer non-numeric columns regardless of input type;

Compatibility with scikit-learn pipelines.

For our purposes, we’re going to use the improved OneHotEncoder and see how much we can simplify the workflow. First, we import the category encoders library.

import category_encoders as ce

Then, let’s try to apply the OneHotEncoder directly to both the training and test sets.

It worked immediately! No need to use LabelEncoder first!

Some observations:","['machine', 'onehotencoder', 'set', 'onehot', 'columns', 'learning', 'encoding', 'dealing', 'data', 'categorical', 'training', 'features', 'test', 'values']","In particular, many machine learning algorithms require that their input is numerical and therefore categorical features must be transformed into numerical features before we can use any of these algorithms.
To illustrate the whole process, I’m going to use the Breast Cancer Data Set from the UCI Machine Learning Repository, which has many categorical features on which to implement the one-hot encoding.
Load the dataThe data we’re going to use is the Breast Cancer Data Set from the UCI Machine Learning Repository.
is fitted to the training set, which means that for each unique value present in the training set, for each feature, a new column is created.
When we try to transform the test set, after having fitted the encoder to the training set, we obtain (again!)",en,['Hugo Ferreira'],2018-06-25 21:42:00.338000+00:00,"{'Scikit Learn', 'Machine Learning', 'Python', 'Data Science'}","{'https://miro.medium.com/fit/c/160/160/1*-y6-EbqAGw6lKWocnrAY1w.jpeg', 'https://miro.medium.com/fit/c/96/96/1*2lwhsmKspCKwlB8U-OIypw.jpeg', 'https://miro.medium.com/max/12000/0*1we2M3lC7fap_g09', 'https://miro.medium.com/max/1200/0*1we2M3lC7fap_g09', 'https://miro.medium.com/fit/c/80/80/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/fit/c/80/80/0*fDXoaBrviuMBHcPB.jpg', 'https://miro.medium.com/fit/c/80/80/1*RmtyuwLMWe8Ng4p_3SGzYw@2x.jpeg', 'https://miro.medium.com/max/60/0*1we2M3lC7fap_g09?q=20', 'https://miro.medium.com/fit/c/160/160/1*2lwhsmKspCKwlB8U-OIypw.jpeg'}",2020-03-05 00:12:26.558226,2.5011212825775146
https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c,My secret sauce to be in top 2% of a kaggle competition,"Competing in kaggle competitions is fun and addictive! And over the last couple of years, I developed some standard ways to explore features and build better machine learning models. These simple, but powerful techniques helped me get a top 2% rank in Instacart Market Basket Analysis competition and I use them outside of kaggle as well. So, let’s get right into it!

One of the most important aspects of building any supervised learning model on numeric data is to understand the features well. Looking at partial dependence plots of a model helps you understand how the model’s output changes with any feature.

But, the problem with these plots is that they are created using a trained model. If we could create these plots from train data directly, it could help us understand the underlying data better. In fact, it can help you with all the following things:

Feature understanding Identifying noisy features (the most interesting part!) Feature engineering Feature importance Feature debugging Leakage detection and understanding Model monitoring

In order to make it easily accessible, I decided to put these techniques into a python package featexp and in this article, we’ll see how it can be used for feature exploration. We’ll use the application dataset from Home Credit Default Risk competition on Kaggle. The task of the competition is to predict defaulters using the data given about them.

Feature Understanding

Scatter plot of feature vs. target doesn’t help

If dependent variable (target) is binary, scatter plots don’t work because all points lie either at 0 or 1. For continuous target, too many data points make it difficult to understand the target vs. feature trend. Featexp creates better plots which help with this problem. Let’s try it out!

Feature vs. target plot of DAYS_BIRTH (age)

Featexp creates equal population bins (X-axis) of a numeric feature. It then calculates target’s mean in each bin and plots it in the left-hand side plot above. In our case, target’s mean is nothing but default rate. The plot tells us that customers with high negative values for DAYS_BIRTH (higher age) have lower default rates. This makes sense since younger people are usually more likely to default. These plots help us understand what the feature is telling about customers and how it will affect the model. The plot on the right shows number of customers in each bin.

2. Identifying noisy features

Noisy features lead to overfitting and identifying them isn’t easy. In featexp, you can pass a test set and compare feature trends in train/test to identify noisy ones. This test set is not the actual test set. Its your local test set/validation set for which you know target.

Comparison of feature trends in train and test

Featexp calculates two metrics to display on these plots which help with gauging noisiness:

Trend correlation (seen in test plot): If a feature doesn’t hold same trend w.r.t. target across train and evaluation sets, it can lead to overfitting. This happens because the model is learning something which is not applicable in test data. Trend correlation helps understand how similar train/test trends are and mean target values for bins in train & test are used to calculate it. Feature above has 99% correlation. Doesn’t seem noisy! Trend changes: Sudden and repeated changes in trend direction could imply noisiness. But, such trend change can also happen because that bin has a very different population in terms of other features and hence, its default rate can’t really be compared with other bins.

Feature below is not holding the same trend and hence, has a low trend correlation of 85%. These two metrics can be used to drop noisy features.

Example of noisy feature

Dropping low trend-correlation features works well when there are a lot of features and they are correlated with each other. It leads to less overfitting and other correlated features avoid information loss. It’s also important to not drop too many important features as it might lead to a drop in performance. Also, you can’t identify these noisy features using feature importance because they could be fairly important and still be very noisy!

Using test data from a different time period works better because then you would be making sure if feature trend holds over time.

get_trend_stats() function in featexp returns a dataframe with trend correlation and changes for each feature.

Dataframe returned by get_trend_stats()

Let’s actually try dropping features with low trend-correlation in our data and see how results improve.

AUC for different feature selections using trend-correlation

We can see that higher the trend-correlation threshold to drop features, higher is the leaderboard (LB) AUC. Not dropping important features further improves LB AUC to 0.74. It’s also interesting and concerning that test AUC doesn’t change as much as LB AUC. Getting your validation strategy right such that local test AUC follows LB AUC is also important. Whole code can be found in featexp_demo notebook.

3. Feature Engineering

The insights that you get by looking at these plots help with creating better features. Just having a better understanding of data can lead to better feature engineering. But, in addition to this, it can also help you in improving the existing features. Let’s look at another feature EXT_SOURCE_1:

Feature vs. target plot of EXT_SOURCE_1

Customers having a high value of EXT_SOURCE_1 have low default rates. But, the first bin (~8% default rate) isn’t following the feature trend (goes up and then down). It has only negative values around -99.985 and a large population. This probably implies that these are special values and hence, don’t follow the feature trend. Fortunately, non-linear models won’t have a problem learning this relationship. But, for linear models like logistic regression, such special values and nulls (which will be shown as a separate bin) should be imputed with a value from a bin with similar default rate instead of simply imputing with feature mean.

4. Feature importance

Featexp also helps you with gauging feature importance. DAYS_BIRTH and EXT_SOURCE_1 both have a good trend. But, population for EXT_SOURCE_1 is concentrated in special value bin implying that feature has the same information for most of the customers and hence, can’t differentiate them well. This tells that it might not be as important as DAYS_BIRTH. Based on XGBoost model’s feature importance, DAYS_BIRTH is actually more important than EXT_SOURCE_1.

5. Feature debugging

Looking at Featexp’s plots helps you in capturing bugs in complex feature engineering codes by doing these two things:

Zero variation features show only a single bin

Checking if the feature’s population distribution looks right. I’ve personally encountered extreme cases like above numerous times due to minor bugs. Always hypothesize what the feature trend will look like before looking at these plots. Feature trend not looking like what you expected might hint towards some problem. And frankly, this process of hypothesizing trends makes building ML models much more fun!

6. Leakage Detection

Data leakage from target to features leads to overfitting. Leaky features have high feature importance. But, understanding why leakage is happening in a feature is difficult. Looking at featexp plots can help you with that.

The feature below has 0% default rate in ‘Nulls’ bin and 100% in all other bins. Clearly, this is an extreme case of leakage. This feature has a value only when the customer has defaulted. Based on what the feature is, this could be because of a bug or the feature is actually populated only for defaulters (in which case it should be dropped). Knowing what the problem is with leaky feature leads to quicker debugging.

Understanding why a feature is leaky

7. Model Monitoring

Since featexp calculates trend correlation between two data sets, it can be easily used for model monitoring. Every time the model is re-trained, the new train data can be compared with a well-tested train data (typically train data from the first time you built the model). Trend correlation can help you monitor if anything has changed in feature w.r.t. its relationship with target.","['kaggle', 'feature', 'secret', 'trend', 'competition', 'sauce', 'target', 'data', 'model', 'help', 'default', 'plots', 'features', 'test']","We’ll use the application dataset from Home Credit Default Risk competition on Kaggle.
For continuous target, too many data points make it difficult to understand the target vs. feature trend.
Using test data from a different time period works better because then you would be making sure if feature trend holds over time.
This probably implies that these are special values and hence, don’t follow the feature trend.
Feature trend not looking like what you expected might hint towards some problem.",en,['Abhay Pawar'],2019-04-24 02:51:25.185000+00:00,"{'Data Science', 'Python', 'Data Visualization', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*muwhOmAYJTjSZetBv1UOaA.png?q=20', 'https://miro.medium.com/max/60/1*RuxmJA0iWrMRCVxNGlBidw.png?q=20', 'https://miro.medium.com/max/640/0*qC-ilcSNCHu-vCHK.png', 'https://miro.medium.com/max/3480/1*muwhOmAYJTjSZetBv1UOaA.png', 'https://miro.medium.com/max/60/1*-NA-fc1LR1yo0JoHp8IFOw.png?q=20', 'https://miro.medium.com/max/3528/1*_FqsA9_SdUca5_0s9Uua4Q.png', 'https://miro.medium.com/max/3200/1*MQQrhVy5NjtD-7mrKT8jOA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1280/0*qC-ilcSNCHu-vCHK.png', 'https://miro.medium.com/max/60/1*Y0SsQz-n2rt_XxkC7y0zdg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/0*qC-ilcSNCHu-vCHK.png?q=20', 'https://miro.medium.com/max/3716/1*-NA-fc1LR1yo0JoHp8IFOw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2124/1*UR-SlR1rZOjp0sjTIaPZ_A.png', 'https://miro.medium.com/fit/c/96/96/1*S46Eqpi5zPsUxjveVnQ6jA.png', 'https://miro.medium.com/max/3200/1*6lSWurF_qOzm1cMEJFuRmA.png', 'https://miro.medium.com/max/60/1*_FqsA9_SdUca5_0s9Uua4Q.png?q=20', 'https://miro.medium.com/max/60/1*tpjxrjbxhH-lJo0hbRerfg.png?q=20', 'https://miro.medium.com/max/3052/1*RuxmJA0iWrMRCVxNGlBidw.png', 'https://miro.medium.com/max/60/1*6lSWurF_qOzm1cMEJFuRmA.png?q=20', 'https://miro.medium.com/max/60/1*UR-SlR1rZOjp0sjTIaPZ_A.png?q=20', 'https://miro.medium.com/max/60/1*MQQrhVy5NjtD-7mrKT8jOA.png?q=20', 'https://miro.medium.com/max/780/1*Y0SsQz-n2rt_XxkC7y0zdg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3524/1*tpjxrjbxhH-lJo0hbRerfg.png', 'https://miro.medium.com/fit/c/160/160/1*S46Eqpi5zPsUxjveVnQ6jA.png'}",2020-03-05 00:12:33.004640,6.445447206497192
https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365,An overview of correlation measures between categorical and continuous variables,"Correlation between two discrete or categorical variables

Broadly speaking, there are two different ways to find association between categorical variables. One set of approaches rely on distance metrics such as Euclidean distance or Manhattan distance while another set of approaches span various statistical metrics such as chi-square test or Goodman Kruskal’s lambda, which was initially developed to analyze contingency tables. Now the mathematical purist out there could correctly argue that distance metrics cannot be a correlation metric since correlation needs to be unit independent which distance by definition can’t be. I do agree with that argument and I will point it out later but for now I include it since many people use distance as a proxy for correlation between categorical variables. Additionally, in certain special situations there is an easy conversion between Pearson correlation and Euclidean distance.

Below, I list some common metrics within both approaches and then discuss some relative strengths and weaknesses of the two broad approaches. Then, I list some commonly used metrics within both approaches and end with a brief discussion of their relative merits.

Distance Metrics

Although the concept of “distance” is often not synonymous with “correlation,” distance metrics can nevertheless be used to compute the similarity between vectors, which is conceptually similar to other measures of correlation. There are many other distance metrics, and my intent here is less to introduce you to all the different ways in which distance between two points can be calculated, and more to introduce the general notion of distance metrics as an approach to measure similarity or correlation. I have noted ten commonly used distance metrics below for this purpose. If you are interested in learning more about these metrics, definitions and formulas can be found here.

Sum of Absolute Distance Sum of Squared Distance Mean-Absolute Error Euclidean Distance Manhattan Distance Chessboard Distance Minkowski Distance Canberra Distance Cosine Distance Hamming Distance

Contingency Table Analysis

When comparing two categorical variables, by counting the frequencies of the categories we can easily convert the original vectors into contingency tables. For example, imagine you wanted to see if there is a correlation between being a man and getting a science grant (unfortunately, there is a correlation but that’s a matter for another day). Your data might have two columns in this case — one for gender which would be Male or Female (assume a binary world for this case) and another for grant (Yes or No). We could take the data from these columns and represent it as a cross tabulation by calculating the pair-wise frequencies

Original data table with two columns having some categorical data

Cross Tabulating the categorical variables and presenting the same data as a contingency table

Contingency tables or cross tabulation display the multivariate frequency distribution of variables and are heavily used in scientific research across disciplines. Due to their heavy historic use in statistical analyses, a family of tests have been developed to determine the significance of the difference between two categories of a variable compared to another categorical variable. A popular approach for dichotomous variables (i.e. variables with only two categories) is built on the chi-squared distribution. We are not interested in testing the statistical significance however, we are more interested in effect size and specifically in the strength of association between the two variables. Thankfully, several coefficients have been defined for this purpose, including several which use the chi-square statistic. Here are some examples:

Relative strengths and weaknesses

Distance metrics, at least to me, are more intuitive and easier to understand. It makes sense that if one variable is perfectly predictive of another variable, when plotted in a high dimensional space, the two variables will overlay or be very close to each other. Since I believe that methods one uses to analyze data be easily explainable to non-statisticians whenever possible , using distance has an obvious appeal. But a big drawback of approaches relying on distance metrics is that they are scale dependent. If you scale your input by a factor of 10, any distance metric will be sensitive to it and change significantly. This behavior is obviously not desirable to understand goodness of fit between different features. Additionally, distance metrics are not easily comparable between variable pairs with different number of categories. Let me illustrate this with an example — let’s say we have 3 columns — gender with two categories (Male represented by 0 and Female represented by 1), grades with three categories (Excellent represented by 2, Good represented by 1 and Poor represented by 0) and college admission (Yes represented by 1 and No represented by 0). We want to compare whether gender is more correlated with college admission or grades are more correlated with college admission. Since, the values of grades range from [0, 2] while gender ranges from [0,1] the distance between college admission (range — [0,1]) and grades will be artificially inflated compared to the distance between college admission and gender. This problem can be easily removed though if you one-hot encode all variables in your matrix before computing correlations such that every categorical variable will only have two values — Yes (1) or No (0).

Another potentially bigger drawback of using distance metrics is that sometimes there isn’t a straightforward conversion of a distance metric into a goodness of fit coefficient which is what we want we are more interested in for the purposes of this blog post. I should note here that if you scale and center your continuous data, Euclidean distance could still be used since in these cases there is an easy conversion of Euclidean distance to Pearson correlation. Of course, the other solution one could try would be to use different cutoff criteria for correlations between two discrete variables compared to two continuous variables. But, according to me that is not ideal since we want a universal scale to compare correlations between all variable pairs.

Although statistical techniques based on analyzing contingency tables suffer from fewer drawbacks compared to distance metrics, there are nonetheless important issues which mostly arise from how the statistical significance test (for example: chi-square statistic) is converted into a measure of association. Some of the coefficients such as Phi are defined only for 2x2 tables. Additionally, the contingency coefficient C suffers from the disadvantage that it does not reach a maximum value of 1. The highest value of C for a 2x2 table is 0.707 and for a 4x4 table it is 0.870. This means that C cannot be used to compare associations among tables with different numbers of categories or in tables with a mix of categorical and continuous variables. Further, other measures such as Cramer’s V can be a heavily biased estimator, especially compared to correlations between continuous variables and will tend to overestimate the strength of the association. One way to mitigate the bias in Cramer’s V is to use a kind of bias correction suggested here. The bias corrected Cramer’s V shown to typically have a much smaller mean square error.","['overview', 'correlation', 'continuous', 'variables', 'measures', 'used', 'categories', 'variable', 'distance', 'represented', 'categorical', 'metrics', 'tables']","Correlation between two discrete or categorical variablesBroadly speaking, there are two different ways to find association between categorical variables.
Now the mathematical purist out there could correctly argue that distance metrics cannot be a correlation metric since correlation needs to be unit independent which distance by definition can’t be.
Then, I list some commonly used metrics within both approaches and end with a brief discussion of their relative merits.
Distance MetricsAlthough the concept of “distance” is often not synonymous with “correlation,” distance metrics can nevertheless be used to compute the similarity between vectors, which is conceptually similar to other measures of correlation.
I have noted ten commonly used distance metrics below for this purpose.",en,['Outside Two Standard Deviations'],2018-09-14 11:40:14.601000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'How To', 'Statistics'}","{'https://miro.medium.com/fit/c/80/80/1*2w-spTiLtXM1u87ayQ35jg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*an5-U2EDule4YCr6Vyhnxw.jpeg', 'https://miro.medium.com/max/920/1*IxrD673HPZFxgSswvie2Yg.png', 'https://miro.medium.com/max/1832/1*OYl2InKjiBNwmU3scyK4Wg.png', 'https://miro.medium.com/fit/c/80/80/1*kbZa4RkOrS2hGKgIsBR8kA.jpeg', 'https://miro.medium.com/max/1000/1*6KNXPv-_3s7cJrcOQxbZCw.png', 'https://miro.medium.com/max/60/1*6KNXPv-_3s7cJrcOQxbZCw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*LXFHFkYQQR1tyNWoHIcSXQ.jpeg', 'https://miro.medium.com/max/60/1*OYl2InKjiBNwmU3scyK4Wg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*an5-U2EDule4YCr6Vyhnxw.jpeg', 'https://miro.medium.com/max/2000/1*6KNXPv-_3s7cJrcOQxbZCw.png', 'https://miro.medium.com/max/38/1*IxrD673HPZFxgSswvie2Yg.png?q=20'}",2020-03-05 00:12:33.836169,0.8315284252166748
https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9,The Search for Categorical Correlation,"All the code appearing in this post is available as part of the dython library on my GitHub page.

For any code related questions, please open an issue on the library’s GitHub page.

Not long ago I stumbled across a data-set of mushrooms on Kaggle, where over 20 different features of edible and poisonous mushrooms were collected and sorted into categories. The idea of seeking patterns that might point on how safe it is to eat a random mushroom seemed like a nice challenge — I even found myself creating a whole storyline of a lost man in the woods behind the kernel I published later on.

While going through other users’ kernels, it was easy to see that Random Forests and other simple methods reach extremely high accuracy without too much effort, so I saw no reason doing so too — I’ve decided to see if can find by myself which features point towards which mushroom I can safely eat, if I’ll ever need to. I realized what I’m actually looking for is the correlation between the features and the mushroom’s type — but that’s a problem, as the features are all categorical, and correlation isn’t defined in that case.

What is correlation?

Before we can discuss about what correlation is not, let’s talk about what it is. In human language, correlation is the measure of how two features are, well, correlated; just like the month-of-the-year is correlated with the average daily temperature, and the hour-of-the-day is correlated with the amount of light outdoors. Formalizing this mathematically, the definition of correlation usually used is Pearson’s R for a data sample (which results in a value in the range [-1,1]):

Pearson’s R for data sample. Taken from Wikipedia

But, as can be seen from the above equation, Pearson’s R isn’t defined when the data is categorical; let’s assume that x is a color feature — how do you subtract yellow from the average of colors? We need something else here.

One common option to handle this scenario is by first using one-hot encoding, and break each possible option of each categorical feature to 0-or-1 features. This will then allow the use of correlation, but it can easily become too complex to analyse. For example, one-hot encoding converts the 22 categorical features of the mushrooms data-set to a 112-features data-set, and when plotting the correlation table as a heat-map, we get something like this:

Correlation of the mushrooms data-set, transformed using one-hot encoding

This is not something that can be easily used for gaining new insights. So we still need something else.

Going categorical

What we need is something that will look like correlation, but will work with categorical values — or more formally, we’re looking for a measure of association between two categorical features. Introducing: Cramér’s V. It is based on a nominal variation of Pearson’s Chi-Square Test, and comes built-in with some great benefits:

Similarly to correlation, the output is in the range of [0,1], where 0 means no association and 1 is full association. (Unlike correlation, there are no negative values, as there’s no such thing as a negative association. Either there is, or there isn’t) Like correlation, Cramer’s V is symmetrical — it is insensitive to swapping x and y

And what was even better — someone already implemented that as a Python function. And here’s my edited version of the original:

def cramers_v(x, y):

confusion_matrix = pd.crosstab(x,y)

chi2 = ss.chi2_contingency(confusion_matrix)[0]

n = confusion_matrix.sum().sum()

phi2 = chi2/n

r,k = confusion_matrix.shape

phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))

rcorr = r-((r-1)**2)/(n-1)

kcorr = k-((k-1)**2)/(n-1)

return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))

When applied to the mushrooms data-set, it looks like this:

Cramer’s V calculated for the mushrooms data-set

Well isn’t that pretty? Just by looking at this heat-map we can see that the odor is highly associated with the class (edible/poisonous) of the mushroom, and that the gill-attachment feature is highly associated with three others.

The curse of symmetry

Thinking over the output of Cramer’s V, I realized I’m losing valuable information due to the symmetry of it. To better demonstrate that, consider the following data-set:

We can see that if the value of x is known, the value of y still can’t be determined, but if the value of y is known — then the value of x is guaranteed. This valuable information is lost when using Cramer’s V due to its symmetry, so to preserve it we need an asymmetric measure of association between categorical features. And this is exactly what Theil’s U is.

Theil’s U, also referred to as the Uncertainty Coefficient, is based on the conditional entropy between x and y — or in human language, given the value of x, how many possible states does y have, and how often do they occur. Just like Cramer’s V, the output value is on the range of [0,1], with the same interpretations as before — but unlike Cramer’s V, it is asymmetric, meaning U(x,y)≠U(y,x) (while V(x,y)=V(y,x), where V is Cramer’s V). Using Theil’s U in the simple case above will let us find out that knowing y means we know x, but not vice-versa.

Implementing the formula as a Python function yields this (full code with the conditional_entropy function can be found on my Github page — link at the top of the post):

def theils_u(x, y):

s_xy = conditional_entropy(x,y)

x_counter = Counter(x)

total_occurrences = sum(x_counter.values())

p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))

s_x = ss.entropy(p_x)

if s_x == 0:

return 1

else:

return (s_x - s_xy) / s_x

Applying this to the mushrooms data-set:

Theil’s U calculated for the mushrooms data-set

This new calculation shed much more light on the associations we’ve seen from Cramer’s V — for example, we now see that while knowing the odor gives a lot of information over the mushroom’s class, this is not in the case the other way around. Theil’s U indeed gives us much more information on the true relations between the different features.

What happens when we mix things up?

So now we have a way to measure the correlation between two continuous features, and two ways of measuring association between two categorical features. But what about a pair of a continuous feature and a categorical feature? For this, we can use the Correlation Ratio (often marked using the greek letter eta). Mathematically, it is defined as the weighted variance of the mean of each category divided by the variance of all samples; in human language, the Correlation Ratio answers the following question: Given a continuous number, how well can you know to which category it belongs to? Just like the two coefficients we’ve seen before, here too the output is on the range of [0,1].

Implementation in Python looks like this:

def correlation_ratio(categories, measurements):

fcat, _ = pd.factorize(categories)

cat_num = np.max(fcat)+1

y_avg_array = np.zeros(cat_num)

n_array = np.zeros(cat_num)

for i in range(0,cat_num):

cat_measures = measurements[np.argwhere(fcat == i).flatten()]

n_array[i] = len(cat_measures)

y_avg_array[i] = np.average(cat_measures)

y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)

numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))

denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))

if numerator == 0:

eta = 0.0

else:

eta = np.sqrt(numerator/denominator)

return eta

Final words

I believe I can declare the search for a measure of association for categorical features a successful one, especially as certain requirements — such as the need for an asymmetric measure — were not expected when starting. These three new metrics are very useful when exploring a data-set which contains categorical features, and helped me gain more insights on data-sets I’ve explored. I can only hope this will be useful to you as it was to me, and if not — well, at least you now know how to identify edible mushrooms.","['correlation', 'association', 'need', 'cramers', 'search', 'categorical', 'mushrooms', 'x', 'features', 'v', 'value']","I realized what I’m actually looking for is the correlation between the features and the mushroom’s type — but that’s a problem, as the features are all categorical, and correlation isn’t defined in that case.
Going categoricalWhat we need is something that will look like correlation, but will work with categorical values — or more formally, we’re looking for a measure of association between two categorical features.
Just like Cramer’s V, the output value is on the range of [0,1], with the same interpretations as before — but unlike Cramer’s V, it is asymmetric, meaning U(x,y)≠U(y,x) (while V(x,y)=V(y,x), where V is Cramer’s V).
So now we have a way to measure the correlation between two continuous features, and two ways of measuring association between two categorical features.
These three new metrics are very useful when exploring a data-set which contains categorical features, and helped me gain more insights on data-sets I’ve explored.",en,['Shaked Zychlinski'],2019-12-26 15:21:27.046000+00:00,"{'Correlation', 'Data Analysis', 'Data Visualization'}","{'https://miro.medium.com/max/58/1*kUuEuJu3B1LNAXiFwUvDIw.png?q=20', 'https://miro.medium.com/max/58/1*5zXKfXX0U2zw68ea_CXaeg.png?q=20', 'https://miro.medium.com/max/546/1*uOBToLtIFDfrNwnXLBv-7w.png', 'https://miro.medium.com/max/3840/1*eqArgnH7PNT76mhB9pQjIg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*hM8X1ekutC5l8TquiCA45g.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*eqArgnH7PNT76mhB9pQjIg.jpeg', 'https://miro.medium.com/max/60/1*uOBToLtIFDfrNwnXLBv-7w.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/1*hM8X1ekutC5l8TquiCA45g.jpeg', 'https://miro.medium.com/max/58/1*XPVvypiS7o6R0Fs4h8SvBA.png?q=20', 'https://miro.medium.com/max/1000/1*3Mx7I537OnQybSOMPvgqEw.png', 'https://miro.medium.com/max/1804/1*kUuEuJu3B1LNAXiFwUvDIw.png', 'https://miro.medium.com/max/60/1*eqArgnH7PNT76mhB9pQjIg.jpeg?q=20', 'https://miro.medium.com/max/1804/1*XPVvypiS7o6R0Fs4h8SvBA.png', 'https://miro.medium.com/max/1372/1*5zXKfXX0U2zw68ea_CXaeg.png', 'https://miro.medium.com/max/60/1*3Mx7I537OnQybSOMPvgqEw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png'}",2020-03-05 00:12:36.170574,2.3344051837921143
https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-39e811c81a0c,Selecting Subsets of Data in Pandas: Part 2,"Part Two: Boolean Indexing

This is part two of a four-part series on how to select subsets of data from a pandas DataFrame or Series. Pandas offers a wide variety of options for subset selection which necessitates multiple articles. This series is broken down into the following four topics.

Become an Expert

If you want to be trusted to make decisions using pandas, you must become an expert. I have completely mastered pandas and have developed courses and exercises that will massively improve your knowledge and efficiency to do data analysis.

Part 1 vs Part 2 subset selection

Part 1 of this series covered subset selection with [] , .loc and .iloc . All three of these indexers use either the row/column labels or their integer location to make selections. The actual data of the Series/DataFrame is not used at all during the selection.

In Part 2 of this series, on boolean indexing, we will select subsets of data based on the actual values of the data in the Series/DataFrame and NOT on their row/column labels or integer locations.

Documentation on boolean selection

I always recommend reading the official documentation in addition to this tutorial when learning about boolean selection. The documentation uses more formal examples with dummy data, but is still an excellent resource.

The documentation use the term boolean indexing but you will also see boolean selection.

Boolean Indexing from pandas documentation

Stack Overflow Data

The data that we will use for this tutorial comes from Stack Overflow’s data explorer, which is a fantastic tool to gather an incredible amount of data from the site. You must know SQL in order to use the data explorer. The data explorer allows you to save queries. Take a look at the query I used to collect the data.

The table below contains data on each question asked on stack overflow tagged as pandas.

The first question was asked March 30, 2011. Since then, more than 56,000 questions have been added as of December 2, 2017.

>>> import pandas as pd

>>> import numpy as np >>> so = pd.read_csv('../../data/stackoverflow_qa.csv')

>>> so.head()

Asking simple questions in plain English

Before we get to the technical definition of boolean indexing, let’s see some examples of the types of questions it can answer.

Find all questions that were created before 2014

Find all questions with a score more than 50

Find all questions with a score between 50 and 100

Find all questions answered by Scott Boston

Find all questions answered by the following 5 users

Find all questions that were created between March, 2014 and October 2014 that were answered by Unutbu and have score less than 5.

Find all questions that have score between 5 and 10 or have a view count of greater than 10,000

Find all questions that are not answered by Scott Boston

You will also see examples like this referred to by the term queries.

All queries have criteria

Each of the above queries have a strict logical criteria that must be checked one row at a time.

Keep or Discard entire row of data

If you were to manually answer the above queries, you would need to scan each row and determine whether the row as a whole meets the criterion or not. If the row meets the criteria, then it is kept and if not, then it is discarded.

Each row will have a True or False value associated with it

When you perform boolean indexing, each row of the DataFrame (or value of a Series) will have a True or False value associated with it depending on whether or not it meets the criterion. True/False values are known as boolean. The documentation refers to the entire procedure as boolean indexing.

Since we are using the booleans to select data, it is sometimes referred to as boolean selection. Essentially, we are using booleans to select subsets of data.

Using [ ] and .loc for boolean selection

We will use the same three indexers, [] and .loc from part 1 to complete our boolean selections. We will do so by placing a sequence of booleans inside of these indexer. The sequence will be the same number of rows/values as the DataFrame/Series it is doing the selection on.

The .iloc indexer can be made to work with boolean selection but is almost never used. A small section towards the end will show why it's unnecessary.

Focus on [ ] for now

To simplify things, we will only use the brackets, [] , which I called just the indexing operator from part 1. We will get to the other indexers a bit later.

Master Data Analysis with Python

Master Data Analysis with Python is an extremely comprehensive course that will help you learn pandas to do data analysis.

I believe that it is the best possible resource available for learning how to data analysis with pandas and provide a 30-day 100% money back guarantee if you are not satisfied.

Use a small DataFrame to get started

Before we make our first boolean selection, let’s simplify matters and use the first five rows of the stack overflow data as our starting DataFrame.

>>> so_head = so.head()

>>> so_head

Manually create a list of booleans

For our first boolean selection, we will not answer any interesting ‘English’ queries and instead just select rows with a list of booleans.

For instance, let’s select the first and third rows by creating the following list:

>>> criteria = [True, False, True, False, False]

We can pass this list of booleans to just the indexing operator and complete our selection:

>>> so_head[criteria]

Wait a second… Isn’t [ ] just for column selection?

The primary purpose of just the indexing operator for a DataFrame is to select one or more columns by using either a string or a list of strings. Now, all of a sudden, this example is showing that entire rows are selected with boolean values. This is what makes pandas, unfortunately, one of the most confusing libraries to use.

Operator Overloading

Just the indexing operator is overloaded. This means, that depending on the inputs, pandas will do something completely different. Here are the rules for the different objects you pass to just the indexing operator.

string — return a column as a Series

list of strings — return all those columns as a DataFrame

a slice — select rows (can do both label and integer location — confusing!)

a sequence of booleans — select all rows where True

In summary, primarily just the indexing operator selects columns, but if you pass it a sequence of booleans it will select all rows that are True .

What do you mean by ‘sequence’?

I keep using the term sequence of booleans to refer to the True/False values. Technically, the most common built-in Python sequence types are lists and tuples. In addition to a list, you will most often be using a pandas Series as your 'sequence' of booleans.

Let’s manually create a boolean Series to select the last three rows of so_head .

>>> s = pd.Series([False, False, True, True, True])

>>> s

0 False

1 False

2 True

3 True

4 True

dtype: bool >>> so_head[s]

Take care when creating a boolean Series by hand

The above example only worked because the index of both the boolean Series and so_head were the exact same. Let's output them so you can clearly see this.

>>> s.index

RangeIndex(start=0, stop=5, step=1) >>> so_head.index

RangeIndex(start=0, stop=5, step=1)

Boolean selection fails when the index doesn’t align

When you are using a boolean Series to do boolean selection, the index of both objects must be the exact same. Let’s create a slightly different Series with a different index than the DataFrame it is indexing on.

>>> s = pd.Series([False, False, True, True, True], index=[2, 3, 4, 5, 6])

>>> s

2 False

3 False

4 True

5 True

6 True

dtype: bool >>> so_head[s] .... IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match

IndexingError : Unalignable boolean Series!

If the index of both the boolean Series and the object you are doing boolean selection on don’t match exactly, you will get the above error. This is one reason, as you will below, why you will almost never create boolean Series by hand like this.

Also use NumPy arrays

You can also use NumPy arrays to do boolean selection. NumPy arrays have no index so you won’t get the error above, but your array needs to be the same exact length as the object you are doing boolean selection on.

>>> a = np.array([True, False, False, False, False])

>>> so_head[a]

Never creating boolean Series by hand

You will likely never create a boolean Series by hand as was done above. Instead, you will produce them based on the values of your data.

Use the comparison operators to create boolean Series

The primary method of creating a Series of booleans is to use one of the six comparison operators:

<

<=

>

>=

==

!=

Use comparison operator with a single column of data

You will almost always use the comparison operators on just a single column or Series of data. For instance, let’s create a boolean Series from the score column. Let's determine if the score is at least 10.

We select the score column and then test the condition that each value is greater than or equal to 10. Notice that this operations gets applied to each value in the Series. A boolean Series is returned.

>>> criteria = so['score'] >= 10

>>> criteria.head(10)

0 False

1 False

2 False

3 False

4 False

5 False

6 True

7 True

8 True

9 False

Name: score, dtype: bool

Finally making a boolean selection

Now that we have our boolean Series stored in the variable criteria , we can pass this to just the indexing operator to select only the rows that have a score of at least 10.

We are going to use the entire so DataFrame for the rest of the tutorial.

>>> so_score_10_or_more = so[criteria]

>>> so_score_10_or_more.head()

How many rows have a score of at least ten

Just by looking at the head of the resulting DataFrame, we don’t know how many rows passed our criterion. Let’s output the shape of both our original and our resulting DataFrame.

>>> so.shape

(56398, 12) >>> so_score_10_or_more.shape

(1505, 12)

Only about 3% of questions get a score of 10 or more.

Boolean selection in one line

Often, you will see boolean selection happen in a single line of code instead of the multiple lines we used above. If the following is confusing for you, then I recommend storing your boolean Series to a variable like I did with criteria above.

It is possible to put the creation of the boolean Series inside of just the indexing operator like this.

>>> so[so['score'] >= 10].head()

Single condition expression

Our first example tested a single condition (whether the score was 10 or more). Let’s test a different single condition and look for all the questions that are answered by Scott Boston. The ans_name variable holds the display names of the people who posted the accepted answer to the question.

We use the == operator to test for equality and again store this result to the variable criteria . Again, we pass this variable to just the indexing operator which completes our selection.

>>> # step 1 - create boolean Series

>>> criteria = so['ans_name'] == 'Scott Boston' >>> # step 2 - do boolean selection

>>> so[criteria].head()

Multiple condition expression

So far, both our boolean selections have involved a single condition. You can, of course, have as many conditions as you would like. To do so, you will need to combine your boolean expressions using the three logical operators and, or and not.

Use & , | , ~

Although Python uses the syntax and , or , and not , these will not work when testing multiple conditions with pandas.

You must use the following operators with pandas:

& for and

for | for or

for ~ for not

Our first multiple condition expression

Let’s find all the questions that have a score of at least 5 and are answered by Scott Boston. To begin, we will create two separate variable to hold each criteria.

>>> criteria_1 = so['score'] >= 5

>>> criteria_2 = so['ans_name'] == 'Scott Boston'

We will then use the and operator, the ampersand & , to combine them

>>> criteria_all = criteria_1 & criteria_2

We can now pass this final criteria to just the indexing operator

>>> so[criteria_all]

If you are enjoying this article, consider purchasing the All Access Pass! which includes all my current and future material for one low price.

Multiple conditions in one line

It is possible to combine the entire expression into a single line. Many pandas users like doing this, others hate it. Regardless, it is a good idea to know how to do so as you will definitely encounter it.

Use parentheses to separate conditions

You must encapsulate each condition in a set of parentheses in order to make this work.

Each condition will be separated like this:

>>> (so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')

We can then drop this expression inside of just the indexing operator

>>> # same result as previous

>>> so[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]

Using an or condition

Let’s find all the questions that have a score of at least 100 or have at least 10 answers.

For the or condition, we use the pipe |

>>> so[(so['score'] >= 100) | (so['answercount'] >= 10)].head()

Reversing a condition with the not operator

The tilde character ~ represents the not operator and reverses a condition. For instance, if we wanted all the questions with score greater than 100, we could do it like this:

>>> so[~(so['score'] <= 100)].head()

Notice that there were parentheses around the condition ‘ score less than equal to 100'. We had to use parentheses here or the operation wouldn't work correctly.

Of course, this trivial example has no need for the not operator and can be replaced with the greater than operator, but it’s easy to verify.

Let’s look back up one example and invert the condition of score at least 100 or number of answers at least 10. To do this, we will have to wrap our entire expression with parentheses like this:

>>> ~((so['score'] >= 100) | (so['answercount'] >= 10))

There is a set of parentheses around each inner expression as well.

Complex conditions

It is possible to build extremely complex conditions to select rows of your DataFrame that meet a very specific criteria. For instance, we can select all questions answered by Scott Boston with score 5 or more OR questions answered by Ted Petrou with answer count 5 or more.

With multiple conditions, its probably best to break out the logic into multiple steps:

>>> criteria_1 = (so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')

>>> criteria_2 = (so['answercount'] >= 5) & (so['ans_name'] == 'Ted Petrou')

>>> criteria_all = criteria_1 | criteria_2

>>> so[criteria_all]

Lots of or conditions in a single column - use isin

Occasionally, we will want to test equality in a single column to multiple values. This is most common in string columns. For instance, let’s say we wanted to find all the questions answered by Scott Boston, Ted Petrou, MaxU, and unutbu.

One way to do this would be with four or conditions.

>>> criteria = ((so['ans_name'] == 'Scott Boston') |

(so['ans_name'] == 'Ted Petrou') |

(so['ans_name'] == 'MaxU') |

(so['ans_name'] == 'unutbu'))

An easier way is to use the Series method isin . Pass it a list of all the items you want to check for equality.

>>> criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou',

'MaxU', 'unutbu'])

>>> criteria.head()

0 False

1 False

2 False

3 False

4 False

Name: ans_name, dtype: bool >>> so[criteria].head()

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!

Combining isin with other criteria

You can use the resulting boolean Series from the isin method in the same way you would from the logical operators. For instance, If we wanted to find all the questions answered by the people above and had score greater than 30 we would do the following:

>>> criteria_1 = so['ans_name'].isin(['Scott Boston', 'Ted Petrou',

'MaxU', 'unutbu'])

>>> criteria_2 = so['score'] > 30

>>> criteria_all = criteria_1 & criteria_2

>>> so[criteria_all].tail()

Use isnull to find rows with missing values

The isnull method returns a boolean Series where True indicates a missing value. For instance, questions that do not have an accepted answer have missing values for ans_name . Let's call isnull on this column.

>>> no_answer = so['ans_name'].isnull()

>>> no_answer.head(6)

0 False

1 False

2 False

3 False

4 False

5 True

Name: ans_name, dtype: bool

This is just another boolean Series which we can pass to just the indexing operator.

>>> so[no_answer].head()

An alias of isnull is the isna method. Alias means it is the same exact method with a different name.

Boolean Selection on a Series

All the examples thus far have taken place on the so DataFrame. Boolean selection on a Series happens almost identically. Since there is only one dimension of data, the queries you ask are usually going to be simpler.

First, let’s select a single column of data as a Series such as the commentcount column.

>>> s = so['commentcount']

>>> s.head()

0 4

1 6

2 0

3 0

4 0

Name: commentcount, dtype: int64

Let’s test for number of comments greater than 10

>>> criteria = s > 10

>>> criteria.head()

0 False

1 False

2 False

3 False

4 False

Name: commentcount, dtype: bool

Notice that there is no column selection here as we are already down to a single column. Let’s pass this criteria to just the indexing operator to select just the values greater than 10.

>>> s[criteria].head()

17 16

76 14

566 11

763 12

781 19

Name: commentcount, dtype: int64

We could have done this in one step like this

>>> s[s > 10].head()

17 16

76 14

566 11

763 12

781 19

Name: commentcount, dtype: int64

If we wanted to find those comments greater than 10 but less than 15 we could have used an and condition like this:

>>> s[(s > 10) & (s < 15)].head()

76 14

566 11

763 12

787 12

837 13

Name: commentcount, dtype: int64

Another possibility is the between method

Pandas has lots of duplicate functionality built in to it. Instead of writing two boolean conditions to select all values inside of a range as was done above, you can use the between method to create a boolean Series. To use, pass it the left and right end points of the range. These endpoints are inclusive.

So, to replicate the previous example, you could have done this:

>>> s[s.between(11, 14)].head()

76 14

566 11

763 12

787 12

837 13

Name: commentcount, dtype: int64

Simultaneous boolean selection with rows and column labels with .loc

The .loc indexer was thoroughly covered in part 1 and will now be covered here to simultaneously select rows and columns. In part 1, it was stated that .loc made selections only by label. This wasn't strictly true as it is also able to do boolean selection along with selection by label.

Remember that .loc takes both a row selection and a column selection separated by a comma. Since the row selection comes first, you can pass it the same exact inputs that you do for just the indexing operator and get the same results.

Let’s take a look at a couple examples from above:

>>> # same as above with [ ]

>>> so.loc[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]

>>> # same as above with [ ]

>>> criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou',

'MaxU', 'unutbu'])

>>> so.loc[criteria].head()

Separate row and column selection with a comma for .loc

The great benefit of .loc is that it allows you to simultaneously do boolean selection along the rows and make column selections by label.

For instance, let’s say we wanted to find all the questions with more than 20k views but only return the creationdate , viewcount , and ans_name columns. You would do the following.

>>> so.loc[so['viewcount'] > 20000, ['creationdate', 'viewcount',

'ans_name']].head(10)

You could have broken each selection into pieces like this:

>>> row_selection = so['viewcount'] > 20000

>>> col_selection = ['creationdate', 'viewcount', 'ans_name']

>>> so.loc[row_selection, col_selection]

Lots of combinations possible with .loc

Remember that .loc can take a string, a list of strings or a slice. You can use all three possible ways to select your data. You can also make very complex boolean selections for your rows.

Let’s select rows with favoritecount between 30 and 40 and every third column beginning from title to the end.

# weird but possible

so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()

Boolean selection for the columns?

It is actually possible to use a sequence of booleans to select columns. You pass a list, Series, or array of booleans the same length as the number of columns to .loc .

Let’s do a simple manual example where we create a list of booleans by hand. First, let’s find out how many columns are in our dataset

>>> so.shape

(56398, 12)

Let’s create a list of 12 booleans

>>> col_bools = [True, False, False] * 4

>>> col_bools

[True,

False,

False,

True,

False,

False,

True,

False,

False,

True,

False,

False]

Use .loc to select all rows with just the True columns from col_bools .

>>> so.loc[:, col_bools].head()

You can simultaneously select rows and columns too. Let’s select the same columns but for rows that have over 500,000 views.

>>> so.loc[so['viewcount'] > 500000, col_bools]

A more practical example

Let’s see a slightly more practical example of doing boolean selection on the columns. Let’s say we flipped 10 coins one-hundred times and store each trial in a column in the DataFrame below

>>> coins = pd.DataFrame(np.random.randint(0, 2, (100, 10)),

columns=list('abcdefghij'))

>>> coins.head()

>>> coins.shape

(100, 10)

If we are interested in selecting only the columns that have more than 50% heads, we could first take the mean of each column like this.

>>> coin_mean = coins.mean()

>>> coin_mean a 0.50

b 0.46

c 0.48

d 0.47

e 0.43

f 0.52

g 0.44

h 0.47

i 0.57

j 0.44

dtype: float64

Let’s test the condition that the percentage is greater than .5

>>> coin_mean > .5

a False

b False

c False

d False

e False

f True

g False

h False

i True

j False

dtype: bool

Finally, we can use this boolean Series to select only the columns that meet our criteria.

>>> coins.loc[:, coins.mean() > .5].head()

Column to column comparisons

All of the previous Series comparisons happened against a single scalar value. It is possible to create a boolean Series by comparing one column to another. For instance, we can find all the questions where there are more answers than score .

>>> criteria = so['answercount'] > so['score']

>>> so[criteria].head()

In one line, the above would have looked like this:

>>> so[so['answercount'] > so['score']]

Almost never use .iloc with boolean selection

First, remember that .iloc uses INTEGER location to make its selections.

You will rarely use .iloc to do boolean selection and almost always use just the indexing operator or .loc . To see why, let's try and run a simple boolean selection to find all the rows that have more than 100,000 views.

>>> so.iloc[so['viewcount'] > 100000]

... NotImplementedError: iLocation based boolean indexing on an integer type is not available

NotImplementedError

The pandas developers have not decided to boolean selection (with a Series) for .iloc so it does not work. You can however convert the Series to a list or a NumPy array as a workaround.

Let’s save our Series to a variable and double-check its type.

>>> criteria = so['viewcount'] > 100000

>>> type(criteria)

pandas.core.series.Series

Let’s grab the underlying NumPy array with the values attribute and pass it to .iloc

>>> a = criteria.values

>>> so.iloc[a].head()

You can make simultaneous column selection as well with integers.

>>> so.iloc[a, [5, 10, 11]].head()

I don’t think I have ever used .iloc for boolean selection as it's not implemented for Series. I added because it's one of the three main indexers in pandas and it's important to know that it's not used much at all for boolean selection.

.loc and [] work the same on a Series for boolean selection

Boolean selection will work identically for .loc as it does with just the indexing operator on a Series. Both the indexers do row selection when passed a boolean Series. Since Series don't have columns, the two indexers are identical in this situation.

>>> s = so['score']

>>> s[s > 100].head()

8 201

17 136

75 199

100 144

106 340

Name: score, dtype: int64 >>> s.loc[s > 100].head()

8 201

17 136

75 199

100 144

106 340

Name: score, dtype: int64

Summary

Boolean Indexing or Boolean Selection is the selection of a subset of a Series/DataFrame based on the values themselves and not the row/column labels or integer location

or is the selection of a subset of a Series/DataFrame based on the values themselves and not the row/column labels or integer location Boolean selection is used to answer common queries like “find all the female engineers with a salary over 150k/year”

To do boolean selection, you first create a sequence of True/False values and pass it to a DataFrame/Series indexer

Each row of data is kept or discarded

The indexing operators are overloaded — change functionality depending on what is passed to them

— change functionality depending on what is passed to them Typically, you will first create a boolean Series with one of the 6 comparison operators

You will pass this boolean series to one of the indexers to make your selection

Use the isin method to test for multiple equalities in the same column

method to test for multiple equalities in the same column Use isnull to find all rows with missing values in a particular column

to find all rows with missing values in a particular column Can use the between Series method to test whether Series values are within a range

Series method to test whether Series values are within a range You can create complex criteria with the and ( & ), or ( | ), and not ( ~ ) logical operators

( ), ( ), and ( ) logical operators When you have multiple conditions in a single line, you must wrap each expression with a parentheses

If you have complex criteria, think about storing each set of criteria into its own variable (i.e. don’t do everything in one line)

If you are only selecting rows, then you will almost always use just the indexing operator

If you are simultaneously doing boolean selection on the rows and selecting column labels then you will use .loc

You will almost never use .iloc to do boolean selection

to do boolean selection Boolean selection works the same for Series as it does for DataFrames

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!","['rows', 'subsets', 'questions', 'column', 'boolean', 'pandas', 'series', 'indexing', 'selection', 'data', 'select', 'score', 'selecting']","Boolean Indexing from pandas documentationStack Overflow DataThe data that we will use for this tutorial comes from Stack Overflow’s data explorer, which is a fantastic tool to gather an incredible amount of data from the site.
Master Data Analysis with PythonMaster Data Analysis with Python is an extremely comprehensive course that will help you learn pandas to do data analysis.
>>> s = pd.Series([False, False, True, True, True], index=[2, 3, 4, 5, 6])>>> s2 False3 False4 True5 True6 Truedtype: bool >>> so_head[s] .... IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not matchIndexingError : Unalignable boolean Series!
>>> a = np.array([True, False, False, False, False])>>> so_head[a]Never creating boolean Series by handYou will likely never create a boolean Series by hand as was done above.
Boolean selection in one lineOften, you will see boolean selection happen in a single line of code instead of the multiple lines we used above.",en,['Ted Petrou'],2020-01-22 21:50:40.829000+00:00,"{'Pandas', 'Data Science', 'Python', 'Jupyter', 'Pydata'}","{'https://miro.medium.com/max/2392/1*0VCNoWqcQimFlIF4Que1yg.png', 'https://miro.medium.com/max/2392/1*Cq4wzRViaRMM8bSKvPkGcg.png', 'https://miro.medium.com/max/674/1*yK_pZQz2EF42pwA2SAOHNQ.png', 'https://miro.medium.com/max/24/1*DKGcSI7oUYVxx8k4ZkffLQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/2404/1*je5Q9XNdtOFmYjae4VewSw.png', 'https://miro.medium.com/max/2390/1*IKUah98urboHA7zoW-5Klg.png', 'https://miro.medium.com/max/60/1*wby1SlKiLrhgIrqV2vF7vg.png?q=20', 'https://miro.medium.com/max/2402/1*3l63FQ17EY74Q4Ju27iIJA.png', 'https://miro.medium.com/max/60/1*uvkIXyZ4zvxsChv6fmobqw.png?q=20', 'https://miro.medium.com/max/2388/1*BtEEHRLa6mrv5IOVlb79ZA.png', 'https://miro.medium.com/max/2400/1*tu4JpGF9WqpSj2Fuoyd8VQ.png', 'https://miro.medium.com/max/60/1*0VCNoWqcQimFlIF4Que1yg.png?q=20', 'https://miro.medium.com/max/548/1*8bjkWWf6FlxTdOMuiZ-KnA.png', 'https://miro.medium.com/max/2400/1*6IeCRgckPO-SI6bj5VUGYA.png', 'https://miro.medium.com/max/60/1*6IeCRgckPO-SI6bj5VUGYA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/60/1*lv3ZDJ597ESQ6APjKSGjNA.png?q=20', 'https://miro.medium.com/max/2386/1*uvkIXyZ4zvxsChv6fmobqw.png', 'https://miro.medium.com/max/60/1*MBxXYiOVvnN0iZ0ci0W9SA.png?q=20', 'https://miro.medium.com/max/60/1*yK_pZQz2EF42pwA2SAOHNQ.png?q=20', 'https://miro.medium.com/max/60/1*W0YWWkeLCJQiq7rG_THDiQ.png?q=20', 'https://miro.medium.com/max/1200/1*6IeCRgckPO-SI6bj5VUGYA.png', 'https://miro.medium.com/max/768/1*IRqHDZ4bxUZh_0Y5IWKL6A.png', 'https://miro.medium.com/max/444/1*tP8Fb4SL816YosGlt1Xvdg.png', 'https://miro.medium.com/max/2374/1*3oa83CF9K6cIgkH4CqMmAg.png', 'https://miro.medium.com/max/2410/1*lv3ZDJ597ESQ6APjKSGjNA.png', 'https://miro.medium.com/max/842/1*AIHGrVQPSREjqipAPiuMRg.png', 'https://miro.medium.com/max/60/1*ttoQ0_o8KkMWwt0yWNf9NA.png?q=20', 'https://miro.medium.com/max/60/1*8bjkWWf6FlxTdOMuiZ-KnA.png?q=20', 'https://miro.medium.com/max/2378/1*8Zyl0ngS-njBW9NVWtcqUg.png', 'https://miro.medium.com/max/2390/1*HtslWkJkW0REIVagSYpQiQ.png', 'https://miro.medium.com/max/60/1*3oa83CF9K6cIgkH4CqMmAg.png?q=20', 'https://miro.medium.com/max/60/1*8Zyl0ngS-njBW9NVWtcqUg.png?q=20', 'https://miro.medium.com/max/60/1*AIHGrVQPSREjqipAPiuMRg.png?q=20', 'https://miro.medium.com/max/60/1*IKUah98urboHA7zoW-5Klg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*VwblGkKV5WaS9NlQeoMUng.png', 'https://miro.medium.com/max/2380/1*MBxXYiOVvnN0iZ0ci0W9SA.png', 'https://miro.medium.com/max/778/1*gkVJafFD0kgBS_scfZwj6A.png', 'https://miro.medium.com/max/60/1*gkVJafFD0kgBS_scfZwj6A.png?q=20', 'https://miro.medium.com/max/60/1*Cq4wzRViaRMM8bSKvPkGcg.png?q=20', 'https://miro.medium.com/max/60/1*3l63FQ17EY74Q4Ju27iIJA.png?q=20', 'https://miro.medium.com/max/2408/1*W0YWWkeLCJQiq7rG_THDiQ.png', 'https://miro.medium.com/max/1216/1*ttoQ0_o8KkMWwt0yWNf9NA.png', 'https://miro.medium.com/max/2388/1*wby1SlKiLrhgIrqV2vF7vg.png', 'https://miro.medium.com/fit/c/96/96/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/2390/1*iQCmVq7-lDEgkB2xx-LNNQ.png', 'https://miro.medium.com/max/60/1*je5Q9XNdtOFmYjae4VewSw.png?q=20', 'https://miro.medium.com/max/60/1*HtslWkJkW0REIVagSYpQiQ.png?q=20', 'https://miro.medium.com/max/158/1*DKGcSI7oUYVxx8k4ZkffLQ.png', 'https://miro.medium.com/max/60/1*BtEEHRLa6mrv5IOVlb79ZA.png?q=20', 'https://miro.medium.com/max/60/1*iQCmVq7-lDEgkB2xx-LNNQ.png?q=20', 'https://miro.medium.com/max/60/1*IRqHDZ4bxUZh_0Y5IWKL6A.png?q=20', 'https://miro.medium.com/max/60/1*tu4JpGF9WqpSj2Fuoyd8VQ.png?q=20'}",2020-03-05 00:12:37.758434,1.5868594646453857
https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931,Visiting: Categorical Features and Encoding in Decision Trees,"When you have categorical features and you are using decision trees, you often have a major issue: how to deal with categorical features?

Often you see the following…:

Postponing the problem: use a machine learning model which handle categorical features, the greatest of solutions!

Deal with the problem now: design matrix, one-hot encoding, binary encoding…

Usually, you WILL want to deal with the problem now, because if you postpone the problem, it means you already found the solution:

You do not postpone problems because in Data Science, they accumulate quickly like hell (good luck remembering every problem encountered, then come back 1 month later without thinking about them and recite them each).

You do not postpone problems without knowing the potential remedy afterwards (otherwise, you might have a working pipeline but no solution to solve it!).

So what is the matter? Let’s go back to the basics of decision trees and encoding, then we can test some good stuff… in three parts:

Machine Learning Implementations: specifications differ

Example ways of Encoding categorical features

Benchmarking Encodings versus vanilla Categorical features

Decision Trees and Encoding

Machine Learning Specification

When using decision tree models and categorical features, you mostly have three types of models:

Models handling categorical features CORRECTLY. You just throw the categorical features at the model in the appropriate format (ex: as factors in R), AND the machine learning model processes categorical features correctly as categoricals. BEST CASE because it fits your needs. Models handling categorical features INCORRECTLY. You just throw the categorical features at the model in the appropriate format (ex: as factors in R), BUT the machine learning model processes categorical features incorrectly by doing wizardry processing to transform them into something usable (like one-hot encoding), unless you are aware of it. WORST CASE EVER because it does not do what you expected to do. Models NOT handling categorical features at all. You have to preprocess manually the categorical features to have them in an appropriate format for the machine learning model (usually: numeric features). But how do you transform (aka ENCODE) them?

We will target specifically the third type of model, because it is what we want to assess. There are many methods to encode categorical features. We are going to check three of them: numeric encoding, one-hot encoding, and binary encoding.

Categorical Encoding Specification

Categorical Encoding refers to transforming a categorical feature into one or multiple numeric features. You can use any mathematical method or logical method you wish to transform the categorical feature, the sky is the limit for this task.

Numeric Encoding

Are you transforming your categorical features by hand or are you doing the work with a computer?

Numerical Encoding is very simple: assign an arbitrary number to each category.

There is no rocket science for the transformation, except perhaps… how do you assign the arbitrary number? Is there a simple way?

The typical case is to let your favorite programming language do the work.

For instance, you might do like this in R…

my_data$cat_feature <- as.numeric(as.factor(my_data$cat_feature))

Such as this:

as.numeric(as.factor(c(""Louise"",

""Gabriel"",

""Emma"",

""Adam"",

""Alice"",

""Raphael"",

""Chloe"",

""Louis"",

""Jeanne"",

""Arthur"")))

This works, this is not brainer, and it encodes the way it wants deterministically (check the ordering and you will see).

One-Hot Encoding

One-Hot Encoding is just a design matrix with the first factor kept. A design matrix removes the first factor to avoid the matrix inversion problem in linear regressions.

Ever heard about One-Hot Encoding and its magic? Here you have it: this is a design matrix where you keep the first factor instead of removing it (how simple!).

To put it clear, just check the picture as it talks for itself better than 1,000 words.

In addition to thinking about what One-Hot Encoding does, you will notice something very quickly:

You have as many columns as you have cardinalities (values) in the categorical variable.

(values) in the categorical variable. You have a bunch of zeroes and only few 1s! (one 1 per new feature)

Therefore, you have to choose between two representations of One-Hot Encoding:

Dense Representation : 0s are stored in memory , which ballons the RAM usage a LOT if you have many cardinalities. But at least, the support for such representation is typically… worldwide.

: , which ballons the RAM usage a LOT if you have many cardinalities. But at least, the support for such representation is typically… worldwide. Sparse Repsentation: 0s are not stored in memory, which makes RAM efficiency a LOT better even if you have millions of cardinalities. However, good luck finding support for sparse matrices for machine learning, because it is not widespread (think: xgboost, LightGBM, etc.).

Again, you usually let your favorite programming language doing the work. Do not loop through each categorical value and assign a column, because this is NOT an efficient at all. It is not difficult, right?

Example in R, “one line”!:

model.matrix(~ cat + 0,

data = data.frame(

cat = as.factor(c(""Louise"",

""Gabriel"",

""Emma"",

""Adam"",

""Alice"",

""Raphael"",

""Chloe"",

""Louis"",

""Jeanne"",

""Arthur""))))

Dense One-Hot Encoding in R example. As usual, the specific order is identical to the numeric version due to as.factor choosing the order arbitrarily!

If you are running out of available memory, what about working with sparse matrices? Doing it in R is no brainer in “one line”!

library(Matrix)

sparse.model.matrix(~ cat + 0,

data = data.frame(

cat = as.factor(c(""Louise"",

""Gabriel"",

""Emma"",

""Adam"",

""Alice"",

""Raphael"",

""Chloe"",

""Louis"",

""Jeanne"",

""Arthur""))))

Sparse One-Hot Encoding in R. There is no difference to the Dense version, except we end up with a sparse matrix (dgCMatrix: sparse column compressed matrix).

Binary Encoding

Power of binaries!

The objective of Binary Encoding… is to use binary encoding to hash the cardinalities into binary values.

By using the power law of binary encoding, we are able to store N cardinalities using ceil(log(N+1)/log(2)) features.

It means we can store 4294967295 cardinalities using only 32 features with Binary Encoding! Isn’t it awesome to not have those 4294697295 features from One-Hot Encoding? (how are you going to learn 4 billion features in a decision tree…? you need a depth of 32 and it is not readable…)

Still as easy in (base) R, you just need to think you are limited to a specified number of bits (will you ever reach 4294967296 cardinalities? If yes, get rid of some categories because you got too many of them…):

my_data <- c(""Louise"",

""Gabriel"",

""Emma"",

""Adam"",

""Alice"",

""Raphael"",

""Chloe"",

""Louis"",

""Jeanne"",

""Arthur"")

matrix(

as.integer(intToBits(as.integer(as.factor(my_data)))),

ncol = 32,

nrow = length(my_data),

byrow = TRUE

)[, 1:ceiling(log(length(unique(my_data)) + 1)/log(2))]

Binary Encoding in base R.

Ugh, the formula is a bit larger than expected. But you get the idea:

Three key operations to perform for binary encoding.

Operation 1 : convert my_data to factor, then to integer (“numeric”), then to numeric binary representation (as a vector of length 32 for each observation), then to integer (“numeric”).

: convert to factor, then to integer (“numeric”), then to numeric binary representation (as a vector of length 32 for each observation), then to integer (“numeric”). Operation 2 : convert the “numeric” to a matrix with 32 columns and the same number of rows as the number of original observations.

: convert the “numeric” to a matrix with 32 columns and the same number of rows as the number of original observations. Operation 3: using the inverse of the binary power property (ceil(log(N+1)/log(2))), remove all the unused columns (the columns with zeroes).

There are, obviously, easier ways to do this. But I am doing this example to show you can do this in base R. No need fancy package stuff.

Benchmarking Performance of Encoding

We are going to benchmark the performance of four types of encoding:

Categorical Encoding (raw, as is)

(raw, as is) Numeric Encoding

One-Hot Encoding

Binary Encoding

We will use rpart as the decision tree learning model, as it is also independent to random seeds.

The experimental design is the following:

We create datasets of one categorical feature with 8 to 8,192 cardinalities (steps of power of 2).

(steps of power of 2). We use 25% or 50% of cardinalities as positive labels to assess performance of the decision tree. This means a ratio of 1:3 or 1:1 .

of the decision tree. This means a . We run 250 times each combination of cardinalities and percentage of positive labels to get a better expected value (mean) of performance.

to get a (mean) of performance. To speed up the computations, we are using 6 parallel threads as One-Hot Encoding is computationally intensive.

as One-Hot Encoding is computationally intensive. The rpart function is limited to a maximum depth of 30 for practical usage, and used with the following parameters:

rpart(label ~ .,

data = my_data,

method = ""class"",

parms = list(split = ""information""),

control = rpart.control(minsplit = 1,

minbucket = 1,

cp = 1e-15,

maxcompete = 1,

maxsurrogate = 1,

usesurrogate = 0,

xval = 1,

surrogatestyle = 1,

maxdepth = 30))

Warning: remember we are doing this on a synthetic dataset with perfect rules. You may get contradictory performance on real world datasets.

Sneak Peak: how are the decision trees looking?

For the sake of example, with 1024 categories and 25% positive labels.

Categorical Encoding

Numeric Encoding

One-Hot Encoding

Binary Encoding

One can understand it as the following:

Categorical encoding well… equality rules so it’s easy to split .

well… . Numeric encoding requires splitting itself , if it splits 30 times in a row on the same branch then it’s over. If a split is frozen afterwards, then it is also over for that branch. Therefore, lot of RNG shaping!

, if it splits 30 times in a row on the same branch then it’s over. One hot encoding requires.. as many splits as there are categories, which means a crazy lot and so much it stops very quickly because if you do one split, one part of the split will be frozen (because it is a perfect rule dataset) => it gives this escalator-like shape , thus very poorly performing.

and so much => it gives this , thus very poorly performing. Binary encoding has less than 30 features in all my cases, therefore each tree should be able to depict all the rules (theory is true, practice is wrong because you need splits to not close on themselves, which is not possible in theory, but possible in practice) => it gives this right-tailed tree shape. When the unbalancing increases, the performance increases because it requires a lower expected value (mean) of splits to perform a perfect split than a perfectly balanced case.

General Accuracy of Encodings

Without looking in depth into the accuracy, we are going to look quickly at the general accuracy of the four encodings we have.

On the picture, we can clearly notice a trend:

Categorical Encoding is the clear winner , with an exact 100% Accuracy at any time.

, with an exact 100% Accuracy at any time. Numeric Encoding is doing an excellent job as long as the number of cardinalities is not too large : from 1,024 cardinalities, its accuracy falls off drastically. Its accuracy over all the tests is around 92%.

: from 1,024 cardinalities, its accuracy falls off drastically. Its accuracy over all the tests is around 92%. One-Hot Encoding is doing an excellent job like Numeric Encoding, except it falls down very quickly : from 128 cardinalities, performing consistency worse than Binary Encoding. Its accuracy over all the runs is around 80%.

: from 128 cardinalities, performing consistency worse than Binary Encoding. Its accuracy over all the runs is around 80%. Binary Encoding is very consistent in performance but not perfect, even with 8,192 cardinalities. Its exact accuracy is approximately 91%.

Balancing Accuracy of Encodings

To look further at the details, we are splitting the balancing ratio (25% and 50%) of the positive label to check for discrepancies.

We are clearly noticing one major trend:

The more the dataset is unbalanced, the more accurate the encodings become.

Inversely: the more the dataset is balanced, the less accurate the encodings become.

We can also notice our Numeric Encoding is getting worse faster versus Binary Encoding when the dataset becomes more balanced: it becomes worse from 1024 cardinalities on a perfectly balanced dataset, while it becomes worse only from 2048 cardinalities on a 1:3 unbalanced dataset.

For One-Hot Encoding, it is even more pronounced: worse 256 cardinalities on a perfectly balanced dataset, to 128 cardinalities on a 1:3 unbalanced dataset.

In addition, there seems to be no reason to use One-Hot Encoding over Numeric Encoding according to the picture.

In addition, there are three specific trends we can capture:

Numeric Encoding is not giving consistent results , as the results are flying around a bit (compare the box plot sizes and you will notice it). It seems it requires a lot more cardinalities to converge to a stable performance . It depicts also more consistency “as the balancing becomes more unbalanced”.

, as the results are flying around a bit (compare the box plot sizes and you will notice it). It seems it requires . It depicts also more consistency “as the balancing becomes more unbalanced”. One-Hot Encoding is extremely consistent. So much consistent there is not much to say about if you want to approximate the predictive power of a categorical feature when alone.

So much consistent there is not much to say about if you want to approximate the predictive power of a categorical feature when alone. Binary Encoding gets consistent when the cardinality increases, while maintaining a stable performance. It depicts also more consistency “as the balancing becomes more unbalanced”. Perhaps someone has a mathematical proof of convergence towards a specific limit, given the cardinality and the balancing ratio?

Training Time

The training time is provided here as an example on dense data using rpart . Each model was run 250 times, with the median for:

25 times per run for Categorical Encoding.

25 times per run for Numeric Encoding.

1 time per run for One-Hot Encoding (too long…).

10 times per run for Binary Encoding.

It shows why you should avoid One-Hot Encoding on rpart , as the training time of the decision tree literally explodes!:

Data is dominated by One-Hot Encoding slowness.

Without One-Hot Encoding scaling issue, we have the following:

A more fair comparison.

As we can clearly notice, if you are ready to spend a bit more time doing computations, then Numeric and Binary Encodings are fine.

Conclusion (tl;dr)

A simple resume with a picture and two lines of text:

Categorical features with large cardinalities (over 1000): Binary

Categorical features with small cardinalities (less than 1000): Numeric","['numeric', 'onehot', 'decision', 'encoding', 'performance', 'trees', 'model', 'categorical', 'features', 'binary', 'accuracy', 'visiting', 'cardinalities']","When you have categorical features and you are using decision trees, you often have a major issue: how to deal with categorical features?
Let’s go back to the basics of decision trees and encoding, then we can test some good stuff… in three parts:Machine Learning Implementations: specifications differExample ways of Encoding categorical featuresBenchmarking Encodings versus vanilla Categorical featuresDecision Trees and EncodingMachine Learning SpecificationWhen using decision tree models and categorical features, you mostly have three types of models:Models handling categorical features CORRECTLY.
You just throw the categorical features at the model in the appropriate format (ex: as factors in R), AND the machine learning model processes categorical features correctly as categoricals.
We are going to check three of them: numeric encoding, one-hot encoding, and binary encoding.
Categorical Encoding SpecificationCategorical Encoding refers to transforming a categorical feature into one or multiple numeric features.",en,[],2017-04-23 20:50:03.775000+00:00,"{'Machine Learning', 'Design', 'Data Science'}","{'https://miro.medium.com/fit/c/80/80/1*1LgTr6bGxTDFnxorWOduPg.jpeg', 'https://miro.medium.com/max/60/1*lEjccubJCtwkfr4HHWc8SA.png?q=20', 'https://miro.medium.com/max/932/1*-c8dsEp94o8uJPYOvw3nfA.png', 'https://miro.medium.com/max/60/1*WMk6QRsMkqVyKhMCGJKGOg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*7Hrwqjb7oJTh4MBc.jpg', 'https://miro.medium.com/max/1210/1*07jRuWWKez4iXK1CBOWY1w.png', 'https://miro.medium.com/max/1145/1*-ZAg5ftPd7KN_cc4Btk7Ww.png', 'https://miro.medium.com/max/60/1*z7KijtBFy35ydz55H8XNWA.png?q=20', 'https://miro.medium.com/max/60/1*LR0s5i1qrLW4iuPxTBCukQ.png?q=20', 'https://miro.medium.com/max/566/1*H7u5y1OmAeeiejUfDDbRcA.png', 'https://miro.medium.com/fit/c/160/160/1*alFoCI-LXbOdi45cy0F6Xg.jpeg', 'https://miro.medium.com/max/60/1*3hdYEX5eixaV4F3wT5OmBg.png?q=20', 'https://miro.medium.com/max/742/1*AD1XkhBRVozu43Tey4xv-w.png', 'https://miro.medium.com/fit/c/80/80/1*alFoCI-LXbOdi45cy0F6Xg.jpeg', 'https://miro.medium.com/max/1296/1*WMk6QRsMkqVyKhMCGJKGOg.png', 'https://miro.medium.com/max/1296/1*uQoAMGOayWbsXpE78p-6uA.png', 'https://miro.medium.com/max/60/1*Vnk7QCx7pM4exR-IBiDwcA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*NEsnmZDw7wXjVHJZvUOFgQ.png', 'https://miro.medium.com/max/1296/1*Vnk7QCx7pM4exR-IBiDwcA.png', 'https://miro.medium.com/max/1296/1*bRPWzRO_rY_eNijBTu9cyw.png', 'https://miro.medium.com/max/1296/1*x-c8RG3lD5s3ccBs_WEGKg.png', 'https://miro.medium.com/max/60/1*V8SJfnGK-9W99KsO5MhtwQ.png?q=20', 'https://miro.medium.com/max/60/1*H7u5y1OmAeeiejUfDDbRcA.png?q=20', 'https://miro.medium.com/max/592/1*V8SJfnGK-9W99KsO5MhtwQ.png', 'https://miro.medium.com/max/1572/1*lEjccubJCtwkfr4HHWc8SA.png', 'https://miro.medium.com/max/1044/1*3hdYEX5eixaV4F3wT5OmBg.png', 'https://miro.medium.com/max/60/1*-ZAg5ftPd7KN_cc4Btk7Ww.png?q=20', 'https://miro.medium.com/max/1296/1*a6xT1bptDKqQFG2B_fjiRQ.png', 'https://miro.medium.com/max/60/1*07jRuWWKez4iXK1CBOWY1w.png?q=20', 'https://miro.medium.com/max/1296/1*z7KijtBFy35ydz55H8XNWA.png', 'https://miro.medium.com/max/60/1*-c8dsEp94o8uJPYOvw3nfA.png?q=20', 'https://miro.medium.com/max/60/1*bRPWzRO_rY_eNijBTu9cyw.png?q=20', 'https://miro.medium.com/max/600/1*vb8xlgaJCxKiaPitVHn-cg.png', 'https://miro.medium.com/max/60/1*uQoAMGOayWbsXpE78p-6uA.png?q=20', 'https://miro.medium.com/max/2290/1*-ZAg5ftPd7KN_cc4Btk7Ww.png', 'https://miro.medium.com/max/60/1*2iqrxmPiwRC-xrHu0nM_Iw.png?q=20', 'https://miro.medium.com/max/60/1*x-c8RG3lD5s3ccBs_WEGKg.png?q=20', 'https://miro.medium.com/max/60/1*a6xT1bptDKqQFG2B_fjiRQ.png?q=20', 'https://miro.medium.com/max/1296/1*2iqrxmPiwRC-xrHu0nM_Iw.png', 'https://miro.medium.com/max/60/1*AD1XkhBRVozu43Tey4xv-w.png?q=20', 'https://miro.medium.com/max/928/1*LR0s5i1qrLW4iuPxTBCukQ.png', 'https://miro.medium.com/max/60/1*IcIIrz8HpO-48vsFxYDJDQ.png?q=20', 'https://miro.medium.com/max/1296/1*IcIIrz8HpO-48vsFxYDJDQ.png', 'https://miro.medium.com/fit/c/96/96/1*alFoCI-LXbOdi45cy0F6Xg.jpeg'}",2020-03-05 00:12:39.394123,1.634688138961792
https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9,"Accuracy, Precision, Recall or F1?","Which metrics to choose?

Often when I talk to organizations that are looking to implement data science into their processes, they often ask the question, “How do I get the most accurate model?”. And I asked further, “What business challenge are you trying to solve using the model?” and I will get the puzzling look because the question that I posed does not really answer their question. I will then need to explain why I asked the question before we start exploring if Accuracy is the be-all and end-all model metric that we shall choose our “best” model from.

So I thought I will explain in this blog post that Accuracy need not necessary be the one-and-only model metrics data scientists chase and include simple explanation of other metrics as well.

Firstly, let us look at the following confusion matrix. What is the accuracy for the model?

Very easily, you will notice that the accuracy for this model is very very high, at 99.9%!! Wow! You have hit the jackpot and holy grail (*scream and run around the room, pumping the fist in the air several times*)!

But….(well you know this is coming right?) what if I mentioned that the positive over here is actually someone who is sick and carrying a virus that can spread very quickly? Or the positive here represent a fraud case? Or the positive here represents terrorist that the model says its a non-terrorist? Well you get the idea. The costs of having a mis-classified actual positive (or false negative) is very high here in these three circumstances that I posed.

OK, so now you realized that accuracy is not the be-all and end-all model metric to use when selecting the best model…now what?

Precision and Recall

Let me introduce two new metrics (if you have not heard about it and if you do, perhaps just humor me a bit and continue reading? :D )

So if you look at Wikipedia, you will see that the the formula for calculating Precision and Recall is as follows:

Let me put it here for further explanation.

Let me put in the confusion matrix and its parts here.

Precision

Great! Now let us look at Precision first.

What do you notice for the denominator? The denominator is actually the Total Predicted Positive! So the formula becomes

True Positive + False Positive = Total Predicted Positive

Immediately, you can see that Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.

Precision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.

Recall

So let us apply the same logic for Recall. Recall how Recall is calculated.

True Positive + False Negative = Actual Positive

There you go! So Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.

For instance, in fraud detection or sick patient detection. If a fraudulent transaction (Actual Positive) is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.

Similarly, in sick patient detection. If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.

F1 Score

Now if you read a lot of other literature on Precision and Recall, you cannot avoid the other measure, F1 which is a function of Precision and Recall. Looking at Wikipedia, the formula is as follows:

F1 Score is needed when you want to seek a balance between Precision and Recall. Right…so what is the difference between F1 Score and Accuracy then? We have previously seen that accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).

I hope the explanation will help those starting out on Data Science and working on Classification problems, that Accuracy will not always be the metric to select the best model from.

Note: I have moved this blog post over to my website with an additional section. Extended post here. Do visit my other blog posts and LinkedIn profile. Thank you!

I wish all readers a FUN Data Science learning journey.","['recall', 'false', 'actual', 'high', 'negative', 'precision', 'model', 'positive', 'predicted', 'accuracy', 'f1']","True Positive + False Negative = Actual PositiveThere you go!
If a fraudulent transaction (Actual Positive) is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.
If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative).
F1 ScoreNow if you read a lot of other literature on Precision and Recall, you cannot avoid the other measure, F1 which is a function of Precision and Recall.
Looking at Wikipedia, the formula is as follows:F1 Score is needed when you want to seek a balance between Precision and Recall.",en,['Koo Ping Shung'],2020-01-22 02:11:27.649000+00:00,"{'Machine Learning', 'Data Science'}","{'https://miro.medium.com/max/60/1*TS2hsRr528UHQG9nDJhIcA.jpeg?q=20', 'https://miro.medium.com/max/1280/1*UYb8WqF3VsAn_GRYRPIWGw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*PPiPQt9Tj4P0C4KJME3_bQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/640/1*UYb8WqF3VsAn_GRYRPIWGw.jpeg', 'https://miro.medium.com/max/60/1*dXkDleGhA-jjZmZ1BlYKXg.png?q=20', 'https://miro.medium.com/max/1520/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg', 'https://miro.medium.com/max/60/1*BBhWQC-m0CLN4sVJ0h5fJQ.jpeg?q=20', 'https://miro.medium.com/max/888/1*7J08ekAwupLBegeUI8muHA.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/948/1*HGd3_eAJ3-PlDQvn-xDRdg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1520/1*PULzWEven_XAZjiMNizDCg.png', 'https://miro.medium.com/max/888/1*C3ctNdO0mde9fa1PFsCVqA.png', 'https://miro.medium.com/max/60/1*T6kVUKxG_Z4V5Fm1UXhEIw.png?q=20', 'https://miro.medium.com/max/60/1*C3ctNdO0mde9fa1PFsCVqA.png?q=20', 'https://miro.medium.com/max/1224/1*TS2hsRr528UHQG9nDJhIcA.jpeg', 'https://miro.medium.com/max/564/1*T6kVUKxG_Z4V5Fm1UXhEIw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*PULzWEven_XAZjiMNizDCg.png?q=20', 'https://miro.medium.com/max/60/1*7J08ekAwupLBegeUI8muHA.png?q=20', 'https://miro.medium.com/max/60/1*UYb8WqF3VsAn_GRYRPIWGw.jpeg?q=20', 'https://miro.medium.com/max/60/1*HGd3_eAJ3-PlDQvn-xDRdg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*PPiPQt9Tj4P0C4KJME3_bQ.jpeg', 'https://miro.medium.com/max/836/1*dXkDleGhA-jjZmZ1BlYKXg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1520/1*BBhWQC-m0CLN4sVJ0h5fJQ.jpeg', 'https://miro.medium.com/max/60/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg?q=20'}",2020-03-05 00:12:46.343796,6.94867205619812
https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc,Logistic Regression — Detailed Overview,"Figure 1: Logistic Regression Model (Source:http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/)

Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications. Logistic Regression is used when the dependent variable(target) is categorical.

For example,

To predict whether an email is spam (1) or (0)

Whether the tumor is malignant (1) or not (0)

Consider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.

From this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.

Simple Logistic Regression

(Full Source code: https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb)

Model

Output = 0 or 1

Hypothesis => Z = WX + B

hΘ(x) = sigmoid (Z)

Sigmoid Function

Figure 2: Sigmoid Activation Function

If ‘Z’ goes to infinity, Y(predicted) will become 1 and if ‘Z’ goes to negative infinity, Y(predicted) will become 0.

Analysis of the hypothesis

The output from the hypothesis is the estimated probability. This is used to infer how confident can predicted value be actual value when given an input X. Consider the below example,

X = [x0 x1] = [1 IP-Address]

Based on the x1 value, let’s say we obtained the estimated probability to be 0.8. This tells that there is 80% chance that an email will be spam.

Mathematically this can be written as,

Figure 3: Mathematical Representation

This justifies the name ‘logistic regression’. Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.

Types of Logistic Regression

1. Binary Logistic Regression

The categorical response has only two 2 possible outcomes. Example: Spam or Not

2. Multinomial Logistic Regression

Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)

3. Ordinal Logistic Regression

Three or more categories with ordering. Example: Movie rating from 1 to 5

Decision Boundary

To predict which class a data belongs, a threshold can be set. Based upon this threshold, the obtained estimated probability is classified into classes.

Say, if predicted_value ≥ 0.5, then classify email as spam else as not spam.

Decision boundary can be linear or non-linear. Polynomial order can be increased to get complex decision boundary.

Cost Function

Figure 4: Cost Function of Logistic Regression

Why cost function which has been used for linear can not be used for logistic?

Linear regression uses mean squared error as its cost function. If this is used for logistic regression, then it will be a non-convex function of parameters (theta). Gradient descent will converge into global minimum only if the function is convex.

Figure 5: Convex and non-convex cost function

Cost function explanation

Figure 6: Cost Function part 1

Figure 7: Cost Function part 2

Simplified cost function

Figure 8: Simplified Cost Function

Why this cost function?

Figure 9: Maximum Likelihood Explanation part-1

Figure 10: Maximum Likelihood Explanation part-2

This negative function is because when we train, we need to maximize the probability by minimizing loss function. Decreasing the cost will increase the maximum likelihood assuming that samples are drawn from an identically independent distribution.

Deriving the formula for Gradient Descent Algorithm

Figure 11: Gradient Descent Algorithm part 1

Figure 12: Gradient Descent part 2

Python Implementation

def weightInitialization(n_features):

w = np.zeros((1,n_features))

b = 0

return w,b def sigmoid_activation(result):

final_result = 1/(1+np.exp(-result))

return final_result

def model_optimize(w, b, X, Y):

m = X.shape[0]



#Prediction

final_result = sigmoid_activation(np.dot(w,X.T)+b)

Y_T = Y.T

cost = (-1/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))

#



#Gradient calculation

dw = (1/m)*(np.dot(X.T, (final_result-Y.T).T))

db = (1/m)*(np.sum(final_result-Y.T))



grads = {""dw"": dw, ""db"": db}



return grads, cost def model_predict(w, b, X, Y, learning_rate, no_iterations):

costs = []

for i in range(no_iterations):

#

grads, cost = model_optimize(w,b,X,Y)

#

dw = grads[""dw""]

db = grads[""db""]

#weight update

w = w - (learning_rate * (dw.T))

b = b - (learning_rate * db)

#



if (i % 100 == 0):

costs.append(cost)

#print(""Cost after %i iteration is %f"" %(i, cost))



#final parameters

coeff = {""w"": w, ""b"": b}

gradient = {""dw"": dw, ""db"": db}



return coeff, gradient, costs def predict(final_pred, m):

y_pred = np.zeros((1,m))

for i in range(final_pred.shape[1]):

if final_pred[0][i] > 0.5:

y_pred[0][i] = 1

return y_pred

Cost vs Number_of_Iterations

Figure 13: Cost Reduction

Train and test accuracy of the system is 100 %

This implementation is for binary logistic regression. For data with more than 2 classes, softmax regression has to be used.

This is an educational post and inspired from Prof. Andrew Ng’s deep learning course.

Full code : https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb","['overview', 'regression', 'function', 'used', 'gradient', 'threshold', 'logistic', 'detailed', 'linear', 'spam', 'value', 'cost']","Figure 1: Logistic Regression Model (Source:http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/)Logistic Regression was used in the biological sciences in early twentieth century.
Linear regression is unbounded, and this brings logistic regression into picture.
Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.
Cost FunctionFigure 4: Cost Function of Logistic RegressionWhy cost function which has been used for linear can not be used for logistic?
Figure 5: Convex and non-convex cost functionCost function explanationFigure 6: Cost Function part 1Figure 7: Cost Function part 2Simplified cost functionFigure 8: Simplified Cost FunctionWhy this cost function?",en,['Saishruthi Swaminathan'],2019-01-18 19:46:44.482000+00:00,"{'Logistic Regression', 'Data', 'Statistics', 'Predictive Analytics'}","{'https://miro.medium.com/max/60/1*TqZ9myxIdLuKNmt8orCeew.png?q=20', 'https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png', 'https://miro.medium.com/max/60/1*uRaeTkF5Ig_DYZwR8HiJMQ.png?q=20', 'https://miro.medium.com/max/3370/1*heGae4aZ-dN-rLsfx2-P9g.jpeg', 'https://miro.medium.com/max/60/1*i_QQvUzXCETJEelf4mLx8Q.png?q=20', 'https://miro.medium.com/max/1600/1*UgYbimgPXf6XXxMy2yqRLw.png', 'https://miro.medium.com/max/1244/1*uRaeTkF5Ig_DYZwR8HiJMQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/926/1*ZyjEj3A_QyR4WY7y5cwIWQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*MFMIEUC_dobhJrRjGK7PBg.jpeg?q=20', 'https://miro.medium.com/max/3904/1*5AYaGPV-gjYUf37d2IhgTQ.jpeg', 'https://miro.medium.com/max/60/1*heGae4aZ-dN-rLsfx2-P9g.jpeg?q=20', 'https://miro.medium.com/max/1636/1*ueEwU1dE0Yu-KpMJanf9AQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/996/1*TqZ9myxIdLuKNmt8orCeew.png', 'https://miro.medium.com/max/56/1*pJEi5f4gdVGezYev9MChBw.jpeg?q=20', 'https://miro.medium.com/max/800/1*UgYbimgPXf6XXxMy2yqRLw.png', 'https://miro.medium.com/max/5120/1*JIpaau-jFfvX2yR9L1YZ6A.jpeg', 'https://miro.medium.com/max/60/1*5AYaGPV-gjYUf37d2IhgTQ.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*2WMVRigPc4Y1q3y8vuPbow@2x.jpeg', 'https://miro.medium.com/max/1096/1*i_QQvUzXCETJEelf4mLx8Q.png', 'https://miro.medium.com/max/60/1*ZyjEj3A_QyR4WY7y5cwIWQ.png?q=20', 'https://miro.medium.com/max/60/1*UgYbimgPXf6XXxMy2yqRLw.png?q=20', 'https://miro.medium.com/max/60/1*r7fhk417IOuq7meXIctGXg.jpeg?q=20', 'https://miro.medium.com/max/3776/1*pJEi5f4gdVGezYev9MChBw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*2WMVRigPc4Y1q3y8vuPbow@2x.jpeg', 'https://miro.medium.com/max/3840/1*r7fhk417IOuq7meXIctGXg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*RqXFpiNGwdiKBWyLJc_E7g.png?q=20', 'https://miro.medium.com/max/60/1*JIpaau-jFfvX2yR9L1YZ6A.jpeg?q=20', 'https://miro.medium.com/max/60/1*ueEwU1dE0Yu-KpMJanf9AQ.png?q=20', 'https://miro.medium.com/max/3872/1*MFMIEUC_dobhJrRjGK7PBg.jpeg'}",2020-03-05 00:12:48.195437,1.8506426811218262
https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea,Basic Time Series Manipulation with Pandas,"As someone who works with time series data on almost a daily basis, I have found the pandas Python package to be extremely useful for time series manipulation and analysis.

This basic introduction to time series data manipulation with pandas should allow you to get started in your time series analysis. Specific objectives are to show you how to:

create a date range

work with timestamp data

convert string data to a timestamp

index and slice your time series data in a data frame

resample your time series for different time period aggregates/summary statistics

compute a rolling statistic such as a rolling average

work with missing data

understand the basics of unix/epoch time

understand common pitfalls of time series data analysis

Let’s get started. If you want to play with real data that you have, you may want to start by using pandas read_csv to read in your file to a data frame, however we’re going to start by playing with generated data.

First import the libraries we’ll be working with and then use them to create a date range

import pandas as pd

from datetime import datetime

import numpy as np date_rng = pd.date_range(start='1/1/2018', end='1/08/2018', freq='H')

This date range has timestamps with an hourly frequency. If we call date_rng we’ll see that it looks like the following:

DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',

'2018-01-01 02:00:00', '2018-01-01 03:00:00',

'2018-01-01 04:00:00', '2018-01-01 05:00:00',

'2018-01-01 06:00:00', '2018-01-01 07:00:00',

'2018-01-01 08:00:00', '2018-01-01 09:00:00',

...

'2018-01-07 15:00:00', '2018-01-07 16:00:00',

'2018-01-07 17:00:00', '2018-01-07 18:00:00',

'2018-01-07 19:00:00', '2018-01-07 20:00:00',

'2018-01-07 21:00:00', '2018-01-07 22:00:00',

'2018-01-07 23:00:00', '2018-01-08 00:00:00'],

dtype='datetime64[ns]', length=169, freq='H')

We can check the type of the first element:

type(date_rng[0]) #returns pandas._libs.tslib.Timestamp

Let’s create an example data frame with the timestamp data and look at the first 15 elements:

df = pd.DataFrame(date_rng, columns=['date']) df['data'] = np.random.randint(0,100,size=(len(date_rng))) df.head(15)

Example data frame — df

If we want to do time series manipulation, we’ll need to have a date time index so that our data frame is indexed on the timestamp.

Convert the data frame index to a datetime index then show the first elements:

df['datetime'] = pd.to_datetime(df['date']) df = df.set_index('datetime') df.drop(['date'], axis=1, inplace=True) df.head()

df with datetime index

What if our ‘time’ stamps in our data are actually string type vs. numerical? Let’s convert our date_rng to a list of strings and then convert the strings to timestamps.

string_date_rng = [str(x) for x in date_rng] string_date_rng #returns ['2018-01-01 00:00:00',

'2018-01-01 01:00:00',

'2018-01-01 02:00:00',

'2018-01-01 03:00:00',

'2018-01-01 04:00:00',

'2018-01-01 05:00:00',

'2018-01-01 06:00:00',

'2018-01-01 07:00:00',

'2018-01-01 08:00:00',

'2018-01-01 09:00:00',...

We can convert the strings to timestamps by inferring their format, then look at the values:

timestamp_date_rng = pd.to_datetime(string_date_rng, infer_datetime_format=True) timestamp_date_rng #returns DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',

'2018-01-01 02:00:00', '2018-01-01 03:00:00',

'2018-01-01 04:00:00', '2018-01-01 05:00:00',

'2018-01-01 06:00:00', '2018-01-01 07:00:00',

'2018-01-01 08:00:00', '2018-01-01 09:00:00',

...

'2018-01-07 15:00:00', '2018-01-07 16:00:00',

'2018-01-07 17:00:00', '2018-01-07 18:00:00',

'2018-01-07 19:00:00', '2018-01-07 20:00:00',

'2018-01-07 21:00:00', '2018-01-07 22:00:00',

'2018-01-07 23:00:00', '2018-01-08 00:00:00'],

dtype='datetime64[ns]', length=169, freq=None)

But what about if we need to convert a unique string format?

Let’s create an arbitrary list of dates that are strings and convert them to timestamps:

string_date_rng_2 = ['June-01-2018', 'June-02-2018', 'June-03-2018'] timestamp_date_rng_2 = [datetime.strptime(x,'%B-%d-%Y') for x in string_date_rng_2] timestamp_date_rng_2 #returns [datetime.datetime(2018, 6, 1, 0, 0),

datetime.datetime(2018, 6, 2, 0, 0),

datetime.datetime(2018, 6, 3, 0, 0)]

What does it look like if we put this into a data frame?

df2 = pd.DataFrame(timestamp_date_rng_2, columns=['date']) df2

Going back to our original data frame, let’s look at the data by parsing on timestamp index:

Say we just want to see data where the date is the 2nd of the month, we could use the index as per below.

df[df.index.day == 2]

The top of this looks like:

We could also directly call a date that we want to look at via the index of the data frame:

df['2018-01-03']

What about selecting data between certain dates?

df['2018-01-04':'2018-01-06']

The basic data frame that we’ve populated gives us data on an hourly frequency, but we can resample the data at a different frequency and specify how we would like to compute the summary statistic for the new sample frequency. We could take the min, max, average, sum, etc., of the data at a daily frequency instead of an hourly frequency as per the example below where we compute the daily average of the data:

df.resample('D').mean()

What about window statistics such as a rolling mean or a rolling sum?

Let’s create a new column in our original df that computes the rolling sum over a 3 window period and then look at the top of the data frame:

df['rolling_sum'] = df.rolling(3).sum()

df.head(10)

We can see that this is computing correctly and that it only starts having valid values when there are three periods over which to look back.

This is a good chance to see how we can do forward or backfilling of data when working with missing data values.

Here’s our df but with a new column that takes the rolling sum and backfills the data:

df['rolling_sum_backfilled'] = df['rolling_sum'].fillna(method='backfill')

df.head(10)

It’s often useful to be able to fill your missing data with realistic values such as the average of a time period, but always remember that if you are working with a time series problem and want your data to be realistic, you should not do a backfill of your data as that’s like looking into the future and getting information you would never have at that time period. Likely you will want to forward fill your data more frequently than you backfill.

When working with time series data, you may come across time values that are in Unix time. Unix time, also called Epoch time is the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970. Using Unix time helps to disambiguate time stamps so that we don’t get confused by time zones, daylight savings time, etc.

Here’s an example of a time t that is in Epoch time and converting unix/epoch time to a regular time stamp in UTC:

epoch_t = 1529272655

real_t = pd.to_datetime(epoch_t, unit='s') real_t #returns Timestamp('2018-06-17 21:57:35')

If I wanted to convert that time that is in UTC to my own time zone, I could simply do the following:

real_t.tz_localize('UTC').tz_convert('US/Pacific') #returns Timestamp('2018-06-17 14:57:35-0700', tz='US/Pacific')

With these basics, you should be all set to work with your time series data.

Here are a few tips to keep in mind and common pitfalls to avoid when working with time series data:","['frequency', 'pandas', 'manipulation', '20180107', 'series', '20180101', 'frame', 'returns', 'date', 'rolling', 'data', 'basic', 'look']","As someone who works with time series data on almost a daily basis, I have found the pandas Python package to be extremely useful for time series manipulation and analysis.
This basic introduction to time series data manipulation with pandas should allow you to get started in your time series analysis.
If you want to play with real data that you have, you may want to start by using pandas read_csv to read in your file to a data frame, however we’re going to start by playing with generated data.
When working with time series data, you may come across time values that are in Unix time.
Here are a few tips to keep in mind and common pitfalls to avoid when working with time series data:",en,['Laura Fedoruk'],2018-06-18 04:49:59.688000+00:00,"{'Pandas', 'Data Science', 'Timeseries', 'Python', 'Programming'}","{'https://miro.medium.com/max/310/1*EAMBuYLGbnQOssQ5iKhh7Q.png', 'https://miro.medium.com/max/208/1*MD2S1B9hTlEPXjeLymyK5w.png', 'https://miro.medium.com/max/60/1*8EtGaf4aiSBAay9lZfAl8A.png?q=20', 'https://miro.medium.com/max/802/1*8EtGaf4aiSBAay9lZfAl8A.png', 'https://miro.medium.com/max/26/1*jM93ti4vsZaXLnCpWdPweQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/26/1*qeBhXKC_SzfAiItiZZopFQ.png?q=20', 'https://miro.medium.com/max/32/1*EAMBuYLGbnQOssQ5iKhh7Q.png?q=20', 'https://miro.medium.com/max/54/1*MD2S1B9hTlEPXjeLymyK5w.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*B-cVk6PZZxDjrLTt0prhtQ.jpeg', 'https://miro.medium.com/max/60/1*jaiEby0lYALbh94e2tWVPA.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/354/1*9vbzx1FO87ZJapWwXKId2A.png', 'https://miro.medium.com/max/28/1*QcnXFyXnNz7vVqYztH__aw.png?q=20', 'https://miro.medium.com/max/506/1*tjMO67HZ4j2HA-d3VR063Q.png', 'https://miro.medium.com/max/28/1*iOWtUCR12XLt5IDDehxKnA.png?q=20', 'https://miro.medium.com/max/46/1*tjMO67HZ4j2HA-d3VR063Q.png?q=20', 'https://miro.medium.com/max/3820/1*jaiEby0lYALbh94e2tWVPA.jpeg', 'https://miro.medium.com/max/356/1*qeBhXKC_SzfAiItiZZopFQ.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/366/1*QcnXFyXnNz7vVqYztH__aw.png', 'https://miro.medium.com/max/408/1*iOWtUCR12XLt5IDDehxKnA.png', 'https://miro.medium.com/max/364/1*jM93ti4vsZaXLnCpWdPweQ.png', 'https://miro.medium.com/max/52/1*9vbzx1FO87ZJapWwXKId2A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*B-cVk6PZZxDjrLTt0prhtQ.jpeg', 'https://miro.medium.com/max/1200/1*jaiEby0lYALbh94e2tWVPA.jpeg'}",2020-03-05 00:12:55.145783,6.949347734451294
https://towardsdatascience.com/an-introduction-to-gpu-optimization-6ea255ef6360,An Introduction to GPU Optimization,"CUDA stands for Compute Unified Device Architecture. These are running at relatively low speeds but provides greater parallelism by employing a large number of ALUs (Arithmetic and Logical Units). Read more here.

This diagram demonstrates the threading model in CUDA (this is quite the same as other architectures in the market such as AMD). To make it simple to start with, we may assume that each CUDA core or a GPU core can run a thread at a time. If we have a large data set, we can break it into pieces. A Grid contains several Blocks, and Block is another matrix which contains a number of threads equal to the size of it. Anyhow, as this is the introduction let’s focus on the bigger picture with a simpler API developed using JAVA.

Think GPU

As we have discussed, each GPU core is capable of running a separate thread. The simplest way to start the analogy is assuming that each cell of the matrix will be calculated by a single GPU core. And since all the cores are running in parallel, all the cells will be computed in parallel. Therefore, our time complexity suddenly drops to O(n). Now, for the 2000 x 2000 matrix we just need 2,000 runs, which is quite easy for a computer to compute. Usually each the threads we discussed before, knows its identity, which is the Block and the Grid it belongs to. Or in more simpler words the cell location of the matrix. Also, the matrix will be loaded into the shared memory of the GPU where we can directly access cell data by indices and process in parallel. Easy right? Let’s check on the code.

GPU Programming with APARAPI

What? Well, APARAPI (A-PARallel-API) is a JAVA wrapper for OpenCL which is the Open Computing Language used for programming GPUs. This supports both CUDA architecture and AMD devices. Also, the API bring in the greater object orientation of JAVA into the picture, which might look like a mess if we directly jump in to the task with C++. Getting started is quite easy. There is a maven dependency. But make sure you have correctly setup OpenCL or CUDA. Simple googling should help you with how. Most of the devices come with them bundled in (OSX and Windows devices).

pom.xml

MatrixMultiplication.java

Elaboration of the above code

Kernel is the part of the code that is being executed with the GPU. The variables that are visible to the Kernel will be copied to the GPU RAM. We are feeding data in terms of linear arrays rather than 2D arrays as this is the way supported by the GPUs. Not that they are not capable of handling 2D arrays, but the way they are handled is through the concept of dimensions (We won’t talk it just yet).

Range range = Range.create(SIZE * SIZE);

The above code allocates memory in the GPU so that SIZE x SIZE amount of threads or less (As available) running in the GPU.

int row = getGlobalId() / SIZE;

int col = getGlobalId() % SIZE;

The above code obtains the Id of the thread from its private memory. With this for that particular thread, we can identify the thread’s cell location. For each cell we would do the following.

for (int i = 0; i < SIZE; i++) {

d[row * SIZE + col] += a[row * SIZE + i] * b[i * SIZE + col];

}

This is the simple sum of multiples of the corresponding cells of the two matrices. We just define the Kernel for a single thread using thread indices, which will be run in parallel for all the threads.

Results

It is fast. But how fast?. This is the output of the above program.

1200 x 1200

Starting single threaded computation

Task finished in 25269ms

Starting GPU computation

Task finished in 1535ms

Following was run only for the GPU component since times were quite large for CPU to compute.

2000 x 2000 Task finished in 3757ms

5000 x 5000 Task finished in 5402ms

Hope you enjoyed reading. Please do try!! Cheers!!","['introduction', 'thread', 'cell', 'quite', 'cuda', 'gpu', 'size', 'optimization', 'x', 'threads', 'matrix', 'running']","Anyhow, as this is the introduction let’s focus on the bigger picture with a simpler API developed using JAVA.
Think GPUAs we have discussed, each GPU core is capable of running a separate thread.
The simplest way to start the analogy is assuming that each cell of the matrix will be calculated by a single GPU core.
Range range = Range.create(SIZE * SIZE);The above code allocates memory in the GPU so that SIZE x SIZE amount of threads or less (As available) running in the GPU.
1200 x 1200Starting single threaded computationTask finished in 25269msStarting GPU computationTask finished in 1535msFollowing was run only for the GPU component since times were quite large for CPU to compute.",en,['Anuradha Wickramarachchi'],2020-03-01 21:42:33.527000+00:00,"{'Cuda', 'Towards Data Science', 'Programming', 'Computer Science', 'Gpu'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/4936/1*HmGsHOTOm5FQTFlW3SHUYg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/2*orEVV_c4Mim13iVbWi6z5A.jpeg', 'https://miro.medium.com/max/620/1*dVo15ilNgnTp862WRmqEGg.jpeg', 'https://miro.medium.com/max/60/1*dVo15ilNgnTp862WRmqEGg.jpeg?q=20', 'https://miro.medium.com/max/60/1*lJ8G9OrAGa5iG8BCloExLQ.jpeg?q=20', 'https://miro.medium.com/max/1240/1*dVo15ilNgnTp862WRmqEGg.jpeg', 'https://miro.medium.com/max/1136/1*6M1CNuVH_uxdb0ryEdAGew.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/2*orEVV_c4Mim13iVbWi6z5A.jpeg', 'https://miro.medium.com/max/60/1*HmGsHOTOm5FQTFlW3SHUYg.png?q=20', 'https://miro.medium.com/max/2202/1*lJ8G9OrAGa5iG8BCloExLQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/58/1*6M1CNuVH_uxdb0ryEdAGew.jpeg?q=20'}",2020-03-05 00:12:57.755178,2.6093955039978027
https://medium.com/vickdata/easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c,Easier Machine Learning with the New Column Transformer from Scikit-Learn,"Photo by Samule Sun on Unsplash

Last week scikit-learn released version 0.20.0, one of the features in this release I am most excited about is the ColumnTransformer. This function allows you to combine several feature extraction or transformation methods into a single transformer. Say, you are working on a machine learning problem, and you have a dataset containing a mixture of categorical and numerical columns. Rather than having to handle each of these separately, and perhaps writing a function to then apply this to new data. These can now be combined into a transformer which can easily be reapplied, and extended.

This is a part of my machine learning workflow I have been keen to simplify, so over the weekend I took a simple data set I had lying around on my laptop, and had a go at applying this to a classification problem.

The data set I am using was taken from the Analytics Vidhya loan prediction competition and can be downloaded here. The purpose of this competition is to predict wether or not a loan application will be successful based on a number of customer features. This contains both categorical and numerical variables, and is a nice simple data set to practice using the new ColumnTransformer.

Data Preparation

To start with I am using pandas to read in both files. Using the dtypes function I can see that there are both numerical and categorical features present in the dataset.

import pandas as pd

train = pd.read_csv('train.csv')

test = pd.read_csv('test.csv')

print(train.shape, test.shape)

print(train.dtypes)

Before going further I am going to drop the Loan_ID column as that will not be used in the model. I am also filling any null values with the most commonly occurring value for each column. There are of course a number of methods I could choose for this but as I am just trying out a new function I am not too worried about the accuracy of the model for now.

train = train.drop(['Loan_ID'], axis=1)

test = test.drop(['Loan_ID'], axis=1) train = train.apply(lambda x:x.fillna(x.value_counts().index[0]))

test = test.apply(lambda x:x.fillna(x.value_counts().index[0]))

Next I’m going to specify the features (X) and target (y). I then use the sklearn train_test_split function to divide the training data into test and train ready to train and validate the model.

feature_set = train.drop(['Loan_Status'], axis=1)

X = feature_set.columns[:len(feature_set.columns)]

y = 'Loan_Status' from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(

train[X], train[y], random_state=0)

ColumnTransformer

The next step is to apply transformation to columns to optimise them for use in the classification model. Usually I would write a function that applies these transformations, so that I can re-use it on both the test and train set, and any holdout data I may have for later validation. However, I am going to try using the ColumnTransformer to simplify these steps.

For simplicity, again I am not too worried about the accuracy of this model, I am going to transform the categorical columns using the sklearn OneHotEncoder. I will also normalize the numerical columns using the Normalizer function.

The ColumnTransformer takes a list of tuples specifying the transformers, and the corresponding columns on which the transformation needs to be applied. The columns can either be entered as strings specifying the column names in a pandas data frame, or as in the code I have used below, as integers which are interpreted as the column positions.

from sklearn.compose import ColumnTransformer

from sklearn.preprocessing import Normalizer, OneHotEncoder colT = ColumnTransformer(

[(""dummy_col"", OneHotEncoder(categories=[['Male', 'Female'],

['Yes', 'No'],

['0','1', '2','3+'],

['Graduate', 'Not Graduate'],

['No', 'Yes'],

['Semiurban', 'Urban', 'Rural']]), [0,1,2,3,4,10]),

(""norm"", Normalizer(norm='l1'), [5,6,7,8,9])])

You will notice in the code above I have used the categories argument of the OnHotEncoder function. This takes a list of all possible categories in each column as a list of lists. This produces one hot encoded columns for all categories even if data does not exist for that category in the column. The reason for doing this is that when using the ColumnTransformer function on new data. If it doesn’t contain the same categories in each feature then the array produced will not be the same shape as the data used to train the model, and you will get an error.

I apply the transformer to the training data as shown below. The first few rows of the output, which is a list of lists containing numerical arrays, is shown beneath the code.

X_train = colT.fit_transform(X_train)

X_train

To transform the X_test data you simply apply the column transformer again.

X_test = colT.transform(X_test)

Training the Model

The data is now ready to be used in any scikit-learn classifier. For simplicity I have just used a RandomForestClassifier model with the default parameters.

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, classification_report

random_forest = RandomForestClassifier()

random_forest.fit(X_train, y_train)

y_pred = random_forest.predict(X_test)

print(classification_report(y_test, y_pred, target_names=['Y', 'N']))

Predicting New Data

To test how the ColumnTransformer would work if we were to use this model to make predictions on previously unseen data. I took a sample of rows from the test csv file I read in earlier. I then simply re-use the column transformer to apply the preprocessing steps.

test_samp = test[:15]

test_samp = colT.transform(test_samp) random_forest.predict(test_samp)

Applying the Random Forest model to predict the loan status gives the following output.

Having used this new tool I have to say I am impressed. It has really simplified my workflow and I am going to be using this a lot in my work from now on. It is really simple to add new transformation steps into the ColumnTransformer, something which I will definitely be looking at over the next few weeks.","['machine', 'function', 'transformer', 'column', 'used', 'going', 'columns', 'learning', 'scikitlearn', 'data', 'model', 'columntransformer', 'easier', 'using']","Say, you are working on a machine learning problem, and you have a dataset containing a mixture of categorical and numerical columns.
I will also normalize the numerical columns using the Normalizer function.
The reason for doing this is that when using the ColumnTransformer function on new data.
X_train = colT.fit_transform(X_train)X_trainTo transform the X_test data you simply apply the column transformer again.
I then simply re-use the column transformer to apply the preprocessing steps.",en,['Rebecca Vickery'],2019-07-26 13:26:08.796000+00:00,"{'Data Science', 'Scikit Learn', 'Python', 'Machine Learning', 'Data Preprocess'}","{'https://miro.medium.com/fit/c/96/96/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/max/10076/1*V5zIWId9Q6dzxcR_zqBGZw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/max/452/1*S_vnb7J6eAAs8UdWJOeI9Q.png', 'https://miro.medium.com/fit/c/80/80/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/max/60/1*S_vnb7J6eAAs8UdWJOeI9Q.png?q=20', 'https://miro.medium.com/max/60/1*o0gDDLfGCvwXyQJ3PN_KLA.png?q=20', 'https://miro.medium.com/max/60/1*ILWs_IHVZg6qNkJKLOHdnw.png?q=20', 'https://miro.medium.com/max/60/1*vp0PxPblhlHakH2vmn6kXw.png?q=20', 'https://miro.medium.com/max/1054/1*ILWs_IHVZg6qNkJKLOHdnw.png', 'https://miro.medium.com/max/1200/1*V5zIWId9Q6dzxcR_zqBGZw.jpeg', 'https://miro.medium.com/max/2092/1*vp0PxPblhlHakH2vmn6kXw.png', 'https://miro.medium.com/max/1680/1*o0gDDLfGCvwXyQJ3PN_KLA.png', 'https://miro.medium.com/max/60/1*V5zIWId9Q6dzxcR_zqBGZw.jpeg?q=20'}",2020-03-05 00:13:00.703916,2.947735548019409
https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62,From Pandas to Scikit-Learn — A new exciting workflow,"The new ColumnTransformer will change workflows from Pandas to Scikit-Learn

This article is available as a Jupyter Notebook on Google’s Colaboratory (open in playground mode to run and edit) and at the Machine Learning Github repository for the Dunder Data Organization.

Become an Expert

If you want to be trusted to make decisions using pandas and scikit-learn, you must become an expert. I have completely mastered both libraries and have developed special techniques that will massively improve your ability and efficiency to do data analysis and machine learning.

Scikit-Learn’s new integration with Pandas

Scikit-Learn will make one of its biggest upgrades in recent years with its mammoth version 0.20 release. For many data scientists, a typical workflow consists of using Pandas to do exploratory data analysis before moving to scikit-learn for machine learning. This new release will make the process simpler, more feature-rich, robust, and standardized.

Summary and goals of this article

This article is aimed at those that use Scikit-Learn as their machine learning library but depend on Pandas as their data exploratory and preparation tool.

It assumes you have some familiarity with both Scikit-Learn and Pandas

We explore the new ColumnTransformer estimator, which allows us to apply separate transformations to different subsets of your data in parallel before concatenating the results together.

estimator, which allows us to apply separate transformations to different subsets of your data in parallel before concatenating the results together. A major pain point for users (and in my opinion the worst part of Scikit-Learn) was preparing a pandas DataFrame with string values in its columns. This process should become much more standardized.

The OneHotEncoder estimator was given a nice upgrade to encode columns with string values.

estimator was given a nice upgrade to encode columns with string values. To help with one hot encoding, we use the new SimpleImputer estimator to fill in missing values with constants

estimator to fill in missing values with constants We will build a custom estimator that does all the “basic” transformations on a DataFrame instead of relying on the built-in Scikit-Learn tools. This will also transform the data with a couple different features not present within Scikit-Learn.

Finally, we explore binning numeric columns with the new KBinsDiscretizer estimator.

A note before we get started

This tutorial is provided as a preview of things to come. The final version 0.20 has not been released. It is very likely that this tutorial will be updated at a future date to reflect any changes.

Continuing…

For those that use Pandas as their exploratory and preparation tool before moving to Scikit-Learn for machine learning, you are likely familiar with the non-standard process of handling columns containing string columns. Scikit-Learn’s machine learning models require the input to be a two-dimensional data structure of numeric values. No string values are allowed. Scikit-Learn never provided a canonical way to handle columns of strings, a very common occurrence in data science.

This lead to numerous tutorials all handling string columns in their own way. Some solutions included turning to Pandas get_dummies function. Some used Scikit-Learn’s LabelBinarizer which does one-hot encoding but was designed for labels (the target variable) and not for the input. Others created their own custom estimators. Even entire packages such as sklearn-pandas were built to support this trouble spot. This lack of standardization made for a painful experience for those wanting to build machine learning models with string columns.

Furthermore, there was poor support for making transformations to specific columns and not to the entire dataset. For instance, it’s very common to standardize continuous features but not categorical features. This will now become much easier.

Upgrading to version 0.20

conda update scikit-learn

or pip:

pip install -U scikit-learn

Introducing ColumnTransformer and the upgraded OneHotEncoder

With the upgrade to version 0.20, many workflows from Pandas to Scikit-Learn should start looking similar. The ColumnTransformer estimator applies a transformation to a specific subset of columns of your Pandas DataFrame (or array).

The OneHotEncoder estimator is not new but has been upgraded to encode string columns. Before, it only encoded columns containing numeric categorical data.

Let’s see how these new additions work to handle string columns in a Pandas DataFrame.

Kaggle Housing Dataset

One of Kaggle’s beginning machine learning competitions is the Housing Prices: Advanced Regression Techniques. The goal is to predict housing prices given about 80 features. There is a mix of continuous and categorical columns. You can download the data from the website or use their command line tool (which is very nice).

Inspect the data

Let’s read in our DataFrame and output the first few rows.

>>> import pandas as pd

>>> import numpy as np >>> train = pd.read_csv(‘data/housing/train.csv’)

>>> train.head()

>>> train.shape

(1460, 81)

Remove the target variable from the training set

The target variable is SalePrice which we remove and assign as an array to its own variable. We will use it later when we do machine learning.

>>> y = train.pop('SalePrice').values

Encoding a single string column

To start off, let’s encode a single string column, HouseStyle , which has values for the exterior of the house. Let’s output the unique counts of each string value.

>>> vc = train['HouseStyle'].value_counts()

>>> vc 1Story 726

2Story 445

1.5Fin 154

SLvl 65

SFoyer 37

1.5Unf 14

2.5Unf 11

2.5Fin 8

Name: HouseStyle, dtype: int64

We have 8 unique values in this column.

Scikit-Learn Gotcha — Must have 2D data

Most Scikit-Learn estimators require that data be strictly 2-dimensional. If we select the column above as train['HouseStyle'] , technically, a Pandas Series is created which is a single dimension of data. We can force Pandas to create a one-column DataFrame, by passing a single-item list to the brackets like this:

>>> hs_train = train[['HouseStyle']].copy()

>>> hs_train.ndim

2

Master Machine Learning with Python

Master Machine Learning with Python is an extremely comprehensive guide that I have written to help you use scikit-learn to do machine learning.

Import, Instantiate, Fit — The three-step process for each estimator

The Scikit-Learn API is consistent for all estimators and uses a three-step process to fit (train) the data.

Import the estimator we want from the module it’s located in Instantiate the estimator, possibly changing its defaults Fit the estimator to the data. Possibly transform the data to its new space if need be.

Below, we import OneHotEncoder , instantiate it and ensure that we get a dense (and not sparse) array returned, and then encode our single column with the fit_transform method.

>>> from sklearn.preprocessing import OneHotEncoder

>>> ohe = OneHotEncoder(sparse=False)

>>> hs_train_transformed = ohe.fit_transform(hs_train)

>>> hs_train_transformed array([[0., 0., 0., ..., 1., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 1., 0., 0.],

...,

[0., 0., 0., ..., 1., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.]])

As expected, it has encoded each unique value as its own binary column.

>>> hs_train_transformed.shape (1460, 8)

If you are enjoying this article, consider purchasing the All Access Pass! which includes all my current and future material for one low price.

We have a NumPy array. Where are the column names?

Notice that our output is a NumPy array and not a Pandas DataFrame. Scikit-Learn was not originally built to be directly integrated with Pandas. All Pandas objects are converted to NumPy arrays internally and NumPy arrays are always returned after a transformation.

We can still get our column name from the OneHotEncoder object through its get_feature_names method.

>>> feature_names = ohe.get_feature_names()

>>> feature_names array(['x0_1.5Fin', 'x0_1.5Unf', 'x0_1Story', 'x0_2.5Fin',

'x0_2.5Unf', 'x0_2Story', 'x0_SFoyer', 'x0_SLvl'], dtype=object)

Verifying our first row of data is correct

It’s good to verify that our estimator is working properly. Let’s look at the first row of encoded data.

>>> row0 = hs_train_transformed[0]

>>> row0 array([0., 0., 0., 0., 0., 1., 0., 0.])

This encodes the 6th value in the array as 1. Let’s use boolean indexing to reveal the feature name.

>>> feature_names[row0 == 1] array(['x0_2Story'], dtype=object)

Now, let’s verify that the first value in our original DataFrame column is the same.

>>> hs_train.values[0] array(['2Story'], dtype=object)

Use inverse_transform to automate this

Just like most transformer objects, there is an inverse_transform method that will get you back your original data. Here we must wrap row0 in a list to make it a 2D array.

>>> ohe.inverse_transform([row0]) array([['2Story']], dtype=object)

We can verify all values by inverting the entire transformed array.

>>> hs_inv = ohe.inverse_transform(hs_train_transformed)

>>> hs_inv array([['2Story'],

['1Story'],

['2Story'],

...,

['2Story'],

['1Story'],

['1Story']], dtype=object) >>> np.array_equal(hs_inv, hs_train.values) True

Applying a transformation to the test set

Whatever transformation we do to our training set, we must apply to our test set. Let’s read in the test set and get the same column and apply our transformation.

>>> test = pd.read_csv('data/housing/test.csv')

>>> hs_test = test[['HouseStyle']].copy()

>>> hs_test_transformed = ohe.transform(hs_test)

>>> hs_test_transformed array([[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 1., 0., 0.],

...,

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 0., 1., 0.],

[0., 0., 0., ..., 1., 0., 0.]])

We should again get 8 columns and we do.

>>> hs_test_transformed.shape

(1459, 8)

This example works nicely, but there are multiple cases where we will run into problems. Let’s examine them now.

Trouble area #1 — Categories unique to the test set

What happens if we have a home with a house style that is unique to just the test set? Say something like 3Story . Let's change the first value of the house styles and see what the default is from Scikit-Learn.

>>> hs_test = test[['HouseStyle']].copy()

>>> hs_test.iloc[0, 0] = '3Story'

>>> hs_test.head(3) HouseStyle

0 3Story

1 1Story

2 2Story >>> ohe.transform(hs_test) ValueError: Found unknown categories ['3Story'] in column 0 during transform

Error: Unknown Category

By default, our encoder will produce an error. This is likely what we want as we need to know if there are unique strings in the test set. If you do have this problem then there could be something much deeper that needs investigating. For now, we will ignore the problem and encode this row as all 0’s by setting the handle_unknown parameter to 'ignore' upon instantiation.

>>> ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')

>>> ohe.fit(hs_train) >>> hs_test_transformed = ohe.transform(hs_test)

>>> hs_test_transformed array([[0., 0., 0., ..., 0., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 1., 0., 0.],

...,

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 0., 1., 0.],

[0., 0., 0., ..., 1., 0., 0.]])

Let’s verify that the first row is all 0's.

>>> hs_test_transformed[0] array([0., 0., 0., 0., 0., 0., 0., 0.])

Trouble area #2 — Missing Values in test set

If you have missing values in your test set (NaN or None), then these will be ignored as long as handle_unknown is set to 'ignore'. Let’s put some missing values in the first couple elements of our test set.

>>> hs_test = test[['HouseStyle']].copy()

>>> hs_test.iloc[0, 0] = np.nan

>>> hs_test.iloc[1, 0] = None

>>> hs_test.head(4) HouseStyle

0 NaN

1 None

2 2Story

3 2Story >>> hs_test_transformed = ohe.transform(hs_test)

>>> hs_test_transformed[:4] array([[0., 0., 0., 0., 0., 0., 0., 0.],

[0., 0., 0., 0., 0., 0., 0., 0.],

[0., 0., 0., 0., 0., 1., 0., 0.],

[0., 0., 0., 0., 0., 1., 0., 0.]])

Trouble area #3 — Missing Values in training set

Missing values in the training set are more of an issue. As of now, the OneHotEncoder estimator cannot fit with missing values.

>>> hs_train = hs_train.copy()

>>> hs_train.iloc[0, 0] = np.nan

>>> ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')

>>> ohe.fit_transform(hs_train) TypeError: '<' not supported between instances of 'str' and 'float'

It would be nice if there was an option to ignore them like what happens when transforming the test set above. As of now, this doesn’t exist and we must impute it.

Must impute missing values

For now, we must impute the missing values. The old Imputer from the preprocessing module got deprecated. A new module, impute , was formed in its place, with a new estimator SimpleImputer and a new strategy, 'constant'. By default, using this strategy will fill missing values with the string ‘missing_value’. We can choose what to set it with the fill_value parameter.

>>> hs_train = train[['HouseStyle']].copy()

>>> hs_train.iloc[0, 0] = np.nan >>> from sklearn.impute import SimpleImputer

>>> si = SimpleImputer(strategy='constant', fill_value='MISSING')

>>> hs_train_imputed = si.fit_transform(hs_train)

>>> hs_train_imputed array([['MISSING'],

['1Story'],

['2Story'],

...,

['2Story'],

['1Story'],

['1Story']], dtype=object)

From here we can encode as we did previously.

>>> hs_train_transformed = ohe.fit_transform(hs_train_imputed)

>>> hs_train_transformed array([[0., 0., 0., ..., 1., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 0., ..., 0., 0., 0.],

...,

[0., 0., 0., ..., 0., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.],

[0., 0., 1., ..., 0., 0., 0.]])

Notice, that we now have an extra column and and an extra feature name.

>>> hs_train_transformed.shape (1460, 9) >>> ohe.get_feature_names() array(['x0_1.5Fin', 'x0_1.5Unf', 'x0_1Story', 'x0_2.5Fin',

'x0_2.5Unf', 'x0_2Story', 'x0_MISSING', 'x0_SFoyer',

'x0_SLvl'], dtype=object)

More on fit_transform

For all estimators, the fit_transform method will first call the fit method and then call the transform method. The fit method finds the key properties that will be used during the transformation. For instance, with the SimpleImputer , if the strategy were ‘mean’, then it would find the mean of each column during the fit method. It would store this mean for every column. When transform is called, it uses this stored mean of every column to fill in the missing values and returns the transformed array.

The OneHotEncoder works analogously. During the fit method, it finds all the unique values for each column and again stores this. When transform is called, it uses these stored unique values to produce the binary array.

Apply both transformations to the test set

We can manually apply each of the two steps above in order like this:

>>> hs_test = test[['HouseStyle']].copy()

>>> hs_test.iloc[0, 0] = 'unique value to test set'

>>> hs_test.iloc[1, 0] = np.nan >>> hs_test_imputed = si.transform(hs_test)

>>> hs_test_transformed = ohe.transform(hs_test_imputed)

>>> hs_test_transformed.shape (1459, 8) >>> ohe.get_feature_names() array(['x0_1.5Fin', 'x0_1.5Unf', 'x0_1Story', 'x0_2.5Fin',

'x0_2.5Unf', 'x0_2Story', 'x0_SFoyer', 'x0_SLvl'],

dtype=object)

Use a Pipeline instead

Scikit-Learn provides a Pipeline estimator that takes a list of transformations and applies them in succession. You can also run a machine learning model as the final estimator. Here we simply impute and encode.

>>> from sklearn.pipeline import Pipeline

Each step is a two-item tuple consisting of a string that labels the step and the instantiated estimator. The output of the previous step is the input to the next step.

>>> si_step = ('si', SimpleImputer(strategy='constant',

fill_value='MISSING'))

>>> ohe_step = ('ohe', OneHotEncoder(sparse=False,

handle_unknown='ignore'))

>>> steps = [si_step, ohe_step]

>>> pipe = Pipeline(steps) >>> hs_train = train[['HouseStyle']].copy()

>>> hs_train.iloc[0, 0] = np.nan

>>> hs_transformed = pipe.fit_transform(hs_train)

>>> hs_transformed.shape (1460, 9)

The test set is easily transformed through each step of the pipeline by simply passing it to the transform method.

>>> hs_test = test[['HouseStyle']].copy()

>>> hs_test_transformed = pipe.transform(hs_test)

>>> hs_test_transformed.shape (1459, 9)

Why just the transform method for the test set?

When transforming the test set, it's important to just call the transform method and not fit_transform . When we ran fit_transform on the training set, Scikit-Learn found all the necessary information it needed in order to transform any other dataset containing the same column names.

Transforming Multiple String Columns

Encoding multiple string columns is not a problem. Select the columns you want and then pass the new DataFrame through the same pipeline again.

>>> string_cols = ['RoofMatl', 'HouseStyle']

>>> string_train = train[string_cols]

>>> string_train.head(3) RoofMatl HouseStyle

0 CompShg 2Story

1 CompShg 1Story

2 CompShg 2Story >>> string_train_transformed = pipe.fit_transform(string_train)

>>> string_train_transformed.shape (1460, 16)

Get individual pieces of the pipeline

It is possible to retrieve each individual transformer within the pipeline through its name from the named_steps dictionary atribute. In this instance, we get the one-hot encoder so that we can output the feature names.

>>> ohe = pipe.named_steps['ohe']

>>> ohe.get_feature_names() array(['x0_ClyTile', 'x0_CompShg', 'x0_Membran', 'x0_Metal',

'x0_Roll', 'x0_Tar&Grv', 'x0_WdShake', 'x0_WdShngl',

'x1_1.5Fin', 'x1_1.5Unf', 'x1_1Story', 'x1_2.5Fin',

'x1_2.5Unf', 'x1_2Story', 'x1_SFoyer', 'x1_SLvl'],

dtype=object)

Use the new ColumnTransformer to choose columns

The brand new ColumnTransformer (part of the new compose module) allows you to choose which columns get which transformations. Categorical columns will almost always need separate transformations than continuous columns.

The ColumnTransformer is currently experimental, meaning that its functionality can change in the future.

The ColumnTransformer takes a list of three-item tuples. The first value in the tuple is a name that labels it, the second is an instantiated estimator, and the third is a list of columns you want to apply the transformation to. The tuple will look like this:

('name', SomeTransformer(parameters), columns)

The columns actually don’t have to be column names. Instead, you can use the integer indexes of the columns, a boolean array, or even a function (which accepts the entire DataFrame as the argument and must return a selection of columns).

You can also use NumPy arrays with the ColumnTransformer , but this tutorial is focused on the integration of Pandas so we will stick with just using DataFrames.

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!

Pass a Pipeline to the ColumnTransformer

We can even pass a pipeline of many transformations to the column transformer, which is what we do here because we have multiple transformations on our string columns.

Below, we reproduce the above imputation and encoding using the ColumnTransformer . Notice that the pipeline is the exact same as above, just with cat appended to each variable name. We will add a different pipeline for the numeric columns in an upcoming section.

>>> from sklearn.compose import ColumnTransformer >>> cat_si_step = ('si', SimpleImputer(strategy='constant',

fill_value='MISSING'))

>>> cat_ohe_step = ('ohe', OneHotEncoder(sparse=False,

handle_unknown='ignore')) >>> cat_steps = [cat_si_step, cat_ohe_step]

>>> cat_pipe = Pipeline(cat_steps)

>>> cat_cols = ['RoofMatl', 'HouseStyle']

>>> cat_transformers = [('cat', cat_pipe, cat_cols)]

>>> ct = ColumnTransformer(transformers=cat_transformers)

Pass the entire DataFrame to the ColumnTransformer

The ColumnTransformer instance selects the columns we want to use, so we simply pass the entire DataFrame to the fit_transform method. The desired columns will be selected for us.

>>> X_cat_transformed = ct.fit_transform(train)

>>> X_cat_transformed.shape (1460, 16)

We can now transform our test set in the same manner.

>>> X_cat_transformed_test = ct.transform(test)

>>> X_cat_transformed_test.shape (1459, 16)

Retrieving the feature names

We have to do a little digging to get the feature names. All the transformers are stored in the named_transformers_ dictionary attribute. We then use the names, the first item from the three-item tuple to select the specific transformer. Below, we select our transformer (there is only one here — a pipeline named ‘cat’).

>>> pl = ct.named_transformers_['cat']

Then from this pipeline we select the one-hot encoder object and finally get the feature names.

>>> ohe = pl.named_steps['ohe']

>>> ohe.get_feature_names() array(['x0_ClyTile', 'x0_CompShg', 'x0_Membran', 'x0_Metal',

'x0_Roll','x0_Tar&Grv', 'x0_WdShake', 'x0_WdShngl',

'x1_1.5Fin', 'x1_1.5Unf', 'x1_1Story', 'x1_2.5Fin',

'x1_2.5Unf', 'x1_2Story', 'x1_SFoyer', 'x1_SLvl'],

dtype=object)

Transforming the numeric columns

The numeric columns will need a different set of transformations. Instead of imputing missing values with a constant, the median or mean is often chosen. And instead of encoding the values, we usually standardize them by subtracting the mean of each column and dividing by the standard deviation. This helps many models like ridge regression produce a better fit.

Using all the numeric columns

Instead of selecting just one or two columns by hand as we did above with the string columns, we can select all of the numeric columns. We do this by first finding the data type of each column with the dtypes attribute and then testing whether the kind of each dtype is 'O'. The dtypes attribute returns a Series of NumPy dtype objects. Each of these has a kind attribute that is a single character. We can use this to find the numeric or string columns. Pandas stores all of its string columns as object which have a kind equal to ‘O’. See the NumPy docs for more on the kind attribute.

>>> train.dtypes.head() Id int64

MSSubClass int64

MSZoning object

LotFrontage float64

LotArea int64

dtype: object

Get the kinds, a one character string representing the dtype.

>>> kinds = np.array([dt.kind for dt in train.dtypes])

>>> kinds[:5] array(['i', 'i', 'O', 'f', 'i'], dtype='<U1')

Assume all numeric columns are non-object. We can also get the categorical columns in this manner.

>>> all_columns = train.columns.values

>>> is_num = kinds != 'O'

>>> num_cols = all_columns[is_num]

>>> num_cols[:5] array(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual'],

dtype=object) >>> cat_cols = all_columns[~is_num]

>>> cat_cols[:5] array(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour'],

dtype=object)

Once we have our numeric column names, we can use the ColumnTransformer again.

>>> from sklearn.preprocessing import StandardScaler >>> num_si_step = ('si', SimpleImputer(strategy='median'))

>>> num_ss_step = ('ss', StandardScaler())

>>> num_steps = [num_si_step, num_ss_step] >>> num_pipe = Pipeline(num_steps)

>>> num_transformers = [('num', num_pipe, num_cols)] >>> ct = ColumnTransformer(transformers=num_transformers)

>>> X_num_transformed = ct.fit_transform(train)

>>> X_num_transformed.shape (1460, 37)

Combining both categorical and numerical column transformations

We can apply separate transformations to each section of our DataFrame with ColumnTransformer . We will use every single column in this example.

We then create a separate pipeline for both categorical and numerical columns and then use the ColumnTransformer to independently transform them. These two transformations happen in parallel. The results of each are then concatenated together.

>>> transformers = [('cat', cat_pipe, cat_cols),

('num', num_pipe, num_cols)]

>>> ct = ColumnTransformer(transformers=transformers)

>>> X = ct.fit_transform(train)

>>> X.shape (1460, 305)

Machine Learning

The whole point of this exercise is to set up our data so that we can do machine learning. We can create one final pipeline and add a machine learning model as the final estimator. The first step in the pipeline will be the entire transformation we just did above. We assigned y way back at the top of the tutorial as the SalePrice . Here, we will just use the fit method instead of fit_transform since our final step is a machine learning model and does no transformations.

>>> from sklearn.linear_model import Ridge >>> ml_pipe = Pipeline([('transform', ct), ('ridge', Ridge())])

>>> ml_pipe.fit(train, y)

We can evaluate our model with the score method, which returns the R-squared value:

>>> ml_pipe.score(train, y) 0.92205

Cross-Validation

Of course, scoring ourselves on the training set is not useful. Let’s do some K-fold cross-validation to get an idea of how well we would do with unseen data. We set a random state so that the splits will be the same throughout the rest of the tutorial.

>>> from sklearn.model_selection import KFold, cross_val_score

>>> kf = KFold(n_splits=5, shuffle=True, random_state=123)

>>> cross_val_score(ml_pipe, train, y, cv=kf).mean() 0.813

Selecting parameters when Grid Searching

Grid searching in Scikit-Learn requires us to pass a dictionary of parameter names mapped to possible values. When using a pipeline, we must use the name of the step followed by a double-underscore and then the parameter name. If there are multiple layers to your pipeline, as we have here, we must continue using double-underscores to move up a level until we reach the estimator whose parameters we would like to optimize.

>>> from sklearn.model_selection import GridSearchCV >>> param_grid = {

'transform__num__si__strategy': ['mean', 'median'],

'ridge__alpha': [.001, 0.1, 1.0, 5, 10, 50, 100, 1000],

}

>>> gs = GridSearchCV(ml_pipe, param_grid, cv=kf)

>>> gs.fit(train, y)

>>> gs.best_params_ {'ridge__alpha': 10, 'transform__num__si__strategy': 'median'} >>> gs.best_score_ 0.819

Getting all the grid search results in a Pandas DataFrame

All the results of the grid search are stored in the cv_results_ attribute. This is a dictionary that can get converted to a Pandas DataFrame for a nice display and it provides a structure that is much easier to manually scan.

>>> pd.DataFrame(gs.cv_results_)

Lots of data from each combination of the parameter grid

Building a custom transformer that does all the basics

There are a few limitations to the above workflow. For instance, it would be nice if the OneHotEncoder gave you the option of ignoring missing values during the fit method. It could simply encode missing values as a row of all zeros. Currently, it forces us to fill the missing values with some string and then encodes this string as a separate column.

Low-frequency strings

Also, string columns that appear only a few times during the training set may not be reliable predictors in the test set. We may want to encode those as if they were missing as well.

Writing your own estimator class

Scikit-Learn provides some help within its documentation on writing your own estimator class. The BaseEstimator class found within the base module provides the get_params and set_params methods for you. The set_params method is necessary when doing a grid search. You can write your own or inherit from the BaseEstimator . There is also a TransformerMixin but it just writes the fit_transform method for you. We do this in one line of code below, so we don’t inherit from it.

The following class BasicTransformer does the following:

Fills in missing values with either the mean or median for numeric columns

Standardizes all numeric columns

Uses one hot encoding for string columns

Does not fill in missing values for categorical columns. Instead, it encodes them as a 0's

Ignores unique values in string columns in the test set

Allows you to choose a threshold for the number of occurrences a value must have in a string column. Strings below this threshold will be encoded as all 0's

It only works with DataFrames and is just experimental and not tested so it will break for some datasets

It is called ‘basic’ because, these are probably the most basic transformations that typically get done to many datasets.

from sklearn.base import BaseEstimator class BasicTransformer(BaseEstimator):



def __init__(self, cat_threshold=None, num_strategy='median',

return_df=False):

# store parameters as public attributes

self.cat_threshold = cat_threshold



if num_strategy not in ['mean', 'median']:

raise ValueError('num_strategy must be either ""mean"" or

""median""')

self.num_strategy = num_strategy

self.return_df = return_df



def fit(self, X, y=None):

# Assumes X is a DataFrame

self._columns = X.columns.values



# Split data into categorical and numeric

self._dtypes = X.dtypes.values

self._kinds = np.array([dt.kind for dt in X.dtypes])

self._column_dtypes = {}

is_cat = self._kinds == 'O'

self._column_dtypes['cat'] = self._columns[is_cat]

self._column_dtypes['num'] = self._columns[~is_cat]

self._feature_names = self._column_dtypes['num']



# Create a dictionary mapping categorical column to unique

# values above threshold

self._cat_cols = {}

for col in self._column_dtypes['cat']:

vc = X[col].value_counts()

if self.cat_threshold is not None:

vc = vc[vc > self.cat_threshold]

vals = vc.index.values

self._cat_cols[col] = vals

self._feature_names = np.append(self._feature_names, col

+ '_' + vals)



# get total number of new categorical columns

self._total_cat_cols = sum([len(v) for col, v in

self._cat_cols.items()])



# get mean or median

num_cols = self._column_dtypes['num']

self._num_fill = X[num_cols].agg(self.num_strategy)

return self



def transform(self, X):

# check that we have a DataFrame with same column names as

# the one we fit

if set(self._columns) != set(X.columns):

raise ValueError('Passed DataFrame has different columns

than fit DataFrame')

elif len(self._columns) != len(X.columns):

raise ValueError('Passed DataFrame has different number

of columns than fit DataFrame')



# fill missing values

num_cols = self._column_dtypes['num']

X_num = X[num_cols].fillna(self._num_fill)



# Standardize numerics

std = X_num.std()

X_num = (X_num - X_num.mean()) / std

zero_std = np.where(std == 0)[0]



# If there is 0 standard deviation, then all values are the

# same. Set them to 0.

if len(zero_std) > 0:

X_num.iloc[:, zero_std] = 0

X_num = X_num.values



# create separate array for new encoded categoricals

X_cat = np.empty((len(X), self._total_cat_cols),

dtype='int')

i = 0

for col in self._column_dtypes['cat']:

vals = self._cat_cols[col]

for val in vals:

X_cat[:, i] = X[col] == val

i += 1



# concatenate transformed numeric and categorical arrays

data = np.column_stack((X_num, X_cat))



# return either a DataFrame or an array

if self.return_df:

return pd.DataFrame(data=data,

columns=self._feature_names)

else:

return data



def fit_transform(self, X, y=None):

return self.fit(X).transform(X)



def get_feature_names():

return self._feature_names

Using our BasicTransformer

Our BasicTransformer estimator should be able to be used just like any other scikit-learn estimator. We can instantiate it and then transform our data.

>>> bt = BasicTransformer(cat_threshold=3, return_df=True)

>>> train_transformed = bt.fit_transform(train)

>>> train_transformed.head(3)

Columns of the DataFrame where the numerical and categorical columns meet

Using our transformer in a pipeline

Our transformer can be part of a pipeline.

>>> basic_pipe = Pipeline([('bt', bt), ('ridge', Ridge())])

>>> basic_pipe.fit(train, y)

>>> basic_pipe.score(train, y) 0.904

We can also cross-validate with it as well and get a similar score as we did with our scikit-learn column transformer pipeline from above.

>>> cross_val_score(basic_pipe, train, y, cv=kf).mean() 0.816

We can use it as part of a grid search as well. It turns out that not including low-count strings did not help this particular model, though it stands to reason it could in other models. The best score did improve a bit, perhaps due to using a slightly different encoding scheme.

>>> param_grid = {

'bt__cat_threshold': [0, 1, 2, 3, 5],

'ridge__alpha': [.1, 1, 10, 100]

} >>> gs = GridSearchCV(p, param_grid, cv=kf)

>>> gs.fit(train, y)

>>> gs.best_params_ {'bt__cat_threshold': 0, 'ridge__alpha': 10} >>> gs.best_score_

0.830

Binning and encoding numeric columns with the new KBinsDiscretizer

There are a few columns that contain years. It makes more sense to bin the values in these columns and treat them as categories. Scikit-Learn introduced the new estimator KBinsDiscretizer to do just this. It not only bins the values, but it encodes them as well. Before you could have done this manually with Pandas cut or qcut functions.

Let’s see how it works with just the YearBuilt column.

>>> from sklearn.preprocessing import KBinsDiscretizer

>>> kbd = KBinsDiscretizer(encode='onehot-dense')

>>> year_built_transformed = kbd.fit_transform(train[['YearBuilt']])

>>> year_built_transformed array([[0., 0., 0., 0., 1.],

[0., 0., 1., 0., 0.],

[0., 0., 0., 1., 0.],

...,

[1., 0., 0., 0., 0.],

[0., 1., 0., 0., 0.],

[0., 0., 1., 0., 0.]])

By default, each bin contains (approximately) an equal number of observations. Let’s sum up each column to verify this.

>>> year_built_transformed.sum(axis=0) array([292., 274., 307., 266., 321.])

This is the ‘quantile’ strategy. You can choose ‘uniform’ to make the bin edges equally spaced or ‘kmeans’ which uses K-means clustering to find the bin edges.

>>> kbd.bin_edges_ array([array([1872. , 1947.8, 1965. , 1984. , 2003. , 2010. ])],

dtype=object)

Processing all the year columns separately with ColumnTransformer

We now have another subset of columns that need separate processing and we can do this with the ColumnTransformer . The following code adds one more step to our previous transformation. We also drop the Id column which was just identifying each row.

>>> year_cols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt',

'YrSold']

>>> not_year = ~np.isin(num_cols, year_cols + ['Id'])

>>> num_cols2 = num_cols[not_year] >>> year_si_step = ('si', SimpleImputer(strategy='median'))

>>> year_kbd_step = ('kbd', KBinsDiscretizer(n_bins=5,

encode='onehot-dense'))

>>> year_steps = [year_si_step, year_kbd_step]

>>> year_pipe = Pipeline(year_steps) >>> transformers = [('cat', cat_pipe, cat_cols),

('num', num_pipe, num_cols2),

('year', year_pipe, year_cols)] >>> ct = ColumnTransformer(transformers=transformers)

>>> X = ct.fit_transform(train)

>>> X.shape (1460, 320)

We cross-validate and score and see that all this work yielded us no improvements.

>>> ml_pipe = Pipeline([('transform', ct), ('ridge', Ridge())])

>>> cross_val_score(ml_pipe, train, y, cv=kf).mean()

0.813

Using a different number of bins for each column might improve our results. Still, the KBinsDiscretizer makes it easy to bin numeric variables.

More goodies in Scikit-Learn 0.20

There are more new features that come with the upcoming release. Check the What’s New section of the docs for more. There are a ton of changes.

Conclusion

This article introduced a new workflow that will be available to Scikit-Learn users who rely on Pandas for the initial data exploration and preparation. A much smoother and feature-rich process for taking a Pandas DataFrame and transforming it so that it is ready for machine learning is now done through the new and improved estimators ColumnTransformer , SimpleImputer , OneHotEncoder , and KBinsDiscretizer .

I am very excited to see this new upgrade and am going to be integrating these new workflows immediately into my projects and teaching materials.

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises)

— The most comprehensive course available to learn pandas. (600+ pages and 300+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!","['machine', 'set', 'column', 'pandas', '00', 'columns', 'workflow', 'exciting', 'scikitlearn', 'data', 'string', 'values']","Become an ExpertIf you want to be trusted to make decisions using pandas and scikit-learn, you must become an expert.
For many data scientists, a typical workflow consists of using Pandas to do exploratory data analysis before moving to scikit-learn for machine learning.
A major pain point for users (and in my opinion the worst part of Scikit-Learn) was preparing a pandas DataFrame with string values in its columns.
Let’s see how these new additions work to handle string columns in a Pandas DataFrame.
ConclusionThis article introduced a new workflow that will be available to Scikit-Learn users who rely on Pandas for the initial data exploration and preparation.",en,['Ted Petrou'],2020-01-22 21:57:25.140000+00:00,"{'Pandas', 'Data Science', 'Scikit Learn', 'Python', 'Machine Learning'}","{'https://miro.medium.com/fit/c/96/96/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/60/1*8RfNuW5cD_t9V4Xjc3sdRw.png?q=20', 'https://miro.medium.com/max/60/1*c0CXQuHv3TCFQ3fqDvSMgQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*0H9qOvDgUXs0KFICewpniQ.png', 'https://miro.medium.com/max/3900/1*q0vdUAZiBV43jOnl9lHf4Q.png', 'https://miro.medium.com/max/60/1*q0vdUAZiBV43jOnl9lHf4Q.png?q=20', 'https://miro.medium.com/max/3944/1*c0CXQuHv3TCFQ3fqDvSMgQ.png', 'https://miro.medium.com/max/60/1*gSZo0-uYFVTkjJg2pVgcnA.png?q=20', 'https://miro.medium.com/max/880/1*gSZo0-uYFVTkjJg2pVgcnA.png', 'https://miro.medium.com/max/444/1*tP8Fb4SL816YosGlt1Xvdg.png', 'https://miro.medium.com/fit/c/160/160/1*VwblGkKV5WaS9NlQeoMUng.png', 'https://miro.medium.com/max/1760/1*gSZo0-uYFVTkjJg2pVgcnA.png', 'https://miro.medium.com/max/3720/1*8RfNuW5cD_t9V4Xjc3sdRw.png', 'https://miro.medium.com/fit/c/80/80/2*0H9qOvDgUXs0KFICewpniQ.png'}",2020-03-05 00:13:04.079506,3.3755905628204346
https://medium.com/botsupply/rule-based-bots-vs-ai-bots-b60cdb786ffa,Rule based bots vs AI bots,"If the bot answers a question logically or solves a given task, it should be considered smart. A lot of personal assistants like Apple Siri, Amazon Alexa and Google Home can be regarded as intelligent. But can we say these assistant bots are Turing smart? Well, when we cannot distinguish between a bot and a human, then the bot can be called Turing smart.

To check whether a bot is human level intelligent or not, a lot of ways are there, and the most prominent one is the Turing test. The Turing test, developed by Alan Turing in 1950, is a test of a machine’s ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. Interactions would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine’s ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test does not check the ability to give correct answers to questions, only how closely answers resemble those a human would give. [1]

To know more about the test and the bots that have won it in the past, check my previous blog.

The question is how to make the bot smart enough that it compares to Human Intelligence. There are mainly two ways of doing so:

Rule-Based Approach: In a rule-based approach, a bot answers questions based on some rules on which it is trained on. The rules defined can be very simple to very complex. The creation of these bots are relatively straightforward using some rule-based approach, but the bot is not efficient in answering questions, whose pattern does not match with the rules on which the bot is trained. One of such languages is AIML (Artificial Intelligence Markup Language): a language based on XML that lets developers write rules for the bot to follow. Also, writing rules for different scenarios is very time taking and it is impossible to write rules for every possible scenario. The bots can handle simple queries but fail to manage complex queries. Hence, the bot can never pass the Turing test if based on some rule-based models.

2. Self learn-able bots: These are the bots that use some Machine Learning-based approaches that make them more efficient than rule-based bots. These bots can be of two types:

2.1 Retrieval based models: These bots are trained on a set of questions and their possible outcomes. For every question, the bot can find the most relevant answers from the sets of all possible answers and then outputs the answer. Although, the bot cannot generate new answers if trained on a lot of question and answers dataset, and if the data set is pre-processed smartly, the bot can handle queries fairly good. The complexity can range from simple rules for a query to complex rules using some machine learning algorithm to find the most appropriate answer. Also, there is no issue with the language and grammar as the answers are pre-determined and it cannot go wrong in syntax manner.

2.2 Generative models: Generative models are better than rule-based models in a way better that they can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers. It also makes them more prone to errors as they need to take the spelling and grammar into account. To make them better at handling these errors, these models need to be trained more precisely. Once trained, they outperform the rule-based models as they can answer complex and unseen queries. Language translation models can be used in creating such a model.

Check my next blog for creating a sequence to sequence learning model that generates its answers.

We’re a team of bot creatives and AI scientists with one common goal:","['bot', 'models', 'trained', 'answers', 'turing', 'rules', 'bots', 'human', 'based', 'vs', 'ai', 'rule', 'rulebased', 'test']","If the bot answers a question logically or solves a given task, it should be considered smart.
Hence, the bot can never pass the Turing test if based on some rule-based models.
Self learn-able bots: These are the bots that use some Machine Learning-based approaches that make them more efficient than rule-based bots.
These bots can be of two types:2.1 Retrieval based models: These bots are trained on a set of questions and their possible outcomes.
Once trained, they outperform the rule-based models as they can answer complex and unseen queries.",en,['Kumar Shridhar'],2017-05-23 17:21:55.691000+00:00,"{'Cognitive', 'Ai Bots', 'Artificial Intelligence', 'Machine Learning', 'Rule Based Bots'}","{'https://miro.medium.com/max/798/1*filKtc1H4v_94ecDxj3VDA.png', 'https://miro.medium.com/freeze/max/60/1*YfANetnnkGbiJ0fvzIxyMQ.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*h8RKAP2nuFEJyNkmj-AbTw.png', 'https://miro.medium.com/max/1000/1*XGaCk6FAa3lI7XuZDbh81Q.jpeg', 'https://miro.medium.com/max/1600/1*nJFIzb8sUMESNT02dVKwkg.png', 'https://miro.medium.com/max/60/1*QDUacsrPqoj29Pz9ffSlJg.png?q=20', 'https://miro.medium.com/max/1200/1*xHQTsakJnMqAIWNXbqNiBQ.png', 'https://miro.medium.com/max/60/1*filKtc1H4v_94ecDxj3VDA.png?q=20', 'https://miro.medium.com/max/1450/1*YfANetnnkGbiJ0fvzIxyMQ.gif', 'https://miro.medium.com/fit/c/96/96/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/max/798/1*wW49XUTI9_FCXy8Q9ML9Mg.png', 'https://miro.medium.com/max/60/1*nJFIzb8sUMESNT02dVKwkg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/max/60/1*xHQTsakJnMqAIWNXbqNiBQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*AIYnLI4kL5oBc40qOWV6Dg.png', 'https://miro.medium.com/fit/c/80/80/1*Un4ofSyOspYd5_1AUJiFSg.jpeg', 'https://miro.medium.com/max/72/1*AIYnLI4kL5oBc40qOWV6Dg.png', 'https://miro.medium.com/max/798/1*QDUacsrPqoj29Pz9ffSlJg.png', 'https://miro.medium.com/max/60/1*wW49XUTI9_FCXy8Q9ML9Mg.png?q=20', 'https://miro.medium.com/max/60/1*XGaCk6FAa3lI7XuZDbh81Q.jpeg?q=20', 'https://miro.medium.com/max/3300/1*xHQTsakJnMqAIWNXbqNiBQ.png'}",2020-03-05 00:13:05.331285,1.250777244567871
https://medium.com/botsupply/generative-model-chatbots-e422ab08461e,Generative Model Chatbots,"THE MODEL

RNN or Recurrent Neural Network is a neural network where the output not only depends on the current input, but to a series of input given in the past. Since the output is influenced by a series of past inputs, it makes RNN very effective in Natural Language Processing as the contexts of next word does not necessarily rely only on the previous word but to a series of words before that.

Depending on the scenario, the RNNs can be used to deal with a variety of tasks. Some of them are listed down below.

Image source: The Unreasonable Effectiveness of Recurrent Neural Networks

If you want to know more about RNN in detail and their use cases, visit RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 — INTRODUCTION TO RNNS.

But, Vanilla RNN faces the problem of vanishing gradient which paved the way for LSTMs that handles the problem very well due to the introduction of memory cells and gates in LSTMs.

LSTMs are type of RNNs that handles the long term dependency problem of RNNs very well due to the introduction of Gates in LSTM. It allows the cells to remember what information needs to be remembered from the previous cells and what needs to be updated. Read more about LSTMs here:

The sequence to sequence model uses two LSTM networks, one each for encoding and decoding respectively. I used three LSTM layers with 512 as layer sizes respectively. Some other parameters that I used were:","['introduction', 'cells', 'series', 'used', 'neural', 'lstm', 'recurrent', 'model', 'rnns', 'rnn', 'problem', 'chatbots', 'generative']","THE MODELRNN or Recurrent Neural Network is a neural network where the output not only depends on the current input, but to a series of input given in the past.
Depending on the scenario, the RNNs can be used to deal with a variety of tasks.
Image source: The Unreasonable Effectiveness of Recurrent Neural NetworksIf you want to know more about RNN in detail and their use cases, visit RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 — INTRODUCTION TO RNNS.
LSTMs are type of RNNs that handles the long term dependency problem of RNNs very well due to the introduction of Gates in LSTM.
Read more about LSTMs here:The sequence to sequence model uses two LSTM networks, one each for encoding and decoding respectively.",en,['Kumar Shridhar'],2018-06-21 17:56:12.426000+00:00,"{'Cognitive', 'Artificial Intelligence', 'Recurrent Neural Network', 'Lstm', 'Seq2seq Learning'}","{'https://miro.medium.com/max/798/1*filKtc1H4v_94ecDxj3VDA.png', 'https://miro.medium.com/freeze/max/60/1*YfANetnnkGbiJ0fvzIxyMQ.gif?q=20', 'https://miro.medium.com/max/60/1*sO-SP58T4brE9EHazHSeGA.png?q=20', 'https://miro.medium.com/max/1668/1*rUWdM3-cUHPzUK3ga5hPfA.png', 'https://miro.medium.com/max/1600/1*nJFIzb8sUMESNT02dVKwkg.png', 'https://miro.medium.com/max/452/1*ai131Am7dIRD_UsFd0wxaw.png', 'https://miro.medium.com/max/60/1*QDUacsrPqoj29Pz9ffSlJg.png?q=20', 'https://miro.medium.com/max/3170/1*sO-SP58T4brE9EHazHSeGA.png', 'https://miro.medium.com/max/60/1*rUWdM3-cUHPzUK3ga5hPfA.png?q=20', 'https://miro.medium.com/max/60/1*filKtc1H4v_94ecDxj3VDA.png?q=20', 'https://miro.medium.com/max/1450/1*YfANetnnkGbiJ0fvzIxyMQ.gif', 'https://miro.medium.com/fit/c/96/96/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/max/798/1*wW49XUTI9_FCXy8Q9ML9Mg.png', 'https://miro.medium.com/max/60/1*nJFIzb8sUMESNT02dVKwkg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*AIYnLI4kL5oBc40qOWV6Dg.png', 'https://miro.medium.com/max/1200/1*sO-SP58T4brE9EHazHSeGA.png', 'https://miro.medium.com/max/72/1*AIYnLI4kL5oBc40qOWV6Dg.png', 'https://miro.medium.com/max/60/1*ai131Am7dIRD_UsFd0wxaw.png?q=20', 'https://miro.medium.com/max/798/1*QDUacsrPqoj29Pz9ffSlJg.png', 'https://miro.medium.com/max/60/1*wW49XUTI9_FCXy8Q9ML9Mg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*mjgpqzsh4UmOooeDDUZltQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*GljF8L_ld119qC4OywbOHg.png'}",2020-03-05 00:13:07.199511,1.8682258129119873
https://towardsdatascience.com/deploying-a-machine-learning-model-as-a-rest-api-4a03b865c166,Deploying a Machine Learning Model as a REST API,"Artwork by Igor Kozak

As a Python developer and data scientist, I have a desire to build web apps to showcase my work. As much as I like to design the front-end, it becomes very overwhelming to take both machine learning and app development. So, I had to find a solution that could easily integrate my machine learning models with other developers who could build a robust web app better than I can.

By building a REST API for my model, I could keep my code separate from other developers. There is a clear division of labor here which is nice for defining responsibilities and prevents me from directly blocking teammates who are not involved with the machine learning aspect of the project. Another advantage is that my model can be used by multiple developers working on different platforms, such as web or mobile.

In this article, I will build a simple Scikit-Learn model and deploy it as a REST API using Flask RESTful. This article is intended especially for data scientists who do not have an extensive computer science background.

About the Model

For this example, I put together a simple Naives Bayes classifier to predict the sentiment of phrases found in movie reviews.

The data came from the Kaggle competition, Sentiment Analysis on Movie Reviews. The reviews are divided into separate sentences and sentences are further divided into separate phrases. All phrases have a sentiment score so that a model can be trained on which words lend a positive, neutral, or negative sentiment to a sentence.

Distribution of ratings from the Kaggle dataset

The majority of phrases had a neutral rating. At first, I tried to use a multinomial Naive Bayes classifier to predict one out of the 5 possible classes. However, because the majority of the data had a rating of 2, the model did not perform very well. I decided to keep it simple because the main point of this exercise is primarily about deploying as a REST API. So, I limited the data to the extreme classes and trained the model to predict only negative or positive sentiment.

It turned out that the multinomial Naive Bayes model was very effective at predicting positive and negative sentiment. You can find a quick overview of the model training process in this Jupyter Notebook Walkthrough. After training the model in a Jupyter notebook, I transferred my code into Python scripts and created a class object for the NLP model. You can find the code in my Github repo at this link. You will also need to pickle or save your model so that you can quickly load the trained model into your API script.

Now that we have the model, let’s deploy this as a REST API.

REST API Guide

Start a new Python script for your Flask app for the API.

Import Libraries and Load Pickles

The code block below contains a lot of Flask boilerplate and the code to load the classifier and vectorizer pickles.

from flask import Flask

from flask_restful import reqparse, abort, Api, Resource

import pickle

import numpy as np

from model import NLPModel app = Flask(__name__)

api = Api(app) # create new model object

model = NLPModel() # load trained classifier

clf_path = 'lib/models/SentimentClassifier.pkl'

with open(clf_path, 'rb') as f:

model.clf = pickle.load(f) # load trained vectorizer

vec_path = 'lib/models/TFIDFVectorizer.pkl'

with open(vec_path, 'rb') as f:

model.vectorizer = pickle.load(f)

Create an argument parser

The parser will look through the parameters that a user sends to your API. The parameters will be in a Python dictionary or JSON object. For this example, we will be specifically looking for a key called query . The query will be a phrase that a user will want our model to make a prediction on whether the phrase is positive or negative.

# argument parsing

parser = reqparse.RequestParser()

parser.add_argument('query')

Resource Class Object

Resources are the main building blocks for Flask RESTful APIs. Each class can have methods that correspond to HTTP methods such as: GET , PUT , POST , and DELETE . GET will be the primary method because our objective is to serve predictions. In the get method below, we provide directions on how to handle the user’s query and how to package the JSON object that will be returned to the user.

class PredictSentiment(Resource):

def get(self):

# use parser and find the user's query

args = parser.parse_args()

user_query = args['query'] # vectorize the user's query and make a prediction

uq_vectorized = model.vectorizer_transform(

np.array([user_query]))

prediction = model.predict(uq_vectorized)

pred_proba = model.predict_proba(uq_vectorized) # Output 'Negative' or 'Positive' along with the score

if prediction == 0:

pred_text = 'Negative'

else:

pred_text = 'Positive'



# round the predict proba value and set to new variable

confidence = round(pred_proba[0], 3) # create JSON object

output = {'prediction': pred_text, 'confidence': confidence}



return output

There is a great tutorial by Flask-RESTful where they build a to-do application and demonstrate how to use the PUT , POST , and DELETE methods.

Endpoints

The following code will set the base url to the sentiment predictor resource. You can imagine that you might have multiple endpoints, each one pointing to a different model that would make different predictions. One example could be an endpoint, '/ratings' , which would direct the user to another model that can predict movie ratings given genre, budget, and production members. You would need to create another resource object for this second model. These can just be added right after one another as shown below.

api.add_resource(PredictSentiment, '/')



# example of another endpoint

api.add_resource(PredictRatings, '/ratings')

Name == Main Block

Not much to say here. Set debug to False if you are deploying this API to production.

if __name__ == '__main__':

app.run(debug=True)

User Requests

Below are some examples of how users can access your API so that they can get predictions.

With the Requests module in a Jupyter Notebook:



params ={'query': 'that movie was boring'}

response = requests.get(url, params)

response.json() url = ' http://127.0.0.1:5000/' params ={'query': 'that movie was boring'}response = requests.get(url, params)response.json() Output: {'confidence': 0.128, 'prediction': 'Negative'}

Using curl in the terminal:



{

""prediction"": ""Negative"",

""confidence"": 0.128

} $ curl -X GET http://127.0.0.1:5000/ -d query='that movie was boring'""prediction"": ""Negative"",""confidence"": 0.128

Using HTTPie in the terminal:

$ http http://127.0.0.1:5000/ query=='that movie was boring' HTTP/1.0 200 OK

Content-Length: 58

Content-Type: application/json

Date: Fri, 31 Aug 2018 18:49:25 GMT

Server: Werkzeug/0.14.1 Python/3.6.3 {

""confidence"": 0.128,

""prediction"": ""Negative""

}

Now, my teammates can add sentiment prediction to their app just by making a request to this API, all without having to mix Python and JavaScript together.

Full app.py code

Sometimes it’s helpful to see all the code in one place.

File Structure

Last thing I want to include is a little overview of the file structure for this simple API.

sentiment-clf/

├── README.md

├── app.py # Flask REST API script

├── build_model.py # script to build and pickle the classifier

├── model.py # script for the classifier class object

├── util.py # helper functions

├── requirements.txt

└── lib/

├── data/ # data from Kaggle

│ ├── sampleSubmission.csv

│ ├── test.tsv

│ └── train.tsv

└── models/ # pickled models for import into API script

├── SentimentClassifier.pkl

└── TFIDFVectorizer.pkl

Deployment

Once you have built your model and REST API and finished testing locally, you can deploy your API just as you would any Flask app to the many hosting services on the web. By deploying on the web, users everywhere can make requests to your URL to get predictions. Guides for deployment are included in the Flask docs.

Closing

This was only a very simple example of building a Flask REST API for a sentiment classifier. The same process can be applied to other machine learning or deep learning models once you have trained and saved them.

In addition to deploying models as REST APIs, I am also using REST APIs to manage database queries for data that I have collected by scraping from the web. This lets me collaborate with a full-stack developer without having to manage the code for their React application. If a mobile developer wants to build an app, then they would only have to become familiar with the API endpoints.","['machine', 'web', 'rest', 'sentiment', 'learning', 'movie', 'flask', 'model', 'api', 'data', 'deploying', 'app', 'code']","By building a REST API for my model, I could keep my code separate from other developers.
In this article, I will build a simple Scikit-Learn model and deploy it as a REST API using Flask RESTful.
I decided to keep it simple because the main point of this exercise is primarily about deploying as a REST API.
REST API GuideStart a new Python script for your Flask app for the API.
ClosingThis was only a very simple example of building a Flask REST API for a sentiment classifier.",en,['Nguyen Ngo'],2019-11-19 19:17:39.365000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'API'}","{'https://miro.medium.com/max/60/1*uezAyMqN2ZynYoWs9uDXvQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*B8cNQmkEfK7Z673yEaQCGw.png?q=20', 'https://miro.medium.com/max/1046/1*fVJHg1UBeiCSmibp9TTNkw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*fVJHg1UBeiCSmibp9TTNkw.png?q=20', 'https://miro.medium.com/max/1886/1*B8cNQmkEfK7Z673yEaQCGw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1476/1*YEpENxFu6kzElHTYNr8QqA.png', 'https://miro.medium.com/max/800/1*uezAyMqN2ZynYoWs9uDXvQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1600/1*uezAyMqN2ZynYoWs9uDXvQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*YEpENxFu6kzElHTYNr8QqA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*tC4-bfix5QcCAnczR9JDDg.jpeg', 'https://miro.medium.com/fit/c/96/96/1*tC4-bfix5QcCAnczR9JDDg.jpeg'}",2020-03-05 00:13:13.931730,6.7312188148498535
https://towardsdatascience.com/deploying-deep-learning-models-part-1-an-overview-77b4d01dd6f7,Deploying deep learning models: Part 1 an overview,"Diagram overviewing the CI/CD deployment process with Kubernetes. Taken from Kubecon slideshow

Recently, academic and industry researchers have conducted a lot of exciting and ground-breaking research in the field of deep learning. They have developed many new models that are incredibly powerful. However, much of this research (outside of the few tech giants) remains just that research, and not part of a production application. Despite the constant flood of new papers it remains incredibly difficult to actually utilize any of these models in production (and that is even when the papers provide code). Deploying machine learning models remains a significant challenge. In this article I will provide an overview of various ways to “productionize” machine learning models and weigh their respective pros/cons without going into too much detail. In subsequent articles I will explore these approaches with actual code and examples.

The Challenge

We will assume that we have already trained the model ourselves or have the trained weights available from the internet. In order to use your model in an application you will have to:

Load your model with its weights Preprocess your data Perform the actual prediction Handle the prediction response data

Sounds simple enough? Well in practice this process can actually be quite complicated.

Like with many things there is not one clear-cut answer about the best way to deploy a machine learning model to a production environment. The questions you should ask yourself are:

What are my requirements? (i.e. how many requests are you expecting per second, what latency is required…etc) How will I evaluate the model’s performance in production? (and how will I collect and store the additional data from interactions) How frequently do I plan on re-training my model? What are the data preprocessing needs? Will the format of the production input data differ drastically from the model training data? Will it come in batches or as a stream? Does the model need to be able to run offline?

These are the basic questions that you should ask before attempting to deploy your model.

Loading model directly into application

This option essentially considers the model a part of the overall application and hence loads it within the application. This approach is easier in certain circumstances than others.

For instance, if the core application itself is written in Python the process can be smooth. Altogether it usually requires adding dependencies to setup/config files and modifying your predict function to be called through the appropiate user interactions. The model is loaded as part of the application and all dependencies must be included in the application.

This process becomes more difficult if your application is not written in Python. For instance, there is no good way to load PyTorch or Caffe into Java programs. Even Tensorflow, which has a Java library requires writing a lot of additional code to fully integrate into the application. Finally, this does not explicitly address the problem of scalibility.

However, as stated previously this route remains beneficial when you want to quickly deploy an application written in Python. It also, remains one of the better options for devices without an internet connection.

Calling an API

The second option involves making an API and calling the API from your application. This can be done in a number of different ways. I have detailed the most common ways here.

Kubernetes

Docker in many respects seems like a natural choice for deploying machine learning models. A model and all of its dependencies can be neatly packaged in one container. Moreover, the server can automatically scale-up by adding more Docker containers when needed. Kubernetes is one of the best ways to manage Docker containers and therefore good for machine learning.

Recently, Kubernetes unveiled Kubeflow which aims at bringing machine learning to the Kubernetes framework. Kubeflow attempts to make it easy to train, test, and deploy your model as well collect evaluation metrics. I plan on covering Kubeflow in a later blog post of its own as it can be quite complicated. For now just understand that it is a full scale package aimed at making it easy to develop and deploy machine learning microservices at scale.

Custom REST-API with Flask/Django

Another option is to create your own REST-API from scratch (this option could possibly be combined with Docker as well) depending on how familiar you are with making APIs. This can be done using Flask relatively easily. I will not go into more detail on how to do this as there are a number of exisiting tutorials that cover exactly this topic. Depending on the number of requests you can usually scale Flask without too much trouble.

However, even this can get quite messy due to the differences in ML frameworks, load times, and specific model preprocessing requirements. Currently, I’m working on a model agnostic instantiation class for use with Flask/Django to make it easier to use models and provide a standardized template for working with models. So instead, of having to remember and implement the different functions for different models you can call model.preprocess() and model.predict() regardless of the backend (more on this in another article as well).

AWS Lambda/Serverless

AWS Lambda is another possible route. You can read AWS’s documentation on how to set this up. There is a good article on using AWS lambda with Caffe2 in “Machine Learnings.”

Other approaches

Apache Beam — I don’t know too much about this method, but the method seems to involve using Beam to do the model preprocessing and then Tensorflow (only framework supported for now) to do the actual prediction. See these slides for more info.

Spark/Flink

Several of the major data processing frameworks such as Spark and Flink are in the process of developing packages to aid in the deployment of machine learning models. Flink has Flink Tensorflow which aims at integrating TF models into Flink streaming pipelines. With Spark there are a number of different packages and attempts at integrating deep learning. Additionally, Yahoo released its own library recently that allows both distributed training and model serving.

Conclusion

In future articles I will go into more detail about building an API with Kubeflow to serve models as well as discuss how to load deep learning models into Java programs and Flink pipelines.","['machine', 'flink', 'models', 'overview', 'production', 'process', 'learning', 'remains', 'deep', 'model', 'data', 'application', 'deploying']","Deploying machine learning models remains a significant challenge.
In this article I will provide an overview of various ways to “productionize” machine learning models and weigh their respective pros/cons without going into too much detail.
KubernetesDocker in many respects seems like a natural choice for deploying machine learning models.
Kubernetes is one of the best ways to manage Docker containers and therefore good for machine learning.
ConclusionIn future articles I will go into more detail about building an API with Kubeflow to serve models as well as discuss how to load deep learning models into Java programs and Flink pipelines.",en,['Isaac Godfried'],2018-06-20 03:27:25.317000+00:00,"{'Api Development', 'Deep Learning', 'Kubernetes', 'Machine Learning', 'Spark'}","{'https://miro.medium.com/max/1200/1*akWVsdGH6XW9SgDIiePq4Q.png', 'https://miro.medium.com/max/4800/1*akWVsdGH6XW9SgDIiePq4Q.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/0*Sx7X1u5ElAJwgTxr.', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*akWVsdGH6XW9SgDIiePq4Q.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*Sx7X1u5ElAJwgTxr.'}",2020-03-05 00:13:16.119745,2.188014507293701
https://towardsdatascience.com/9-essential-newsletters-for-data-scientists-e225e4227318,9 Essential Newsletters for Data Scientists,"The Problem

It’s been clear for some time now that digital media is the future. However, only over these past few years have we truly experienced the consequences of digital media at scale. While fake news may take center stage due to influence and various scandals, the problem is bigger than just fake news.

The real problem lies in differentiating signal from noise.

We are constantly bombarded with information to the point where it’s become increasingly difficult to find the metaphorical ‘value needle’ in a haystack the size of the internet.

This predicament leaves us with more questions than answers at the moment.

In a world of clickbait titles and fake news, how do we survive these landmines unscathed? With an endless amount of content available on the web today, wow do we sift through all of it and focus our attention on what matters?

These are undoubtedly difficult questions. So difficult in fact, that there are thousands of talented people out there trying to solve them on daily basis.

The Solution

It’s become clear to me that solution to these problems all depend on perfecting one concept above all else — content curation.

“Content curation is the process of gathering information relevant to a particular topic or area of interest.”

In a perfect world, you shouldn’t have to search the World Wide Web just to find authentic, valuable information or resources on a specific topic. That task can and should be delegated to others.

Throughout the rest of this post, I’ll be focused on my favorite form of content curation — newsletters. We’ll go over why newsletters are so useful for data science specifically, then I’ll share every data science newsletter that I subscribe to at the moment.

What About Data Science?

Due to the fast-growing, multi-dimensional nature of the data science skillset, newsletters can offer an immense amount of value.

The field shows no sign of slowing down at the moment. There seems like there is an interesting breakthrough or project every other week. New technologies and techniques are extremely common as well.

In order to keep your data-driven repertoire at it’s best, you’ll need to develop a growth mindset and be open to learning new things. Newsletters are a perfect way to help you stay up to date with anything of note that’s happening in the field.

Think of your newsletter subscriptions as an elite force of smart, specialized people working to bring you information that‘s worth your time.

Luckily for Data Scientists, there are tons of excellent newsletters out there to look into. You’ll find my subscription list below in no particular order for anything regarding data science, machine learning, artificial intelligence, or hacking in general. This has been iterated on and grown out over the last three years.

Author’s Note: I only subscribe to a small subset of the excellent newsletters out there, so don’t be afraid to branch out and try others. Besides, you can always unsubscribe after one issue if it doesn’t meet standards. Nothing to lose!","['scientists', 'newsletters', 'information', 'web', 'essential', 'content', 'world', 'data', 'youll', 'fake', 'difficult', 'science']","We’ll go over why newsletters are so useful for data science specifically, then I’ll share every data science newsletter that I subscribe to at the moment.
What About Data Science?
Due to the fast-growing, multi-dimensional nature of the data science skillset, newsletters can offer an immense amount of value.
Luckily for Data Scientists, there are tons of excellent newsletters out there to look into.
You’ll find my subscription list below in no particular order for anything regarding data science, machine learning, artificial intelligence, or hacking in general.",en,['Conor Dewey'],2020-01-23 03:13:55.460000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Technology'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*1A18MmlW_Z04UdwIcXnMVg.jpeg', 'https://miro.medium.com/max/9600/1*OQbphzA8DpxH0iRpxr3lwg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*OQbphzA8DpxH0iRpxr3lwg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*1A18MmlW_Z04UdwIcXnMVg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*OQbphzA8DpxH0iRpxr3lwg.png'}",2020-03-05 00:13:23.619632,7.498923063278198
https://towardsdatascience.com/build-develop-and-deploy-a-machine-learning-model-to-predict-cars-price-using-gradient-boosting-2d4d78fddf09,"Build, Develop and Deploy a Machine Learning Model to predict cars price using Gradient Boosting.","Data Modeling

Now we came to the main task in all this process, which is Data Modeling, for this purpose I will use 4 Machine Learning models dedicated for Regression problems, at the end I will do a Benchmarking table to compare each model r2_score and select the best one. The used models are : K Nearest Neighbors regression, Multiple Linear Regression, Decision Tree Regression and Gradient Boosting Regression.

Data Transformation

I Intentionally let this part until the Data Modeling instead of doing it with Data Preprocessing for some visualization purposes.

At the moment I still have 2 categorical features which are the fuel_type and mark , the aim of this section is to preprocess those features in order to make them numerical so that they will fit into our model. In literature there are two famous ways of categorical variable transformations, the first one is label encoding, and the second one is the one hot encoding, for this use case we will use the one hot position and the reason why I choose this kind of data labeling is because I will not need any kind of data normalization later, and also this has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.

Features engineering and Data Transformation

Result of one hot encoding on the data frame

As shown in the figure above, after transforming the categorical features to numerical ones by using the one hot encoding, we got a wide data frame.

Data Splitting

Usually we split our data into three parts : Training , validation and Testing set, but for simplicity we will use only train and test with 20% in test size, and the rest for training.

Gradient Boosting Regression

Boosting is another ensemble technique for creating collection of powerful predictors, and Gradient Boosting is a technique for producing regression models consisting of collections of regressors.

An ensemble is a collection of predictors whose predictions are combined usually by some sort of weighted average or vote in order to provide an overall prediction that takes its guidance from the collection itself. So boosting is an ensemble technique in which learners are learned sequentially with early learners fitting simple models to the data and then analyzing the data for errors, those errors identify problems or particular instances of the data that are difficult or hard to fit, as a consequence later models focus primarily on those examples trying to get them right.

At the end, all the models contribute with weights and the set is combined into some overall predictors, so boosting is a method of converting a sequence of weak learners into a very complex predictor, it’s a way of increasing the complexity of a particular model initial learners tend to be very simple and then the weighted combination can grow more and more complex as learners are added.

The Math behind Gradient Boosting

This algorithm is an instance of gradient boosting, it’s called gradient boosting because it’s related to a gradient descent sort of procedure.

First we make a set of predictions ŷ(i) for each data point.

for each data point. We can calculate the error in our predictions, let’s call it J(y, ŷ) , and J just relates the accuracy of ŷ in modelling y.

, and J just relates the accuracy of ŷ in modelling y. For mean squared error MSE : J(y, ŷ) = Σ ( y(i) - ŷ(i) )² .

So now we can try to think about adjusting our prediction ŷ to try to reduce the error above : ŷ(i)= ŷ(i) + α *∇J(y, ŷ) , with : ∇J(y, ŷ) = y(i)- ŷ(i)

to try to reduce the error above : , with : Each learner is estimating the gradient of the loss function.

Gradient Descent : take sequence of steps to reduce J .

. Sum of predictors, weighted by step size alpha.

Gradient Boosting Regressor — Code Snapshot

Snapshot from the used code in the GBR model

Interpreting Residual VS Predicted values

After building the GBR model, it is mostly recommended to plot the error distribution to verify the following assumptions :

Normally distributed

Homoscedastic (The same variance at every X)

Independent

which is already verified in our case as you can check on the notebook for more details, however we need to observe the residual plot to make sure it doesn’t follow a non-linear or Heteroscedasticity distribution.

Residual VS Predicted values

As we can see from the plot above, the residuals roughly form a “horizontal band” around the 0 line, this suggest that the variances of the error terms are equal, furthermore no one residual “stands out” from the basic random pattern of residuals which involve that there is no outliers.

Models Benchmarking

So after trying many regression models to fit our data set, it’s time to draw a Benchmarking table that will summarize all the results we have got.

╔═══════════════════╦════════════════════╦═══════════════╗

║ Model ║ R^2 score ║ RMSE ║

╠═══════════════════╬════════════════════╬═══════════════╣

║ KNN ║ 0.56 ║ 37709.67 ║

║ Linear Regression ║ 0.62 ║ 34865.07 ║

║ Gradient Boosting ║ 0.80 ║ 25176.16 ║

║ Decision Tree ║ 0.63 ║ 34551.17 ║

╚═══════════════════╩════════════════════╩═══════════════╝

It appears that the Gradient Boosting model won the battle as it was expected with the lowest RMSE value and the highest R² score.","['machine', 'models', 'regression', 'predict', 'cars', 'error', 'ŷi', 'boosting', 'deploy', 'price', 'learning', 'ŷ', 'learners', 'model', 'data', 'develop', 'using', 'gradient']","The used models are : K Nearest Neighbors regression, Multiple Linear Regression, Decision Tree Regression and Gradient Boosting Regression.
Gradient Boosting RegressionBoosting is another ensemble technique for creating collection of powerful predictors, and Gradient Boosting is a technique for producing regression models consisting of collections of regressors.
The Math behind Gradient BoostingThis algorithm is an instance of gradient boosting, it’s called gradient boosting because it’s related to a gradient descent sort of procedure.
Models BenchmarkingSo after trying many regression models to fit our data set, it’s time to draw a Benchmarking table that will summarize all the results we have got.
╔═══════════════════╦════════════════════╦═══════════════╗║ Model ║ R^2 score ║ RMSE ║╠═══════════════════╬════════════════════╬═══════════════╣║ KNN ║ 0.56 ║ 37709.67 ║║ Linear Regression ║ 0.62 ║ 34865.07 ║║ Gradient Boosting ║ 0.80 ║ 25176.16 ║║ Decision Tree ║ 0.63 ║ 34551.17 ║╚═══════════════════╩════════════════════╩═══════════════╝It appears that the Gradient Boosting model won the battle as it was expected with the lowest RMSE value and the highest R² score.",en,['Ayoub Rmidi'],2020-02-29 12:38:05.256000+00:00,"{'Data Science', 'Python', 'Data Visualization', 'Web Scraping', 'Machine Learning'}","{'https://miro.medium.com/max/1200/1*dwMOG0jhLjKk6SJKr1MFtQ.png', 'https://miro.medium.com/fit/c/96/96/1*5OKABVGgBbMUrgJ9QtfPNQ.png', 'https://miro.medium.com/max/60/1*dwMOG0jhLjKk6SJKr1MFtQ.png?q=20', 'https://miro.medium.com/max/60/1*m6q1FzndJWFmdHnHOLce1Q.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*5OKABVGgBbMUrgJ9QtfPNQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1808/1*6-hGdI-ckzLF1yAZNIkOTg.png', 'https://miro.medium.com/max/1368/1*NAwFASBGE5PT1wLCy8t72A.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/freeze/max/34/1*PV-NOumJzilc5R2bGyEbRQ.gif?q=20', 'https://miro.medium.com/max/60/1*PMD4A4_2vCIx4yG91F_s4Q.png?q=20', 'https://miro.medium.com/max/60/1*M-IYtFSBg7AS-w217qBs-Q.png?q=20', 'https://miro.medium.com/max/660/1*PV-NOumJzilc5R2bGyEbRQ.gif', 'https://miro.medium.com/max/60/1*oSaavpwH3zdf8RWaPsgFWg.png?q=20', 'https://miro.medium.com/max/3616/1*dwMOG0jhLjKk6SJKr1MFtQ.png', 'https://miro.medium.com/max/60/1*6-hGdI-ckzLF1yAZNIkOTg.png?q=20', 'https://miro.medium.com/max/1782/1*oSaavpwH3zdf8RWaPsgFWg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*NAwFASBGE5PT1wLCy8t72A.png?q=20', 'https://miro.medium.com/max/60/1*MT0MyWqNDz1ZGuuh2LVl4A.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1228/1*M-IYtFSBg7AS-w217qBs-Q.png', 'https://miro.medium.com/max/1686/1*m6q1FzndJWFmdHnHOLce1Q.png', 'https://miro.medium.com/max/1838/1*PMD4A4_2vCIx4yG91F_s4Q.png', 'https://miro.medium.com/max/596/1*MT0MyWqNDz1ZGuuh2LVl4A.png'}",2020-03-05 00:13:30.595635,6.976002931594849
https://towardsdatascience.com/fraud-detection-under-extreme-class-imbalance-c241854e60c,Fraud Detection Under Extreme Class Imbalance,"A popular field in data science is fraud analytics. This might include credit/debit card fraud, anti-money laundering or cyber-security. One thing common in all these fields is the level of class imbalance. Generally, only a small percentage of the total number of transactions is actual fraud. Take credit card fraud for example. Of the 1000 transactions of a given user, only 1 of them is an actual fraud. This might be because the client’s credit card information was stolen or the vendor’s PoS device was compromised. This needs to be caught as soon as possible to minimize the financial damage to both the client and the vendor. At the same time, we need to be aware of false positives. Naturally, a credit card owner will not be happy if the credit card is blocked by the bank when no actual fraud had taken place. In this blog, I will cover some common strategies used to uncover financial and cyber misdemeanor while minimizing false alarms.

Unsupervised Learning Models

Unsupervised learning models are useful when transaction records are not tagged. This is especially true for internet traffic data, where there is no clear way to tell if a specific internet transaction is malicious in nature. Under such circumstances, anomaly detection is common.

Anomaly Detection

Imagine you have a certain behavior while browsing the internet. You transfer a certain amount of data while browsing YouTube and a certain amount of data texting via Facebook messenger. Now, let’s aggregate that to an entire organization. The endpoints, on average, will have predictable port usage. Now, imagine one of the endpoints happen to use a certain port (that it usually never uses), to access a server port (that may has either been blacklisted for sending malware). That might be a potential port scan or even torrent usage on the corporate network. Another example could be a sudden high usage of one or a number of ports, indicating a DDoS attack. There are plenty of unsupervised learning algorithms, from K-Means to Gaussian Mixture Models.

For time series data, it is possible to look at the standard deviations of the data points to look for outliers. Certain outliers in time series may show distinct patterns and can be detected using Fourier Transform or Hidden Markov Model.

For more details, check out my blog on Clustering Based Unsupervised Learning.

Supervised Learning Models

Certain fraud detection datasets come with tags. Let’s take credit card fraud for example. If the bank suspects a fraud, they can call the cardholder to check if the card is actually stolen. This information can be used as training data for a new model build. Supervised learning models have their own set of opportunities as challenges, as I will cover in this section.

Mislabeled Data

In fraud detection problems, the dataset is already horribly imbalanced. Imagine having mislabeled data on top of that? Unfortunately, the real world is not as clean as Kaggle. You will get extremely messy data. My personal general strategy is to visualize the data using K-Means to check if the labeling actually makes sense.

However, this is very subjective and will depend on the use case. If done right, we can get a much more reliable dataset to train our model. On the downside, there is the possibility of target leakage.

Model Complexity

Naturally, for most fraud detection use cases, the models tend to be more complex. Considering the level of granularity and feature engineering needed, a simple linear regression might not help. That being said, one needs to be aware of overfitting their dataset. A learning curve is generally useful to check if whether the model has high bias or high variance.

If the model is high biased, then it is possible to look into something more complex like decision trees, random forest or even neural network. Generally, in financial institutions, ensemble models are commonly used. Certain engineered features work best on separate models (usually due to drastic difference in variance). Or even use unsupervised learning models in conjunction with supervised models (especially when the possibility of mislabels is significant).

For a refresher on learning curves, ensemble models and other basics to refresh upon, check out my Data Science Interview Guide.

Penalized Models

You can use the same algorithms but give them a different perspective on the problem. Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.

Performance Metrics

The most common methodology for evaluating model performance is the classification score. However, when only 2% of your dataset is of one class (fraud) and 98% some other class (non-fraud), misclassification scores don’t really make sense. You can be 98% accurate and still catch none of the fraud. Also, remember that we care about false positives when dealing with fraud. In this section, I will talk about some alternative performance metrics.

Confusion Matrix

Confusion Matrix is a common favorite when analyzing misclassification. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class

The diagonals represent the classes that have been correctly classified. This helps as we not only know which classes are being misclassified but also what they are being misclassified as.

Precision, Recall and F1-Score

For a better look at misclassification, we often use the following metric to get a better idea of true positives (TP), true negatives (TN), false positive (FP) and false negative (FN).

Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.

Recall is the ratio of correctly predicted positive observations to all the observations in actual class.

F1-Score is the weighted average of Precision and Recall.

The higher the F1-Score, the better the model. For all three metric, 0 is the worst while 1 is the best.

ROC Curves

The Receiver Operating Characteristic or ROC curves is a very popular metric in the field of fraud analytics (as well as digital signal processing).

The 45 degree line is the random line, where the Area Under the Curve or AUC is 0.5 . The further the curve from this line, the higher the AUC and better the model. The highest a model can get is an AUC of 1, where the curve forms a right angled triangle. The ROC curve can also help debug a model. For example, if the bottom left corner of the curve is closer to the random line, it implies that the model is misclassifying at Y=0. Whereas, if it is random on the top right, it implies the errors are occurring at Y=1. Also, if there are spikes on the curve (as opposed to being smooth), it implies the model is not stable. When dealing with fraud models, ROC is your best friend. For more information, read Receiver Operating Characteristic Curves Demystified (in Python).

Re-sampling

The final recommendation I have is re-sampling. If it is possible to get more data to make the classes more balanced, this would be the simplest and best approach. If not, creating synthetic data is also a possibility. This section will cover these methodologies.

Over-sampling

Over-sampling the fraud records in the model can help improve classification. However, realize that it needs to be representative of the real world (where frauds are actually rare relative to non-frauds). SMOTE and ADASYN are common algorithms for this. Autoencoders can also be used for creating synthetic fraud data.

Under-sampling

Under-sampling non-frauds is another technique that had been used. Generating centroid based on a clustering method (e.g. K-Means) is a common strategy for this.

Final Remarks

Ideally what I have mentioned in this blog will help you come up with better ways of dealing with fraud models (whether for work, school or Kaggle competitions). A lot of the things I have mentioned can be found in more detail in my other blogs:","['common', 'models', 'learning', 'fraud', 'extreme', 'curve', 'class', 'imbalance', 'model', 'data', 'card', 'certain', 'detection']","This might include credit/debit card fraud, anti-money laundering or cyber-security.
Unsupervised Learning ModelsUnsupervised learning models are useful when transaction records are not tagged.
Or even use unsupervised learning models in conjunction with supervised models (especially when the possibility of mislabels is significant).
However, when only 2% of your dataset is of one class (fraud) and 98% some other class (non-fraud), misclassification scores don’t really make sense.
Autoencoders can also be used for creating synthetic fraud data.",en,['Syed Sadat Nazrul'],2018-06-29 17:34:44.929000+00:00,{'Machine Learning'},"{'https://miro.medium.com/max/2328/1*e-XffrY5d3mTf-ZzVM6sgg.png', 'https://miro.medium.com/max/60/1*pJsRlL6QLTLTHdegtUPdkg.png?q=20', 'https://miro.medium.com/max/2560/1*v8493OnMDLmt6IrRnWeqnA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*RsR3YZmL-c2x7WZggOLrhA.png?q=20', 'https://miro.medium.com/max/60/1*pY-mC4WVMunmnn5ui9s0Ng.jpeg?q=20', 'https://miro.medium.com/max/2560/1*KIWLBfmhwaXmxuCPrjxjAg.jpeg', 'https://miro.medium.com/max/2560/1*SUOfSQ4gIB19_wn83WKopg.jpeg', 'https://miro.medium.com/max/970/1*CFb_3aU73R9EZrfHfGV0kA.png', 'https://miro.medium.com/max/750/1*D16HfhYgqPL0ex7BZz_Qqw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*SUOfSQ4gIB19_wn83WKopg.jpeg?q=20', 'https://miro.medium.com/max/60/1*yOjdqV61gJ8Xrd8uxCZyMw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*8I0_wgEjC0CPRDAovzrfJQ.png', 'https://miro.medium.com/max/60/1*KIWLBfmhwaXmxuCPrjxjAg.jpeg?q=20', 'https://miro.medium.com/max/60/1*v8493OnMDLmt6IrRnWeqnA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*8I0_wgEjC0CPRDAovzrfJQ.png', 'https://miro.medium.com/max/1340/1*gLrUFxK3E3ltmDy4OX9PFQ.png', 'https://miro.medium.com/max/944/1*oLbGM94kT9wuSCXY9wZArA.png', 'https://miro.medium.com/max/60/1*Cxdd6AsjU7Hzj-ax6Zv-tg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1600/1*RsR3YZmL-c2x7WZggOLrhA.png', 'https://miro.medium.com/max/2560/1*pY-mC4WVMunmnn5ui9s0Ng.jpeg', 'https://miro.medium.com/max/60/1*oLbGM94kT9wuSCXY9wZArA.png?q=20', 'https://miro.medium.com/max/1200/1*v8493OnMDLmt6IrRnWeqnA.png', 'https://miro.medium.com/max/60/1*fY7UgscD6dUavqT7P5ZKCg.png?q=20', 'https://miro.medium.com/max/2296/1*pJsRlL6QLTLTHdegtUPdkg.png', 'https://miro.medium.com/max/60/1*e-XffrY5d3mTf-ZzVM6sgg.png?q=20', 'https://miro.medium.com/max/2288/1*fY7UgscD6dUavqT7P5ZKCg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*CFb_3aU73R9EZrfHfGV0kA.png?q=20', 'https://miro.medium.com/max/2560/1*yOjdqV61gJ8Xrd8uxCZyMw.png', 'https://miro.medium.com/max/60/1*D16HfhYgqPL0ex7BZz_Qqw.png?q=20', 'https://miro.medium.com/max/1780/1*Cxdd6AsjU7Hzj-ax6Zv-tg.png', 'https://miro.medium.com/max/2362/1*BQosKd3FHZWsfKRF6pdVtQ.png', 'https://miro.medium.com/max/60/1*BQosKd3FHZWsfKRF6pdVtQ.png?q=20', 'https://miro.medium.com/max/60/1*gLrUFxK3E3ltmDy4OX9PFQ.png?q=20'}",2020-03-05 00:13:37.330627,6.734991788864136
https://towardsdatascience.com/encoding-categorical-features-21a2651a065c,Encoding Categorical Features,"LabelEncoder & OneHotEncoder

The labelEncoder and OneHotEncoder only works on categorical features. We need first to extract the categorial featuers using boolean mask.

# Categorical boolean mask

categorical_feature_mask = X.dtypes==object # filter categorical columns using mask and turn it into a list

categorical_cols = X.columns[categorical_feature_mask].tolist()

LabelEncoder converts each class under specified feature to a numerical value. Let’s go through the steps to see how to do it.

Instantiate a LabelEncoder object:

# import labelencoder

from sklearn.preprocessing import LabelEncoder # instantiate labelencoder object

le = LabelEncoder()

Apply LabelEncoder on each of the categorical columns:

# apply le on categorical feature columns

X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col)) X[categorical_cols].head(10)

Note that the output of LabelEncoder is still a dataframe. The results is shown as following:

As we can see, all the categorical feature columns are binary class. But if the categorical feature is multi class, LabelEncoder will return different values for different classes. See the following example, the ‘Neighborhood’ feature has as many as 24 classes.

In this case, using LabelEncoder only is not a good choice, since it brings in a natural ordering for different classes. For example, under ‘Neighborhood’ feature, class_a has value 5 but class_b has value 24, is class_b ‘greater’ than class_a? The answer is obviously no. Thus allowing model learning this result will lead to poor performance. Therefore, for dataframe containing multi class features, a further step of OneHotEncoder is needed. Let’s see the steps to do it.

Instantiate OneHotEncoder object:

# import OneHotEncoder

from sklearn.preprocessing import OneHotEncoder # instantiate OneHotEncoder

ohe = OneHotEncoder(categorical_features = categorical_feature_mask, sparse=False )

# categorical_features = boolean mask for categorical columns

# sparse = False output an array not sparse matrix

We need to specify categorical feature using its mask inside OneHotEncoder. The sparse=False argument outputs a non-sparse matrix.

Apply OneHotEncoder on DataFrame:

# apply OneHotEncoder on categorical feature columns

X_ohe = ohe.fit_transform(X) # It returns an numpy array

Note that the output is a numpy array, not a dataframe. For each class under a categorical feature, a new column is created for it. For example, there are 20 columns created for the ten binary class categorical features.

DictVectorizer

As we can see, the LabelEncoder and OneHotEncoder usually need to be used together as two steps procedure. An more convenient way is using DictVectorizer which can achieve these two steps all at once.

First, we need to convert the dataframe into a dictionary. This can be achieved by Pandas to_dict method.

# turn X into dict

X_dict = X.to_dict(orient='records') # turn each row as key-value pairs # show X_dict

X_dict

The orient='records' is required to turn the data frame into a {column:value} format. The result is a list of dictionaries, among which each dictionary represent one sample. Note that, in this case we don’t need to extract the categorical features, we can convert the whole dataframe into a dict. This is one advantage compared to LabelEncoder and OneHotEncoder.

Now we instantiate a DictVectorizer:

# DictVectorizer

from sklearn.feature_extraction import DictVectorizer # instantiate a Dictvectorizer object for X

dv_X = DictVectorizer(sparse=False)

# sparse = False makes the output is not a sparse matrix

The sparse=False makes the output to be a non-sparse matrix.

DictVectorizer fit and transform on the converted dict:

# apply dv_X on X_dict

X_encoded = dv_X.fit_transform(X_dict) # show X_encoded

X_encoded

The result is a numpy array:

Each row represents a sample and each column represents a feature. If we want to know what feature for each column, we can check the vocabulary of this DictVectorizer:

# vocabulary

vocab = dv_X.vocabulary_ # show vocab

vocab

Get Dummies

Pandas get_dummies method is a very straight forward one step procedure to get the dummy variables for categorical features. The advantage is you can directly apply it on the dataframe and the algorithm inside will recognize the categorical features and perform get dummies operation on it. Here is how to do it:

# Get dummies

X = pd.get_dummies(X, prefix_sep='_', drop_first=True) # X head

X.head()","['onehotencoder', 'feature', 'import', 'need', 'encoding', 'labelencoder', 'class', 'categorical', 'output', 'features', 'dataframe', 'using']","LabelEncoder & OneHotEncoderThe labelEncoder and OneHotEncoder only works on categorical features.
# Categorical boolean maskcategorical_feature_mask = X.dtypes==object # filter categorical columns using mask and turn it into a listcategorical_cols = X.columns[categorical_feature_mask].tolist()LabelEncoder converts each class under specified feature to a numerical value.
The results is shown as following:As we can see, all the categorical feature columns are binary class.
But if the categorical feature is multi class, LabelEncoder will return different values for different classes.
For example, there are 20 columns created for the ten binary class categorical features.",en,['Yang Liu'],2018-09-20 16:57:26.358000+00:00,"{'Get Dummies', 'Dictvectorizer', 'Label Encoder', 'Categorical Data', 'One Hot Encoder'}","{'https://miro.medium.com/max/60/1*GZE_3HzGgNq6qsI1CyOI8g.png?q=20', 'https://miro.medium.com/max/588/1*Qv7kCRIodAe_UZDobunxkw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/32/1*Qv7kCRIodAe_UZDobunxkw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*oxsK-yZHI-uyhZfY6UWYTw.jpeg', 'https://miro.medium.com/max/352/1*3s-JILdyTZ0JoH5rVkIoAg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1092/1*a0LfmA26PIqrfZ0-VrqqlA.png', 'https://miro.medium.com/fit/c/96/96/1*oxsK-yZHI-uyhZfY6UWYTw.jpeg', 'https://miro.medium.com/max/60/1*5uXHXtM_dNOPnGBXxDbQ3w.jpeg?q=20', 'https://miro.medium.com/max/60/1*a0LfmA26PIqrfZ0-VrqqlA.png?q=20', 'https://miro.medium.com/max/24/1*3s-JILdyTZ0JoH5rVkIoAg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/4032/1*j8Z6Y7Be9kIdrD_aPekjAw.png', 'https://miro.medium.com/max/1184/1*w8uIBZaNMr4Zr9X5AYpEsA.png', 'https://miro.medium.com/max/60/1*w8uIBZaNMr4Zr9X5AYpEsA.png?q=20', 'https://miro.medium.com/max/60/1*e1kC8s1_lHpq8QgXdxo0sg.png?q=20', 'https://miro.medium.com/max/1312/1*GZE_3HzGgNq6qsI1CyOI8g.png', 'https://miro.medium.com/max/60/1*SN2Zm8mTWNf4UwMGsl_NGw.png?q=20', 'https://miro.medium.com/max/8064/1*5uXHXtM_dNOPnGBXxDbQ3w.jpeg', 'https://miro.medium.com/max/1200/1*5uXHXtM_dNOPnGBXxDbQ3w.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/4052/1*e1kC8s1_lHpq8QgXdxo0sg.png', 'https://miro.medium.com/max/786/1*wdBlWX7tSq5WUarg_rKinQ.png', 'https://miro.medium.com/max/60/1*wdBlWX7tSq5WUarg_rKinQ.png?q=20', 'https://miro.medium.com/max/60/1*j8Z6Y7Be9kIdrD_aPekjAw.png?q=20', 'https://miro.medium.com/max/2014/1*SN2Zm8mTWNf4UwMGsl_NGw.png'}",2020-03-05 00:13:44.638718,7.307502508163452
https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63,Categorical Data,"Hence they have a sense of order amongst them. In general, there is no generic module or function to map and transform these features into numeric representations based on order automatically. Hence we can use a custom encoding\mapping scheme.

gen_ord_map = {'Gen 1': 1, 'Gen 2': 2, 'Gen 3': 3,

'Gen 4': 4, 'Gen 5': 5, 'Gen 6': 6} poke_df['GenerationLabel'] = poke_df['Generation'].map(gen_ord_map)

poke_df[['Name', 'Generation', 'GenerationLabel']].iloc[4:10]

Pokémon generation encoding

It is quite evident from the above code that the map(…) function from pandas is quite helpful in transforming this ordinal feature.

Encoding Categorical Attributes

If you remember what we mentioned earlier, typically feature engineering on categorical data involves a transformation process which we depicted in the previous section and a compulsory encoding process where we apply specific encoding schemes to create dummy variables or features for each category\value in a specific categorical attribute.

You might be wondering, we just converted categories to numerical labels in the previous section, why on earth do we need this now? The reason is quite simple. Considering video game genres, if we directly fed the GenreLabel attribute as a feature in a machine learning model, it would consider it to be a continuous numeric feature thinking value 10 (Sports) is greater than 6 (Racing) but that is meaningless because the Sports genre is certainly not bigger or smaller than Racing, these are essentially different values or categories which cannot be compared directly. Hence we need an additional layer of encoding schemes where dummy features are created for each unique value or category out of all the distinct categories per attribute.

One-hot Encoding Scheme

Considering we have the numeric representation of any categorical attribute with m labels (after transformation), the one-hot encoding scheme, encodes or transforms the attribute into m binary features which can only contain a value of 1 or 0. Each observation in the categorical feature is thus converted into a vector of size m with only one of the values as 1 (indicating it as active). Let’s take a subset of our Pokémon dataset depicting two attributes of interest.

poke_df[['Name', 'Generation', 'Legendary']].iloc[4:10]

Subset of our Pokémon dataset

The attributes of interest are Pokémon Generation and their Legendary status. The first step is to transform these attributes into numeric representations based on what we learnt earlier.

from sklearn.preprocessing import OneHotEncoder, LabelEncoder # transform and map pokemon generations

gen_le = LabelEncoder()

gen_labels = gen_le.fit_transform(poke_df['Generation'])

poke_df['Gen_Label'] = gen_labels # transform and map pokemon legendary status

leg_le = LabelEncoder()

leg_labels = leg_le.fit_transform(poke_df['Legendary'])

poke_df['Lgnd_Label'] = leg_labels poke_df_sub = poke_df[['Name', 'Generation', 'Gen_Label',

'Legendary', 'Lgnd_Label']]

poke_df_sub.iloc[4:10]

Attributes with transformed (numeric) labels

The features Gen_Label and Lgnd_Label now depict the numeric representations of our categorical features. Let’s now apply the one-hot encoding scheme on these features.

# encode generation labels using one-hot encoding scheme

gen_ohe = OneHotEncoder()

gen_feature_arr = gen_ohe.fit_transform(

poke_df[['Gen_Label']]).toarray()

gen_feature_labels = list(gen_le.classes_)

gen_features = pd.DataFrame(gen_feature_arr,

columns=gen_feature_labels) # encode legendary status labels using one-hot encoding scheme

leg_ohe = OneHotEncoder()

leg_feature_arr = leg_ohe.fit_transform(

poke_df[['Lgnd_Label']]).toarray()

leg_feature_labels = ['Legendary_'+str(cls_label)

for cls_label in leg_le.classes_]

leg_features = pd.DataFrame(leg_feature_arr,

columns=leg_feature_labels)

In general, you can always encode both the features together using the fit_transform(…) function by passing it a two dimensional array of the two features together (Check out the documentation!). But we encode each feature separately, to make things easier to understand. Besides this, we can also create separate data frames and label them accordingly. Let’s now concatenate these feature frames and see the final result.

poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=1)

columns = sum([['Name', 'Generation', 'Gen_Label'],

gen_feature_labels, ['Legendary', 'Lgnd_Label'],

leg_feature_labels], [])

poke_df_ohe[columns].iloc[4:10]

One-hot encoded features for Pokémon generation and legendary status

Thus you can see that 6 dummy variables or binary features have been created for Generation and 2 for Legendary since those are the total number of distinct categories in each of these attributes respectively. Active state of a category is indicated by the 1 value in one of these dummy variables which is quite evident from the above data frame.

Consider you built this encoding scheme on your training data and built some model and now you have some new data which has to be engineered for features before predictions as follows.

new_poke_df = pd.DataFrame([['PikaZoom', 'Gen 3', True],

['CharMyToast', 'Gen 4', False]],

columns=['Name', 'Generation', 'Legendary'])

new_poke_df

Sample new data

You can leverage scikit-learn’s excellent API here by calling the transform(…) function of the previously build LabeLEncoder and OneHotEncoder objects on the new data. Remember our workflow, first we do the transformation.

new_gen_labels = gen_le.transform(new_poke_df['Generation'])

new_poke_df['Gen_Label'] = new_gen_labels new_leg_labels = leg_le.transform(new_poke_df['Legendary'])

new_poke_df['Lgnd_Label'] = new_leg_labels new_poke_df[['Name', 'Generation', 'Gen_Label', 'Legendary',

'Lgnd_Label']]

Categorical attributes after transformation

Once we have numerical labels, let’s apply the encoding scheme now!

new_gen_feature_arr = gen_ohe.transform(new_poke_df[['Gen_Label']]).toarray()

new_gen_features = pd.DataFrame(new_gen_feature_arr,

columns=gen_feature_labels) new_leg_feature_arr = leg_ohe.transform(new_poke_df[['Lgnd_Label']]).toarray()

new_leg_features = pd.DataFrame(new_leg_feature_arr,

columns=leg_feature_labels) new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=1)

columns = sum([['Name', 'Generation', 'Gen_Label'],

gen_feature_labels,

['Legendary', 'Lgnd_Label'], leg_feature_labels], []) new_poke_ohe[columns]

Categorical attributes after one-hot encoding

Thus you can see it’s quite easy to apply this scheme on new data easily by leveraging scikit-learn’s powerful API.

You can also apply the one-hot encoding scheme easily by leveraging the to_dummies(…) function from pandas .

gen_onehot_features = pd.get_dummies(poke_df['Generation'])

pd.concat([poke_df[['Name', 'Generation']], gen_onehot_features],

axis=1).iloc[4:10]

One-hot encoded features by leveraging pandas

The above data frame depicts the one-hot encoding scheme applied on the Generation attribute and the results are same as compared to the earlier results as expected.

Dummy Coding Scheme

The dummy coding scheme is similar to the one-hot encoding scheme, except in the case of dummy coding scheme, when applied on a categorical feature with m distinct labels, we get m - 1 binary features. Thus each value of the categorical variable gets converted into a vector of size m - 1. The extra feature is completely disregarded and thus if the category values range from {0, 1, …, m-1} the 0th or the m - 1th feature column is dropped and corresponding category values are usually represented by a vector of all zeros (0). Let’s try applying dummy coding scheme on Pokémon Generation by dropping the first level binary encoded feature ( Gen 1 ).

gen_dummy_features = pd.get_dummies(poke_df['Generation'],

drop_first=True)

pd.concat([poke_df[['Name', 'Generation']], gen_dummy_features],

axis=1).iloc[4:10]

Dummy coded features for Pokémon g eneration

If you want, you can also choose to drop the last level binary encoded feature ( Gen 6 ) as follows.

gen_onehot_features = pd.get_dummies(poke_df['Generation'])

gen_dummy_features = gen_onehot_features.iloc[:,:-1]

pd.concat([poke_df[['Name', 'Generation']], gen_dummy_features],

axis=1).iloc[4:10]

Dummy coded features for Pokémon g eneration

Based on the above depictions, it is quite clear that categories belonging to the dropped feature are represented as a vector of zeros (0) like we discussed earlier.

Effect Coding Scheme

The effect coding scheme is actually very similar to the dummy coding scheme, except during the encoding process, the encoded features or feature vector, for the category values which represent all 0 in the dummy coding scheme, is replaced by -1 in the effect coding scheme. This will become clearer with the following example.

gen_onehot_features = pd.get_dummies(poke_df['Generation'])

gen_effect_features = gen_onehot_features.iloc[:,:-1]

gen_effect_features.loc[np.all(gen_effect_features == 0,

axis=1)] = -1.

pd.concat([poke_df[['Name', 'Generation']], gen_effect_features],

axis=1).iloc[4:10]

Effect coded features for Pokémon g eneration

The above output clearly shows that the Pokémon belonging to Generation 6 are now represented by a vector of -1 values as compared to zeros in dummy coding.

Bin-counting Scheme

The encoding schemes we discussed so far, work quite well on categorical data in general, but they start causing problems when the number of distinct categories in any feature becomes very large. Essential for any categorical feature of m distinct labels, you get m separate features. This can easily increase the size of the feature set causing problems like storage issues, model training problems with regard to time, space and memory. Besides this, we also have to deal with what is popularly known as the ‘curse of dimensionality’ where basically with an enormous number of features and not enough representative samples, model performance starts getting affected often leading to overfitting.

Hence we need to look towards other categorical data feature engineering schemes for features having a large number of possible categories (like IP addresses). The bin-counting scheme is a useful scheme for dealing with categorical variables having many categories. In this scheme, instead of using the actual label values for encoding, we use probability based statistical information about the value and the actual target or response value which we aim to predict in our modeling efforts. A simple example would be based on past historical data for IP addresses and the ones which were used in DDOS attacks; we can build probability values for a DDOS attack being caused by any of the IP addresses. Using this information, we can encode an input feature which depicts that if the same IP address comes in the future, what is the probability value of a DDOS attack being caused. This scheme needs historical data as a pre-requisite and is an elaborate one. Depicting this with a complete example would be currently difficult here but there are several resources online which you can refer to for the same.

Feature Hashing Scheme

The feature hashing scheme is another useful feature engineering scheme for dealing with large scale categorical features. In this scheme, a hash function is typically used with the number of encoded features pre-set (as a vector of pre-defined length) such that the hashed values of the features are used as indices in this pre-defined vector and values are updated accordingly. Since a hash function maps a large number of values into a small finite set of values, multiple different values might create the same hash which is termed as collisions. Typically, a signed hash function is used so that the sign of the value obtained from the hash is used as the sign of the value which is stored in the final feature vector at the appropriate index. This should ensure lesser collisions and lesser accumulation of error due to collisions.

Hashing schemes work on strings, numbers and other structures like vectors. You can think of hashed outputs as a finite set of b bins such that when hash function is applied on the same values\categories, they get assigned to the same bin (or subset of bins) out of the b bins based on the hash value. We can pre-define the value of b which becomes the final size of the encoded feature vector for each categorical attribute that we encode using the feature hashing scheme.

Thus even if we have over 1000 distinct categories in a feature and we set b=10 as the final feature vector size, the output feature set will still have only 10 features as compared to 1000 binary features if we used a one-hot encoding scheme. Let’s consider the Genre attribute in our video game dataset.

unique_genres = np.unique(vg_df[['Genre']])

print(""Total game genres:"", len(unique_genres))

print(unique_genres) Output

------

Total game genres: 12

['Action' 'Adventure' 'Fighting' 'Misc' 'Platform' 'Puzzle' 'Racing'

'Role-Playing' 'Shooter' 'Simulation' 'Sports' 'Strategy']

We can see that there are a total of 12 genres of video games. If we used a one-hot encoding scheme on the Genre feature, we would end up having 12 binary features. Instead, we will now use a feature hashing scheme by leveraging scikit-learn’s FeatureHasher class, which uses a signed 32-bit version of the Murmurhash3 hash function. We will pre-define the final feature vector size to be 6 in this case.

from sklearn.feature_extraction import FeatureHasher fh = FeatureHasher(n_features=6, input_type='string')

hashed_features = fh.fit_transform(vg_df['Genre'])

hashed_features = hashed_features.toarray()

pd.concat([vg_df[['Name', 'Genre']], pd.DataFrame(hashed_features)],

axis=1).iloc[1:7]

Feature Hashing on the Genre attribute

Based on the above output, the Genre categorical attribute has been encoded using the hashing scheme into 6 features instead of 12. We can also see that rows 1 and 6 denote the same genre of games, Platform which have been rightly encoded into the same feature vector.

Conclusion

These examples should give you a good idea about popular strategies for feature engineering on discrete, categorical data. If you read Part 1 of this series, you would have seen that it is slightly challenging to work with categorical data as compared to continuous, numeric data but definitely interesting! We also talked about some ways to handle large feature spaces using feature engineering but you should also remember that there are other techniques including feature selection and dimensionality reduction methods to handle large feature spaces. We will cover some of these methods in a later article.

Next up will be feature engineering strategies for unstructured text data. Stay tuned!","['feature', 'generation', 'vector', 'encoding', 'data', 'categorical', 'values', 'features', 'scheme', 'value']","Encoding Categorical AttributesIf you remember what we mentioned earlier, typically feature engineering on categorical data involves a transformation process which we depicted in the previous section and a compulsory encoding process where we apply specific encoding schemes to create dummy variables or features for each category\value in a specific categorical attribute.
Hence we need to look towards other categorical data feature engineering schemes for features having a large number of possible categories (like IP addresses).
Feature Hashing SchemeThe feature hashing scheme is another useful feature engineering scheme for dealing with large scale categorical features.
ConclusionThese examples should give you a good idea about popular strategies for feature engineering on discrete, categorical data.
If you read Part 1 of this series, you would have seen that it is slightly challenging to work with categorical data as compared to continuous, numeric data but definitely interesting!",en,"['Dipanjan', 'Dj']",2019-03-27 13:44:31.750000+00:00,"{'Feature Engineering', 'Data Science', 'Machine Learning', 'Programming', 'Tds Feature Engineering'}","{'https://miro.medium.com/fit/c/160/160/1*Wy0cxXZpX-FrMWeXsqOOpg.png', 'https://miro.medium.com/max/1200/1*FgMeHrpzkMgDc1RCrl8JNw.jpeg', 'https://miro.medium.com/max/60/1*FgMeHrpzkMgDc1RCrl8JNw.jpeg?q=20', 'https://miro.medium.com/max/718/1*IKqNF9wJ11xdlIT5-EQ50Q.png', 'https://miro.medium.com/max/1062/1*wRIkttjEX1Udy4pZaPSZeg.png', 'https://miro.medium.com/max/1500/0*FwubnnoNlt6Coo9j.png', 'https://miro.medium.com/max/954/1*-EbaK-Nn5L7pNulJzx39YQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1384/1*ychLO4DAe5cvD1UwUuvjZw.png', 'https://miro.medium.com/max/60/1*za1jYH-6ooFfccLxdggpKg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*WiBsYgBy-GGRZzvNUBvQFQ.png?q=20', 'https://miro.medium.com/max/60/1*pCnqiKj-Hrdn7FajO0sh1Q.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/964/1*RrtvTvnqAYtI__CVRsqB8Q.png', 'https://miro.medium.com/max/60/0*iKsDex5fUBQoYTju.png?q=20', 'https://miro.medium.com/max/60/1*RrtvTvnqAYtI__CVRsqB8Q.png?q=20', 'https://miro.medium.com/max/60/1*vMZDgwJ1Fvzj-zOfGN21ug.png?q=20', 'https://miro.medium.com/max/60/0*zQCDjMFdx-4uo3ec.png?q=20', 'https://miro.medium.com/max/1974/1*WiBsYgBy-GGRZzvNUBvQFQ.png', 'https://miro.medium.com/max/60/1*ClxVY4HgIWzwL3zxdqOhQg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/972/1*ClxVY4HgIWzwL3zxdqOhQg.png', 'https://miro.medium.com/max/932/1*XlsFdg01yZh1IMPhmhw3Wg.png', 'https://miro.medium.com/max/60/1*IKqNF9wJ11xdlIT5-EQ50Q.png?q=20', 'https://miro.medium.com/max/60/1*MKqRnwOdBfD33tgLmWM7lw.png?q=20', 'https://miro.medium.com/max/956/1*vMZDgwJ1Fvzj-zOfGN21ug.png', 'https://miro.medium.com/max/1580/0*iKsDex5fUBQoYTju.png', 'https://miro.medium.com/max/60/1*_dhZ4-lsPVUzRdHUWbtBNQ.png?q=20', 'https://miro.medium.com/max/60/1*XlsFdg01yZh1IMPhmhw3Wg.png?q=20', 'https://miro.medium.com/max/532/1*MKqRnwOdBfD33tgLmWM7lw.png', 'https://miro.medium.com/max/640/1*keUxOv3nOszfDf8BmD4iyQ.png', 'https://miro.medium.com/max/60/1*keUxOv3nOszfDf8BmD4iyQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*Wy0cxXZpX-FrMWeXsqOOpg.png', 'https://miro.medium.com/max/60/1*j-RLI_vxx-MA7pexi3G5QQ.png?q=20', 'https://miro.medium.com/max/850/1*j-RLI_vxx-MA7pexi3G5QQ.png', 'https://miro.medium.com/max/60/1*ychLO4DAe5cvD1UwUuvjZw.png?q=20', 'https://miro.medium.com/max/60/0*FwubnnoNlt6Coo9j.png?q=20', 'https://miro.medium.com/max/1880/1*za1jYH-6ooFfccLxdggpKg.png', 'https://miro.medium.com/max/964/1*pCnqiKj-Hrdn7FajO0sh1Q.png', 'https://miro.medium.com/max/2560/1*FgMeHrpzkMgDc1RCrl8JNw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*-EbaK-Nn5L7pNulJzx39YQ.png?q=20', 'https://miro.medium.com/max/60/1*wRIkttjEX1Udy4pZaPSZeg.png?q=20', 'https://miro.medium.com/max/2048/0*zQCDjMFdx-4uo3ec.png', 'https://miro.medium.com/max/964/1*_dhZ4-lsPVUzRdHUWbtBNQ.png'}",2020-03-05 00:13:46.803674,2.1649563312530518
https://medium.com/tech-vision/random-forest-classification-with-h2o-python-for-beginners-b31f6e4ccf3c,Random Forest Classification with H2O [Python][for Beginners],"H2O is an opensource machine learning platform that facilitates you to build models based on data that you have. This article will let you discover how H2O machine learning can apply for simple classification problem.

Note : For this tutorial, you need to setup H2O in your python environment.

import h2o

from h2o.estimators import H2ORandomForestEstimator

To create a Random Forest Classification model H2ORandomForestEstimator will instantiate you a model object.

h2o.init()

Check whether if it is possible to connect to an existing H2O instance. If it fails, attempt to create a local H2O instance at localhost:54321.

Copy ‘iris.csv’ file into your project folder. This file contains the data that required to train your model. You need to add headers to the data set manually.

Figure 1 : Adding headers to the data set

# Load data from CSV

data = h2o.import_file('iris.csv')

Read the iris.csv file and load the data as an H2O frame.

'''

Iris data set description

-------------------------



1. sepal length in cm

2. sepal width in cm

3. petal length in cm

4. petal width in cm

5. class:

Iris Setosa

Iris Versicolour

Iris Virginica



'''

Based on sepal length, sepal width, petal length and petal width data it is required to identify the class that each iris flower belongs to.

# Input parameters that are going to train

training_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

# Output parameter train against input parameters

response_column = 'class'

Define the training parameters, input and target parameters.

# Split data into train and testing

train, test = data.split_frame(ratios=[0.8])

Split the data set into train and test. The testing data will help you to verify the validity of your model after creating it. And it will also prevent model over fitting to the given data.

# Define model

model = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10)



# Train model

model.train(x=training_columns, y=response_column, training_frame=train)

Define the model with required parameters and train it.

# Model performance

performance = model.model_performance(test_data=test)



print performance

Finally, it is time to see the performance of the model.

Figure 2: Test Results

You can see that the model identifies the class of iris flowers correctly without having any misprediction. When you run this program, the answers can be slightly varied. Because random forest algorithm uses randomly created trees for ensemble learning. And also when splitting data for training and testing, H2O is using a random splitting which can change the data in each frame.

Full Project","['forest', 'set', 'beginners', 'train', 'sepal', 'petal', 'h2o', 'length', 'pythonfor', 'width', 'data', 'model', 'random', 'classification', 'parameters']","import h2ofrom h2o.estimators import H2ORandomForestEstimatorTo create a Random Forest Classification model H2ORandomForestEstimator will instantiate you a model object.
Figure 1 : Adding headers to the data set# Load data from CSVdata = h2o.import_file('iris.csv')Read the iris.csv file and load the data as an H2O frame.'''
Iris data set description-------------------------1. sepal length in cm2. sepal width in cm3. petal length in cm4. petal width in cm5. class:Iris SetosaIris VersicolourIris Virginica'''Based on sepal length, sepal width, petal length and petal width data it is required to identify the class that each iris flower belongs to.
# Split data into train and testingtrain, test = data.split_frame(ratios=[0.8])Split the data set into train and test.
And also when splitting data for training and testing, H2O is using a random splitting which can change the data in each frame.",en,['Roshan Alwis'],2016-10-30 12:50:18.055000+00:00,"{'Machine Learning', 'Data Science'}","{'https://miro.medium.com/max/984/1*krbMzHkXtNeXUh3fKMjJHg.png', 'https://miro.medium.com/max/60/1*krbMzHkXtNeXUh3fKMjJHg.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*GOgFVH5CffOcNcL5.', 'https://miro.medium.com/fit/c/160/160/1*AAaJa2cajphLjyke0DVLGw.png', 'https://miro.medium.com/fit/c/160/160/0*GOgFVH5CffOcNcL5.', 'https://miro.medium.com/fit/c/80/80/2*WB9JQgyyaBv3sCtj5tqPkQ.jpeg', 'https://miro.medium.com/max/1000/1*QNtpWNresupHbiV7Xe59gg.png', 'https://miro.medium.com/max/60/1*JQihCBaH-PyPGPl_69M8Xg.png?q=20', 'https://miro.medium.com/max/500/1*QNtpWNresupHbiV7Xe59gg.png', 'https://miro.medium.com/max/930/1*JQihCBaH-PyPGPl_69M8Xg.png', 'https://miro.medium.com/max/60/1*QNtpWNresupHbiV7Xe59gg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*8TyJoz2oyfux06hOJujerQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*qxCVcWzjLboFJkt4FJl2jQ@2x.jpeg', 'https://miro.medium.com/max/96/1*DiTIRWkijrTmt1IM3GsMGA.jpeg'}",2020-03-05 00:13:48.151822,1.3481478691101074
https://towardsdatascience.com/a-simple-and-intuitive-explanation-of-hintons-capsule-networks-b59792ad46b1,A simple and intuitive explanation of Hinton’s Capsule Networks,"The Deep Learning resurgence

Back in 2012, a paper was published in the area of machine learning that would spark a resurgence in the field if AI. The paper of the first published research of its kind to use a Deep Convolutional Neural Network for image recognition. It shattered all previous records of algorithms for image recognition.

Since then, everyone’s hopped on the deep learning train, really for one simple reason: it’s been working. In fact, there really hasn’t been a single field in computer vision or natural language process where deep learning has been applied, where it didn’t improve the results. Researchers have been trying all kinds of crazy configurations too and applying the techniques to a host of new applications, an exciting time for AI indeed.

A few of those techniques are in the figure below. Things like stacking many layers, multiple sub-networks, changing convolution size, and skip connections have all played a role in boosting accuracy and advancing the research.

A few of the many techniques developed for deep learning in computer vision

… So what’s the catch?

Well deep learning isn’t going to work everywhere.

It’s obviously done very well at very many things over the past few years, but when you use anything for an extended period of time, some of its flaws and weaknesses start to show up.

The failures of Deep Learning

The main building block of a Convolutional Neural Network (CNN) is the convolution operation. In a deep network, the basic job of a convolution (and its weights) is basically to “detect” key features. When we train a deep network we are effectively tweaking the weights of our CNN’s convolution operations to detect or be “activated” by certain features in the image.

If we train a network on a dataset for face detection, then some of the convolutions in the network might be triggered by eyes while others may be triggered by ears. If we have all of the components (or at least a certain amount) to make up a face like eyes, ears, nose, and mouth, then our CNN will tell us that it has detected a face, hooray!!

But that’s unfortunately where the reach of CNNs end. Let’s take a look at the example figure below to see why. The image of the face on the left has all of the components mentioned above. A CNN handles this case just fine: it sees all the components, i.e the convolutions activate on all the right features. So our network says yes, that is a face!

An example of where a CNN would fail. Source

The tricky part is that for a CNN, the image on the right is also a face. Why you say? Well to a network it has all the features of a face. When the convolution operations are applied they will be activated on all of those features!

An important thing to understand is that higher-level features combine lower-level features as a weighted sum: activation of a preceding layer are multiplied by the following layer neuron’s weights and added, before being passed to activation non-linearity. Nowhere in this information flow are the relationships between features taken into account.

Thus, we can say that the main failure of CNNs is that they do not carry any information about the relative relationships between features. This is simply a flaw in the core design of CNNs since they are based on the basic convolution operation applied to scalar values.

How Capsules can help

Hinton argues that in order to correctly do image recognition, it is important to preserve hierarchical pose relationships between image features. Capsules introduce a new building block that can be used in deep learning to better model these relationships inside the network. When these relationships are built into the network, it becomes very easy for it to understand that the thing that it sees is just another view of something that it has seen before, since it’s not just relying on independent features; it’s now putting those features together to form a more complete representation of its “knowledge”.

The key to this richer feature representation is the use of vectors rather than scalers. Let’s review the basic operations of a convolution in a CNN: matrix multiply (i.e scalar waiting), add it all up, scalar-to-scalar mapping (i.e the activation function). Here are the steps:

scalar weighting of input scalars Sum of weighted input scalars scalar-to-scalar non-linearity

The change for Capsule networks can be broken down simply by using vectors instead of scalars:

matrix multiplication of input vectors scalar weighting of input vectors sum of weighted input vectors vector-to-vector non-linearity

Capsules vs traditional neurons

Vectors help because the help us encode more information, and not just any kind of information, relational and relative information. Imagine that instead of taking just the scalar activation of a feature, we considered its vector containing something like containing [likelihood, orientation, size]. The original scalar version might work something like the diagram on the left; it detects a face even though the eyes and lips are huge relative to the face (95% likelihood)! Capsules on the other hand due to their richer vector information see that the sizes of the features are different and therefore output a lower likelihood for the detection of the face! Pay special attention to the scores in the diagrams to fully understand.","['convolution', 'capsule', 'network', 'face', 'information', 'intuitive', 'cnn', 'vectors', 'simple', 'image', 'hintons', 'learning', 'deep', 'networks', 'explanation', 'features']","The Deep Learning resurgenceBack in 2012, a paper was published in the area of machine learning that would spark a resurgence in the field if AI.
Since then, everyone’s hopped on the deep learning train, really for one simple reason: it’s been working.
In fact, there really hasn’t been a single field in computer vision or natural language process where deep learning has been applied, where it didn’t improve the results.
A few of the many techniques developed for deep learning in computer vision… So what’s the catch?
How Capsules can helpHinton argues that in order to correctly do image recognition, it is important to preserve hierarchical pose relationships between image features.",en,['George Seif'],2019-05-04 13:44:18.631000+00:00,"{'Innovation', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Technology'}","{'https://miro.medium.com/max/1015/1*QruQniKjyUwa2QWckPl6kg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1204/1*_nzxfHm_GWW7e_09UDdqyA.png', 'https://miro.medium.com/max/60/1*9tzKuG6lY9WSc6zPI0ysAA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*3tnSj6KgC1A4gclyOgqDxw.png?q=20', 'https://miro.medium.com/max/60/1*QruQniKjyUwa2QWckPl6kg.png?q=20', 'https://miro.medium.com/max/60/1*_nzxfHm_GWW7e_09UDdqyA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/max/960/1*3tnSj6KgC1A4gclyOgqDxw.png', 'https://miro.medium.com/max/60/1*t6GMJCq-vWiSJ1qTH7RB2A.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*AL4k3d3Qov_uGKeJaB-vkA.png?q=20', 'https://miro.medium.com/max/2030/1*QruQniKjyUwa2QWckPl6kg.png', 'https://miro.medium.com/max/2310/1*t6GMJCq-vWiSJ1qTH7RB2A.jpeg', 'https://miro.medium.com/max/954/1*AL4k3d3Qov_uGKeJaB-vkA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2000/1*9tzKuG6lY9WSc6zPI0ysAA.png'}",2020-03-05 00:13:54.241623,6.08871865272522
https://towardsdatascience.com/generating-new-ideas-for-machine-learning-projects-through-machine-learning-ce3fee50ec2,Generating New Ideas for Machine Learning Projects Through Machine Learning,"The motivation for this project was for me to learn using a recurrent neural network (RNN) to generate quotes similar to my favorite philosophers and thinkers. I had seen many other people generating music, jokes and even molecules using RNNs. I was pumped to do the same, but for philosophy.

From the web, I had collected about ~5000 quotes from thinkers like Camus, Nietzsche, Wittgenstein, Feynman, David Hume, Stephen Hawking, and James Carse.

What skipped my eye completely was that the projects that I took as inspiration usually had a dataset that went into millions and all I had with myself was 5000 sentences. Naively and blindly, I marched on and failed repeatedly in getting my artificial philosopher to work. Finally, after three failed experiments I got it to work, which I followed up by building a generator for machine learning ideas.

The unreasonable stubbornness of RNNs when the training corpus is small

Here’s the first failed experiment I did with RNNs:

I thought the corpus is not big enough to train a word-level language model, so I went for a character-level language model

I trained a very simple one-layer LSTM/GRU-based recurrent network on the corpus but it didn’t do well

It output English-looking but ultimately gibberish sentences such as “I can to the somethe we and the can to the come to this tore to the contrange tore to the conservati”

My second failed experiment:

I thought perhaps one layer of recurrent units isn’t enough and so I experimented with two layers and also played around with higher learning rate.

Nothing worked. I was still getting gibberish such as: “I real to yest portication found that in you are for the man of there some of the more to yor to you”

My third failed experiment:

Because of a small corpus, I thought perhaps a generative adversarial framework will work better

But that didn’t work too and I realized that GAN for LSTMs is hard and there are papers on it but training is hard and quality of output not that good.

My GAN after much training was even worse. It generated text that was absolute rubbish: “x11114111411141114111”

After so many failed attempts, I was VERY frustrated that my artificial philosopher will always remain a pipe dream.

Why can’t I get my artificial philosopher to work? (Not my photo, but you wouldn’t know) via Pixabay

My conclusion from these failed attempts was that the culprit was the small text corpus.

Perhaps 5000 quotes were not enough to generate similar quotes of good quality?

So, as my next experiment, I wanted to try pre-existing word embeddings such as word2vec rather than forcing the network to learn the embeddings from scratch. But, before doing that, I decided to take advice on Reddit’s machine learning subreddit. On the thread that I started, someone pointed me to a poster accepted into 2018 NeurIPS conference titled: “Transfer Learning for Style-Specific Text Generation”. I followed ideas from that paper and they worked like a charm.

What is transfer learning?

Transfer learning is a simple but powerful idea. It means using an existing model that’s trained on a very large dataset as a starting point and tweaking it to work well on your domain-specific dataset.

Image via Intel’s developer website

In the computer vision community, transfer learning has been used for long. The idea is to use a publicly available model such as VGG that was trained on the ImageNet dataset with 14 million images across 20,000 categories and use activations of its last layer as the input to an additional task-specific layer. The additional layer is then trained specifically on your small, domain-specific dataset for prediction, classification or any other task.

The surprising beauty of using pre-trained models is that you for free, you get to use all the concepts that the pre-trained model has learned across millions of images across hundreds of hours of training.

A pre-trained model is compressed knowledge.

These pre-learned concepts and activations enable you to predict and classify on your small, domain-specific dataset. This magic happens due to the fact that all “natural” datasets share similar characteristics. Most images share many characteristics: from primitive concepts of shapes, lines and edges to high-level concepts such as textures and effect of light and shadows.

Without pre-trained models, you’d have to learn all these concepts from scratch and your dataset may not contain enough examples to do that. With a pre-trained model, you go straight to learning what’s important and different in your domain / problem and not bothering about common things found across datasets.

Transfer learning in text and NLP using pre-trained language models

Recently, transfer learning has started cropping up in NLP and exciting possibilities have opened up. There’s Google’s massive BERT model and then there’s ULMFit.

These language models are trained on publicly available textual corpus (such as parliamentary records, Wikipedia, etc) and implicitly encode knowledge of English. Hence, they enable machine learning tasks such as classification and prediction on text even when your dataset is very small.

And that’s precisely what I wanted! My dataset of quotes was small (~5k) and my objective was to generate new quotes with a style similar to those of my favorite thinkers.

Code for text generation from a small dataset

For generating style-specific text from my small corpus, I followed the paper linked from the Reddit thread and that led me to FastAI’s lesson on classifying IMDB reviews using a pre-trained model. The lesson was about classification but as I was going through FastAI library’s documentation, I discovered that even generating text is made trivial thanks to helper functions in the library.

My code and dataset is present in this repository. You’ll require PyTorch v1, FastAI v1 and Pandas. Note that my code is literally a copy-paste from FastAI text module’s documentation. They’ve made it that easy to use pre-trained models.

What pre-trained model do we use? It’s a 3-layer AWD-LSTM model developed by Salesforce’s research team that’s trained on 100 million tokens from Wikipedia articles. I encourage you to read more details on this specific model but a key benefit of using pre-trained models is that you can get away by not understanding the underlying details. Just like you’ll most likely not care about how Pytorch and Numpy work under the hood, you can also afford to not care how AWD-LSTM works under the hood.

This level of abstraction provided by pre-trained models is truly revolutionary.

Now, anyone can assemble a state of the art deep learning model in their respective domain without requiring months and years of effort. (But it pays to know the details when you’re not getting results)

Aritifical Philosopher: new philosophical insights generated by my neural network

What would my artificial philosopher spit out? (Photo by Pixabay)

When I ran my model, I literally couldn’t believe what came out of it. In excitement, I tweeted about it:

My network said: “In the world there is no man who is not a slave” and this sounded so too-good-to-be-true that I first checked whether it was simply repeating a memorized quote from the dataset. When I didn’t find it, I googled the exact phrase to see if this idea has been expressed before. Lo-and-behold, I didn’t find it on Google too.

Here is a run with 100 quotes generated by the neural network. These are not modified by me in any way. I’ve literally copy-pasted these from my notebook. (I’m bolding the ones which are intriguing and possibly unique).

'por las vida de los mundo de los mundo en el chi',

'the truth is that we can not be sure of what we do not know .',

'according to the standard , man is the real animal .',

""it is my business that i think that 's what i do n't know ."",

'after a adventure , it was not until that point that old ideas were drawn up .',

'the lives of a finite player player must be avoided .',

'a human being is also an animal .',

'i had to turn',

""i want people to be happy but i do n't want to be ourselves ."",

'there is a greater freedom for the philosophers and for the philosophers .',

'for a moment is not merely a thought , but a tragedy .',

'at this stage it is the true art .',

'i am the bridge , which is the bridge and the one that carries the bridge .',

'it is the wisdom that the world has not yet seen established at all .',

'the future is great because we love everything .',

'what is the belief in the right to man ?',

'nature is a perfect field of vision .',

't',

'to learn to draw is to impose a law on the law of physics .',

'the feeling of absurdity : that is , as we see why we are here .',

'he who fights with monsters knows what he is .',

'no longer culture is a sign of self - mastery .',

'when the universe is rotating , i will make a very very big decision .',

'today this is probably the most so - called mystery of science .',

'it is not a matter of fact , a reason for being ashamed of love .',

'the world around me , i believe , it is the world itself .',

'the subject must always be a man who is not alone . the',

""some people n't want to be stupid ."",

'the good dream is that you have to struggle for the good and the bad .',

'there is no community without the strong and just no one to live across .',

'i am not the man who is myself .',

'i felt that i had to quite cease to exist when i was touched .',

'the above principle of infinite players is perfectly good .',

'the stars are small and solitary , though they are neither great nor bad .',

'all souls are ignorance .',

'the limits of my language are always your limits .',

'the world is a world without any real purpose .',

'beyond the curve of the days become his favorite .',

'i continue to believe in this condition of life',

'here is the origin of all things .',

'we have to live and let live in order that we can create a universe .',

'a man is very much the most fertile man .',

'this world comes out of nowhere .',

'to live is to be happy .',

'the present man has a reason to be able to give birth to a dancing star .',

""it 's surprising to say that the real world had a beginning :"",

'the first thing is to die at heart .',

'and how i learned to walk and dance , i must sing .',

'as long as the mind is limited by its laws , it can not be understood .',

'the weakness of the apes is not infinite , but our surprise .',

'at the end of a conversation there is an invincible summer .',

'les de la vida pour la vie a es se non het la vida .',

'i say the last thing , in the end , is to die .',

'what does man understand is that the true man is a unjust child .',

'the soul is a dead cause .',

""it seems that there is nothing less than a child 's love ."",

'that is why the world is governed by the laws of physics .',

'the king is a genius who goes to school to be a public relations yes .',

'the child is born of this solitude .',

'i am a tree among trees .',

'we have never got the least of ideas and ideas .',

'every age in the middle ages is born of the holy spirit of peace .',

'but no one is willing to strive for peace , justice or reason .',

""but n't the time is going to happen if what breathe is bad ."",

'at the heart of all beauty lies something monstrous and full of things .',

'really , my heart is never .',

'yes , it is to say that there is a very rapid increase in human affairs .',

'everything in the world is like a dead world .',

'the good man is a man who wants to play my own .',

'there are no real truths , but it is a perpetual truth that is true .',

'you imagine that he can not live without knowing how to live .',

'the problems of life are not universal , but the future of mankind .',

'no one can build a bridge across the river of life .',

'passion is the desire to be strong . however it is necessary to love the weak .',

'in the end one must have experience to live with envy .',

'from the dark horizon of my future a future will never be seen .',

'he who does not know has the meaning , his reason to do .',

'no one has any idea how youth ... must have learned how to walk .',

'it is true that mersault was a very poor soil .',

'this is where we see and where there are , and what we see here .',

'a species of th',

'there are no boundaries between those limits of physical limits .',

'man is one who has the advantage of being overcome',

'woman is a woman . she is a tahu .',

'to live is to live alone',

'the fate of a man must be as great as the rank of man .',

'all artists of the twentieth century are quite free to live alone !',

'there is no justification of the state for the astonishment of the world .',

'there is evidence that in every human being , a human being must win the match .',

'the world is worth living .',

'the dumb man is not a bad bad conscience but a bad liar',

'because we have to choose between being understood we have a friend and love .',

'the mother of child dignity is a mother or mother .',

'it is the art of value that we do not understand .',

'a writer has been written with a definite idea of what is really in his universe',

'they believe that something is rare for the rare .',

'every step forward in the world preclude a future and there is a certain future .',

'and continuing that is the horror of original conservation .',

'solitude is often an activity .',

'one concerns me that things can never be forgotten .',

'i love people who have no happiness , but not lest they become strong .'

You must have seen equally impressive generated text in other articles. But I think what’s impressive here is the quality of generated text given that my training set was extremely small (5k sentences). This is only possible using a pre-trained model.

It may not seem like much but I think an idea like “the limits of my language are always your limits” seems like something the language philosopher Ludwig Wittgenstein might have said. In fact, when you Google this phrase, you find no exact results but Google recommends checking the Wikipedia article on Wittgenstein.

Getting Wittgenstein to say new things after he’s dead

In reality, Wittgenstein had said: “The limits of my language mean the limits of my world” and our model has smartly (and in a grammatically accurate fashion) changed it to something new.

Similarly, the generated quote “the present man has a reason to be able to give birth to a dancing star” is reminiscent of Nietzsche because he has mentioned “dancing star” in his books but he never said it in context of the present man. I may be reading too much into it, but to me, the generated quote represents the idea that we’ve become so technologically advanced that we can give rise to really complicated machinery (like a dancing star) and we’ve become so competitive that we have a reason to do that. (Is my neural network warning us of the potential dangers of AI and the inevitability of it?)

Let’s give rise to the dancing star: generating new machine learning ideas

Remember that my corpus for philosophy quotes was ~5000 sentences. I wondered how this approach will perform if I were to give it an even smaller corpus.

I decided that generating machine learning ideas would be fun. To my knowledge, nobody else so far has tried doing that. So I collected titles of all the machine learning projects that students at Stanford’s CS229 class had submitted from the year 2004 to 2017. The dataset includes 2500 ideas comprising of five to seven words each. The dataset and the corresponding notebook are available in my repository. (Note: I don’t own the copyright to ideas. It’s collected merely for research and exploration)

The project seemed exciting but my main worry was that the domain of machine learning project ideas is very narrow and contained niche and technical terms. I thought the model will mostly spit out memorized ideas, the same as the ones in the dataset.

However, to my surprise, it generated some very novel ideas (in bold, followed by my commentary):

“ a different genre of social video game via digital camera behavior ”. There’s no “social video game” phrase in the dataset, so this must be new.

”. There’s no “social video game” phrase in the dataset, so this must be new. “ a machine learning approach to learning to recognize events from academic topics ”. There’s no “academic topics” phrase in the dataset.

”. There’s no “academic topics” phrase in the dataset. “ predicting what is the difference in frequency in data ” <- there’s no “Frequency in data” phrase in the dataset.

” <- there’s no “Frequency in data” phrase in the dataset. “ Using learning to predict features for identifying gene expression ” <- actually a novel idea!

” <- actually a novel idea! “ classifying human gene expression in optical images ” <- there’s no project idea on classifying human gene expression.

” <- there’s no project idea on classifying human gene expression. “ making an image of the world ”. I think this is an interesting project suggestion where you have to come up with one image / graphic that represents the entire world.

”. I think this is an interesting project suggestion where you have to come up with one image / graphic that represents the entire world. “ predicting the dimensions of human behavior ”. Possibly a suggestion for unsupervised classification on all the different ways that humans behave?

”. Possibly a suggestion for unsupervised classification on all the different ways that humans behave? “ reinforcement learning to improve professional learning”. Training dataset doesn’t have the phrase “professional learning”. How do you do improve learning ability in professional courses by using ideas from reinforcement learning? I was really impressed by this one as it seems both valuable and doable.

Training dataset doesn’t have the phrase “professional learning”. How do you do improve learning ability in professional courses by using ideas from reinforcement learning? I was really impressed by this one as it seems both valuable and doable. “ a single expression of what ‘s on the corporate market ? ”. How do you combine all indicators available for stock markets to come up with one indicator that’s most informative?

”. How do you combine all indicators available for stock markets to come up with one indicator that’s most informative? “ types of cardiac processes ”. Unsupervised learning to cluster similar patterns of cardiac processes to help in predicting and analyzing the ones that can lead to a cardiac arrest.

”. Unsupervised learning to cluster similar patterns of cardiac processes to help in predicting and analyzing the ones that can lead to a cardiac arrest. “ in the natural history of human interaction ”. Using human migration dataset, how do you classify historical human interactions. Can you generate new insights on human interactions that historians and anthropologists have missed?

”. Using human migration dataset, how do you classify historical human interactions. Can you generate new insights on human interactions that historians and anthropologists have missed? “ classifying the convolutional characteristics of remote — sensing images ”. The dataset doesn’t have the phrase “convolutional characteristics”. This project sounds like a fun research project for anyone who’s interested in the theory behind CNNs.

”. The dataset doesn’t have the phrase “convolutional characteristics”. This project sounds like a fun research project for anyone who’s interested in the theory behind CNNs. “classifying and predicting the event reviews” <- wow, the dataset doesn’t have the phrase “event reviews”. Just like IMDB reviews, can we collect event reviews (of plays or rock concerts) and predict for future events which ones are going to be successful and which ones will be unsuccessful?

If you want unfiltered output from the model, here are 100 ideas that it generated. I’ve not modified anything (just bolding the ones I think are interesting and novel).

'the problem is right : grasping and extracting the face of pose',

'applying machine learning to text treatment',

'machine learning techniques for learning through machine learning',

'a machine learning approach to predicting career success from a single object',

'using machine learning to predict the outcome of a machine learning approach',

'based on stock prices',

'identifying stock price models for machine learning techniques',

'a study in time travel time series analysis of musical features',

'vectors in the amazon impact network',

'classification of web articles in facebook',

'dynamic signal processing in post - secondary order data',

'copy selection with machine learning techniques',

'interpretation of user classification',

'the application of deep learning to fairness in using a semantic framework',

""creating a different entity 's portfolio"",

'using supervised learning of blind data',

'system classification for driving automatic vehicle design and classification with gene expression',

'based on public documents from text expression',

'semantic learning for music',

'machine learning for cancer prediction',

'learning static variations with deep learning for learning options',

'image classification for svm',

'satellite imagery classification',

'making decision selection from a single object',

'object detection using product preferences',

'speech detection with deep learning',

'genomic data based on stock trading',

'learning to predict approach to handwriting',

'classification of musical features from the composer data',

'semantic social network and smartphone features',

'machine learning techniques',

'using real - time information to predict the popularity of the market',

'video game classification',

'a learning task for time series players',

'using a single machine learning approach for a single learning approach to learning to identify other environments',

'multiple - genre classification of fraud

prediction for a mass neural network',

'learning of human activity recognition from analysis of text',

""an nba player 's approach to learning and character forecasting through video game ecg"",

'playing a vocal instrument in local mri learning',

'real - time music recordings',

'finding new artistic and artistic features in music videos',

'an analysis of musical genres',

'predicting a single image - specific musical style',

'a cost approach to crime prediction',

'automatic user prediction and automated review recognition',

'food processing via machine learning',

'human activity recognition using multi - label fantasy',

'predicting a match in the keystroke poker',

'estimation of game types',

'ai identification of deep learning in locomotion monitoring using neural networks',

'the value of collaborative attention projecting for real - time playing',

'the sea level and low speed : the two waves',

'learning to predict the price of beer and personal genomes',

'trading and removing a novel image from the text',

'real - time news user identification on google gestures',

'removing and re - learning to play game and lyrics',

'rapid - mass dynamics with acoustic images',

'real - time music direction',

""what 's your right ?"",

'exploring event and music',

'human activity prediction using machine learning',

'model of architecture in california',

'vs light crime',

'adaptive learning for image recognition',

'predicting the approach of human activity using machine learning',

'the win given trajectories',

'a machine learning approach to online design',

'a massive based multi - layer feature unsupervised approach for multi - agent music',

'can you learn from a single hand',

'reaction with the media',

'measurement of time to order over time',

'how people can stop : learning the objects of blood and blood',

'machine learning for autonomous vehicles',

'vehicle types in neural networks',

'building a model for what does it store ?',

'for enhanced identification of machine learning techniques',

""exploring new york city 's public image through machine learning"",

'a novel approach to career image recognition',

'in general game playing',

'structure classification for adaptation of text',

'a variance learning approach for speech recognition',

'the optimization of a non - peer temporal layer',

""a distinguishing feature of a song 's legal expression"",

'learning to sound in english : learning to learn using word learning',

'information sharing with adaptive neural networks',

'playing the game with multi - touch neural networks',

'recursive estimation of dynamic and static images',

'predicting the quality of the net - style result in the media',

'the character of the sea snake robot',

'predicting the stock market price of machine learning',

'using inverted nucleotide data to predict the price of convolutional protein models',

'search engine',

'using twitter data to predict prices in high - cost trading',

'a machine learning approach',

'creating a new approach to building a deep learning approach',

'fingerprint learning component',

'machine learning techniques for functional change learning for the building of new york city college football networks',

'predicting cancer risk of breast cancer risk',

'cancer diagnosis and prediction',

'stock market classification',

'identifying the outcome of the news media'

I haven’t checked thoroughly, but random checks tell me that most of the generated ideas are unique. I think the reason why the generated text isn’t memorized from training corpus is because we’re using a pre-trained model. The pre-trained language model was trained on Wikipedia and hence it has strong opinions on how concepts and words are related even before seeing training data.

For a model that’s initialized randomly, the easiest way to reduce training data is to remember the training corpus. This results in over-fitting. However, for a pre-trained model, if the network tries to learn the training corpus, it can only do that if it first forgets previously learned weights. And since that leads to a higher error, the easier way is to accommodate training corpus within the context of earlier learned weights. Hence, the network is forced to generalize and generates grammatically correct sentences (thanks to pre-training on Wikipedia) but using domain-specific concepts and words (thanks to your dataset).

What would you train using this approach?

Before pre-trained models were available, you needed a huge corpus of text to do anything meaning. Now, even a small dataset is enough to do interesting things. Let me know in comments what project ideas come to your mind that could use a small text corpus along with a pre-trained model.

Some ideas to get your neurons firing:

Using your tweets, train a model that tweets like you

Using data dump from your WhatsApp, make a bot that chats like you

For your company, classify support tickets into BUG or FEATURE REQUEST

Make a bot that generates quotes similar to your favorite author

Make your own customized AUTO-REPLY drafter for Gmail

Provided a photo and an Instagram account, generate caption in the style of the account’s previous captions

Generate new blog post ideas for your blog (based on previous blog posts titles)

Also, it’ll be super cool if you end up implementing a machine learning project idea generated by my model (or the one contained in this post). You’ll be part of the world’s first project that a machine has thought of which a human implements!

Thanks for reading so far. Let me know your thoughts and questions in comments.

PS: Check out my previous hands-on tutorial on Bayesian Neural Networks

Thanks Nirant Kasliwal for reviewing the draft of this post and giving helpful suggestions.

Follow me on Twitter

I regularly tweet on AI, deep learning, startups, science and philosophy. Follow me on https://twitter.com/paraschopra","['machine', 'ideas', 'generating', 'pretrained', 'human', 'dataset', 'learning', 'world', 'man', 'model', 'projects', 'using']","Finally, after three failed experiments I got it to work, which I followed up by building a generator for machine learning ideas.
Let’s give rise to the dancing star: generating new machine learning ideasRemember that my corpus for philosophy quotes was ~5000 sentences.
I decided that generating machine learning ideas would be fun.
So I collected titles of all the machine learning projects that students at Stanford’s CS229 class had submitted from the year 2004 to 2017.
“ a machine learning approach to learning to recognize events from academic topics ”.",en,['Paras Chopra'],2018-12-20 06:39:08.932000+00:00,"{'Neural Networks', 'Deep Learning', 'Transfer Learning', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1098/1*yABe7shcoWlFKfidyzrOhg.png', 'https://miro.medium.com/max/58/1*bQXMnvI8U__bPay1FJavFw.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1232/1*bQXMnvI8U__bPay1FJavFw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*LckTzKicHoE_xRRBRk1ThA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/640/1*LaJRmXfHq-SRJM2UEMHQfA.jpeg', 'https://miro.medium.com/max/52/1*yABe7shcoWlFKfidyzrOhg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*cienkGywx5tieNpNoyQL3A.png?q=20', 'https://miro.medium.com/max/1280/1*LaJRmXfHq-SRJM2UEMHQfA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2624/1*cienkGywx5tieNpNoyQL3A.png', 'https://miro.medium.com/max/1500/1*tmh4aAYfP-1SGqpqAaim3w.png', 'https://miro.medium.com/max/60/1*LaJRmXfHq-SRJM2UEMHQfA.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*LckTzKicHoE_xRRBRk1ThA.png', 'https://miro.medium.com/max/60/1*tmh4aAYfP-1SGqpqAaim3w.png?q=20'}",2020-03-05 00:13:57.284603,3.042980432510376
https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d,Google Colab Free GPU Tutorial,"Now you can develop deep learning applications with Google Colaboratory -on the free Tesla K80 GPU- using Keras, Tensorflow and PyTorch.

Hello! I will show you how to use Google Colab, Google’s free cloud service for AI developers. With Colab, you can develop deep learning applications on the GPU for free.

Thanks to KDnuggets!

I am happy to announce that this blog post was selected as KDnuggets Silver Blog for February 2018! Read this on KDnuggets.

What is Google Colab?

Google Colab is a free cloud service and now it supports free GPU!

You can;

improve your Python programming language coding skills.

programming language coding skills. develop deep learning applications using popular libraries such as Keras, TensorFlow, PyTorch, and OpenCV.

The most important feature that distinguishes Colab from other free cloud services is; Colab provides GPU and is totally free.

Detailed information about the service can be found on the faq page.

Getting Google Colab Ready to Use

Creating Folder on Google Drive

Since Colab is working on your own Google Drive, we first need to specify the folder we’ll work. I created a folder named “app” on my Google Drive. Of course, you can use a different name or choose the default Colab Notebooks folder instead of app folder.

I created an empty “app” folder

Creating New Colab Notebook

Create a new notebook via Right click > More > Colaboratory

Right click > More > Colaboratory

Rename notebook by means of clicking the file name.

Setting Free GPU

It is so simple to alter default hardware (CPU to GPU or vice versa); just follow Edit > Notebook settings or Runtime>Change runtime type and select GPU as Hardware accelerator.

Running Basic Python Codes with Google Colab

Now we can start using Google Colab.

I will run some Basic Data Types codes from Python Numpy Tutorial.

It works as expected :) If you do not know Python which is the most popular programming language for AI, I would recommend this simple and clean tutorial.

Running or Importing .py Files with Google Colab

Run these codes first in order to install the necessary libraries and perform authorization.

When you run the code above, you should see a result like this:

Click the link, copy verification code and paste it to text box.

After completion of the authorization process, you should see this:

Now you can reach you Google Drive with:

install Keras:

!pip install -q keras

upload mnist_cnn.py file to app folder which is located on your Google Drive.

mnist_cnn.py file

run the code below to train a simple convnet on the MNIST dataset.","['folder', 'python', 'simple', 'colab', 'free', 'tutorial', 'gpu', 'service', 'google', 'using', 'app']","I will show you how to use Google Colab, Google’s free cloud service for AI developers.
Google Colab is a free cloud service and now it supports free GPU!
The most important feature that distinguishes Colab from other free cloud services is; Colab provides GPU and is totally free.
Getting Google Colab Ready to UseCreating Folder on Google DriveSince Colab is working on your own Google Drive, we first need to specify the folder we’ll work.
Running Basic Python Codes with Google ColabNow we can start using Google Colab.",en,[],2019-03-25 09:51:13.750000+00:00,"{'Python', 'Google', 'Google Colab', 'Free Gpu', 'Gpu'}","{'https://miro.medium.com/max/1192/1*lb2htyPfbC5Y9VF8IZGqdQ.png', 'https://miro.medium.com/max/2466/1*Mw8_NcnS-a0TyDG9TVHqqg.png', 'https://miro.medium.com/max/2122/1*jE_CBuejVzTT_3ecSjk86w.png', 'https://miro.medium.com/max/60/1*rHxgzJWoos7f4AYF90PkzQ.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*a37uQ1IoRxUkSeMUw-BFAg.jpeg', 'https://miro.medium.com/max/60/1*keRD5wndUyzoxgNUwfWfsQ.png?q=20', 'https://miro.medium.com/max/1032/1*I1TO_CtAolkNTPDK-vp4Hg.png', 'https://miro.medium.com/max/60/1*cIrmYPaA5HHR1yLj2UPgAQ.png?q=20', 'https://miro.medium.com/max/1048/1*02ylPr7JIn_qiJkc4iprpw.png', 'https://miro.medium.com/max/2130/1*Sm0CLQDJjX0uJMMjLuuhYA.png', 'https://miro.medium.com/max/58/1*9x6GVBOwbAEsx7h8k5ruBw.jpeg?q=20', 'https://miro.medium.com/max/60/1*I1TO_CtAolkNTPDK-vp4Hg.png?q=20', 'https://miro.medium.com/max/740/1*WNovJnpGMOys8Rv7YIsZzA.png', 'https://miro.medium.com/max/2004/1*zyxag4hs2vCY1DejIJveZg.png', 'https://miro.medium.com/max/794/1*cIrmYPaA5HHR1yLj2UPgAQ.png', 'https://miro.medium.com/max/484/1*EPbmqr--SxC0crhMxoaS9Q.png', 'https://miro.medium.com/max/60/1*7XLisHAnGGnflIYyqQja8Q.jpeg?q=20', 'https://miro.medium.com/max/60/1*Sm0CLQDJjX0uJMMjLuuhYA.png?q=20', 'https://miro.medium.com/max/1626/1*D-xR_CzTP3_MMt_8UqIj4Q.png', 'https://miro.medium.com/max/60/1*Wx-XLmFKjir-jxcVWp2i9g.png?q=20', 'https://miro.medium.com/max/1074/1*Om46o5HRFOC7RgXaWELV-w.png', 'https://miro.medium.com/max/926/1*rHxgzJWoos7f4AYF90PkzQ.jpeg', 'https://miro.medium.com/max/786/1*kGvfrNrRHwfv1jWtguufkg.png', 'https://miro.medium.com/max/4996/1*E2UfDvleKBbhydHxMZtQ2g.png', 'https://miro.medium.com/max/364/1*Cy19qeGZzgllJrtAqOH4OQ.png', 'https://miro.medium.com/fit/c/80/80/1*Wy0cxXZpX-FrMWeXsqOOpg.png', 'https://miro.medium.com/max/60/1*02ylPr7JIn_qiJkc4iprpw.png?q=20', 'https://miro.medium.com/max/2636/1*4AJ2EEn-xtvGAiwsNlDmNQ.png', 'https://miro.medium.com/max/600/1*l8MbafXh9TGm3NE4ULtPQg.png', 'https://miro.medium.com/max/1896/1*gjyZxq2tUORKLi3Fp_-sEg.png', 'https://miro.medium.com/max/1856/1*vtTvpFVdCcsmEXtQA6k2Kw.png', 'https://miro.medium.com/max/60/1*D324zKvU1Ivu-RvKrOG7Ew.png?q=20', 'https://miro.medium.com/max/2000/1*keRD5wndUyzoxgNUwfWfsQ.png', 'https://miro.medium.com/max/60/1*Cy19qeGZzgllJrtAqOH4OQ.png?q=20', 'https://miro.medium.com/max/956/1*9y7lbgBmG99ZVkGr5b7arQ.png', 'https://miro.medium.com/max/60/1*gjyZxq2tUORKLi3Fp_-sEg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*nIvslm5rv4goxDK0pLDjBA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*IOB-UbprvTgHQIIKRg_R6g.png', 'https://miro.medium.com/max/60/1*lb2htyPfbC5Y9VF8IZGqdQ.png?q=20', 'https://miro.medium.com/max/60/1*jE_CBuejVzTT_3ecSjk86w.png?q=20', 'https://miro.medium.com/max/1440/1*Kbta9F_ZiRQmvETa-JkOSA.png', 'https://miro.medium.com/max/60/1*vtTvpFVdCcsmEXtQA6k2Kw.png?q=20', 'https://miro.medium.com/max/60/1*4AJ2EEn-xtvGAiwsNlDmNQ.png?q=20', 'https://miro.medium.com/max/60/1*zyxag4hs2vCY1DejIJveZg.png?q=20', 'https://miro.medium.com/max/60/1*E2UfDvleKBbhydHxMZtQ2g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*a37uQ1IoRxUkSeMUw-BFAg.jpeg', 'https://miro.medium.com/max/60/1*Mw8_NcnS-a0TyDG9TVHqqg.png?q=20', 'https://miro.medium.com/max/2324/1*SwDEbzteA0EeNDcq8m_tdA.png', 'https://miro.medium.com/max/60/1*D-xR_CzTP3_MMt_8UqIj4Q.png?q=20', 'https://miro.medium.com/max/640/1*9x6GVBOwbAEsx7h8k5ruBw.jpeg', 'https://miro.medium.com/max/60/1*qPFwOR1l8DPwXqAcotrWCA.png?q=20', 'https://miro.medium.com/max/1056/1*qPFwOR1l8DPwXqAcotrWCA.png', 'https://miro.medium.com/max/782/1*emOY5nIyYphREEqo6e86jg.png', 'https://miro.medium.com/max/60/1*Om46o5HRFOC7RgXaWELV-w.png?q=20', 'https://miro.medium.com/max/720/1*Kbta9F_ZiRQmvETa-JkOSA.png', 'https://miro.medium.com/max/60/1*emOY5nIyYphREEqo6e86jg.png?q=20', 'https://miro.medium.com/max/60/1*EPbmqr--SxC0crhMxoaS9Q.png?q=20', 'https://miro.medium.com/max/1784/1*D324zKvU1Ivu-RvKrOG7Ew.png', 'https://miro.medium.com/max/60/1*ICwiBXUgxwq7i6f_zyn-Nw.jpeg?q=20', 'https://miro.medium.com/max/60/1*WNovJnpGMOys8Rv7YIsZzA.png?q=20', 'https://miro.medium.com/max/1546/1*Wx-XLmFKjir-jxcVWp2i9g.png', 'https://miro.medium.com/max/1806/1*7XLisHAnGGnflIYyqQja8Q.jpeg', 'https://miro.medium.com/max/60/1*Kbta9F_ZiRQmvETa-JkOSA.png?q=20', 'https://miro.medium.com/max/60/1*SwDEbzteA0EeNDcq8m_tdA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BEc6O6Z6fCy2NDHL9K462A@2x.jpeg', 'https://miro.medium.com/max/60/1*9y7lbgBmG99ZVkGr5b7arQ.png?q=20', 'https://miro.medium.com/max/60/1*kGvfrNrRHwfv1jWtguufkg.png?q=20', 'https://miro.medium.com/max/3416/1*ICwiBXUgxwq7i6f_zyn-Nw.jpeg'}",2020-03-05 00:13:58.535452,1.2498488426208496
https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09,GAN — What is Generative Adversarial Networks GAN?,"To create something from nothing is one of the greatest feelings, ... It’s heaven. By Prince

GAN is about creating, like drawing a portrait or composing a symphony. This is hard compared to other deep learning fields. It is much easier to identify a Monet painting than painting one, by computers or by people. But it brings us closer to understanding intelligence. GAN leads us to thousands of GAN research papers written in recent years. In developing games, we hire many production artists to create animation. Some of the tasks are routine. By applying automation with GAN, we may one day focus ourselves on the creative sides rather than repeating routine tasks daily. Being a part of the GAN series, this article covers the GAN concept and its algorithm.

What does GAN do?

The main focus for GAN (Generative Adversarial Networks) is to generate data from scratch, mostly images but other domains including music have been done. But the scope of application is far bigger than this. Just like the example below, it generates a zebra from a horse. In reinforcement learning, it helps a robot to learn much faster.

Right side image created by CycleGAN Source

Generator and discriminator

GAN composes of two deep networks, the generator, and the discriminator. We will first look into how a generator creates images before learning how to train it.

First, we sample some noise z using a normal or uniform distribution. With z as an input, we use a generator G to create an image x (x=G(z)). Yes, it sounds magical and we will explain it one-step at a time.

Conceptually, z represents the latent features of the images generated, for example, the color and the shape. In Deep learning classification, we don’t control the features the model is learning. Similarly, in GAN, we don’t control the semantic meaning of z. We let the training process to learn it. i.e. we do not control which byte in z determines the color of the hair. To discover its meaning, the most effective way is to plot the generated images and examine ourselves. The following images are generated by progressive GAN using random noise z!

We can change one particular dimension in z gradually and visualize its semantic meaning.

Images produced in CoGAN

So what is this magic generator G? The following is the DCGAN which is one of the most popular designs for the generator network. It performs multiple transposed convolutions to upsample z to generate the image x. We can view it as the deep learning classifier in the reverse direction.

A generator in DCGAN. Source

But a generator alone will just create random noise. Conceptually, the discriminator in GAN provides guidance to the generator on what images to create. Let’s consider a GAN’s application, CycleGAN, that uses a generator to convert real scenery into a Monet style painting.

Modified form CycleGAN

By training with real images and generated images, GAN builds a discriminator to learn what features make images real. Then the same discriminator will provide feedback to the generator to create paintings that look like the real Monet paintings.

So how is it done technically? The discriminator looks at real images (training samples) and generated images separately. It distinguishes whether the input image to the discriminator is real or generated. The output D(X) is the probability that the input x is real, i.e. P(class of input = real image).

We train the discriminator just like a deep network classifier. If the input is real, we want D(x)=1. If it is generated, it should be zero. Through this process, the discriminator identifies features that contribute to real images.

On the other hand, we want the generator to create images with D(x) = 1 (matching the real image). So we can train the generator by backpropagation this target value all the way back to the generator, i.e. we train the generator to create images that towards what the discriminator thinks it is real.

We train both networks in alternating steps and lock them into a fierce competition to improve themselves. Eventually, the discriminator identifies the tiny difference between the real and the generated, and the generator creates images that the discriminator cannot tell the difference. The GAN model eventually converges and produces natural look images.

This discriminator concept can be applied to many existing deep learning applications also. The discriminator in GAN acts as a critic. We can plug the discriminator into existing deep learning solutions to provide feedback to make it better.

Backpropagation

Now, we will go through some simple equations. The discriminator outputs a value D(x) indicating the chance that x is a real image. Our objective is to maximize the chance to recognize real images as real and generated images as fake. i.e. the maximum likelihood of the observed data. To measure the loss, we use cross-entropy as in most Deep Learning: p log(q). For real image, p (the true label for real images) equals to 1. For generated images, we reverse the label (i.e. one minus label). So the objective becomes:

On the generator side, its objective function wants the model to generate images with the highest possible value of D(x) to fool the discriminator.

We often define GAN as a minimax game which G wants to minimize V while D wants to maximize it.

Once both objective functions are defined, they are learned jointly by the alternating gradient descent. We fix the generator model’s parameters and perform a single iteration of gradient descent on the discriminator using the real and the generated images. Then we switch sides. Fix the discriminator and train the generator for another single iteration. We train both networks in alternating steps until the generator produces good quality images. The following summarizes the data flow and the gradients used for the backpropagation.

The pseudo-code below puts everything together and shows how GAN is trained.

Usually with k=1 Source

Generator diminished gradient

However, we encounter a gradient diminishing problem for the generator. The discriminator usually wins early against the generator. It is always easier to distinguish the generated images from real images in early training. That makes V approaches 0. i.e. - log(1 -D(G(z))) → 0. The gradient for the generator will also vanish which makes the gradient descent optimization very slow. To improve that, the GAN provides an alternative function to backpropagate the gradient to the generator.

More thoughts

In general, the concept of generating data leads us to great potential but unfortunately great dangers. There are many other generative models besides GAN. For example, OpenAI’s GPT-2 generates paragraphs that may look like what a journalist writes. Indeed, OpenAI decides not to open their dataset and trained model because of its possible misuse.

Yes, that is the basic concept that leads to a few thousand research papers. As always, simplicity works. Here, we cover the concept. But the potentials are far bigger than what we described. To know why it becomes so popular, let’s see some of the potential applications:","['create', 'adversarial', 'generator', 'learning', 'gan', 'networks', 'discriminator', 'deep', 'images', 'real', 'generated', 'z', 'generative']","The main focus for GAN (Generative Adversarial Networks) is to generate data from scratch, mostly images but other domains including music have been done.
To discover its meaning, the most effective way is to plot the generated images and examine ourselves.
Modified form CycleGANBy training with real images and generated images, GAN builds a discriminator to learn what features make images real.
Our objective is to maximize the chance to recognize real images as real and generated images as fake.
It is always easier to distinguish the generated images from real images in early training.",en,['Jonathan Hui'],2019-12-26 06:06:47.367000+00:00,"{'Data Science', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Computer Vision'}","{'https://miro.medium.com/max/2940/1*ihK3whUAZ_0UeK4SJicYFw.png', 'https://miro.medium.com/max/3034/1*6So6q3dWurG8qrmwk1y3jw.jpeg', 'https://miro.medium.com/max/3200/1*roO-E4KTolB-wttrs-u16g.jpeg', 'https://miro.medium.com/fit/c/80/80/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/6120/1*lfXSqpTVsr0JcCFp1SfDHA.jpeg', 'https://miro.medium.com/fit/c/96/96/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/60/1*9qW0I-2M6qKGBwifhnPKPQ.png?q=20', 'https://miro.medium.com/max/3700/1*Hpkkiry1SQ1VCf-f9FhgDA.png', 'https://miro.medium.com/max/60/1*5imY1J0AbDvgCp_yrvUCHA.png?q=20', 'https://miro.medium.com/max/60/1*n235XEigXKL3ktL08d-CZA.jpeg?q=20', 'https://miro.medium.com/freeze/max/60/1*dWd0lVTbnu80UZM641gCbw.gif?q=20', 'https://miro.medium.com/max/60/1*ihK3whUAZ_0UeK4SJicYFw.png?q=20', 'https://miro.medium.com/max/1200/1*lfXSqpTVsr0JcCFp1SfDHA.jpeg', 'https://miro.medium.com/max/2912/1*3vK0FfloSUAHPWPpH3jjOA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg', 'https://miro.medium.com/max/3200/1*8TTaFyyh_WyKWE--H-Pl0A.jpeg', 'https://miro.medium.com/max/60/1*6So6q3dWurG8qrmwk1y3jw.jpeg?q=20', 'https://miro.medium.com/max/2448/1*n235XEigXKL3ktL08d-CZA.jpeg', 'https://miro.medium.com/max/60/1*roO-E4KTolB-wttrs-u16g.jpeg?q=20', 'https://miro.medium.com/max/60/1*lfXSqpTVsr0JcCFp1SfDHA.jpeg?q=20', 'https://miro.medium.com/max/60/1*M_YipQF_oC6owsU1VVrfhg.jpeg?q=20', 'https://miro.medium.com/max/3184/1*hlFyF-klXQunFpmoeA89jQ.png', 'https://miro.medium.com/max/3440/1*4xAHMaUGXeOQnNJhzjq-4Q.jpeg', 'https://miro.medium.com/max/60/1*hlFyF-klXQunFpmoeA89jQ.png?q=20', 'https://miro.medium.com/max/60/1*4xAHMaUGXeOQnNJhzjq-4Q.jpeg?q=20', 'https://miro.medium.com/max/60/1*_uFUaxXIEjCDm_UTzbyleA.png?q=20', 'https://miro.medium.com/max/60/1*H1QNuCajWXy2eFLH1kVFfg.png?q=20', 'https://miro.medium.com/max/60/1*8TTaFyyh_WyKWE--H-Pl0A.jpeg?q=20', 'https://miro.medium.com/max/4156/1*ULAGAYoGGr5eEB2377ArYA.png', 'https://miro.medium.com/max/3200/1*_uFUaxXIEjCDm_UTzbyleA.png', 'https://miro.medium.com/max/60/1*Hpkkiry1SQ1VCf-f9FhgDA.png?q=20', 'https://miro.medium.com/max/3200/1*9qW0I-2M6qKGBwifhnPKPQ.png', 'https://miro.medium.com/max/3200/1*M_YipQF_oC6owsU1VVrfhg.jpeg', 'https://miro.medium.com/max/896/1*dWd0lVTbnu80UZM641gCbw.gif', 'https://miro.medium.com/max/60/1*3vK0FfloSUAHPWPpH3jjOA.jpeg?q=20', 'https://miro.medium.com/max/4800/1*H1QNuCajWXy2eFLH1kVFfg.png', 'https://miro.medium.com/max/60/1*ULAGAYoGGr5eEB2377ArYA.png?q=20', 'https://miro.medium.com/max/2024/1*5imY1J0AbDvgCp_yrvUCHA.png'}",2020-03-05 00:13:59.372868,0.8364057540893555
https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714,Understanding LSTM and its diagrams,"Although we don’t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.

But when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.

I think it’s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.

The naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.

A lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.

Theoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.

Then later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.

At a first sight, this looks intimidating. Let’s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the “memory” of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.

Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.

The way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.

The first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.

The second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.

On the LSTM diagram, the top “pipe” is the memory pipe. The input is the old memory (a vector). The first cross ✖ it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.

Then the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the ✖ below the + sign.

After these two operations, you have the old memory C_t-1 changed to the new memory C_t.

Now lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it’s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.

Now the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.

The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.

These two ✖ signs are the forget valve and the new memory valve.

And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.

The above diagram is inspired by Christopher’s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn’t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing “Cell”.

I like the Christopher’s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can’t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.

The second reason I don’t like the following diagram is that the computation you perform within the unit should be ordered, but you can’t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.

The following diagram tries to represent this “delay” or “order” with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.

But these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:

This is the forget gate (valve) that shuts the old memory:

This is the new memory valve and the new memory:

These are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big “Cell”):

This is the output valve and output of the LSTM unit:","['network', 'old', 'previous', 'diagram', 'lstm', 'neural', 'memory', 'output', 'valve', 'diagrams', 'understanding', 'unit']","Although we don’t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit.
The naive way to let neural network accept a time series data is connecting several neural networks together.
Theoretically the naively connected neural network, so called recurrent neural network, can work.
h_t-1 is the output from the previous LSTM unit and C_t-1 is the “memory” of the previous unit, which I think is the most important input.
So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory.",en,['Shi Yan'],2017-11-15 10:27:13.180000+00:00,"{'Neural Networks', 'Deep Learning', 'Recurrent Neural Network', 'Machine Learning', 'Lstm'}","{'https://miro.medium.com/fit/c/160/160/1*3yjjoZ--eh8E84rH1xRIhQ.jpeg', 'https://miro.medium.com/fit/c/80/80/0*orixWf7jAmLV61wo', 'https://miro.medium.com/max/2042/1*Hil-MwFIs_6l4naBNbUQXg.png', 'https://miro.medium.com/fit/c/80/80/2*R6303tLavDAf6jJAsMlaJQ.jpeg', 'https://miro.medium.com/max/60/1*Hil-MwFIs_6l4naBNbUQXg.png?q=20', 'https://miro.medium.com/max/2382/1*9ZgZxRSVAvJEsaoWtz69yg.png', 'https://miro.medium.com/max/60/1*a0LOUiBzyKB6J-wSFwwQ1Q.jpeg?q=20', 'https://miro.medium.com/max/60/1*OouvKl3kimURBWNfVEutpg.png?q=20', 'https://miro.medium.com/max/2382/1*mOhp-z4KM0Qm6Y0RY7YgMQ.png', 'https://miro.medium.com/max/692/1*oS5taVAKcIII1qNduYnLJA.jpeg', 'https://miro.medium.com/max/160/1*0ykYC2ubehxNL5XrIIbscg.png', 'https://miro.medium.com/max/60/1*9ZgZxRSVAvJEsaoWtz69yg.png?q=20', 'https://miro.medium.com/max/60/1*oS5taVAKcIII1qNduYnLJA.jpeg?q=20', 'https://miro.medium.com/max/2382/1*dVHmV-0Wery0KJQPAisHBA.png', 'https://miro.medium.com/max/1714/1*2Jodip6JvUlo-kNtKyB96A.png', 'https://miro.medium.com/max/60/1*2Jodip6JvUlo-kNtKyB96A.png?q=20', 'https://miro.medium.com/max/60/1*6dHvFSMCpWdm9LMvwslKag.png?q=20', 'https://miro.medium.com/max/60/1*lNDzNHVxLSKJEpCsP4KD4w.png?q=20', 'https://miro.medium.com/max/60/1*laH0_xXEkFE0lKJu54gkFQ.png?q=20', 'https://miro.medium.com/max/1714/1*oCsCuHNvNg74Yrsjfn4eoA.png', 'https://miro.medium.com/max/60/1*S0Y1A3KXYO7_eSug_KsK-Q.png?q=20', 'https://miro.medium.com/max/60/1*mOhp-z4KM0Qm6Y0RY7YgMQ.png?q=20', 'https://miro.medium.com/max/710/1*a0LOUiBzyKB6J-wSFwwQ1Q.jpeg', 'https://miro.medium.com/max/1714/1*Z6J0hIE_EuFMteXGtZT4uA.png', 'https://miro.medium.com/fit/c/96/96/1*nenbWEBSd0-qTBarv3PnAA.jpeg', 'https://miro.medium.com/max/60/1*oCsCuHNvNg74Yrsjfn4eoA.png?q=20', 'https://miro.medium.com/max/60/1*dVHmV-0Wery0KJQPAisHBA.png?q=20', 'https://miro.medium.com/max/2382/1*lNDzNHVxLSKJEpCsP4KD4w.png', 'https://miro.medium.com/max/1156/1*laH0_xXEkFE0lKJu54gkFQ.png', 'https://miro.medium.com/max/60/1*Z6J0hIE_EuFMteXGtZT4uA.png?q=20', 'https://miro.medium.com/max/2382/1*eXmq2ZW1O0k5VSm-BUJ2Mg.png', 'https://miro.medium.com/max/1714/1*6dHvFSMCpWdm9LMvwslKag.png', 'https://miro.medium.com/fit/c/160/160/1*nenbWEBSd0-qTBarv3PnAA.jpeg', 'https://miro.medium.com/max/6872/1*S0Y1A3KXYO7_eSug_KsK-Q.png', 'https://miro.medium.com/max/60/1*eXmq2ZW1O0k5VSm-BUJ2Mg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*JAI6AWapx2N6wRi_', 'https://miro.medium.com/max/2312/1*laH0_xXEkFE0lKJu54gkFQ.png', 'https://miro.medium.com/max/1714/1*OouvKl3kimURBWNfVEutpg.png'}",2020-03-05 00:14:00.200440,0.8275716304779053
https://towardsdatascience.com/chatbots-are-cool-a-framework-using-python-part-1-overview-7c69af7a7439?fbclid=IwAR1fQdns3AM3JHTnN5jiUzBnXfQ74gWrog17e7CS20emMkltpWrwc4Lu_wc,Chatbots are cool! A framework using Python,"What is Chatbot?

Chatbot is a tool to retrieve information and generate humanlike conversation. It is mainly a dialog system aimed to solve/serve a specific purpose. Depending upon the design, chatbots generally fall into 4 categories.

Open Domain

Open Domain bots are otherwise known as Generalist bots. Today we use Alexa, Google Home, Siri, Cortana which fall under this category(open domain/generative-based). These bots try to imitate humanlike conversation. Again, it answers questions (like FAQ’s) asked by most humans. However, they cannot answer a specific domain based question. For example: How did my company sales division performed in the last quarter? That is one of the reason, open domain/retrieval-based bots is impossible to build.

Closed Domain

Closed Domain bots are otherwise known as Specialist bots. Depending upon the type, it can be easy (retrieval-based) or hard(generative-based) to develop. The bot discussed in this article is a specialist bot and it falls under the closed domain/retrieval-based category. Other bots in this category include — order a pizza, book flights/restaurants/hotel/appointments. On the other hand, generative bots include customer service chatbots which try to imitate like a agent while answering the questions from customer. These bots are hard to build since the bots try to make the customer believe that they are talking to a actual human.

How Chatbots work?

Chatbots needs to understand the following to respond to an user question.

What is the user talking about? (Intent) Did the user mention anything specific? (Entities) What should the bot ask to get further details from the user? (Dialog/Maintaining Context) How to fulfill the user request? (Response/Fulfillment)

Let us use a Flight Bot example shown below to understand each of these pieces in detail. The user says “I want to book a ticket from New York to Seattle departing on September 15 and returning on September 19 for 2 people”.

Intent

The intent of the user is to book flights.

Entities

Entities are also known as keywords or slots. Here there are multiple entities.

From - New York

To - Seattle

Departure date - September 15

Return date - September 19

#people - 2

Dialog/Maintaining Context

Dialogs are back and forth communication between bot and user. A context let’s the bot know what state the bot is currently in. There are three states — Previous, Present and Future. When the user initiates the dialog, the bot reiterates the user itinerary and then checks with the user “Is this info correct?”. Here the previous state is blank, present state is “user validation” and future state is to “Provide a response based on user validation”. When the user responds “Yes”, then the bot state changes to “User validation”, “Provide a response based on user validation” and “Book a flight” for Previous, present and future state respectively. Without maintaining the context, bots cannot establish the back and forth communication. In the flight bot example, if the context is not maintained the bot would be asking “Is this info correct?” every time until the user gives up.

Response/Fulfillment

Fulfilling the user request is the final step in the bot conversation. Here the bot provides the results in the form of links “See all results”. So when the user clicks the link, they will be able to see the flights and make a reservation.

Generative-based bots use AI and Machine learning to generate user responses. So they need not have to understand the Intents and Entities to respond to a user.

I found this article which covers a lot of topics that we discussed so far and also the bot frameworks that you can use to build chatbots. It is a good read.

Let’s get to work

Okay, now it is time to deploy the Kelly movie bot. It is a simple bot that answers questions about movies. The user can ask about ratings, #people voted for the movie, genre, movie overview, similar movies, imdb and tmdb links, budget, revenue and adult content. The dataset for this exercise is taken from Kaggle — movies_metadata. The entire code for this project can be found in Github.

Kelly Design

One important thing to note with this design is that, the data and processing is all handled in the local system. Even though we use IBM, it is used as an API service and none of the internal data is sent to IBM. This way the entire design can be implemented in your workplace without having to worry about data transfers.

Kelly Design

Step 1 (User asks question):

Users can interact with Kelly via Slack. Once the user post a question, it is passed to the backend system for analysis.

Step 2 and 3 (NLP processing and Return the NLP results):

All the natural language processing happens in step 2. This includes IBM Watson processing, similarity search, recommendation based on collaborative filtering. After the NLP processing is completed, we have three outputs from it

Intents — What the user is trying to ask or query? Entities — What is the exact field or column they are looking for? Dialog/Interaction — Provide the appropriate request/response for the user question.

Step 4 and 5(Query the data):

Currently, the data resides in a excel file. However, you can add multiple databases/excel files if needed, to access different sources. Based on the results from step 3, the appropriate database/excel file is queried and the results are returned.

Step 6 (Post the result to user):

The results obtained from the backend is posted to user via Slack

Step 7 (Log maintenance):

The interactions between the users are logged and stored in a text file. Also, if the bot is not able to identify the user questions it will add those questions to a followup file.

Main.py

Package Imports

As usual, we define the program by importing the packages. Notice something, where it says “slack.slack_commands” and “nlp.nlp_commands”. This would import the python program slack_commands and nlp_commands from slack and nlp folder respectively. In addition, a couple of functions from the config file is imported.

Variables Initialization

User context is maintained in the “context” variable. Once the chat begins, the context variable generated by IBM Watson looks like below,

{'conversation_id': '76e59c57-8257-4390-ae15-ba75e7576476',

'system': {'_node_output_map': {'node_2_1541443578896': [0]},

'branch_exited': True,

'branch_exited_reason': 'completed',

'dialog_request_counter': 1,

'dialog_stack': [{'dialog_node': 'root'}],

'dialog_turn_counter': 1,

'initialized': True}}

It has a “conversation_id” which is used to track the state (previous/present and future state) of conversation flow. During the start of the conversation it is assigned to a empty dictionary value.

In addition to maintaining a conversation, we also should let the bot know when to stop a conversation. This is achieved by the “current_action” variable. An example of the ending a conversation is shown below.","['cool', 'bot', 'questions', 'processing', 'user', 'framework', 'bots', 'python', 'based', 'context', 'conversation', 'state', 'results', 'chatbots', 'using']","On the other hand, generative bots include customer service chatbots which try to imitate like a agent while answering the questions from customer.
Here the previous state is blank, present state is “user validation” and future state is to “Provide a response based on user validation”.
Without maintaining the context, bots cannot establish the back and forth communication.
Response/FulfillmentFulfilling the user request is the final step in the bot conversation.
Also, if the bot is not able to identify the user questions it will add those questions to a followup file.",en,['Sundar Krishnan'],2018-12-11 15:07:46.309000+00:00,"{'Chatbot Design', 'Slack', 'Bots', 'Chatbots', 'NLP'}","{'https://miro.medium.com/max/40/1*wNo9PtuWodOKBIP3ZZZGMQ.png?q=20', 'https://miro.medium.com/max/60/1*3V7TqnKdRRwiO1quD14LAw.png?q=20', 'https://miro.medium.com/max/60/1*IkrtCZtpgc2sQrnNyIzUdQ.png?q=20', 'https://miro.medium.com/max/4920/1*kfXkK3PuhFgDkHE9qiNWoA.png', 'https://miro.medium.com/max/1200/1*KMPLydX_DmGNe24-WlRNDA.jpeg', 'https://miro.medium.com/max/2560/1*U4aFxcK-cNsKCuhLkY0MGA.jpeg', 'https://miro.medium.com/max/3840/1*I-7Rxw2eN9hXeoTPN33I9A.jpeg', 'https://miro.medium.com/max/3840/1*kWXyrtDjUTkqRs6vQLXkPQ.jpeg', 'https://miro.medium.com/max/60/1*Jca7Xael8WQg0ausY88cIA.png?q=20', 'https://miro.medium.com/max/5264/1*80Kr1c7iKJb8sDa-TO7IEQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*VayUtMM7XuGHBqKoBTrQHg.jpeg?q=20', 'https://miro.medium.com/max/60/1*kfXkK3PuhFgDkHE9qiNWoA.png?q=20', 'https://miro.medium.com/max/4992/1*dG0jWb4x3GeXBqAbpsYqvA.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*I-7Rxw2eN9hXeoTPN33I9A.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*2XP7RzuXjH0wrpEoX2EBsA@2x.jpeg', 'https://miro.medium.com/max/2108/1*KqEWzIDJssildxzl9uP5Pg.png', 'https://miro.medium.com/max/60/1*c_9GSfmEQyNSHIP60J9QZA.png?q=20', 'https://miro.medium.com/max/60/1*U4aFxcK-cNsKCuhLkY0MGA.jpeg?q=20', 'https://miro.medium.com/max/60/1*kWXyrtDjUTkqRs6vQLXkPQ.jpeg?q=20', 'https://miro.medium.com/max/5356/1*NfidDAU6SOznG-1qq4jQWQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/5036/1*IkrtCZtpgc2sQrnNyIzUdQ.png', 'https://miro.medium.com/max/60/1*IRqvnjkyBNsoGZc-yqtoPQ.png?q=20', 'https://miro.medium.com/max/1092/1*gqSiURnxtzSRbliKNR2MYQ.png', 'https://miro.medium.com/max/60/1*NfidDAU6SOznG-1qq4jQWQ.png?q=20', 'https://miro.medium.com/max/60/1*yTX2DAaPh9Rn39aI0OIK-w.png?q=20', 'https://miro.medium.com/max/5604/1*Jca7Xael8WQg0ausY88cIA.png', 'https://miro.medium.com/max/1832/1*Kxt9_h3RQLsVW8uUtShZ0A.png', 'https://miro.medium.com/max/3084/1*3V7TqnKdRRwiO1quD14LAw.png', 'https://miro.medium.com/max/1652/1*wNo9PtuWodOKBIP3ZZZGMQ.png', 'https://miro.medium.com/max/1160/1*Mp_bdrIIXoWrXJeSBEtSRg.png', 'https://miro.medium.com/max/60/1*cH536JJL3BAWwGmvWm7CXQ.jpeg?q=20', 'https://miro.medium.com/max/38/1*-8A9UhJStZd23FPjmpfmuQ.png?q=20', 'https://miro.medium.com/max/1596/1*yTX2DAaPh9Rn39aI0OIK-w.png', 'https://miro.medium.com/max/1604/1*-8A9UhJStZd23FPjmpfmuQ.png', 'https://miro.medium.com/max/60/1*e8v1xC0NTgoduh_ei9F7Pw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*dG0jWb4x3GeXBqAbpsYqvA.png?q=20', 'https://miro.medium.com/max/2552/1*c_9GSfmEQyNSHIP60J9QZA.png', 'https://miro.medium.com/max/60/1*KMPLydX_DmGNe24-WlRNDA.jpeg?q=20', 'https://miro.medium.com/max/60/1*iX3cSskvnR8SVAITapBUkA.png?q=20', 'https://miro.medium.com/max/56/1*KqEWzIDJssildxzl9uP5Pg.png?q=20', 'https://miro.medium.com/max/5276/1*wUAcGFy4yXkib8_sjQSZhA.png', 'https://miro.medium.com/max/58/1*Kxt9_h3RQLsVW8uUtShZ0A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*2XP7RzuXjH0wrpEoX2EBsA@2x.jpeg', 'https://miro.medium.com/max/60/1*trFuErE5W5Xq3WZrz8tFIQ.png?q=20', 'https://miro.medium.com/max/3488/1*IRqvnjkyBNsoGZc-yqtoPQ.png', 'https://miro.medium.com/max/4536/1*UV1ulr0tQHKzs3au_XUPRw.png', 'https://miro.medium.com/max/3840/1*cH536JJL3BAWwGmvWm7CXQ.jpeg', 'https://miro.medium.com/max/60/1*UV1ulr0tQHKzs3au_XUPRw.png?q=20', 'https://miro.medium.com/max/2800/1*KMPLydX_DmGNe24-WlRNDA.jpeg', 'https://miro.medium.com/max/60/1*DUP4AvHDev4oTpj72L5gZg.png?q=20', 'https://miro.medium.com/max/60/1*Mp_bdrIIXoWrXJeSBEtSRg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2484/1*DUP4AvHDev4oTpj72L5gZg.png', 'https://miro.medium.com/max/60/1*i-8fomq2JH0h2AfF3QS8VA.jpeg?q=20', 'https://miro.medium.com/max/60/1*80Kr1c7iKJb8sDa-TO7IEQ.png?q=20', 'https://miro.medium.com/max/3840/1*VayUtMM7XuGHBqKoBTrQHg.jpeg', 'https://miro.medium.com/max/2992/1*trFuErE5W5Xq3WZrz8tFIQ.png', 'https://miro.medium.com/max/2560/1*iX3cSskvnR8SVAITapBUkA.png', 'https://miro.medium.com/max/60/1*wUAcGFy4yXkib8_sjQSZhA.png?q=20', 'https://miro.medium.com/max/60/1*gqSiURnxtzSRbliKNR2MYQ.png?q=20', 'https://miro.medium.com/max/4800/1*i-8fomq2JH0h2AfF3QS8VA.jpeg', 'https://miro.medium.com/max/2048/1*e8v1xC0NTgoduh_ei9F7Pw.png'}",2020-03-05 00:14:06.920575,6.719621658325195
https://towardsdatascience.com/tagged/chatbots,Chatbots – Towards Data Science,,"['data', 'chatbots', 'science']",,,[],,{'Chatbots'},"{'https://cdn-images-1.medium.com/fit/t/1600/480/1*bxmYrHGV9wJPKLt6-MQO2w.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/1*LIGvbNrFShuYptAEDBi13w.jpeg', 'https://cdn-images-1.medium.com/fit/t/1600/480/1*lh6l7tvV7x89Q45P5qoRFw.png', 'https://cdn-images-1.medium.com/fit/t/1600/480/1*NKVm-AoDLc0KdN-Ls7Ts_g.png', 'https://cdn-images-1.medium.com/fit/t/1600/480/0*8jr0YvwZAfrBLF7F.gif', 'https://cdn-images-1.medium.com/fit/c/72/72/2*6NTBd7mtsPOSq9oO7_ddsw.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/2*4zFKgS-W-rA89ZHi1Ku80A.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/2*9ETRn-s8wU1dK_syonS-8Q.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/1*JPo0k0PRZYkVBkhs15skEg@2x.jpeg', 'https://cdn-images-1.medium.com/fit/t/1600/480/0*C8RKMhKEMUxrl-oz', 'https://cdn-images-1.medium.com/fit/c/72/72/2*xqnGM8rzg9MIq6nPFjJv1Q.jpeg', 'https://cdn-images-1.medium.com/letterbox/198/72/50/50/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png?source=logoAvatar-lo_js3uDwrszocc---7f60cf5620c9', 'https://cdn-images-1.medium.com/fit/t/1600/480/1*gzB4vITuRz6fKL9j9FyTkA.png', 'https://cdn-images-1.medium.com/fit/c/72/72/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://cdn-images-1.medium.com/fit/t/1600/480/1*vYnrJZvpMq6DO23esMFx-g.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/2*oBM4rR-W3VS_mtPKrbcIpA.jpeg', 'https://cdn-images-1.medium.com/fit/t/1600/480/1*k9jUwLFWMPfX904Tx0pB3A.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/0*_CwYR2OmNap47tQO.jpg', 'https://cdn-images-1.medium.com/fit/t/1600/480/0*E5fkENpHnaOtEbY4', 'https://cdn-images-1.medium.com/fit/c/72/72/1*lJ8e2wSZOtlZwP_OyPc7-g.jpeg', 'https://cdn-images-1.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png', 'https://cdn-images-1.medium.com/fit/c/72/72/2*-luBJGk7bijQtII3tXYWHA.png'}",2020-03-05 00:14:08.962591,2.0410141944885254
https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8,Music Genre Classification with Python,"Objective

Companies nowadays use music classification, either to be able to place recommendations to their customers (such as Spotify, Soundcloud) or simply as a product (for example Shazam). Determining music genres is the first step in that direction. Machine Learning techniques have proved to be quite successful in extracting trends and patterns from the large pool of data. The same principles are applied in Music Analysis also.

In this article, we shall study how to analyse an audio/music signal in Python. We shall then utilise the skills learnt to classify music clips into different genres.

Audio Processing with Python

Sound is represented in the form of an audio signal having parameters such as frequency, bandwidth, decibel etc. A typical audio signal can be expressed as a function of Amplitude and Time.

These sounds are available in many formats which makes it possible for the computer to read and analyse them. Some examples are:

mp3 format

WMA (Windows Media Audio) format

wav (Waveform Audio File) format

Audio Libraries

Python has some great libraries for audio processing like Librosa and PyAudio.There are also built-in modules for some basic audio functionalities.

We will mainly use two libraries for audio acquisition and playback:

1. Librosa

It is a Python module to analyze audio signals in general but geared more towards music. It includes the nuts and bolts to build a MIR(Music information retrieval) system. It has been very well documented along with a lot of examples and tutorials.

For a more advanced introduction which describes the package design principles, please refer to the librosa paper at SciPy 2015.

Installation

pip install librosa

or

conda install -c conda-forge librosa

To fuel more audio-decoding power, you can install ffmpeg which ships with many audio decoders.

2. IPython.display.Audio

IPython.display.Audio lets you play audio directly in a jupyter notebook.

Loading an audio file

import librosa

x , sr = librosa.load(audio_path) audio_path = '../ T08-violin .wav'x , sr = librosa.load(audio_path) print(type(x), type(sr))

<class 'numpy.ndarray'> <class 'int'> print(x.shape, sr)

(396688,) 22050

This returns an audio time series as a numpy array with a default sampling rate(sr) of 22KHZ mono. We can change this behaviour by saying:

librosa.load(audio_path, sr=44100)

to resample at 44.1KHz, or

librosa.load(audio_path, sr=None)

to disable resampling.

The sample rate is the number of samples of audio carried per second, measured in Hz or kHz.

Playing Audio

Using, IPython.display.Audio to play the audio

import IPython.display as ipd

ipd.Audio(audio_path)

This returns an audio widget in the jupyter notebook as follows:

screenshot of the Ipython audio widget

This widget won’t work here, but it will work in your notebooks. I have uploaded the same to SoundCloud so that we can listen to it.

You can even use an mp3 or a WMA format for the audio example.

Visualizing Audio

Waveform

We can plot the audio array using librosa.display.waveplot :

%matplotlib inline

import matplotlib.pyplot as plt

import librosa.display plt.figure(figsize=(14, 5))

librosa.display.waveplot(x, sr=sr)

Here, we have the plot of the amplitude envelope of a waveform.

Spectrogram

A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. Spectrograms are sometimes called sonographs, voiceprints, or voicegrams. When the data is represented in a 3D plot, they may be called waterfalls. In 2-dimensional arrays, the first axis is frequency while the second axis is time.

We can display a spectrogram using. librosa.display.specshow.

X = librosa.stft(x)

Xdb = librosa.amplitude_to_db(abs(X))

plt.figure(figsize=(14, 5))

librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')

plt.colorbar()

The vertical axis shows frequencies (from 0 to 10kHz), and the horizontal axis shows the time of the clip. Since we see that all action is taking place at the bottom of the spectrum, we can convert the frequency axis to a logarithmic one.

librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')

plt.colorbar()

Writing Audio

librosa.output.write_wav saves a NumPy array to a WAV file.

librosa.output.write_wav('example.wav', x, sr)

Creating an audio signal

Let us now create an audio signal at 220Hz. An audio signal is a numpy array, so we shall create one and pass it into the audio function.

import numpy as np

sr = 22050 # sample rate

T = 5.0 # seconds

t = np.linspace(0, T, int(T*sr), endpoint=False) # time variable

x = 0.5*np.sin(2*np.pi*220*t)# pure sine wave at 220 Hz Playing the audio

ipd.Audio(x, rate=sr) # load a NumPy array Saving the audio

librosa.output.write_wav('tone_220.wav', x, sr)

So, here it is- first sound signal created by you.🙌

Feature extraction

Every audio signal consists of many features. However, we must extract the characteristics that are relevant to the problem we are trying to solve. The process of extracting features to use them for analysis is called feature extraction. Let us study about few of the features in detail.

Zero Crossing Rate

The zero crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back. This feature has been used heavily in both speech recognition and music information retrieval. It usually has higher values for highly percussive sounds like those in metal and rock.

Let us calculate the zero crossing rate for our example audio clip.

# Load the signal

x, sr = librosa.load('../T08-violin.wav') #Plot the signal:

plt.figure(figsize=(14, 5))

librosa.display.waveplot(x, sr=sr)

# Zooming in

n0 = 9000

n1 = 9100

plt.figure(figsize=(14, 5))

plt.plot(x[n0:n1])

plt.grid()

There appear to be 6 zero crossings. Let’s verify with librosa.

zero_crossings = librosa.zero_crossings(x[n0:n1], pad=False)

print(sum(zero_crossings)) 6

It indicates where the ”centre of mass” for a sound is located and is calculated as the weighted mean of the frequencies present in the sound. Consider two songs, one from a blues genre and the other belonging to metal. Now as compared to the blues genre song which is the same throughout its length, the metal song has more frequencies towards the end. So spectral centroid for blues song will lie somewhere near the middle of its spectrum while that for a metal song would be towards its end.

librosa.feature.spectral_centroid computes the spectral centroid for each frame in a signal:

spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]

spectral_centroids.shape

(775,) # Computing the time variable for visualization

frames = range(len(spectral_centroids))

t = librosa.frames_to_time(frames) # Normalising the spectral centroid for visualisation

def normalize(x, axis=0):

return sklearn.preprocessing.minmax_scale(x, axis=axis) #Plotting the Spectral Centroid along the waveform

librosa.display.waveplot(x, sr=sr, alpha=0.4)

plt.plot(t, normalize(spectral_centroids), color='r')

There is a rise in the spectral centroid towards the end.

Spectral Rolloff

It is a measure of the shape of the signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies.

librosa.feature.spectral_rolloff computes the rolloff frequency for each frame in a signal:

spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]

librosa.display.waveplot(x, sr=sr, alpha=0.4)

plt.plot(t, normalize(spectral_rolloff), color='r')

The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.

Let’ work with a simple loop wave this time.

x, fs = librosa.load('../simple_loop.wav')

librosa.display.waveplot(x, sr=sr)

librosa.feature.mfcc computes MFCCs across an audio signal:

mfccs = librosa.feature.mfcc(x, sr=fs)

print mfccs.shape

(20, 97) #Displaying the MFCCs:

librosa.display.specshow(mfccs, sr=sr, x_axis='time')

Here mfcc computed 20 MFCC s over 97 frames.

We can also perform feature scaling such that each coefficient dimension has zero mean and unit variance:

import sklearn

mfccs = sklearn.preprocessing.scale(mfccs, axis=1)

print(mfccs.mean(axis=1))

print(mfccs.var(axis=1)) librosa.display.specshow(mfccs, sr=sr, x_axis='time')

Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.

librosa.feature.chroma_stft is used for computation","['signal', 'numpy', 'centroid', 'frequency', 'srsr', 'rate', 'python', 'features', 'classification', 'audio', 'music', 'spectral', 'genre']","Audio Processing with PythonSound is represented in the form of an audio signal having parameters such as frequency, bandwidth, decibel etc.
A typical audio signal can be expressed as a function of Amplitude and Time.
LibrosaIt is a Python module to analyze audio signals in general but geared more towards music.
librosa.output.write_wav('example.wav', x, sr)Creating an audio signalLet us now create an audio signal at 220Hz.
An audio signal is a numpy array, so we shall create one and pass it into the audio function.",en,['Parul Pandey'],2018-12-19 06:23:43.268000+00:00,"{'Data Science', 'Artificial Intelligence', 'Python', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*JttzQLEFRzAb3clp7dkfKg.png?q=20', 'https://miro.medium.com/max/946/1*JttzQLEFRzAb3clp7dkfKg.png', 'https://miro.medium.com/max/9184/0*EfWqZYlS-GfMmqLj', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/1402/1*7jZRCE58z2QTulOIIHw_LA.png', 'https://miro.medium.com/max/1978/1*akRbhl8739UEDuKHkOUR1Q.png', 'https://miro.medium.com/max/60/1*kLln6ejJJsTDn9B0zLALUA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*6ar_u2SROhmOnvgimnI48Q.png?q=20', 'https://miro.medium.com/max/1200/0*EfWqZYlS-GfMmqLj', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/802/1*6ar_u2SROhmOnvgimnI48Q.png', 'https://miro.medium.com/max/1352/1*31ZW3GzKofk4jsSyRMwubg.png', 'https://miro.medium.com/max/944/1*pOp0miIvDc1wxvS1iwrwCQ.png', 'https://miro.medium.com/max/892/1*1Vu8GXxY_CmMBLmcypEjvw.png', 'https://miro.medium.com/max/60/1*Re6tj77KWLypP3gY7MaXjA.png?q=20', 'https://miro.medium.com/max/60/1*a6OcRvqCNDEix02CI5dK6w.png?q=20', 'https://miro.medium.com/max/60/1*31ZW3GzKofk4jsSyRMwubg.png?q=20', 'https://miro.medium.com/max/1414/1*tspcooMfQnmYPwxMKWhTmg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1396/1*m3v38dqyykefBp0OiNRBPg.png', 'https://miro.medium.com/max/770/1*a6OcRvqCNDEix02CI5dK6w.png', 'https://miro.medium.com/max/882/1*kLln6ejJJsTDn9B0zLALUA.png', 'https://miro.medium.com/max/60/1*pOp0miIvDc1wxvS1iwrwCQ.png?q=20', 'https://miro.medium.com/max/60/1*m3v38dqyykefBp0OiNRBPg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/60/1*7jZRCE58z2QTulOIIHw_LA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1356/1*Re6tj77KWLypP3gY7MaXjA.png', 'https://miro.medium.com/max/60/1*ktoXTt51zFSTMgiuv5ZCgg.png?q=20', 'https://miro.medium.com/max/684/1*ktoXTt51zFSTMgiuv5ZCgg.png', 'https://miro.medium.com/max/60/1*akRbhl8739UEDuKHkOUR1Q.png?q=20', 'https://miro.medium.com/max/60/1*1Vu8GXxY_CmMBLmcypEjvw.png?q=20', 'https://miro.medium.com/max/60/1*tspcooMfQnmYPwxMKWhTmg.png?q=20', 'https://miro.medium.com/max/60/0*EfWqZYlS-GfMmqLj?q=20'}",2020-03-05 00:14:15.476926,6.5133376121521
https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33,Deep Neural Networks for Regression Problems,"First : Processing the dataset

We will not go deep in processing the dataset, all we want to do is getting the dataset ready to be fed into our models .

We will get rid of any features with missing values, then we will encode the categorical features, that’s it.

Load the dataset :

Load train and test data into pandas DataFrames

Combine train and test data to process them together

combined.describe()

let’s define a function to get the columns that don’t have any missing values

Get the columns that do not have any missing values .

Let’s see how many columns we got

[out]:

Number of numerical columns with no nan values : 25

Number of nun-numerical columns with no nan values : 20

Histogram of the features

The correlation between the features

From the correlation heat map above, we see that about 15 features are highly correlated with the target.

One Hot Encode The Categorical Features :

We will encode the categorical features using one hot encoding.

[out]:

There were 45 columns before encoding categorical features

There are 149 columns after encoding categorical features

Now, split back combined dataFrame to training data and test data","['encode', 'regression', 'train', 'neural', 'columns', 'dataset', 'deep', 'networks', 'data', 'categorical', 'values', 'missing', 'features', 'test', 'problems']","First : Processing the datasetWe will not go deep in processing the dataset, all we want to do is getting the dataset ready to be fed into our models .
We will get rid of any features with missing values, then we will encode the categorical features, that’s it.
Load the dataset :Load train and test data into pandas DataFramesCombine train and test data to process them togethercombined.describe()let’s define a function to get the columns that don’t have any missing valuesGet the columns that do not have any missing values .
One Hot Encode The Categorical Features :We will encode the categorical features using one hot encoding.
[out]:There were 45 columns before encoding categorical featuresThere are 149 columns after encoding categorical featuresNow, split back combined dataFrame to training data and test data",en,"[""Mohammed Ma'Amari""]",2018-10-25 20:05:10.825000+00:00,"{'Neural Networks', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Kaggle'}","{'https://miro.medium.com/max/60/1*PO0jxykz1hv-aSN5kkjItg.png?q=20', 'https://miro.medium.com/max/1080/1*vUKwarc7rCouMzSt0Ksakw.jpeg', 'https://miro.medium.com/max/60/1*mXbUrGB9yB9RrBscwugP9g.png?q=20', 'https://miro.medium.com/max/60/1*cBbEbSv-BWn1BCJg0c7fPw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1742/1*mhgimekf-wVw4gvFQr3Ecg.png', 'https://miro.medium.com/max/60/1*vUKwarc7rCouMzSt0Ksakw.jpeg?q=20', 'https://miro.medium.com/max/1898/1*PO0jxykz1hv-aSN5kkjItg.png', 'https://miro.medium.com/max/1898/1*wcJqKzBsbLARqMjwvv0Xjw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2160/1*vUKwarc7rCouMzSt0Ksakw.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/1*QDfpZrkAUQkg5w_jrXUW9Q.jpeg', 'https://miro.medium.com/fit/c/160/160/1*QDfpZrkAUQkg5w_jrXUW9Q.jpeg', 'https://miro.medium.com/max/1708/1*mx0xGJPdknNRZmiZOv_VGg.png', 'https://miro.medium.com/max/1904/1*mXbUrGB9yB9RrBscwugP9g.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1438/1*cBbEbSv-BWn1BCJg0c7fPw.png', 'https://miro.medium.com/max/60/1*wcJqKzBsbLARqMjwvv0Xjw.png?q=20', 'https://miro.medium.com/max/60/1*mhgimekf-wVw4gvFQr3Ecg.png?q=20', 'https://miro.medium.com/max/60/1*mx0xGJPdknNRZmiZOv_VGg.png?q=20'}",2020-03-05 00:14:18.578543,3.1006157398223877
https://medium.com/lean-in-women-in-tech-india/google-colab-the-beginners-guide-5ad3b417dfa,Google Colab — The Beginner’s Guide,"Whether you are a student interested in exploring Machine Learning but struggling to conduct simulations on enormous datasets, or an expert playing with ML desperate for extra computational power, Google Colab is the perfect solution for you. Google Colab or “the Colaboratory” is a free cloud service hosted by Google to encourage Machine Learning and Artificial Intelligence research, where often the barrier to learning and success is the requirement of tremendous computational power.

Benefits of Colab

Besides being easy to use (which I’ll describe later), the Colab is fairly flexible in its configuration and does much of the heavy lifting for you.

Python 2.7 and Python 3.6 support

Free GPU acceleration

Pre-installed libraries: All major Python libraries like TensorFlow, Scikit-learn, Matplotlib among many others are pre-installed and ready to be imported.

Built on top of Jupyter Notebook

Collaboration feature (works with a team just like Google Docs): Google Colab allows developers to use and share Jupyter notebook among each other without having to download, install, or run anything other than a browser.

Supports bash commands

Google Colab notebooks are stored on the drive

If you prefer to read more before getting started, I recommend the Google Colab FAQ, Google Colab Documentation and Code Snippets, and advice from the helpful community of users on Stack Overflow.

Let’s Begin!

Create a Colab Notebook

Open Google Colab. Click on ‘New Notebook’ and select Python 2 notebook or Python 3 notebook.

OR

Open Google Drive. Create a new folder for the project. Click on ‘New’ > ‘More’ > ‘Colaboratory’.

Setting GPU Accelerator

The default hardware of Google Colab is CPU or it can be GPU.

Click on ‘Edit’ > ‘Notebook Settings’ > ‘Hardware Accelerator’ > ‘GPU’.

OR

Click on ‘Runtime’ > ‘Hardware Accelerator’ > ‘GPU’.

Running a Cell

Make sure the runtime is connected. The notebook shows a green check and ‘Connected’ on the top right corner. There are various runtime options in ‘Runtime’.

OR

To run the current cell, press SHIFT + ENTER.

Bash Commands

Bash commands can be run by prefixing the command with ‘!’.

Cloning a git repository

!git clone [git clone url]

Directory commands !ls, !mkdir.

!ls

This command outputs the folders /content and /drive (if it has been mounted). Run the following snippet to change the current folder.

import sys

sys.path.append(‘[Folder name]’)

Download from the web

!wget [url] -p drive/[Folder Name]

Installing Libraries

Although most of the commonly used Python libraries are pre-installed, new libraries can be installed using the below packages:

!pip install [package name]

OR

!apt-get install [package name]

Upload local files

from google.colab import files

uploaded = files.upload()

Select the files for upload

For multiple files, the individual key names can be obtained by looping through the uploaded files.

for file in uploaded.keys():

print('Uploaded file ""{name}"" with length {length} bytes'.format(name=file, length=len(uploaded[file])))

Mounting Google Drive

Run the following code.

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools

!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null

!apt-get update -qq 2>&1 > /dev/null

!apt-get -y install -qq google-drive-ocamlfuse fuse

from google.colab import auth

auth.authenticate_user()

from oauth2client.client import GoogleCredentials

creds = GoogleCredentials.get_application_default()

import getpass

!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL

vcode = getpass.getpass()

!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

Click on the link and enter the api key.

!mkdir -p drive

!google-drive-ocamlfuse drive

Your drive is now mounted. You can use any files and folders in your drive by using the path as follows

!ls /content/drive/[folder name]

/content is the root folder of Google Colab and has to be appended to all paths used in the notebook.

Importing from existing .py scripts

Upload any existing .py scripts to a folder on drive. Consider a script ‘script.py’ uploaded to folder ‘Project’.

To import any module

import sys

sys.path.append(‘Project’)

import script

Run an existing .py script

To run a script

!python3 /content/drive/Project/script.py

Check CPU and RAM specifications

!cat /proc/cpuinfo

!cat /proc/meminfo

Check GPU specifications

from tensorflow.python.client import device_lib

device_lib.list_local_devices()

Colab provides the Tesla K80 GPU.

This should get you started with Google Colab. Feel free to ask questions!","['import', 'beginners', 'install', 'folder', 'python', 'drive', 'colab', 'run', 'google', 'libraries', 'notebook', 'guide']","Google Colab or “the Colaboratory” is a free cloud service hosted by Google to encourage Machine Learning and Artificial Intelligence research, where often the barrier to learning and success is the requirement of tremendous computational power.
Supports bash commandsGoogle Colab notebooks are stored on the driveIf you prefer to read more before getting started, I recommend the Google Colab FAQ, Google Colab Documentation and Code Snippets, and advice from the helpful community of users on Stack Overflow.
Create a Colab NotebookOpen Google Colab.
Setting GPU AcceleratorThe default hardware of Google Colab is CPU or it can be GPU.
This should get you started with Google Colab.",en,['Vishakha Lall'],2018-06-29 07:31:54.632000+00:00,"{'Artificial Intelligence', 'Google', 'Machine Learning', 'Google Colab', 'Gpu'}","{'https://miro.medium.com/fit/c/80/80/0*hNYLcyd0kEShHoX6.', 'https://miro.medium.com/max/1800/0*yjymDLqN-xvjIw3K.jpg', 'https://miro.medium.com/fit/c/160/160/1*7G6m0sGltZ4aCoHhAiXSPw.jpeg', 'https://miro.medium.com/max/164/1*vQyswys35o_Z8lk3CH3kfg.png', 'https://miro.medium.com/fit/c/160/160/1*GDstrpCXlHLE_t4HA3BzYQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*eexmOTluNLJKrHc93iBWKg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*87qAlkZLjEOK4HFhMpIwgA.jpeg', 'https://miro.medium.com/max/60/0*yjymDLqN-xvjIw3K.jpg?q=20', 'https://miro.medium.com/max/900/0*yjymDLqN-xvjIw3K.jpg', 'https://miro.medium.com/fit/c/96/96/1*GDstrpCXlHLE_t4HA3BzYQ.jpeg'}",2020-03-05 00:14:19.416379,0.836798906326294
https://medium.com/the-python-corner/using-virtual-environments-with-python-7166d3bfa218,Using virtual environments with Python,"Guys, the Python corner has a new home and it’s a great place, so the article you are looking for is now available for free at the following url:

Happy coding! :)

Davide Mastromatteo","['virtual', 'mastromatteo', 'guys', 'place', 'davide', 'python', 'great', 'free', 'following', 'looking', 'urlhappy', 'using', 'environments']","Guys, the Python corner has a new home and it’s a great place, so the article you are looking for is now available for free at the following url:Happy coding!
:)Davide Mastromatteo",en,['Davide Mastromatteo'],2020-03-03 13:10:07.798000+00:00,"{'Virtualenv', 'Virtualenvwrapper', 'Featured', 'Python', 'Virtual Environment'}","{'https://miro.medium.com/fit/c/96/96/1*IfTSjB7FxrJlvUdSgVGX4Q.jpeg', 'https://miro.medium.com/fit/c/160/160/1*IfTSjB7FxrJlvUdSgVGX4Q.jpeg', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/max/582/1*9QYHKltS-5Tzr6zOlNn8HQ.png', 'https://miro.medium.com/fit/c/80/80/2*-BAXbfc1gdb2tUfBwX2bVg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*AFSreQr7UQr-D8f-yCvA_g.png', 'https://miro.medium.com/fit/c/80/80/0*cu4Pztlg9excQP7i.jpg', 'https://miro.medium.com/fit/c/80/80/0*000ZlntQkYrNiPB-.png'}",2020-03-05 00:14:20.947609,1.5312304496765137
https://medium.com/@charlie.b.ohara/building-a-flask-rest-api-with-docker-94ca4219f460,Docker for Python Developers,"Last fall I taught myself how to use Docker and wrote a piece titled Docker for Rails Developers. Since then, I have fallen in love with Python and landed a job as a data engineer. At work we are building a product that includes a Flask REST API, so I wanted to get more familiar with Flask.

Big Picture

Understanding the terminology Docker uses may help you understand why the technology exists. “Docker” is another term for a longshoreman, who is the person responsible for loading and unloading cargo ships. Back in the day, this process was incredibly inefficient. It took a huge amount of manpower to carry all the goods on to the ship and then carry all the cargo off the ship.

The use of shipping containers revolutionized global trade. Transporting goods is now incredibly efficient because shipping containers get loaded and unloaded onto ships with a crane. In a similar spirit, Docker containers provide an efficient way to package and transport our code.

“Using Docker, everything required to make a piece of software run is packaged into isolated containers. Unlike VMs, containers do not bundle a full operating system — only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it’s deployed.” https://www.docker.com/what-docker

Why not just use virtualenv?

If you are Python developer, you are probably familiar with virtualenv. It is a tool used for separating the package dependencies for different applications you are working on. When I run the command pip3 install Flask, I am asking the pip3 package manager to add the Flask binary file into the /usr/local/bin/ directory on my computer. The Flask binary file contains machine code the computer needs to run Flask.

Say you are working on a Flask application, and you know it works with a certain version of Flask. Over time, the maintainers working on Flask may decide to make some changes to the framework that negatively affect your project. If you are not using a virtualenv, and you globally install the latest Flask release by running pip3 install -U Flask, your application will no longer work.

To avoid this problem, virtualenv exists. For example, when you create a virtualenv named flask0.12, it will create a directory called flask0.12 with the directory flask0.12/bin/ inside of it. When you activate the virtualenv, you will be accessing the binary files in the flask0.12/bin/ directory, not the global /usr/local/bin/ directory.

Using virtualenv is great for working on my computer, but what if I want to deploy my Flask application so others can use it? How do I share all my project code as well as all the package dependencies to a web server? This is where Docker can step in and help out.

Why not just use a virtual machine?

If you are familiar with virtual machines, you may be asking yourself why should I use Docker? I already have experiencing spinning up a virtual machine on my computer using Vagrant, and separating out my projects into different virtual machines. If this is what you are thinking — consider this! Virtual environments take up a lot of space (CPU, RAM, storage) on your computer because each application requires an entire guest operating system. Docker does not.

I’d highly suggest watching this short video before reading any further

A virtual machine (VM) is essentially a computer within a computer. The reason why developers use virtual machines is to make sure that the code environment on their local machine will match the environment that other developers on their team are working in and match the environment of the production server.

How do VMs work? A hypervisor is a piece of software, firmware, or hardware that VMs run on top of. The hypervisors themselves run on physical computers, referred to as the host machine. The host machine provides the VMs with resources, including RAM and CPU.

The VM that is running on the host machine’s hypervisor is often called a guest machine. This guest machine contains both the application and whatever it needs to run that application (e.g. system binaries and libraries). It also carries an entire virtualized hardware stack of its own, including virtualized network adapters, storage, and CPU — which means it has its own full-fledged guest operating system.

Since the VM has a virtual operating system of its own, the hypervisor plays an essential role in providing the VMs with a platform to manage and execute this guest operating system. It allows for host computers to share their resources amongst the virtual machines that are running as guests on top of them.

Well then how does Docker work?

The one big difference between containers and VMs is that containers share the host system’s kernel with other containers. The kernel is a computer program that is the core of a computer’s operating system, with complete control over everything in the system. Therefore, Docker does not require each project have its own its own full-fledged guest operating system.

How does this work? The description above states that “multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space.”

What is user space? System memory in Linux can be divided into two distinct regions: kernel space and user space. Kernel space is where the kernel (the core of the operating system) executes and provides its services. User space is that set of memory locations in which user processes (executing instance of a program) run.

All user programs (containerized or not) function by manipulating data, but where does this data live? This data is most commonly stored in memory and on disk. The kernel provides an API to these applications via system calls. Example system calls include allocating memory (variables) or opening a file.

When a container is started, a program is loaded into memory from the container image. Once the program in the container is running, it still needs to make system calls into kernel space. Essentially containers communicate directly with the kernel of the host operating system, which is what makes Docker a more resource-efficient option compared to VMs.

Docker Engine

When we refer to Docker, we are really talking about the Docker Engine, which is the program that creates and runs the Docker container from the Docker image file. It runs natively on Linux systems and is made up of:

The Docker Client is the command line interface (what we communicate with via the terminal).

is the command line interface (what we communicate with via the terminal). A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.

which specifies interfaces that programs can use to talk to the daemon and instruct it what to do. The Docker Daemon runs in the background on the host computer, and it is what actually executes commands sent to the Docker Client — like building, running, and distributing your containers.

Download Docker

Curious to try it out for yourself? Well the first thing you need to do is download Docker for your operating system. The Docker documentation has improved quite a bit over the last 6 months, so it should be pretty easy.

Now that you have Docker installed on your computer, let’s get up and running quickly by building a very simple Flask application. The code for this simple Flask application is available on github to make it easier to follow along.

Setup Flask App

These are the steps I took in the terminal snapshot above:

Make a new directory for your project

Set up a virtual environment in the directory with the command virtualenv venv

Activate the virtual environment with source venv/bin/activate

Make a file requirements.txt that has all your dependencies in it. For this simple Flask app, all you need is Flask==0.11.1

that has all your dependencies in it. For this simple Flask app, all you need is Install your dependencies with pip install -r requirements.txt

Make a directory within your project director called app/

Add the code below into app/main.py

Now that we have our basic Flask app, we can create a Docker image. An image is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software. It is essentially a snapshot of the code and the environment we need to run that code. We will make a Docker image from our Dockerfile, so create this file in your project folder and copy and paste the code below.

FROM tells Docker what image to pull from the Docker hub repository. We are going to take advantage of this convenient image which has Python, Flask, Nginx, and uWSGI. Nginx is a web server, so it takes care of the HTTP connections and also can serve static files directly and efficiently. uWSGI is the application server that runs your Python code and communicates talks with Nginx.

COPY will copy the app/ folder we created on our computer into the Docker container. A container is an actual running instance of a Docker image.

Now that we have a Dockerfile, we can build a Docker image. To do so, we need to be in the root directory of our project in the terminal. Our project filesystem should look like this:

docker_for_flask_developers/



Dockerfile



requirements.txt



app/

main.py



venv/

So while I am in my docker_for_flask_developers/ folder, I am going to run the command co to communicate with Docker that it is time to create snapshot of my code and its environment:

This tells Docker to build an image from the Dockerfile. Docker will pull down the base image tiangolo/uwsgi-nginx-flask:flask from Docker Hub, then copy our app code into the container. Important note: Every time you change your app code, you need to copy the updated app/ folder into the container, so you will need to build the image again.

Now that we have created a Docker image, we can finally spin up a Docker container. Enter docker run -p 80:80 -t simple-flask in your terminal:

To give our container access to traffic over port 80, we use the -p flag and specify the port on the host that maps to the port inside the container. In our case we want 80 for each, so we include -p 80:80 in our command. If you open up your browser and enter 0.0.0.0:80 (or you can just use 0.0.0.0 since port 80 is the default port for HTTP traffic), you should see your Flask app running in the browser on your computer.

Pretty neat right?! Congrats! You got a Flask application up and running using Docker!","['virtual', 'computer', 'developers', 'docker', 'python', 'image', 'run', 'flask', 'container', 'system', 'containers', 'code']","Last fall I taught myself how to use Docker and wrote a piece titled Docker for Rails Developers.
In a similar spirit, Docker containers provide an efficient way to package and transport our code.
Docker EngineWhen we refer to Docker, we are really talking about the Docker Engine, which is the program that creates and runs the Docker container from the Docker image file.
Docker will pull down the base image tiangolo/uwsgi-nginx-flask:flask from Docker Hub, then copy our app code into the container.
Now that we have created a Docker image, we can finally spin up a Docker container.",en,"[""Charlie Brooke O'Hara""]",2018-12-17 01:58:36.298000+00:00,"{'Flask', 'Docker', 'Python'}","{'https://miro.medium.com/fit/c/80/80/1*GwGIgHi9ngu0bsSrr_v79g.png', 'https://miro.medium.com/max/5104/1*JHLMWhU4tT_CYAqCeotgXA.png', 'https://miro.medium.com/max/60/1*TUmbTSSsT-rdAi7KlHF5xA.jpeg?q=20', 'https://miro.medium.com/max/60/1*JHLMWhU4tT_CYAqCeotgXA.png?q=20', 'https://miro.medium.com/max/60/1*rbUndRLRFjfH8vkoPVQUfA.jpeg?q=20', 'https://miro.medium.com/max/3012/0*Ez913dMsRbQk1Ua6.', 'https://miro.medium.com/max/60/1*_f0mk7pRNpr7V_DmvvCQww.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*p41Ne9H5d4CuVKi6vGpsUg@2x.jpeg', 'https://miro.medium.com/max/60/1*z8zGgQa938A70StKgNNUZw.png?q=20', 'https://miro.medium.com/max/60/1*qM5rlgPBvqGM160FbjyQfA.png?q=20', 'https://miro.medium.com/max/60/1*PEsstUWsMpI2ZeNWUDTycg.png?q=20', 'https://miro.medium.com/max/50/1*BDpD0BbvUzZSe5icqW9KzA.png?q=20', 'https://miro.medium.com/max/2048/1*TUmbTSSsT-rdAi7KlHF5xA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*MIOg6Hd39cSLugg_kQ7ufw.jpeg', 'https://miro.medium.com/max/5064/1*qM5rlgPBvqGM160FbjyQfA.png', 'https://miro.medium.com/max/5108/1*z8zGgQa938A70StKgNNUZw.png', 'https://miro.medium.com/max/1200/1*rbUndRLRFjfH8vkoPVQUfA.jpeg', 'https://miro.medium.com/max/60/1*gs8HkbRkWVXctrk9Eo1kWQ.png?q=20', 'https://miro.medium.com/max/1484/1*_f0mk7pRNpr7V_DmvvCQww.png', 'https://miro.medium.com/fit/c/160/160/1*HN6cIWMwbxpPJzTC_x0bhA.jpeg', 'https://miro.medium.com/fit/c/96/96/1*HN6cIWMwbxpPJzTC_x0bhA.jpeg', 'https://miro.medium.com/max/2808/1*rbUndRLRFjfH8vkoPVQUfA.jpeg', 'https://miro.medium.com/max/2076/1*BDpD0BbvUzZSe5icqW9KzA.png', 'https://miro.medium.com/max/60/0*Ez913dMsRbQk1Ua6.?q=20', 'https://miro.medium.com/max/4184/1*PEsstUWsMpI2ZeNWUDTycg.png', 'https://miro.medium.com/max/5112/1*gs8HkbRkWVXctrk9Eo1kWQ.png'}",2020-03-05 00:14:22.719825,1.7722158432006836
https://medium.com/@alexwalling/skyscanner-flight-api-incorporate-travel-data-into-your-app-c319c586c390?fbclid=IwAR2FqFqhskqLZ0z8rTAs48dbVl38kKIzA3J6koqoymY_f6lItg5naKwEBIo,Skyscanner Flight API: Incorporate Travel Data into Your App,"Last year, we wrote a blog post about the Google Flights API (QPX Express API). In that post, we highlighted what we love about the API, explained how to make an API call, and discussed some relevant project ideas. After hearing about the Google Flights API’s depreciation on April 10th, 2018, as travel lovers we started looking for a replacement API for our developer community. That’s where the Skyscanner API steps in to save the day!

We are extremely excited to announce our new partnership with Skyscanner. For the first time ever, developers will be able to access and implement Skyscanner’s Flight Search directly into their applications through a public API, exclusively listed on RapidAPI.

Skyscanner Travel APIs directly connect you to all the data you need to build an innovative website or app. Your customers will be able to take advantage of the best deals on flights, hotels or car hire from wherever they are in the world — either as three independent travel solutions or integrated together.

As a leader in the travel space, our partnership with Skyscanner is especially strategic for us at RapidAPI.

What Data is Available With the Skyscanner Flight Search API?

Over the past 16 years, the Skyscanner team has been building and developing one of the best all-in-one travel apps in the world. Their platform aggregates travel information from across the internet and puts it into one place for users to consume and plug into their products.

Skyscanner offers an unprecedented dataset via their Developer API, and we can’t wait to see what our community builds on top of it. All of Skyscanner’s amazing flight data will now be available to developers directly on the RapidAPI platform via a single click.

The main function of the Sky Scanner API is searching for flights. This can be done in two ways: cached search and live search.

Cached search is much simpler to use, and is free. It searches through the existing data on the Skyscanner platform, giving you a good estimate of available routes (using the Browse Routes endpoint) and flight prices (using the Browse Quotes endpoint).

Live search is a little more complex, but it lets you perform a real-time query of all the different airlines and booking agents to get real booking options. To use that, you’ll need to create a search session with the details of the flight you’re searching. The search results will load in the background, and you can poll them.

In both approaches, you can query flights between any two locations in the world at any dates. The information returned by the API will include flight dates, carriers, and prices.

How to Use the Skyscanner Flight Search API

Step 0: Setup a RapidAPI Account

To connect to the Skyscanner Flight Search API, you will first need to set up an account on RapidAPI. Simply head over to the RapidAPI login page to sign up for an account.

Step 1: Make an API Request

As soon as you are logged in to RapidAPI, you will instantly be able to make requests to the API. A great way to test out the functionality of an API is to use the RapidAPI website to make a test API call from within the browser.

To do this, navigate to the Skyscanner Flight Search functions page, select the endpoint you want to call, and click the “Test Function” button.

Once you’ve made a test API call, you are ready to add the API to your project. Every API on RapidAPI has a code snippet provided to help get you started. Just copy to code snippet directly into your program and select the Unirest Library for the programming language your application is using. Unirest is a set of lightweight HTTP libraries available in multiple languages, which makes connecting to the Skyscanner API a breeze.

Prepare for takeoff…

We cannot wait to see our community begin using the Skyscanner API and all of the amazing functionality it has to offer. Please leave comments, questions and any ideas you have on how you might use the Skyscanner Flight Search API in the comments down below — we’d love your feedback!","['rapidapi', 'travel', 'flights', 'incorporate', 'data', 'api', 'search', 'skyscanner', 'test', 'directly', 'using', 'app', 'flight']","Last year, we wrote a blog post about the Google Flights API (QPX Express API).
Skyscanner Travel APIs directly connect you to all the data you need to build an innovative website or app.
What Data is Available With the Skyscanner Flight Search API?
How to Use the Skyscanner Flight Search APIStep 0: Setup a RapidAPI AccountTo connect to the Skyscanner Flight Search API, you will first need to set up an account on RapidAPI.
Please leave comments, questions and any ideas you have on how you might use the Skyscanner Flight Search API in the comments down below — we’d love your feedback!",en,['Alex Walling'],2018-11-01 09:50:03.230000+00:00,"{'API', 'Developer', 'Programmer'}","{'https://miro.medium.com/max/60/0*Rhje4PqKzdBBJPdh.png?q=20', 'https://miro.medium.com/max/3200/0*Rhje4PqKzdBBJPdh.png', 'https://miro.medium.com/fit/c/160/160/1*W1k25Tn7BUMkH8N3xyvufg.jpeg', 'https://miro.medium.com/max/3200/0*FOPA_9Xh2U93Noc7.png', 'https://miro.medium.com/max/60/0*FOPA_9Xh2U93Noc7.png?q=20', 'https://miro.medium.com/max/1200/0*Rhje4PqKzdBBJPdh.png', 'https://miro.medium.com/fit/c/96/96/1*W1k25Tn7BUMkH8N3xyvufg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*W1k25Tn7BUMkH8N3xyvufg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*GpHmBp1UMcFgIR3-qiVpsg.jpeg'}",2020-03-05 00:14:24.471282,1.7504565715789795
https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6,Train/Test Split and Cross Validation in Python,"Hi everyone! After my last post on linear regression in Python, I thought it would only be natural to write a post about Train/Test Split and Cross Validation. As usual, I am going to give a short overview on the topic and then give an example on implementing it in Python. These are two rather important concepts in data science and data analysis and are used as tools to prevent (or at least minimize) overfitting. I’ll explain what that is — when we’re using a statistical model (like linear regression, for example), we usually fit the model on a training set in order to make predications on a data that wasn’t trained (general data). Overfitting means that what we’ve fit the model too much to the training data. It will all make sense pretty soon, I promise!

What is Overfitting/Underfitting a Model?

As mentioned, in statistics and machine learning we usually split our data into two subsets: training data and testing data (and sometimes to three: train, validate and test), and fit our model on the train data, in order to make predictions on the test data. When we do that, one of two thing might happen: we overfit our model or we underfit our model. We don’t want any of these things to happen, because they affect the predictability of our model — we might be using a model that has lower accuracy and/or is ungeneralized (meaning you can’t generalize your predictions on other data). Let’s see what under and overfitting actually mean:

Overfitting

Overfitting means that model we trained has trained “too well” and is now, well, fit too closely to the training dataset. This usually happens when the model is too complex (i.e. too many features/variables compared to the number of observations). This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. It is because this model is not generalized (or not AS generalized), meaning you can generalize the results and can’t make any inferences on other data, which is, ultimately, what you are trying to do. Basically, when this happens, the model learns or describes the “noise” in the training data instead of the actual relationships between variables in the data. This noise, obviously, isn’t part in of any new dataset, and cannot be applied to it.

Underfitting

In contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data. As you probably guessed (or figured out!), this is usually the result of a very simple model (not enough predictors/independent variables). It could also happen when, for example, we fit a linear model (like linear regression) to data that is not linear. It almost goes without saying that this model will have poor predictive ability (on training data and can’t be generalized to other data).

An example of overfitting, underfitting and a model that’s “just right!”

It is worth noting the underfitting is not as prevalent as overfitting. Nevertheless, we want to avoid both of those problems in data analysis. You might say we are trying to find the middle ground between under and overfitting our model. As you will see, train/test split and cross validation help to avoid overfitting more than underfitting. Let’s dive into both of them!

Train/Test Split

As I said before, the data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.

Train/Test Split

Let’s see how to do this in Python. We’ll do this using the Scikit-Learn library and specifically the train_test_split method. We’ll start with importing the necessary libraries:

import pandas as pd

from sklearn import datasets, linear_model

from sklearn.model_selection import train_test_split

from matplotlib import pyplot as plt

Let’s quickly go over the libraries I’ve imported:

Pandas — to load the data file as a Pandas data frame and analyze the data. If you want to read more on Pandas, feel free to check out my post!

— to load the data file as a Pandas data frame and analyze the data. If you want to read more on Pandas, feel free to check out my post! From Sklearn , I’ve imported the datasets module, so I can load a sample dataset, and the linear_model, so I can run a linear regression

, I’ve imported the datasets module, so I can load a sample dataset, and the linear_model, so I can run a linear regression From Sklearn, sub-library model_selection , I’ve imported the train_test_split so I can, well, split to training and test sets

sub-library , I’ve imported the train_test_split so I can, well, split to training and test sets From Matplotlib I’ve imported pyplot in order to plot graphs of the data

OK, all set! Let’s load in the diabetes dataset, turn it into a data frame and define the columns’ names:

# Load the Diabetes dataset

columns = “age sex bmi map tc ldl hdl tch ltg glu”.split() # Declare the columns names

diabetes = datasets.load_diabetes() # Call the diabetes dataset from sklearn

df = pd.DataFrame(diabetes.data, columns=columns) # load the dataset as a pandas data frame

y = diabetes.target # define the target variable (dependent variable) as y

Now we can use the train_test_split function in order to make the split. The test_size=0.2 inside the function indicates the percentage of the data that should be held over for testing. It’s usually around 80/20 or 70/30.

# create training and testing vars

X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)

print X_train.shape, y_train.shape

print X_test.shape, y_test.shape (353, 10) (353,)

(89, 10) (89,)

Now we’ll fit the model on the training data:

# fit a model

lm = linear_model.LinearRegression() model = lm.fit(X_train, y_train)

predictions = lm.predict(X_test)

As you can see, we’re fitting the model on the training data and trying to predict the test data. Let’s see what (some of) the predictions are:

predictions[0:5]

array([ 205.68012533, 64.58785513, 175.12880278, 169.95993301,

128.92035866])

Note: because I used [0:5] after predictions, it only showed the first five predicted values. Removing the [0:5] would have made it print all of the predicted values that our model created.

Let’s plot the model:

## The line / model

plt.scatter(y_test, predictions)

plt.xlabel(“True Values”)

plt.ylabel(“Predictions”)

And print the accuracy score:

print “Score:”, model.score(X_test, y_test) Score: 0.485829586737

There you go! Here is a summary of what I did: I’ve loaded in the data, split it into a training and testing sets, fitted a regression model to the training data, made predictions based on this data and tested the predictions on the test data. Seems good, right? But train/test split does have its dangers — what if the split we make isn’t random? What if one subset of our data has only people from a certain state, employees with a certain income level but not other income levels, only women or only people at a certain age? (imagine a file ordered by one of these). This will result in overfitting, even though we’re trying to avoid it! This is where cross validation comes in.

Cross Validation

In the previous paragraph, I mentioned the caveats in the train/test split method. In order to avoid this, we can perform something called cross validation. It’s very similar to train/test split, but it’s applied to more subsets. Meaning, we split our data into k subsets, and train on k-1 one of those subset. What we do is to hold the last subset for test. We’re able to do it for each of the subsets.

Visual Representation of Train/Test Split and Cross Validation. H/t to my DSI instructor, Joseph Nelson!

There are a bunch of cross validation methods, I’ll go over two of them: the first is K-Folds Cross Validation and the second is Leave One Out Cross Validation (LOOCV)

K-Folds Cross Validation

In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set.

Visual representation of K-Folds. Again, H/t to Joseph Nelson!

Here is a very simple example from the Sklearn documentation for K-Folds:

from sklearn.model_selection import KFold # import KFold

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array

y = np.array([1, 2, 3, 4]) # Create another array

kf = KFold(n_splits=2) # Define the split - into 2 folds

kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator print(kf) KFold(n_splits=2, random_state=None, shuffle=False)

And let’s see the result — the folds:

for train_index, test_index in kf.split(X):

print(“TRAIN:”, train_index, “TEST:”, test_index)

X_train, X_test = X[train_index], X[test_index]

y_train, y_test = y[train_index], y[test_index] ('TRAIN:', array([2, 3]), 'TEST:', array([0, 1]))

('TRAIN:', array([0, 1]), 'TEST:', array([2, 3]))

As you can see, the function split the original data into different subsets of the data. Again, very simple example but I think it explains the concept pretty well.

Leave One Out Cross Validation (LOOCV)

This is another method for cross validation, Leave One Out Cross Validation (by the way, these methods are not the only two, there are a bunch of other methods for cross validation. Check them out in the Sklearn website). In this type of cross validation, the number of folds (subsets) equals to the number of observations we have in the dataset. We then average ALL of these folds and build our model with the average. We then test the model against the last fold. Because we would get a big number of training sets (equals to the number of samples), this method is very computationally expensive and should be used on small datasets. If the dataset is big, it would most likely be better to use a different method, like kfold.

Let’s check out another example from Sklearn:

from sklearn.model_selection import LeaveOneOut

X = np.array([[1, 2], [3, 4]])

y = np.array([1, 2])

loo = LeaveOneOut()

loo.get_n_splits(X)





for train_index, test_index in loo.split(X):

print(""TRAIN:"", train_index, ""TEST:"", test_index)

X_train, X_test = X[train_index], X[test_index]

y_train, y_test = y[train_index], y[test_index]

print(X_train, X_test, y_train, y_test)

And this is the output:

('TRAIN:', array([1]), 'TEST:', array([0]))

(array([[3, 4]]), array([[1, 2]]), array([2]), array([1]))

('TRAIN:', array([0]), 'TEST:', array([1]))

(array([[1, 2]]), array([[3, 4]]), array([1]), array([2]))

Again, simple example, but I really do think it helps in understanding the basic concept of this method.

So, what method should we use? How many folds? Well, the more folds we have, we will be reducing the error due the bias but increasing the error due to variance; the computational price would go up too, obviously — the more folds you have, the longer it would take to compute it and you would need more memory. With a lower number of folds, we’re reducing the error due to variance, but the error due to bias would be bigger. It’s would also computationally cheaper. Therefore, in big datasets, k=3 is usually advised. In smaller datasets, as I’ve mentioned before, it’s best to use LOOCV.","['overfitting', 'cross', 'python', 'dataset', 'training', 'data', 'model', 'ive', 'traintest', 'test', 'validation', 'split']","After my last post on linear regression in Python, I thought it would only be natural to write a post about Train/Test Split and Cross Validation.
As you will see, train/test split and cross validation help to avoid overfitting more than underfitting.
Train/Test SplitAs I said before, the data we use is usually split into training data and test data.
There are a bunch of cross validation methods, I’ll go over two of them: the first is K-Folds Cross Validation and the second is Leave One Out Cross Validation (LOOCV)K-Folds Cross ValidationIn K-Folds Cross Validation we split our data into k different subsets (or folds).
Leave One Out Cross Validation (LOOCV)This is another method for cross validation, Leave One Out Cross Validation (by the way, these methods are not the only two, there are a bunch of other methods for cross validation.",en,['Adi Bronshtein'],2019-02-27 14:52:04.610000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Data Analysis', 'Statistics'}","{'https://miro.medium.com/max/1220/1*2f6x7rNuN0_zbW3_O5vOEA.png', 'https://miro.medium.com/max/60/1*tBErXYVvTw2jSUYK7thU2A.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1896/1*4G__SV580CxFj78o9yUXuQ.png', 'https://miro.medium.com/max/2892/1*J2B_bcbd1-s1kpWOu_FZrg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*iHQeqD1jkhi1ihuxX_30lg.png?q=20', 'https://miro.medium.com/max/1192/1*iHQeqD1jkhi1ihuxX_30lg.png', 'https://miro.medium.com/fit/c/96/96/2*HQE_-QZzbq_LaZCtksiYjQ.jpeg', 'https://miro.medium.com/max/2272/1*-8_kogvwmL1H6ooN1A1tsQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png', 'https://miro.medium.com/fit/c/160/160/2*HQE_-QZzbq_LaZCtksiYjQ.jpeg', 'https://miro.medium.com/max/948/1*4G__SV580CxFj78o9yUXuQ.png', 'https://miro.medium.com/max/60/1*-8_kogvwmL1H6ooN1A1tsQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*2f6x7rNuN0_zbW3_O5vOEA.png?q=20', 'https://miro.medium.com/max/60/1*J2B_bcbd1-s1kpWOu_FZrg.png?q=20', 'https://miro.medium.com/max/60/1*4G__SV580CxFj78o9yUXuQ.png?q=20'}",2020-03-05 00:14:30.667971,6.195702314376831
https://medium.com/weareservian/migrating-dialogflow-webhook-from-cloud-function-to-app-engine-6835a3dd52c1,Migrating a Dialogflow webhook from Cloud Functions to App Engine,"In Dialogflow we may want to integrate with other external systems in order to send information from the user and dynamically populate the response. This would be essential if a chatbot needs to book movie tickets, check flight times or stream conversation data to analytics platforms.

The easiest way to create a webhook endpoint, and the one suggested in the Dialogflow documentation, is a Cloud Function. Cloud Functions are serverless, autoscaling, very cheap and can run functions in several languages, so they are a good way to limit overhead. However there is no dedicated instance when using Cloud Functions. The cold start times can mean slow response times, and therefore a hindered experience for users of your Dialogflow chatbot.

If you are suffering from extended response times, or your function has to do a lot of work, such as calling multiple external services, another solution is to move away from serverless and onto a dedicated server on Google’s App Engine. Using App Engine will allow you to have a dedicated instance for your Dialogflow fulfillments, and offers more options on the size and speed of the server needed the handle larger functions.

Setting the Webhook Endpoint

The endpoint URL can be set in Dialogflow in the fulfillment section.

Setting the URL endpoint trigger for the webhook

For each intent the webhook then can be enabled. The Dialogflow request will be sent to the webhook, and in turn the response will need to come back from the webhook.

Setting the webhook toggle

Cloud Functions

Google Cloud Functions are a great way to implement the webhook as there are no servers to provision and they scale automatically. We set the http trigger for Dialogflow to hit…

Creating the cloud function trigger

Then add our source code to handle each Dialogflow event and send the response back. Notice here that the Firebase framework allows you to set the function inside index.js that is evoked by the trigger.

Cloud Functions are powered by Firebase so we need to import the Firebase module, and the Firebase function framework will then take care of initiating express and parsing our request and response jsons using body parser.

const functions = require('firebase-functions'); exports.dialogflowFirebaseFulfillment = functions.https.onRequest(app)

Our app in this case is the Actions on Google module designed to handle Dialogflow events via the Google Assistant.

const {dialogflow} = require('actions-on-google'); const app = dialogflow({debug: true});

Then for each intent we can specify some behaviour such as the response to give. The conv.ask() method here will send the SSML response once this intent is trigged. In the app.intent() function we could also call another external services to populate the wait time of the coffee, for example, or send our conversation data to a database such as BigQuery or Chatbase.

app.intent('Default Welcome Intent', (conv) => {

const ssml = '<speak>Hi, can I please take your order?</speak>'

return conv.ask(ssml) });

Moving to App Engine

Cloud Functions however, suffer from the drawback of having roughly ~10s cold start times. The server is not waiting to receive your function, but is instead triggered when it is needed, which can lead to slow chatbot conversation response times, especially on the first intent.

The advantage of using Cloud Functions is that they are cheap and readily available but with the release of App Engine Standard Edition, which has a free tier, the same result can be achieved on a dedicated server without too much operational overhead.

To move to App Engine we can use the same index.js and package.json files, but we also need to define our App Engine build in an app.yaml file. For App Engine Standard Environment we just need to set the runtime (for Node.js only 8 is available on Standard).

# [START gae_quickstart_yaml] runtime: nodejs8 # [END gae_quickstart_yaml]

In the index.js file we need to start express and body parser as we will no longer be using the Firebase framework that automatically handles this when using a Cloud Function.

const express = require('express'); const bodyParser = require('body-parser') const express_app = express();

Define the Google on Actions app in the same way to handle intent conversation requests.

app.intent('Default Welcome Intent', (conv) => {

const ssml = '<speak>Hi, can I please take your order?</speak>'

return conv.ask(ssml) });

Then set the body parser to handle the request and response jsons from the Actions on Google app and set the express app to listen for requests coming into the endpoint.

express_app.use(bodyParser.urlencoded({ extended: true }));

express_app.use(bodyParser.json({type: 'application/json'}));

express_app.post('/', app);

express_app.listen(8080);

Initial the GCloud SDK using gcloud init and select the appropriate Google project. Navigate to the directory you index.js , app.yaml , and package.json file are stored and simply type gcloud app deploy to push the files up to the App Engine instance.

Set the webhook endpoint in Dialogflow to the new App Engine site for post requests to be handled by the new app.

Information on the improvement in performance to follow…","['function', 'webhook', 'cloud', 'dialogflow', 'engine', 'response', 'functions', 'times', 'migrating', 'using', 'app']","Cloud Functions are serverless, autoscaling, very cheap and can run functions in several languages, so they are a good way to limit overhead.
However there is no dedicated instance when using Cloud Functions.
To move to App Engine we can use the same index.js and package.json files, but we also need to define our App Engine build in an app.yaml file.
For App Engine Standard Environment we just need to set the runtime (for Node.js only 8 is available on Standard).
Set the webhook endpoint in Dialogflow to the new App Engine site for post requests to be handled by the new app.",en,['Scott Shellien-Walker'],2018-10-24 20:54:54.514000+00:00,"{'App Engine', 'Actions On Google', 'Nodejs', 'Google Assistant', 'Dialogflow'}","{'https://miro.medium.com/max/60/1*nqNrzPEtwBE3VWN8e_yQiw.png?q=20', 'https://miro.medium.com/max/60/1*tmgFrTEfVmPys7eWQzCybA.png?q=20', 'https://miro.medium.com/max/3628/1*RC_tp2twH28k715ESZX_7w.png', 'https://miro.medium.com/fit/c/160/160/1*Vsqo59XpzZ_epuiPU63jRQ.png', 'https://miro.medium.com/max/1330/1*Vrda0sfXeFQKvXlGPsn0Ag.png', 'https://miro.medium.com/fit/c/80/80/0*gBtU1x6cultK10FP.jpg', 'https://miro.medium.com/max/1440/0*_fi92ydmqtVpt9vI.jpg', 'https://miro.medium.com/max/60/1*oKbYWb3VZZ-fPt8b0HXXFA.png?q=20', 'https://miro.medium.com/max/60/1*Vrda0sfXeFQKvXlGPsn0Ag.png?q=20', 'https://miro.medium.com/max/60/0*_fi92ydmqtVpt9vI.jpg?q=20', 'https://miro.medium.com/fit/c/80/80/0*zvmUdlH-sl9XLzAA', 'https://miro.medium.com/fit/c/160/160/0*oohz1LOCoQ81_oix', 'https://miro.medium.com/fit/c/96/96/0*oohz1LOCoQ81_oix', 'https://miro.medium.com/max/3658/1*tmgFrTEfVmPys7eWQzCybA.png', 'https://miro.medium.com/max/1600/1*nqNrzPEtwBE3VWN8e_yQiw.png', 'https://miro.medium.com/max/3630/1*oKbYWb3VZZ-fPt8b0HXXFA.png', 'https://miro.medium.com/max/392/1*Q_4p7QWmgDvwbxonpy6xYg.png', 'https://miro.medium.com/fit/c/80/80/1*-gtQJ08fht22E5wx7JCQNA.png', 'https://miro.medium.com/max/720/0*_fi92ydmqtVpt9vI.jpg', 'https://miro.medium.com/max/60/1*RC_tp2twH28k715ESZX_7w.png?q=20'}",2020-03-05 00:14:31.527569,0.85959792137146
https://medium.com/swlh/how-to-build-a-chatbot-with-dialog-flow-chapter-4-external-api-for-fulfilment-3ab934fd7a00,How to build a chatbot with Dialog flow | Chapter 4— External API for Fulfilment,"Fulfilment

Executing the code that will complete the action requested by the user to fulfil the intent of the user.

Code might require us to update the datbase

Once the code is generated a response is generated to tex response that’s sent to the user..which is in-turn sent to the agent (bot) which in turn is sent to the app that the user used to interact with your agent.

…..

Example:

We are gonna get the stock price of Apple from an external source.

The Intrinio platform

Alternative for Indian Market could be: https://kite.trade/startups

Signup and go to your ccount section..you should find your API access keys

API explorer:

https://intrinio.com/api-explorer?formula=historical_data&values=eyJpZGVudGlmaWVyIjoiQUFQTCIsIml0ZW0iOiIiLCJzZXF1ZW5jZSI6MH0%3D

Documentation

http://docs.intrinio.com/#introduction","['usercode', 'turn', 'code', 'flow', 'user', 'update', 'dialog', 'used', 'chatbot', 'generated', 'build', 'api', 'fulfilment', 'response', 'sent', 'userwhich', 'chapter', 'external']","FulfilmentExecuting the code that will complete the action requested by the user to fulfil the intent of the user.
Code might require us to update the datbaseOnce the code is generated a response is generated to tex response that’s sent to the user..which is in-turn sent to the agent (bot) which in turn is sent to the app that the user used to interact with your agent.
Example:We are gonna get the stock price of Apple from an external source.
The Intrinio platformAlternative for Indian Market could be: https://kite.trade/startupsSignup and go to your ccount section..you should find your API access keysAPI explorer:https://intrinio.com/api-explorer?formula=historical_data&values=eyJpZGVudGlmaWVyIjoiQUFQTCIsIml0ZW0iOiIiLCJzZXF1ZW5jZSI6MH0%3DDocumentationhttp://docs.intrinio.com/#introduction",en,['Moses Sam Paul'],2018-09-12 09:29:54.849000+00:00,"{'Bots', 'Dialogflow', 'Chatbot Development', 'Chatbots', 'Chatbot Platforms'}","{'https://miro.medium.com/fit/c/160/160/1*Xd2uZaVHfrGOP14W_3UQRg.jpeg', 'https://miro.medium.com/max/26/1*J17zBlM5apwTFI-Oti5KKg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*8sPnUUwNX27Ibcn0yK1Kow.jpeg', 'https://miro.medium.com/max/60/1*hU1UikCBGB-w-IlSKzSbDw.png?q=20', 'https://miro.medium.com/max/1516/1*JjvhHNzie7Lpp4JJ9nUMxA.png', 'https://miro.medium.com/max/60/1*3hCBBzJ61I5VEIA-R5PknA.png?q=20', 'https://miro.medium.com/max/60/1*ljcn06KOXmDzdR_x2Hyxqw.png?q=20', 'https://miro.medium.com/max/28/1*kD0yFk7LsqPaARmMW_RLTw.png?q=20', 'https://miro.medium.com/max/60/1*I00OURG_uk_jIY85wRUmsw.png?q=20', 'https://miro.medium.com/max/1552/1*9Q7E9GkwcxsRm_D5zkNoKA.png', 'https://miro.medium.com/max/60/1*v70kUD9HALqnRUvG-b8XXg.png?q=20', 'https://miro.medium.com/max/1916/1*ljcn06KOXmDzdR_x2Hyxqw.png', 'https://miro.medium.com/max/60/1*JjvhHNzie7Lpp4JJ9nUMxA.png?q=20', 'https://miro.medium.com/max/60/1*tZKE2D6wLbgOdWMuNQ4CiQ.png?q=20', 'https://miro.medium.com/max/508/1*kD0yFk7LsqPaARmMW_RLTw.png', 'https://miro.medium.com/max/1518/1*xiTqCTwCppOkKTQWo3Gsew.png', 'https://miro.medium.com/fit/c/80/80/1*iUOmQH3pEEytA5Mwwuclxg.jpeg', 'https://miro.medium.com/max/2400/1*hU1UikCBGB-w-IlSKzSbDw.png', 'https://miro.medium.com/max/1376/1*Uu_DYQ0hb9Yojej97ZEIGw.png', 'https://miro.medium.com/max/422/1*IOJrKVmLnRcFz3E_KrrN_Q.png', 'https://miro.medium.com/max/1994/1*gnfDERldSJeR_zIZBOLsfg.png', 'https://miro.medium.com/max/60/1*NN_bmUPwKxDNdTN5MbfS9w.png?q=20', 'https://miro.medium.com/max/60/1*gnfDERldSJeR_zIZBOLsfg.png?q=20', 'https://miro.medium.com/max/60/1*McdNpmjxQj0Iq0kFXEwxOg.png?q=20', 'https://miro.medium.com/max/60/1*Uu_DYQ0hb9Yojej97ZEIGw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*8sPnUUwNX27Ibcn0yK1Kow.jpeg', 'https://miro.medium.com/max/60/1*YqDjlKFwScoQYQ62DWEdig.png?q=20', 'https://miro.medium.com/max/60/1*ouK9XR4xuNWtCes-TIUNAw.png?q=20', 'https://miro.medium.com/max/2542/1*v70kUD9HALqnRUvG-b8XXg.png', 'https://miro.medium.com/max/60/1*-Y1u4OB9CByCDW15MWdbBA.png?q=20', 'https://miro.medium.com/max/1506/1*tZKE2D6wLbgOdWMuNQ4CiQ.png', 'https://miro.medium.com/max/1336/1*McdNpmjxQj0Iq0kFXEwxOg.png', 'https://miro.medium.com/max/4000/1*ouK9XR4xuNWtCes-TIUNAw.png', 'https://miro.medium.com/max/1982/1*x_tdDi0H-v8Bm65Pk0PiTQ.png', 'https://miro.medium.com/max/1948/1*I00OURG_uk_jIY85wRUmsw.png', 'https://miro.medium.com/max/522/1*J17zBlM5apwTFI-Oti5KKg.png', 'https://miro.medium.com/max/60/1*9Q7E9GkwcxsRm_D5zkNoKA.png?q=20', 'https://miro.medium.com/max/1404/1*-Y1u4OB9CByCDW15MWdbBA.png', 'https://miro.medium.com/max/4000/1*YqDjlKFwScoQYQ62DWEdig.png', 'https://miro.medium.com/max/60/1*x_tdDi0H-v8Bm65Pk0PiTQ.png?q=20', 'https://miro.medium.com/max/2276/1*NN_bmUPwKxDNdTN5MbfS9w.png', 'https://miro.medium.com/max/1480/1*3hCBBzJ61I5VEIA-R5PknA.png', 'https://miro.medium.com/max/1200/1*hU1UikCBGB-w-IlSKzSbDw.png', 'https://miro.medium.com/fit/c/80/80/1*vTG13sWBDAFir-PlEvsg9w.jpeg', 'https://miro.medium.com/max/60/1*xiTqCTwCppOkKTQWo3Gsew.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*D4KAER7h6bCkTU-m7Aw9DA.jpeg'}",2020-03-05 00:14:32.503289,0.9757199287414551
https://towardsdatascience.com/customer-segmentation-report-for-arvato-financial-solutions-b08a01ac7bc0,Customer Segmentation Report for Arvato Financial Solutions,"Introduction

In this project supervised and unsupervised learning techniques are used to analyze demographics data of customers of a mail-order sales company in Germany against demographics information for the general population. The goal of this project is to characterize customers segment of population, and to build a model that will be able to predict customers for Arvato Financial Solutions.

The data for this project is provided by Udacity partners at Bertelsmann Arvato Analytics, and represents a real-life data science task. It includes general population dataset, customer segment data set, dataset of mailout campaign with response and test dataset that needs to make predictions.

Problem Statement

There are four main parts of the project:

Data Preprocessing

In this part we need to preprocess data for further analysis.

Missing values by columns and rows will be analysed, data will be divided by types followed by subsequent transformations.

2. Customer Segmentation

In this part we need to analyze general population and customer segment data sets and use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company.

I will use principal component analysis (PCA) technique for dimensionality reduction. Then, elbow curve will be used to identify the best number of clusters for KMeans algorithm. Finaly, I will apply KMeans to make segmentation of population and customers and determine description of target cluster for the company.

3. Supervised Learning Model

In this part we need to build machine learning model using response of marketing campaign and use model to predict which individuals are most likely to convert into becoming customers for the company.

I will use several machine learning classifiers and choose the best using analysis of learning curve. Then, I will parametrize the model and make predictions.

4. Kaggle Competition

The results of this part need to be submitted for Kaggle competition

Metrics

Area under the receiver operating characteristic curve (ROC_AUC) from predicted probabilities will be used to evaluate performances of the models. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random target person more highly than a random non-target person. Thus, ROC analysis provides tools to select possibly optimal models for customer prediction task.

Results and Discussion

Data Preprocessing

There are two data description Excel spreadsheets and four data files associated with this project:

AZDIAS: Demographics data for the general population of Germany. It has 891211 persons (rows) and 366 features (columns).

Descriptive statistics for the first few attributes of AZDIAS data set

CUSTOMERS : Demographics data for customers of a mail-order company. It has 191652 rows and 369 features.

Descriptive statistics for the first few attributes of CUSTOMERS data set

MAILOUT_TRAIN : Demographics data for individuals who were targets of a marketing campaign; 42982 persons and 367 features including response of people.

: Demographics data for individuals who were targets of a marketing campaign; 42982 persons and 367 features including response of people. MAILOUT_TEST : Demographics data for individuals who were targets of a marketing campaign; 42833 persons and 366 features.

Unfortunately, there are a lot of missing values in these datasets and not all of the listed features have explanation in a given Excel spreadsheets.

Analysis of columns and rows with missing values

First, I created python dictionary of missing values codes where the key in a “key”: value pair is attribute and value is a list of missing codes parsed from DIAS Attributs — Values 2017.xlsx.

Interestingly, there are only 275 out of 366 items in the dictionary, meaning that there are a lot of features that are not listed in the given attribute description file as well as some of the attributes missing values are simply not entered and listed in dataset as numpy not a number (np.nan).

First three items from missing keys dictionary

Next, values that correspond to missing value codes of AZDIAS dataset were converted to np.nan values and the final number of missing values were analyzed for each attribute.

The analysis demonstrates that most of the columns have less than 30% of missing data while there are 41 attributes with more than 30% of missing data (see the distribution of missing values in these columns the below). These 41 attributes were dropped from analysis.

Attributes with more than 30% of missing values dropped from analysis

Additionally, there are other columns that were dropped based on the following reasons:

column with unique values, LNR

categorical columns with more than 10 categories to avoid many additional attributes after one-hot encoding (CAMEO_INTL_2015 is exclusion)

columns with information repetition from another feature (e.g. fein vs grob)

some attributes, for which description was not given and it is hard to predict meaning and type of column (categorical vs ordinal vs mixed).

The assement of missing values by rows demonstrates that the maximum number of missing data in each row is 233 attributes out of 303 attributes left after dropping columns. The distribution of amount of missing data in each row demonstrates that most of the rows have less than 25 missing attributes. So, the data was devided into two subsets: azdias with <=25 missing attributes (737235 rows) and azdias with > 25 missing attributes (153986 rows). Comparison of distribution of values for 6 randomly choosen columns demonstrates that there is similar distribution in two data sets (see bar plot for 6 different attributes with few nan vs many nan datasets below).","['arvato', 'attributes', 'rows', 'report', 'demographics', 'columns', 'population', 'customer', 'learning', 'data', 'customers', 'missing', 'solutions', 'values', 'financial', 'segmentation']","Missing values by columns and rows will be analysed, data will be divided by types followed by subsequent transformations.
: Demographics data for individuals who were targets of a marketing campaign; 42982 persons and 367 features including response of people.
MAILOUT_TEST : Demographics data for individuals who were targets of a marketing campaign; 42833 persons and 366 features.
The distribution of amount of missing data in each row demonstrates that most of the rows have less than 25 missing attributes.
So, the data was devided into two subsets: azdias with <=25 missing attributes (737235 rows) and azdias with > 25 missing attributes (153986 rows).",en,['Elena Ivanova'],2018-12-12 00:13:30.222000+00:00,"{'Kaggle Competition', 'Data Science', 'Machine Learning', 'Pipeline', 'Data Processing'}","{'https://miro.medium.com/max/1984/1*Uw0_JYGOdvqDgFFWHFXq7Q.png', 'https://miro.medium.com/max/60/1*TLlWWZFnnseAqSizGV7AUw.png?q=20', 'https://miro.medium.com/max/1560/1*SPPEfGuBZFVatzQ9Mm2dzg.png', 'https://miro.medium.com/max/2328/1*TLlWWZFnnseAqSizGV7AUw.png', 'https://miro.medium.com/max/3864/1*Bo2QSg9B2feaEmsBIWuVAg.png', 'https://miro.medium.com/max/60/1*CpH4XB0JNBNo4Dt8MjlQ_A.png?q=20', 'https://miro.medium.com/max/1604/1*dwtVqwlHehwPwT2G8blfCQ.png', 'https://miro.medium.com/max/60/1*SYQ2acWhtnXToLylZJ4wnQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*7iEKMakdu3GEXlb6cvd3OA.png?q=20', 'https://miro.medium.com/max/3408/1*mMvQxMrPiOQAcRLePQuAxA.png', 'https://miro.medium.com/max/1552/1*557ztb6eseeLid_uyigFVg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*zE9kRFtVn3uy3n760tIdCA.png?q=20', 'https://miro.medium.com/max/1632/1*9ciJSIsgZPaLiVX-iOZ52g.png', 'https://miro.medium.com/max/60/1*jT_W6u56Ru_PBMvKQ6rakA.png?q=20', 'https://miro.medium.com/max/60/1*557ztb6eseeLid_uyigFVg.png?q=20', 'https://miro.medium.com/max/60/1*9ciJSIsgZPaLiVX-iOZ52g.png?q=20', 'https://miro.medium.com/max/3852/1*jT_W6u56Ru_PBMvKQ6rakA.png', 'https://miro.medium.com/max/1500/1*zE9kRFtVn3uy3n760tIdCA.png', 'https://miro.medium.com/max/1200/1*n428hvrPFp9oK60n7GJwhw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*mMvQxMrPiOQAcRLePQuAxA.png?q=20', 'https://miro.medium.com/max/60/1*SPPEfGuBZFVatzQ9Mm2dzg.png?q=20', 'https://miro.medium.com/max/60/1*jRKD_Vy-gBoHufj8P_XkuQ.png?q=20', 'https://miro.medium.com/max/3364/1*7iEKMakdu3GEXlb6cvd3OA.png', 'https://miro.medium.com/max/40/1*Uw0_JYGOdvqDgFFWHFXq7Q.png?q=20', 'https://miro.medium.com/max/4084/1*CpH4XB0JNBNo4Dt8MjlQ_A.png', 'https://miro.medium.com/max/3632/1*jRKD_Vy-gBoHufj8P_XkuQ.png', 'https://miro.medium.com/fit/c/160/160/1*o86iEcO5P5wO7UFtK9qJZA.jpeg', 'https://miro.medium.com/max/60/1*AGmpjVzB1aHwpWAvbWbdJQ.png?q=20', 'https://miro.medium.com/max/1640/1*AGmpjVzB1aHwpWAvbWbdJQ.png', 'https://miro.medium.com/max/60/1*Bo2QSg9B2feaEmsBIWuVAg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*o86iEcO5P5wO7UFtK9qJZA.jpeg', 'https://miro.medium.com/max/60/1*n428hvrPFp9oK60n7GJwhw.png?q=20', 'https://miro.medium.com/max/60/1*dUYYR_S_Xx9CROl8rRwRwQ.png?q=20', 'https://miro.medium.com/max/3588/1*75q4Fs4G5-nc2GwVD50BTw.png', 'https://miro.medium.com/max/60/1*75q4Fs4G5-nc2GwVD50BTw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/4552/1*n428hvrPFp9oK60n7GJwhw.png', 'https://miro.medium.com/max/1956/1*dUYYR_S_Xx9CROl8rRwRwQ.png', 'https://miro.medium.com/max/2560/1*SYQ2acWhtnXToLylZJ4wnQ.png', 'https://miro.medium.com/max/60/1*dwtVqwlHehwPwT2G8blfCQ.png?q=20'}",2020-03-05 00:14:39.604711,7.1004111766815186
https://medium.com/google-developer-experts/the-full-stack-guide-to-actions-for-google-assistant-e1765edd075b,The Full-Stack Guide to Actions for Google Assistant,"The Full-Stack Guide to Actions for Google Assistant

Plus: How We Taught Google Assistant to Teach You Spanish

If you’re currently building or thinking about developing a custom Google Assistant Action, this post is for you!

My friend Daniel Gwerzman and I recently released Spanish Lesson, an Assistant that will help you learn Spanish by teaching you a number of new words every day, and reading you sample sentences in Spanish for you to translate into English.

Since we had so much fun building the app, we thought we would show you how we did it, including some interesting code snippets. Our goal is to help you save some precious time when developing your own Actions, and to show you how fun they are to create!

The Spanish Lesson Logo :)

So how did this whole thing get started?

Some months ago, my life partner Ariella got a new Google Home device. She was very excited about it and tried all sort of things. At some point, I heard her ask the device: “Hey Google, Teach me Spanish”, to which the device responded: “I’m sorry, I don’t know how to help with that yet.”

Coincidentally, earlier that same day, I read Daniel Gwerzman’s article in which he explains why now is a good time to build actions on Google. He had some very good points — people are lazy and would rather talk than type, and since it’s still the early days of the platform, there are many opportunities to have a big impact in the technology space.

So when the Assistant responded to Ariella that it can’t teach her Spanish “yet,” it struck me: why don’t I make that happen?!

The next day, I pinged Daniel, and asked him if he was ready to embark on a new adventure. Daniel was very excited about the idea, and we started collaborating. We sat together and designed a persona, which was the basis for creating the texts and possible dialog flows. We learned a lot throughout this process, and we will probably publish another post from the product / UX point of view in a few weeks.

However, the focus of this post is the technical part of creating an Assistant Action — the challenges that we had, the stack we chose, basically sharing with you how the architecture of a complete solution for providing a real-life, complex, Action on Google Assistant.

We are going to cover the tech decisions that we made, and share our experience with the outcomes of our choices.

Template, Dialogflow, or Actions SDK?

There are currently three approaches to building Assistant actions: ready-made templates, Dialogflow, and Actions SDK.

The ready-made templates are great for use cases such as creating a Trivia Game or Flash Cards app. When you use a template, you don’t need to write a single line of code, just fill-in some spreadsheet and the action is created for you, based on the information that you fill. This can be very useful for school teachers, who can easily create games for their students.

In our case, however, we needed more power: we wanted to be able to keep track of the user’s progress, and actually mixing Spanish and English in a single app is quite a challenge, as you will see in a minute. So we had to choose between Dialogflow and Actions SDK.

Dialogflow gives you a nice user interface for building conversation flows (in some cases you can even get away without writing any code), and also incorporates some AI to help you figure out the user intent.

Actions SDK gives you “bare-bone” access to user input, and it is up to you to provide a backend which will parse that input and generate the appropriate responses.

We decided to go with Dialogflow, as it could handle some of the flows for us (e.g. asking the user a yes or no question, and understanding the user response), and it would also let us prototype quickly.

Dialogflow’s built-in capabilities proved very useful to us. For instance, if a user didn’t know how to translate the sentence they were given, they could say, “I don’t know” to get the answer and skip to the next one.

Quickly after we published the app, we realized users had many ways to say they didn’t know the sentence — “I have no idea,” “I forgot,” “what is it,” and even the good old, “idk.”","['know', 'actions', 'assistant', 'fullstack', 'spanish', 'user', 'daniel', 'dialogflow', 'help', 'dont', 'google', 'guide']","The Full-Stack Guide to Actions for Google AssistantPlus: How We Taught Google Assistant to Teach You SpanishIf you’re currently building or thinking about developing a custom Google Assistant Action, this post is for you!
Our goal is to help you save some precious time when developing your own Actions, and to show you how fun they are to create!
So when the Assistant responded to Ariella that it can’t teach her Spanish “yet,” it struck me: why don’t I make that happen?!
However, the focus of this post is the technical part of creating an Assistant Action — the challenges that we had, the stack we chose, basically sharing with you how the architecture of a complete solution for providing a real-life, complex, Action on Google Assistant.
There are currently three approaches to building Assistant actions: ready-made templates, Dialogflow, and Actions SDK.",en,['Uri Shaked'],2019-05-18 08:43:06.277000+00:00,"{'Startup', 'Google Assistant', 'Internet of Things', 'Programming', 'Technology'}","{'https://miro.medium.com/max/60/1*ZepeEsnaiON6967tw5cSkw.png?q=20', 'https://miro.medium.com/max/60/1*dHpIq3TCUAaGkNH4HtG-ew.png?q=20', 'https://miro.medium.com/max/2390/1*AIOfl9B8pu9bizmlm2Qvlg.png', 'https://miro.medium.com/max/2020/1*ZepeEsnaiON6967tw5cSkw.png', 'https://miro.medium.com/max/46/1*np1-dWzL-1BpfG6awxFPeA.png?q=20', 'https://miro.medium.com/max/60/1*PrQW42seBpwbBjqEytI6bQ.png?q=20', 'https://miro.medium.com/max/2478/1*AgIMXgTJy5ruUMzWF1leeA.png', 'https://miro.medium.com/fit/c/80/80/1*jZj1Zeiwv34V6kwM6CwoVQ.jpeg', 'https://miro.medium.com/max/60/1*5AmPP_9kl_rZsrwIwoQ8dw.png?q=20', 'https://miro.medium.com/max/3492/1*6i5vbeqyuvn7tRi2o4G0sg.png', 'https://miro.medium.com/max/1776/1*PrQW42seBpwbBjqEytI6bQ.png', 'https://miro.medium.com/max/1162/1*L9RKfVAEgyp5HkAYJYo19w.png', 'https://miro.medium.com/max/1726/1*np1-dWzL-1BpfG6awxFPeA.png', 'https://miro.medium.com/fit/c/96/96/1*Eue5o9nV1YB3JdkeduFCLQ.jpeg', 'https://miro.medium.com/max/60/1*AgIMXgTJy5ruUMzWF1leeA.png?q=20', 'https://miro.medium.com/max/60/1*g6a8f4rqv4VWhGol1-sK0Q.png?q=20', 'https://miro.medium.com/max/1512/1*dHpIq3TCUAaGkNH4HtG-ew.png', 'https://miro.medium.com/fit/c/160/160/1*Zkhl4Zz43z2_iR_ADlP-rg.png', 'https://miro.medium.com/fit/c/80/80/1*J0BoXIqZemyIW3V25DK6CQ.png', 'https://miro.medium.com/max/60/1*L9RKfVAEgyp5HkAYJYo19w.png?q=20', 'https://miro.medium.com/max/1574/1*5AmPP_9kl_rZsrwIwoQ8dw.png', 'https://miro.medium.com/fit/c/160/160/1*Eue5o9nV1YB3JdkeduFCLQ.jpeg', 'https://miro.medium.com/max/2010/1*g6a8f4rqv4VWhGol1-sK0Q.png', 'https://miro.medium.com/max/2394/1*A5OoPixM1I1p-tx4_aElUA.png', 'https://miro.medium.com/max/60/1*A5OoPixM1I1p-tx4_aElUA.png?q=20', 'https://miro.medium.com/max/60/1*6i5vbeqyuvn7tRi2o4G0sg.png?q=20', 'https://miro.medium.com/max/60/1*AIOfl9B8pu9bizmlm2Qvlg.png?q=20', 'https://miro.medium.com/max/1200/1*6i5vbeqyuvn7tRi2o4G0sg.png', 'https://miro.medium.com/fit/c/80/80/0*70yYjDWQlfL9WjoD.jpg'}",2020-03-05 00:14:41.104532,1.4988207817077637
https://medium.com/google-cloud/datalab-bigquery-fast-dataset-queries-to-build-image-file-repository-857ec94275ae,Datalab + BigQuery = fast! dataset queries to build Image file repository,"Part 2: BigQuery is really fast for SQL queries on large datasets. Query, download image files from Open Images to Cloud Storage to build your own image repository.

“round orange light effects” by Sebastian Kanczok on Unsplash

You can use Google Datalab and BigQuery for fast database queries using SQL-like syntax; this is the 2nd in a series. The first article is here.

Google Datalab is an interactive tool created to explore, analyze, transform and visualize data and can build Machine Learning (ML) models; it runs on Google’s Cloud Compute Engine.

BigQuery is cloud-based big data analytics web service for fast processing of very large read-only data sets, using SQL-like syntax. Basically, what previously might have been done on a pc or network computer using dedicated resources and an installed database, can now be accessed through a computer with an internet connection. All the heavy lifting and processing is done in the cloud to achieve the same result in a more efficient manner.

Here’s an overview of our process — we will use Datalab to access Google BigQuery Open Images dataset. Open Images is a dataset consisting of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories. Google tried to make the dataset as practical as possible: the labels cover more real-life entities than the 1000 ImageNet classes, there are enough images to train a deep neural network from scratch.

Connecting to the public dataset is simple. Once connected, we will look at the tables, some of which are very large, and list their structure and peek into their data. The process is astonishingly fast!

Before you begin, ensure that:

You signed on to a Google Cloud account.

Google Compute Engine VM is created and active.

Machine Learning and Dataflow APIs are enabled.

You have an active project, Datalab and an active notebook.

The Open Images dataset has 4 tables. For each of the 4, we will:

list the # of rows

peek into each one

list the fields

preview a sample of the data content

run other queries [on some tables] to validate the speed of BigQuery!

Section 1: Look at the tables — structure, sample data, run some queries on them, view the results and understand the tables.

#import tools



import google.datalab.bigquery as bq

import matplotlib.pyplot as plot

import numpy as np

import pandas as pd

from pandas.io import gbq

import plotly.plotly as py

import plotly.graph_objs as go

from plotly.tools import FigureFactory as FF #describe the 1st table - dataset.table fields, types, and if available -> mode, description

%%bq tables describe --name ""bigquery-public-data.open_images.annotations_bbox""

Output showing annotations_bbox table structure

The 4 tables are annotations_bbox, dict, images, labels.

Here’s some operations on the annotations_bbox table (field list shown above)— source, label name, confidence (of the label with the image), x and y mins and max, and other image parameters.:

#define table1 and list # of data rows

table1=bq.Table('bigquery-public-data.open_images.annotations_bbox')

table1.metadata.rows

# of rows in annotations_bbox = 2,070,219

#query to show a sample of data from selected fields

bq.Query.from_table(table1).execute(sampling=bq.Sampling.default(

fields=['image_id',

'label_name',

'confidence',

'x_min',

'y_min',

'is_depiction'])).result()

Sample of selected fields from annotations_bbox table

Here’s another query on this large dataset — note the speed! the query below counts the # of images label sources — humans and machines.

%%bq query --name sources

SELECT source AS sname, COUNT(source) as watchnum

FROM `bigquery-public-data.open_images.labels`

GROUP BY sname #execute the query defined above

sources.execute().result()

Query result — 1.5s for 810MB processed!

Here’s another query that runs fast — this one is to count the different label name types across millions of rows — in the images table.

counts the # of images label sources — humans and machines.

%%bq query --name image_types

SELECT DISTINCT label_display_name as lname, COUNT(label_display_name)as namecount

FROM `bigquery-public-data.open_images.dict`

GROUP BY lname #execute the query defined above

image_types.execute().result()

Results for image types — 19,587 types of images classified

There are other queries that you can lookup in the github repo here, or the gist below.

Section 2: Selecting specific type of image, querying them, then opening the files from the specified URL and saving them

Create a query specifying images labelled as “traffic signs”

Get the urls for each of those files

Access, download and save each of them

Copy them to a storage bucket, presumably for Image processing or to build an AI model

#insert your project id

#then create a query specifying what kind of images you want to see; I wanted traffic signs, so my query is pulling titles with traffic sign in their names

# then put the results in a dataframe (df1)

projectid = ""input your project id""

test_query = """"""SELECT original_url FROM [bigquery-public-data.open_images.images] WHERE title LIKE '%traffic sign%' GROUP BY original_url""""""

df1 = gbq.read_gbq(test_query, projectid)

Our query for traffic sign images resulted in 75 hits

#insert your project id

import urllib

import urllib2

bucket = 'gs://' + 'open-images-urls/' + '*.jpg' bucket_storage = 'gs://' + 'open-images-urls/' #take dataframe and make it a list to make for easier reading

url_list=df1['original_url'].tolist()

#convert from unicode to list of bytes,string, otherwise there is a 'u' in front of each list element

my_list = [str(url_list[x]) for x in range(len(url_list))] #for each element of the list (each url containing 1 image of the type specified in the query)

#read the file

#initialize the counter y, then for each file save, update it, which also is used for adding to the file name that it will be saved as - for example, temp1, temp2 and so on

#once file is accessed through the url (website) and opened, save it for yourself to do ML image processing or whatever you want

#print a message after each file is saved

#----------------------------------------------

import cStringIO # *much* faster than StringIO

import urllib

from PIL import Image

y=0

for x in my_list:

file_write = ""temp"" + ""-"" + str(y) + "".jpg""

y = y + 1

file = urllib.urlopen(x)

im = cStringIO.StringIO(file.read()) # constructs a StringIO holding the image

img = Image.open(im)

img.save(file_write)

print(""writing file: "", file_write)

The files from the URLs matching our query for ‘traffic signs’ — being written to Datalab VM

Now, for the last step, we will copy the files to our (your) Google storage bucket, where you will presumably do all kinds of analysis or AI image modeling with them. The purpose of this exercise is to demonstrate how Google Bigquery can be used to query, isolate and download image files for processing from a large dataset. Most likely, you would have used the Datalab itself for image processing. But in many real work situations, you may need to look for and retrieve specific types of files for further processing. This is one way to do it.

#specify your Google cloud storage bucket

#use gsutil to copy the files to that bucket

bucket_storage = 'gs://' + 'open-images-urls/'

!gsutil cp '*.jpg' $bucket_storage

Files are copied with gsutil cp command to specified Google Storage bucket

Thank you for reading! Here are some other articles you might be interested in:","['file', 'traffic', 'files', 'fast', 'queries', 'query', 'list', 'dataset', 'image', 'repository', 'build', 'images', 'datalab', 'data', 'google', 'processing', 'bigquery']","Query, download image files from Open Images to Cloud Storage to build your own image repository.
BigQuery is cloud-based big data analytics web service for fast processing of very large read-only data sets, using SQL-like syntax.
Here’s an overview of our process — we will use Datalab to access Google BigQuery Open Images dataset.
The Open Images dataset has 4 tables.
Most likely, you would have used the Datalab itself for image processing.",en,['Hari Santanam'],2018-10-17 01:24:09.136000+00:00,"{'Datalab', 'Cloud', 'Bigquery', 'Big Data', 'Image Processing'}","{'https://miro.medium.com/max/60/0*C2qX21AlOw2dQYMa?q=20', 'https://miro.medium.com/max/900/1*d024Gb67DGgrrTKtZyprmQ.png', 'https://miro.medium.com/max/9856/0*C2qX21AlOw2dQYMa', 'https://miro.medium.com/max/912/1*Nw-xjIo9uWqAAughL8qS9A.png', 'https://miro.medium.com/max/60/1*G-T7mqJ6X1Xrw6aKlQUmdQ.png?q=20', 'https://miro.medium.com/max/1626/1*tz78jPpbv_Lp85zglI74mA.png', 'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/60/1*xBBcJWiOqOCyOkRhRCgNQQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*BqRktQCiXmLWWvxo3Mpetw.png?q=20', 'https://miro.medium.com/max/60/1*zGa5ox0m5TaDKtsgw9Z-FQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/max/1200/0*C2qX21AlOw2dQYMa', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/602/1*SsxvQR3kiwVwjEA_9dtKIQ.png', 'https://miro.medium.com/max/60/1*d024Gb67DGgrrTKtZyprmQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*MWS_W_S-BF2J-vKHYEYUlw.jpeg', 'https://miro.medium.com/max/60/1*tz78jPpbv_Lp85zglI74mA.png?q=20', 'https://miro.medium.com/max/44/1*SsxvQR3kiwVwjEA_9dtKIQ.png?q=20', 'https://miro.medium.com/max/1184/1*BqRktQCiXmLWWvxo3Mpetw.png', 'https://miro.medium.com/fit/c/160/160/1*MWS_W_S-BF2J-vKHYEYUlw.jpeg', 'https://miro.medium.com/max/60/1*qdkf7SPUNCvD8xZqxyWVow.png?q=20', 'https://miro.medium.com/max/2044/1*zGa5ox0m5TaDKtsgw9Z-FQ.png', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/1276/1*xBBcJWiOqOCyOkRhRCgNQQ.jpeg', 'https://miro.medium.com/max/60/1*Nw-xjIo9uWqAAughL8qS9A.png?q=20', 'https://miro.medium.com/max/992/1*qdkf7SPUNCvD8xZqxyWVow.png', 'https://miro.medium.com/max/1408/1*G-T7mqJ6X1Xrw6aKlQUmdQ.png'}",2020-03-05 00:14:43.101192,1.996659755706787
https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60,PCA using Python (scikit-learn),"Original image (left) with Different Amounts of Variance Retained

My last tutorial went over Logistic Regression using Python. One of the things learned was that you can speed up the fitting of a machine learning algorithm by changing the optimization algorithm. A more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA). If your learning algorithm is too slow because the input dimension is too high, then using PCA to speed it up can be a reasonable choice. This is probably the most common application of PCA. Another common application of PCA is for data visualization.

To understand the value of using PCA for data visualization, the first part of this tutorial post goes over a basic visualization of the IRIS dataset after applying PCA. The second part uses PCA to speed up a machine learning algorithm (logistic regression) on the MNIST dataset.

With that, let’s get started! If you get lost, I recommend opening the video below in a separate tab.

PCA using Python Video

The code used in this tutorial is available below

PCA for Data Visualization

PCA to Speed-up Machine Learning Algorithms

PCA for Data Visualization

For a lot of machine learning applications it helps to be able to visualize your data. Visualizing 2 or 3 dimensional data is not that challenging. However, even the Iris dataset used in this part of the tutorial is 4 dimensional. You can use PCA to reduce that 4 dimensional data into 2 or 3 dimensions so that you can plot and hopefully understand the data better.

Load Iris Dataset

The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below will load the iris dataset.

import pandas as pd url = "" https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data # load dataset into Pandas DataFrame

df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])

Original Pandas df (features + target)

Standardize the Data

PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.

from sklearn.preprocessing import StandardScaler features = ['sepal length', 'sepal width', 'petal length', 'petal width'] # Separating out the features

x = df.loc[:, features].values # Separating out the target

y = df.loc[:,['target']].values # Standardizing the features

x = StandardScaler().fit_transform(x)

The array x (visualized by a pandas dataframe) before and after standardization

PCA Projection to 2D

The original data has 4 columns (sepal length, sepal width, petal length, and petal width). In this section, the code projects the original data which is 4 dimensional into 2 dimensions. I should note that after dimensionality reduction, there usually isn’t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation.

from sklearn.decomposition import PCA pca = PCA(n_components=2) principalComponents = pca.fit_transform(x) principalDf = pd.DataFrame(data = principalComponents

, columns = ['principal component 1', 'principal component 2'])

PCA and Keeping the Top 2 Principal Components

finalDf = pd.concat([principalDf, df[['target']]], axis = 1)

Concatenating DataFrame along axis = 1. finalDf is the final DataFrame before plotting the data.

Concatenating dataframes along columns to make finalDf before graphing

Visualize 2D Projection

This section is just plotting 2 dimensional data. Notice on the graph below that the classes seem well separated from each other.

fig = plt.figure(figsize = (8,8))

ax = fig.add_subplot(1,1,1)

ax.set_xlabel('Principal Component 1', fontsize = 15)

ax.set_ylabel('Principal Component 2', fontsize = 15)

ax.set_title('2 component PCA', fontsize = 20) targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']

colors = ['r', 'g', 'b']

for target, color in zip(targets,colors):

indicesToKeep = finalDf['target'] == target

ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']

, finalDf.loc[indicesToKeep, 'principal component 2']

, c = color

, s = 50)

ax.legend(targets)

ax.grid()

2 Component PCA Graph

Explained Variance

The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert 4 dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this. By using the attribute explained_variance_ratio_, you can see that the first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. Together, the two components contain 95.80% of the information.

pca.explained_variance_ratio_

PCA to Speed-up Machine Learning Algorithms

One of the most important applications of PCA is for speeding up machine learning algorithms. Using the IRIS dataset would be impractical here as the dataset only has 150 rows and only 4 feature columns. The MNIST database of handwritten digits is more suitable as it has 784 feature columns (784 dimensions), a training set of 60,000 examples, and a test set of 10,000 examples.

Download and Load the Data

You can also add a data_home parameter to fetch_mldata to change where you download the data.

from sklearn.datasets import fetch_mldata mnist = fetch_mldata('MNIST original')

The images that you downloaded are contained in mnist.data and has a shape of (70000, 784) meaning there are 70,000 images with 784 dimensions (784 features).

The labels (the integers 0–9) are contained in mnist.target. The features are 784 dimensional (28 x 28 images) and the labels are simply numbers from 0–9.

Split Data into Training and Test Sets

Typically the train test split is 80% training and 20% test. In this case, I chose 6/7th of the data to be training and 1/7th of the data to be in the test set.

from sklearn.model_selection import train_test_split # test_size: what proportion of original data is used for test set

train_img, test_img, train_lbl, test_lbl = train_test_split( mnist.data, mnist.target, test_size=1/7.0, random_state=0)

Standardize the Data

The text in this paragraph is almost an exact copy of what was written earlier. PCA is effected by scale so you need to scale the features in the data before applying PCA. You can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the dataset’s features. Note you fit on the training set and transform on the training and test set. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() # Fit on training set only.

scaler.fit(train_img) # Apply transform to both the training set and the test set.

train_img = scaler.transform(train_img)

test_img = scaler.transform(test_img)

Import and Apply PCA

Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.

from sklearn.decomposition import PCA # Make an instance of the Model

pca = PCA(.95)

Fit PCA on training set. Note: you are fitting PCA on the training set only.

pca.fit(train_img)

Note: You can find out how many components PCA choose after fitting the model using pca.n_components_ . In this case, 95% of the variance amounts to 330 principal components.

Apply the mapping (transform) to both the training set and the test set.

train_img = pca.transform(train_img)

test_img = pca.transform(test_img)

Apply Logistic Regression to the Transformed Data

Step 1: Import the model you want to use

In sklearn, all machine learning models are implemented as Python classes

from sklearn.linear_model import LogisticRegression

Step 2: Make an instance of the Model.

# all parameters not specified are set to their defaults

# default solver is incredibly slow which is why it was changed to 'lbfgs'

logisticRegr = LogisticRegression(solver = 'lbfgs')

Step 3: Training the model on the data, storing the information learned from the data

Model is learning the relationship between digits and labels

logisticRegr.fit(train_img, train_lbl)

Step 4: Predict the labels of new data (new images)

Uses the information the model learned during the model training process

The code below predicts for one observation

# Predict for One Observation (image)

logisticRegr.predict(test_img[0].reshape(1,-1))

The code below predicts for multiple observations at once

# Predict for One Observation (image)

logisticRegr.predict(test_img[0:10])

Measuring Model Performance

While accuracy is not always the best metric for machine learning algorithms (precision, recall, F1 Score, ROC Curve, etc would be better), it is used here for simplicity.

logisticRegr.score(test_img, test_lbl)

Timing of Fitting Logistic Regression after PCA

The whole point of this section of the tutorial was to show that you can use PCA to speed up the fitting of machine learning algorithms. The table below shows how long it took to fit logistic regression on my MacBook after using PCA (retaining different amounts of variance each time).

Time it took to fit logistic regression after PCA with different fractions of Variance Retained

Image Reconstruction from Compressed Representation

The earlier parts of the tutorial have demonstrated using PCA to compress high dimensional data to lower dimensional data. I wanted to briefly mention that PCA can also take the compressed representation of the data (lower dimensional data) back to an approximation of the original high dimensional data. If you are interested in the code that produces the image below, check out my github.

Original Image (left) and Approximations (right) of the original data after PCA

Closing Thoughts

This is a post that I could have written on for a lot longer as PCA has many different uses. I hope this post helps you with whatever you are working on. My next machine learning tutorial goes over Understanding Decision Trees for Classification (Python). If you any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter. If you want to learn how I made some of my graphs or how to utilize Pandas, Matplotlib, or Seaborn libraries, please consider taking my Python for Data Visualization LinkedIn Learning course.","['machine', 'dimensional', 'pca', 'python', 'learning', 'scikitlearn', 'principal', 'data', 'variance', 'training', 'component', 'using']","A more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA).
To understand the value of using PCA for data visualization, the first part of this tutorial post goes over a basic visualization of the IRIS dataset after applying PCA.
PCA using Python VideoThe code used in this tutorial is available belowPCA for Data VisualizationPCA to Speed-up Machine Learning AlgorithmsPCA for Data VisualizationFor a lot of machine learning applications it helps to be able to visualize your data.
pca.explained_variance_ratio_PCA to Speed-up Machine Learning AlgorithmsOne of the most important applications of PCA is for speeding up machine learning algorithms.
I wanted to briefly mention that PCA can also take the compressed representation of the data (lower dimensional data) back to an approximation of the original high dimensional data.",en,['Michael Galarnyk'],2019-11-04 17:56:19.628000+00:00,"{'Data Science', 'Scikit Learn', 'Python', 'Machine Learning', 'Pca'}","{'https://miro.medium.com/max/2100/1*Gob8ZbScyM7hHUHjvrMJYg.png', 'https://miro.medium.com/max/60/1*duZ0MeNS6vfc35XtYr88Bg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1050/1*Gob8ZbScyM7hHUHjvrMJYg.png', 'https://miro.medium.com/max/60/1*Qxyo-uDrmsUzdxIe7Nnsmg.png?q=20', 'https://miro.medium.com/max/3200/1*xL0FR9PGmu0GmZmyLCUyzw.png', 'https://miro.medium.com/fit/c/160/160/2*FsG9-gMI-jiPBDcH0LLl7g.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*xKUK0wLnLHAJYS1zbt-7wA.png?q=20', 'https://miro.medium.com/max/3200/1*Qxyo-uDrmsUzdxIe7Nnsmg.png', 'https://miro.medium.com/max/3200/1*4Q1kH0zKeHrnHF7Eg_yhTQ.png', 'https://miro.medium.com/max/2186/1*duZ0MeNS6vfc35XtYr88Bg.png', 'https://miro.medium.com/max/1704/1*xKUK0wLnLHAJYS1zbt-7wA.png', 'https://miro.medium.com/max/3006/1*7jUCr36YguAMKNHTN4Gt8A.png', 'https://miro.medium.com/max/60/1*xL0FR9PGmu0GmZmyLCUyzw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/2*FsG9-gMI-jiPBDcH0LLl7g.jpeg', 'https://miro.medium.com/max/60/1*Qt_pYlwBeHtTewnEdksYKQ.png?q=20', 'https://miro.medium.com/max/60/1*Gob8ZbScyM7hHUHjvrMJYg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*4Q1kH0zKeHrnHF7Eg_yhTQ.png?q=20', 'https://miro.medium.com/max/60/1*7jUCr36YguAMKNHTN4Gt8A.png?q=20', 'https://miro.medium.com/max/1644/1*Qt_pYlwBeHtTewnEdksYKQ.png'}",2020-03-05 00:14:49.745478,6.644285440444946
https://medium.com/the-infinite-machine/python-app-engine-2017-building-a-simple-flask-app-b55f60755204,Python App Engine 2017: Building a simple Flask app,"This article is part of a series where I’m intending to dig into App Engine deeply, investigating what’s really going on under the covers by testing hypotheses, performing experiments.

To do that I need a simple App Engine application that I can add experiments to, then run them and get results.

Also, in this previous article I wrote about getting up to speed with the recent changes in App Engine. I’ve been using App Engine for years, but I haven’t paid attention to changes in tooling and techniques until now, so I’m making an effort to get up to date.

If I’m going to build an hypothesis testing app, why not use the latest recommended App Engine techniques, and maybe even learn something along the way?

Having worked through the Quickstart guide, I saw this:

urh, what’s a Flask?

I’ve been using webapp2 until now. I’ve never heard of this Flask thing. Maybe the next bit of the docs, Getting Started with Flask on App Engine Standard Environment is illuminating?

Nope, I still don’t know what the taters are. Ok Google, what’s the taters?

Googling gives me this link about Flask. Apparently it’s a whole thing…

Oh, a microframework based Werkzeug, Jinja 2 and good intentions? uh. Well, I do know what Jinja 2 is, that’s the templating system I’ve been using with webapp2. Nice.

What does Flask’s webpage say?

Oh, Flask is fun! Well, you should have told me that before!

No, not as fun as this.

But that little code snippet tells me a lot.

Apparently I can just make a Flask “app” object (hopefully a WSGI app?) by passing in my module name (because why?). But what’s cool is that you can decorate a function with a route and turn it into a web handler:

@app.route(""/"")

def hello():

return ""Hello World!""

Really? That’s pretty awesome. In webapp2, you have to make a stodgy class, descending from webapp2.requesthandler , and it all feels a bit more clunky than this. I actually don’t like using classes all that much in Python; they’re a necessity in App Engine particularly for ndb, but I’d prefer to stay in pure function land. I’ll have more to say about that in subsequent articles. But for now, nice.

Ok, so how do we do this in App Engine? Let’s go work through Getting Started with Flask on App Engine Standard Environment.

Getting Started with Flask on App Engine Standard Environment

Ok. In the Quickstart article I managed to get 1 and 2 done. Let’s look at 3.

First of all, Eclipse and PyDev? I know a lot of people will grumble about those, but that’s what I already use. Tops. However, since I installed gcloud I don’t have things quite hooked up correctly; the servicesdemo project has a lot of red underlines and none of the nice IDE features work because I haven’t got source paths and etc set up.

I just… I just can’t work like this :-(

Ok. So I’m going through Getting Started with Flask on App Engine Standard Environment. Yup, got python, got eclipse, got pydev. Now to create a project:

Cool, ok, let’s create one of those. This is going to be my hypothesis testing app, and it’s going to be deployed to my emlyn-experiments project. So let’s just call it emlyn-experiments, ok.

Now the interesting bit:

Ok Google, where on God’s green earth are my App Engine source folders?

So before gcloud, I downloaded the App Engine sdk, put it in a folder, and you know, that’s where it was. So this step was easy. But gcloud installed as a package in that a-la-peanut-butter-sandwiches magic debian package way. So, where’d it go? Let’s click “Where is it?” in the docs:

uh, no

Ok Google, that’s bullshit.

A bit of googling, I can’t find anyone that knows. Stack Overflow doesn’t even seem to know.

aaaaaaaaaahhhhhhhh!!!!!

Ok, let’s just fall back to scratching around in the filesystem like a rodent, dammit.

I found something promising here: /usr/lib/google-cloud-sdk/platform/google_appengine

You can see me pasting in the folder, and BAM, we have liftoff. Let’s just assume this is correct and I’ll go crying later if it’s wrong.

Ok, the next bit says we’re going to make a user comment form, but I don’t really want to. Let’s follow the guide, but create the hypothesis testing app.

The Hypothesis Testing App

Before I keep going, I’d better have a little thinky about what this app will be.

It needs to do let me add experiments, run them, and get results.

There are a lot of ways this could be. An heuristic I use is, when you have multiple options and you don’t know which to use, do the simplest thing.

The simplest thing would be this:

There is a main page, which is just a list of links, a big switchboard. Each is an experiment.

To add an experiment, I add code to the app and deploy it. That should include adding a link for that experiment somehow.

To run the experiment, I click the link.

How to see results? I think I can assume there’ll be a GCP Datastore object which is some kind of top level reporting object for an experiment. A generic way to view this object would be to have it dump out a JSON representation of itself, and slap that on a page. I could just override to_dict() .

So I’ll need a report page, which takes a datastore object key, retrieves the object, calls to_dict() , and shows me the result. If the experiment is underway, I can refresh the page to get an update.

Then, the link that kicks off an experiment should:

kick off the experiment

send me to the report page with the key for an object that’ll show me results.

App of the Year!!!

So, for this Flask app, I’ll need two horrible server side pages. That sounds doable!

Building the app

So let’s look at this Flask quickstart guide, see how to do this.

Ok. The Hypothesis Testing app can use this structure, but I’ll need to modify it a little.

I probably wont use any CSS (because Times New Roman is how I roll).

I’ll need two templates, one for the experiment switchboard and one for the report page.

I might break main.py out into multiple files (switchboard and report).

And I’ll be needing a background service separate from the default service, so I’ll need background.yaml , and a queue .yaml to define the background queue (see my post on Services for a quick explanation of this, lol).

I’ll create a bunch of blank files for now. Here:

What’s the next bit? Oh, “Setting up libraries to enable development”. Ok. And a quick read…

Vendoring!

vendoring is built in… cookieeeee?

So, Python has a whole package management system, as do many languages. It’s normal to install packages using pip install . They go somewhere magic on your machine, and Python can just find them.

However, App Engine doesn’t support that. Instead, we need to add packages to our code base, and deploy them along with the rest of our code.

The package management system supports installing packages to a specific directory instead of to the magic place. Then, in your App Engine code, you need to do a thing called vendoring to make Python find these packages at run time.

When I first started with App Engine, I didn’t know about vendoring, and couldn’t make head or tail of how to make Python’s import mechanisms do anything smart. Python’s import mechanisms drive me crazy generally, they have turned my hair grey and made most of it fall out.

Then, I found out about vendoring, and have slowly been converting over to it. But it was a custom solution by a third party developer, involving arcane python files and holding your mouth just so as you sprinkle the material components into the sacrificial urn.

Now, here’s Google just casually telling me that I’m Doing It Wrong:

So apparently there’s a built in library, google.appengine.ext.vendor, that just does this for me.

I will note that the way I do this in a production app is to use a requirements.txt file containing my packages (eg: flask ), and use a build process to invoke pip on this, sorting out these third party dependencies in an orderly fashion. But this method will do fine for this app.

Note: appengine_config.py is some kind of magic bean. I think it runs once when a new instance is initialised for one of your app versions, but I don’t know exactly. It might be something to experiment on a bit another day.

So I’ve added appengine_config.py as above, and installed flask with the pip command. Here’s my app folder now:

That escalated quickly…

Ooh, that installed a lot of stuff in the lib folder. Cool bananas.

Setting up the yamls

The next bit of the guide says to set up app.yaml. I’ll set up app.yaml, queue.yaml, and background.yaml , like this:

app.yaml

runtime: python27

api_version: 1

threadsafe: true

automatic_scaling:

max_idle_instances: 2



handlers:

- url: /.*

script: main.app builtins:

- deferred: on

background.yaml

runtime: python27

api_version: 1

threadsafe: true

service: background

automatic_scaling:

max_idle_instances: 2 handlers:

- url: /.*

script: main.app builtins:

- deferred: on

queue.yaml

queue:

- name: default

rate: 100/s

- name: background

target: background

rate: 100/s

See Python App Engine 2017: Prioritisation of tasks using Services for details on why I’ve done this.

Creating the basic pages

Ok. The next bit of the guide is about setting up handler. I need two GET handlers, one for the switchboard, one for the report page.

I’ll put the switchboard at / , and the report page at /report .

For the first shot at this, I’ll make both these pages not really do anything.

Note: To get Eclipse to play nicely with the vendored libraries, I’ve added the lib folder to the project’s PYTHONPATH, like this:

Ok, here’s my main.py:

import logging from flask import Flask app = Flask(__name__)

def switchboard():

return 'Switchboard' @app .route('/')def switchboard():return 'Switchboard'

def report():

return 'Report' @app .route('/report')def report():return 'Report'

def server_error(e):

# Log the error and stacktrace.

logging.exception('An error occurred during a request.')

return 'An internal error occurred.', 500 @app .errorhandler(500)def server_error(e):# Log the error and stacktrace.logging.exception('An error occurred during a request.')return 'An internal error occurred.', 500

And let’s run it:

Makes sense?

That works. Now, let’s refactor this a bit to put switchboard and report in their own files.

main.py

import logging from flask import Flask app = Flask(__name__) from handlers.switchboard import get_switchboard

from handlers.report import get_report get_switchboard(app)

get_report(app)

def server_error(e):

# Log the error and stacktrace.

logging.exception('An error occurred during a request.')

return 'An internal error occurred.', 500 @app .errorhandler(500)def server_error(e):# Log the error and stacktrace.logging.exception('An error occurred during a request.')return 'An internal error occurred.', 500

I’ll throw an empty __init__.py file into the handlers folder, so it works as a package. Then,

switchboard.py





def switchboard():

return 'Switchboard' def get_switchboard(app): @app .route('/')def switchboard():return 'Switchboard' return switchboard

report.py





def report():

return 'Report' def get_report(app): @app .route('/report')def report():return 'Report' return report

I don’t know if that’s recommended, but it seems like a good structure to me. Ok.

Switchboard

Here I want to list the experiments, one button per experiment.

I don’t have any experiments yet, so let’s make a hello world experiment, which creates a hello world object that we can report on.

Model

By object, I mean a hello world model object. I’ll pop it in a model folder.

Like this:

model/helloworld.py

from google.appengine.ext import ndb class HelloWorld(ndb.model.Model):

stored = ndb.DateTimeProperty(auto_now_add=True)

updated = ndb.DateTimeProperty(auto_now=True) def to_dict(self):

return {

""key"": self.key.urlsafe() if self.key else None,

""stored"": str(self.stored) if self.stored else None,

""updated"": str(self.updated) if self.stored else None

}

I don’t want to dive into ndb model objects here. The gist is that this class is something you can store to the Google Cloud Platform Datastore, and load back up. It’s got two persistent attributes, stored and updated, which are automatically set to the time the object was first stored, and the time the object was last updated, respectively. It also has a Key, which is implied.

I’ve added a to_dict() method which returns a JSON compatible dictionary full of attributes in the object. We’ll need that for report.

Experiments

I need a way to define an experiment. Let’s do it in a functional way.

I need an experiment to have

a displayable name, and

a function that takes no arguments, actually kicks off the experiment, and returns the key to a datastore object which I can report on.

Here’s a HelloWorld experiment.

experiments/helloworld.py

from model.helloworld import HelloWorld def HelloWorldExperiment():

def Go():

lhw = HelloWorld()

lhw.put()

return lhw.key



return ""Hello World"", Go

The Go method actually performs the experiment. It creates and stores a HelloWorld model object, and returns its key.

HelloWorldExperiment returns a pair; the name of the experiment, and the Go function.

Great. Now how do we do the switchboard?

Switchboard

Here’s an updated switchboard function:

handlers/switchboard.py

from flask import render_template from experiments.helloworld import HelloWorldExperiment



def switchboard():

experiments = [

HelloWorldExperiment()

]



return render_template(

""switchboard.html"",

experiments = experiments

) def get_switchboard(app): @app .route('/')def switchboard():experiments = [HelloWorldExperiment()return render_template(""switchboard.html"",experiments = experiments return switchboard

There are a few new concepts here.

In the inner switchboard function, I’m constructing a list of experiments (just what is returned by HelloWorldExperiment at the moment, ie: the pair of name and function), then passing it to render_template. This magically finds templates/switchboard.html , and renders it using jinja2 with the array experiments in the context.

Remember that HelloWorldExperiment() returns a pair of name and function? This means experiments is a list of these pairs.

So now we need a template that does something useful with this array.

templates/switchboard.html

<html>

<head>

<title>Experiments</title>

</head>

<body>

<h1>Experiments</h1>

{% for ex in experiments %}

<p>

<a href=""report?key={{ex[1]().urlsafe()}}"">{{ex[0]}}</a>

</p>

{% endfor %}

</body>

</html>

This iterates through the experiments, showing each one as a link with the text being the experiment name, and the href being reports?key=<key> . <key> is the key returned from calling the experiment function; the urlsafe() method turns it into something we can put on the querystring.

Lastly, let’s throw in a simple update to the report handler which will just display the key.

handlers/report.py

from google.appengine.ext import ndb

from flask import request



def report():

keystr = request.args.get('key')

return str(ndb.Key(urlsafe=keystr))

return report def get_report(app): @app .route('/report')def report():keystr = request.args.get('key')return str(ndb.Key(urlsafe=keystr))return report

Let’s take a look:

Let’s click Hello World :

That’s working; you’re looking at the string representation of the ndb Key for the model object that we created. Nice!

Ok, let’s do the report page in earnest.

Report

Alright. This is going to work as follows:

get the key

load the object for the key

call to_dict() on it

convert that to a string representation

return that as the report.

Here it is:

handlers/report.py

from google.appengine.ext import ndb

from flask import request, render_template

import json



def report():

keystr = request.args.get('key')

key = ndb.Key(urlsafe=keystr)

obj = key.get()

return render_template(

""report.html"",

objjson = json.dumps(

obj.to_dict(),

indent=2,

sort_keys=True

),

keystr = keystr

) def get_report(app): @app .route('/report')def report():keystr = request.args.get('key')key = ndb.Key(urlsafe=keystr)obj = key.get()return render_template(""report.html"",objjson = json.dumps(obj.to_dict(),indent=2,sort_keys=True),keystr = keystr return report

You can see it gets the string representation of the key, constructs the key, loads the object using the key, then renders the template, passing a json string representation of the object (this uses to_dict() on the model object).

What about the template?

<html>

<head>

<title>Report for {{keystr}}</title>

<link rel=""stylesheet"" type=""text/css"" href=""/static/style.css"">

</head>

<body>

<h1>Report for {{keystr}}</h1>

<h3><a href=""/"">< experiments</a></h3>

<pre>{{objjson|safe}}</pre>

</body>

</html>

Simple template; just grabs the json and throws it onto the page.

Let’s refresh the report page and see what it looks like now:

That’s pretty good!

I’m going to upload this to app_engine:

and give it a shot…

Success!

—

Ok, that’s a basic (!) framework I can use for experiments on App Engine in future articles. Flask was pretty awesome, ey? That’s some clean code!

btw, here’s the repo:

Thanks for reading this.","['object', 'report', 'import', 'lets', '2017', 'experiment', 'simple', 'python', 'need', 'flask', 'engine', 'experiments', 'building', 'app']","Also, in this previous article I wrote about getting up to speed with the recent changes in App Engine.
If I’m going to build an hypothesis testing app, why not use the latest recommended App Engine techniques, and maybe even learn something along the way?
Maybe the next bit of the docs, Getting Started with Flask on App Engine Standard Environment is illuminating?
Apparently I can just make a Flask “app” object (hopefully a WSGI app?)
main.pyimport logging from flask import Flask app = Flask(__name__) from handlers.switchboard import get_switchboardfrom handlers.report import get_report get_switchboard(app)get_report(app)def server_error(e):# Log the error and stacktrace.",en,"[""Emlyn O'Regan""]",2017-02-04 07:49:49.134000+00:00,"{'Google Cloud Platform', 'App Engine', 'Python', 'Google App Engine'}","{'https://miro.medium.com/max/60/1*y__4uYhn3NkT9S4C594stA.png?q=20', 'https://miro.medium.com/max/960/1*LA6ZnPhXv5m40LiFpedkfw.gif', 'https://miro.medium.com/max/1000/1*CDsvr4xvvx8mOejr1RwWKg.gif', 'https://miro.medium.com/max/960/1*CWtnDJeXLgBG0PuVKYOHrg.gif', 'https://miro.medium.com/max/60/1*fUo9p1TAwUxm5CpSPM0kTg.png?q=20', 'https://miro.medium.com/max/60/1*UQJwzGLSduYD27i-AK447A.png?q=20', 'https://miro.medium.com/max/1706/1*oFMAIA3QOFVI2uSpx0L_dg.png', 'https://miro.medium.com/max/1718/1*R_d0-IoN-DrGkn4NirJSTg.png', 'https://miro.medium.com/max/60/1*BlNKpCutTkc0k_VobWcEBA.png?q=20', 'https://miro.medium.com/max/1324/1*xG5KZ-lhe-QoPgT-G_UT5w.png', 'https://miro.medium.com/freeze/max/60/1*ol3RGka2C-h62SKX_1ZOYw.gif?q=20', 'https://miro.medium.com/max/60/1*D13KAqMZqDEeQ-XToZn3Ug.png?q=20', 'https://miro.medium.com/max/322/1*hYaqy3xB0OXWfk0LmdOeOw.png', 'https://miro.medium.com/max/60/1*R_d0-IoN-DrGkn4NirJSTg.png?q=20', 'https://miro.medium.com/max/60/1*z3UyQpKgzRnulwNGF_ccNw.png?q=20', 'https://miro.medium.com/max/1000/1*ol3RGka2C-h62SKX_1ZOYw.gif', 'https://miro.medium.com/max/602/1*Vq8HL766RyG7Tkxx3j_dYQ.gif', 'https://miro.medium.com/freeze/max/60/1*LA6ZnPhXv5m40LiFpedkfw.gif?q=20', 'https://miro.medium.com/max/2718/1*Oo8EyJBjBPoilPpKFnynAw.png', 'https://miro.medium.com/freeze/max/60/1*CDsvr4xvvx8mOejr1RwWKg.gif?q=20', 'https://miro.medium.com/max/60/1*wnUDBXwHIEJ47GAtnrcHnA.png?q=20', 'https://miro.medium.com/max/2880/1*wnUDBXwHIEJ47GAtnrcHnA.png', 'https://miro.medium.com/max/1154/1*0A32ElXy1qKodWy1_hUHTg.png', 'https://miro.medium.com/max/640/1*OjWkD-leGSCkbd3WfeJ8qw.jpeg', 'https://miro.medium.com/max/3058/1*-dIAYyvrZIDuThHQzNcBuA.png', 'https://miro.medium.com/max/60/1*OjWkD-leGSCkbd3WfeJ8qw.jpeg?q=20', 'https://miro.medium.com/max/906/1*D13KAqMZqDEeQ-XToZn3Ug.png', 'https://miro.medium.com/freeze/max/60/1*A5kKd8J3_8TVCeHFgaE4pg.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*RRUfAf9hJnccGrgDFXmtAg.png', 'https://miro.medium.com/max/60/1*hYaqy3xB0OXWfk0LmdOeOw.png?q=20', 'https://miro.medium.com/max/36/1*sKJQExmD9VF5hVoL_kakug.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*e6T1Qq7B9VQJcAajAMlmCQ.jpeg', 'https://miro.medium.com/max/60/1*-dIAYyvrZIDuThHQzNcBuA.png?q=20', 'https://miro.medium.com/max/96/1*e6T1Qq7B9VQJcAajAMlmCQ.jpeg', 'https://miro.medium.com/max/1106/1*cVgG8w0n1K9InN-zVGsdEg.jpeg', 'https://miro.medium.com/max/572/1*dGF0yJkB22IM91Ln5vaLEg.png', 'https://miro.medium.com/max/1424/1*fUo9p1TAwUxm5CpSPM0kTg.png', 'https://miro.medium.com/max/2880/1*84h6gbcZufvcl5PZ4YY24A.png', 'https://miro.medium.com/max/60/1*xG5KZ-lhe-QoPgT-G_UT5w.png?q=20', 'https://miro.medium.com/max/1762/1*Fz_DJsvcaclaUmvcmuJs-A.png', 'https://miro.medium.com/max/1730/1*UQJwzGLSduYD27i-AK447A.png', 'https://miro.medium.com/fit/c/160/160/0*wWBGE-SflM2pWgTi.jpg', 'https://miro.medium.com/fit/c/96/96/0*wWBGE-SflM2pWgTi.jpg', 'https://miro.medium.com/freeze/max/60/1*CWtnDJeXLgBG0PuVKYOHrg.gif?q=20', 'https://miro.medium.com/max/60/1*0A32ElXy1qKodWy1_hUHTg.png?q=20', 'https://miro.medium.com/max/760/1*sKJQExmD9VF5hVoL_kakug.png', 'https://miro.medium.com/max/1200/1*-dIAYyvrZIDuThHQzNcBuA.png', 'https://miro.medium.com/max/60/1*cVgG8w0n1K9InN-zVGsdEg.jpeg?q=20', 'https://miro.medium.com/max/60/1*kd0YaXaDmGp_VikBpCLaIg.png?q=20', 'https://miro.medium.com/max/60/1*oFMAIA3QOFVI2uSpx0L_dg.png?q=20', 'https://miro.medium.com/max/60/1*iJUyaA9kcjZz4U2pU3s8Nw.png?q=20', 'https://miro.medium.com/max/1122/1*A5kKd8J3_8TVCeHFgaE4pg.gif', 'https://miro.medium.com/max/60/1*Oo8EyJBjBPoilPpKFnynAw.png?q=20', 'https://miro.medium.com/max/1444/1*iJUyaA9kcjZz4U2pU3s8Nw.png', 'https://miro.medium.com/max/60/1*84h6gbcZufvcl5PZ4YY24A.png?q=20', 'https://miro.medium.com/max/60/0*OJWVbW2KvuXwe0O9.png?q=20', 'https://miro.medium.com/max/1412/1*z3UyQpKgzRnulwNGF_ccNw.png', 'https://miro.medium.com/max/2144/1*kd0YaXaDmGp_VikBpCLaIg.png', 'https://miro.medium.com/max/60/1*dGF0yJkB22IM91Ln5vaLEg.png?q=20', 'https://miro.medium.com/max/1678/1*BlNKpCutTkc0k_VobWcEBA.png', 'https://miro.medium.com/freeze/max/48/1*Vq8HL766RyG7Tkxx3j_dYQ.gif?q=20', 'https://miro.medium.com/max/1622/1*y__4uYhn3NkT9S4C594stA.png', 'https://miro.medium.com/max/1050/0*OJWVbW2KvuXwe0O9.png', 'https://miro.medium.com/max/60/1*Fz_DJsvcaclaUmvcmuJs-A.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*wWBGE-SflM2pWgTi.jpg', 'https://miro.medium.com/max/94/1*e6T1Qq7B9VQJcAajAMlmCQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*QxjoeD_w0I12iWy2unDWOA.jpeg'}",2020-03-05 00:14:51.770475,2.023913621902466
https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80,Gradient Boosting vs Random Forest,"In this post, I am going to compare two popular ensemble methods, Random Forests (RF) and Gradient Boosting Machine (GBM). GBM and RF both are ensemble learning methods and predict (regression or classification) by combining the outputs from individual trees (we assume tree-based GBM or GBT). They have all the strengths and weaknesses of the ensemble methods mentioned in my previous post. So, here we compare them only with respect to each other.

GBM and RF differ in the way the trees are built: the order and the way the results are combined. It has been shown that GBM performs better than RF if parameters tuned carefully [1,2].

Gradient Boosting: GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree.

A) Real-world application

A great application of GBM is anomaly detection in supervised learning settings where data is often highly unbalanced such as DNA sequences, credit card transactions or cybersecurity.

Reference [3] presents a more specific application in this context, supervised anomaly detection task with a learning to rank approach. Learning to rank means the application of machine learning in the construction of ranking models for information retrieval systems. This results in finding the anomalies with the highest precision without giving too many genuine examples to the experts. According to this manuscript, gradient boosting has shown to be a powerful method on real-life datasets to address learning to rank problems due to its two main features:

It performs the optimization in function space (rather than in parameter space) which makes the use of custom loss functions much easier.

Boosting focuses step by step on difficult examples that give a nice strategy to deal with unbalanced datasets by strengthening the impact of the positive class.

B) Strengths of the model

Since boosted trees are derived by optimizing an objective function, basically GBM can be used to solve almost all objective function that we can write gradient out. This including things like ranking and poission regression, which RF is harder to achieve.

C) Weaknesses of the model","['forest', 'function', 'rf', 'learning', 'methods', 'gbm', 'vs', 'trees', 'random', 'ensemble', 'rank', 'boosting', 'gradient']","In this post, I am going to compare two popular ensemble methods, Random Forests (RF) and Gradient Boosting Machine (GBM).
GBM and RF both are ensemble learning methods and predict (regression or classification) by combining the outputs from individual trees (we assume tree-based GBM or GBT).
They have all the strengths and weaknesses of the ensemble methods mentioned in my previous post.
Gradient Boosting: GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree.
Learning to rank means the application of machine learning in the construction of ranking models for information retrieval systems.",en,['Abolfazl Ravanshad'],2019-08-01 17:16:45.742000+00:00,"{'Data Science', 'Ensemble Learning', 'Machine Learning', 'Data Analysis', 'Algorithms'}","{'https://miro.medium.com/fit/c/80/80/0*Hi65oWiCFGkLUPoA.jpg', 'https://miro.medium.com/fit/c/80/80/2*i9nJ2AW_szQp4jOe4qntjw.jpeg', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/160/160/1*oS-tqrDhil00hmxgvNEjzw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*czLBAIH3EyO7s9kTPMWPDQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*oS-tqrDhil00hmxgvNEjzw.jpeg'}",2020-03-05 00:14:52.846214,1.074739933013916
https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248,Decision Trees Explained Easily,"Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.

Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.

Simple Decision Tree

These are similar to flowcharts.","['set', 'regression', 'rules', 'nodes', 'decision', 'trees', 'tree', 'node', 'easily', 'classification', 'explained', 'smaller']","Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression.
Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules.
The final result is a tree with decision nodes and leaf nodes.
The topmost decision node in a tree which corresponds to the best predictor called root node.
Decision trees can handle both categorical and numerical data.",en,['Chirag Sehra'],2018-01-19 18:55:19.105000+00:00,"{'Explained', 'Decision Tree', 'Machine Learning', '12 Steps', 'Entropy'}","{'https://miro.medium.com/max/1636/1*UkQ2GHsopx3P1AiGWN7f0Q.png', 'https://miro.medium.com/max/60/1*UkQ2GHsopx3P1AiGWN7f0Q.png?q=20', 'https://miro.medium.com/max/60/1*nSP14B4NqKCfP-tlo8D9vA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*AwaLx8M4i8dEBWi9lik4fw.jpeg', 'https://miro.medium.com/max/60/1*JAEY3KP7TU2Q6HN6LasMrw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*BeCp-PVf0I4MCLIG', 'https://miro.medium.com/fit/c/96/96/1*VjBMyS2KLmsO-cgTfPEGlw.jpeg', 'https://miro.medium.com/fit/c/80/80/0*AxCvJ6lCPLNlEIMH.jpg', 'https://miro.medium.com/max/2560/1*nSP14B4NqKCfP-tlo8D9vA.jpeg', 'https://miro.medium.com/max/1200/1*nSP14B4NqKCfP-tlo8D9vA.jpeg', 'https://miro.medium.com/max/698/0*KLhgaRH43lGDOKSN.png', 'https://miro.medium.com/fit/c/160/160/1*VjBMyS2KLmsO-cgTfPEGlw.jpeg', 'https://miro.medium.com/max/820/1*JAEY3KP7TU2Q6HN6LasMrw.png', 'https://miro.medium.com/max/60/0*KLhgaRH43lGDOKSN.png?q=20'}",2020-03-05 00:14:54.540841,1.6936287879943848
https://towardsdatascience.com/a-gentle-introduction-to-credit-risk-modeling-with-data-science-part-2-d7b87806c9df,A Gentle Introduction to Credit Risk Modeling with Data Science — Part 2,"In our last post, we started using Data Science for Credit Risk Modeling by analyzing loan data from Lending Club.

We’ve raised some possible indications that the loan grades assigned by Lending Club are not as optimal as possible.

Over the next posts, our objective will be using Machine Learning to beat those loan grades.

Soon this guy will take your job AND generate your credit score.

We will do this by conceptualizing a new credit score predictive model in order to predict loan grades.

In this post, we will use Data Science and Exploratory Data Analysis to delve deeper into some of the Borrower Variables, such as annual income and employment status and see how they affect other variables.

This is crucial to help us visualize and understand what kind of public are we dealing with, allowing us to come up with an Economic Profile.

Economic Profile

Would our friend W get a loan grade B?

In the dataset, we have some variables from each borrower’s economic profile, such as:

Income: annual income in USD

annual income in USD Employment Length : how many years of employment at the current job.

: how many years of employment at the current job. Employment Title: job title provided by the borrower in the loan application

We will analyze each of these variables and how they interact within our dataset.

Show me the Money

We have two variables reflecting borrowers’ income, depending on the nature of the application: single annual income or joint annual income.

Single applications are the ones filed by only one person, while joint applications are filed by two or more people.

As it can be seen in the countplot above, the quantity of joint applications is negligible.

Let’s generate a couple of violin plots and analyze the annual income for single and joint applications together.

Violin plots are similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram).

Our first violin plot looks weird.

Before digging deeper, let’s zoom in and try to understand why it has this format by generating a distribution plot for annual incomes from single applications.

Annual Incomes for Single Applications

We can observe a few particularities:

It is heavily skewed — deviates from the gaussian distribution.

deviates from the gaussian distribution. It is heavily peaked.

It has a long right tail.

These aspects are usually observed in distributions which are fit by a Power Law.

A power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a power of another.

Throughout the years, some scientists have analyzed a variety of distributions. There is reasonable amount of work indicating that some of these distributions could be fit by a Power Law, such as:

For our dataset, if we only consider employed individuals (or income greater than zero), we could very informally say that we have a Power Law candidate distribution — this would be in line with the first paper referenced above ( Distributions of Korean Household Incomes ).

However, formally proving a distribution’s goodness-of-fit is not a trivial task and thus would require a reasonable amount of work, which is outside the scope of this article. If you’re interested into this subject, check out this excellent paper from Aaron Clauset.

Coming back to our problem, let’s zoom into the distribution for annual joint incomes and see how they differ from the annual single incomes data distribution.

Interestingly enough, we have a different animal here.

Our distribution is unimodal and resembles the gaussian distribution, being skewed to the left.

Income versus Loan Amount

We will check the relationship between Income and Loan Amount by generating a boxplot.

But in order to do this we’ll look at a subset from our data, where income is less than USD 120K per year.

The reason for this is that applications with income above this limit are quite not statistically representative in our population — from 880K loans, only 10% have annual incomes higher than USD 120K.

If we don’t cap our annual income, we would have a lot of outliers and our boxplot would look like the one on the left:

From the right side boxplot, we have a few highlights:

Fully Paid status quartile distribution is very different from Charged Off . Moreover, it is similar to Current , In Grade Period and Issued . Intuitively, this leads us to think that Lending Club has been more selective with its newer loans.

status quartile distribution is very different from . Moreover, it is similar to , and . Intuitively, this leads us to think that Lending Club has been more selective with its newer loans. Charged Off and Default statuses hold similarities in terms of the quartile distribution, differing from all the others.

This looks like good news in terms of importance of the income variable for predicting loan grades.

Will these variables represent critical information for our model?

Let’s add one more dimension to the analysis by generating the boxplots for Income versus Loan Grade.

Unsurprisingly, A graded loans have a median income that is superior to other grades, and the same can be said about the other quartiles.

Note however that F, G and B graded loans hold a similar income quartile distribution, lacking consistency.

Perhaps income is not that critical when determining LC’s loan grades?

We will find that later.

Employment Title

While annual income gives a good indication on the financial situation of each of the borrowers, employment title can also give us some valuable insights. Some professions have a higher turnover, are riskier or more stressful than others, which might impact borrowers financial capacity.

Let’s start by generating a count plot for employment titles considering annual income lower than USD 120K.

Most applications have a null employment title. Other than that, top 5 occupations for this subset are:

Teacher Manager Registered Nurse RN (possibly Registered Nurse) Supervisor

Having a lot of applications with “None” as employment title could be reason for concern. This could be explained by some possible scenarios, among them:

Unemployed people are getting loans

Some of the applications are being filed without this information

Let’s investigate this further by checking the annual income from people with “None” as employment title.

All the quartiles seem to be a little bit above zero, which wouldn’t be a surprise.

But there are also a lot of outliers — some people with no employment title and an annual income of more than USD 250K, for example.

Maybe Compliance and KYC are not doing a good job?

At this point, we don’t know.

Before answering this question, let me introduce you to NINJA Loans.

NINJA Loans

In financial markets, NINJA Loan is a slang term for a loan extended to a borrower with “no income, no job and no assets.” Whereas most lenders require the borrower to show a stable stream of income or sufficient collateral, a NINJA loan ignores the verification process.

Let’s see if the applications with no income have actually got a loan.

We can conclude that many people with almost zero annual income actually got loans.

At the same time, there are current loans which were granted to people with zero annual income.

Bottomline for now is that we can’t safely assume this is a problem with Compliance or KYC. This scenario could also relate to LC’s risk appetite, or simply bad data.

But there is some indication that LC’s risk appetite has been steadily growing, since some of the current loans were accepted without any annual income.

The Good, the Bad and the Wealthy

Just out of curiosity, let’s check the employment titles for individuals with an annual income that is superior to USD 120K.

Again, most of the applications within this subset don’t have an employment title. For the rest of them, top 5 positions by frequency are:

Director Manager Vice President Owner President

We can see that we have more senior level positions as well as high level self-employed professionals such as attorneys and physicians. No surprises here either.

We must remember that this subset represents around 80K loans, which stands for less than 10% of the entire population.

Time is Money

From a P2P loan investor strategy, it is important to understand the size of the cash flows (income amount) and the stability. Employment length can be a good proxy for that.

However, it is known that employment length is self reported by LC loan applicants. Lending Club does not verify it.

So the question is — is it reliable as a credit risk predictor?

From the count plot below, we can see that we have many applications where the borrower has more than 10 years of employment tenure.

Let’s see how employment length correlates with the loan status.

Since we’re talking about two categorical variables, we will generate some dummy variables and see how the pearson correlation between them looks like through a heatmap.

Unfortunately, there are no significant linear correlations between the dummy variables related to employment length and loan status.

Would the correlations between employment length and the loan grade show a different story?

Same old — no absolute correlation scores above 0.4.

Our quest does not end here, though.

We will continue expanding our analysis in a next post, where we will analyze hybrid loan x borrower characteristics, such as debt to income ratio, delinquency rates, and also other types of data such as geographic data and macroeconomic variables.

Getting to understand this public will help us achieve our objective in the longer run, which is creating a new machine learning model for predicting loan grades and credit default.

Rafael V. Pierre is a Data Scientist based in Amsterdam, NL. He is currently an MSc. candidate in Information Studies, Data Science at the University of Amsterdam. He is also a Data Science, AI & Analytics Consultant at Weet Analytics, a company helping companies unlock unique business opportunities out of data.

You Might Also Like","['introduction', 'credit', 'variables', 'income', 'gentle', 'modeling', 'title', 'employment', 'loans', 'data', 'loan', 'annual', 'distribution', 'applications', 'risk', 'science']","In our last post, we started using Data Science for Credit Risk Modeling by analyzing loan data from Lending Club.
Show me the MoneyWe have two variables reflecting borrowers’ income, depending on the nature of the application: single annual income or joint annual income.
Let’s generate a couple of violin plots and analyze the annual income for single and joint applications together.
Coming back to our problem, let’s zoom into the distribution for annual joint incomes and see how they differ from the annual single incomes data distribution.
He is also a Data Science, AI & Analytics Consultant at Weet Analytics, a company helping companies unlock unique business opportunities out of data.",en,['Rafael Pierre'],2019-01-04 11:29:30.841000+00:00,"{'Data Science', 'Exploratory Data Analysis', 'Machine Learning', 'Towards Data Science', 'Credit Risk'}","{'https://miro.medium.com/max/4382/1*LpF5JcCuLsp9G30LO9e2fQ.png', 'https://miro.medium.com/max/60/1*MTJ3wzba3os_6ArrE2reWQ.png?q=20', 'https://miro.medium.com/max/3644/1*aryZsc9Ai_hMniFA2lhPYw.png', 'https://miro.medium.com/max/60/1*uxePT9XpzuS-lc1lNaY5sQ.jpeg?q=20', 'https://miro.medium.com/max/3880/1*mrSNefMqfa6-R_1n9SffMg.jpeg', 'https://miro.medium.com/max/60/1*5ytA3AWXTFAmgaa5T7I_yQ.png?q=20', 'https://miro.medium.com/max/60/1*ojM0Hdxkv89EXY3hDHRtJQ.jpeg?q=20', 'https://miro.medium.com/max/3000/1*uxePT9XpzuS-lc1lNaY5sQ.jpeg', 'https://miro.medium.com/max/3202/1*bLYvB9K0o1qBJ0_PiL3D5w.png', 'https://miro.medium.com/max/60/1*mrSNefMqfa6-R_1n9SffMg.jpeg?q=20', 'https://miro.medium.com/max/3180/1*OVrl8gJtc8i1f2gqIdD5wA.png', 'https://miro.medium.com/max/60/1*253COT74n0AcGvcgP3A-vg.png?q=20', 'https://miro.medium.com/max/60/1*WVqBKmGbrX0PhJE3MIF8uQ.png?q=20', 'https://miro.medium.com/max/3164/1*-VcaBtXPPjSrzUFbx2nLPQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*OzKuJi0wQwCp_HK5Xw2SIg.png?q=20', 'https://miro.medium.com/max/60/1*Xiai97vyDzYrVg0AXXO41g.png?q=20', 'https://miro.medium.com/max/3172/1*MTJ3wzba3os_6ArrE2reWQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*bLYvB9K0o1qBJ0_PiL3D5w.png?q=20', 'https://miro.medium.com/max/7574/1*Rw8aCvjzAHoKHwC-cS6SQg.png', 'https://miro.medium.com/max/4784/1*gFtroFM2JnFK9BLgUpdU0Q.png', 'https://miro.medium.com/fit/c/96/96/2*vySDg6ZZbzXmjAuJ_2mb8w.png', 'https://miro.medium.com/max/60/1*Gg6b7ok5yiBzJEb_WGSjSw.png?q=20', 'https://miro.medium.com/max/60/1*aryZsc9Ai_hMniFA2lhPYw.png?q=20', 'https://miro.medium.com/max/3180/1*5ytA3AWXTFAmgaa5T7I_yQ.png', 'https://miro.medium.com/max/60/1*-VcaBtXPPjSrzUFbx2nLPQ.png?q=20', 'https://miro.medium.com/max/60/1*uS5BiQiKcQEbXvoe_al-WQ.png?q=20', 'https://miro.medium.com/max/1800/1*5W4L4B-86yLz1E-7MKadOQ.jpeg', 'https://miro.medium.com/max/60/1*OVrl8gJtc8i1f2gqIdD5wA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2352/1*Xiai97vyDzYrVg0AXXO41g.png', 'https://miro.medium.com/max/60/1*gFtroFM2JnFK9BLgUpdU0Q.png?q=20', 'https://miro.medium.com/max/2400/1*Gg6b7ok5yiBzJEb_WGSjSw.png', 'https://miro.medium.com/max/1368/1*253COT74n0AcGvcgP3A-vg.png', 'https://miro.medium.com/max/2480/1*3DwNevk-ZgM06IKTurBZ1Q.jpeg', 'https://miro.medium.com/max/5308/1*WVqBKmGbrX0PhJE3MIF8uQ.png', 'https://miro.medium.com/fit/c/160/160/2*vySDg6ZZbzXmjAuJ_2mb8w.png', 'https://miro.medium.com/max/60/1*rYsWOaqaKvv3GlvdvBt-VQ.png?q=20', 'https://miro.medium.com/max/60/1*Rw8aCvjzAHoKHwC-cS6SQg.png?q=20', 'https://miro.medium.com/max/6088/1*OzKuJi0wQwCp_HK5Xw2SIg.png', 'https://miro.medium.com/max/2560/1*ojM0Hdxkv89EXY3hDHRtJQ.jpeg', 'https://miro.medium.com/max/3840/1*rYsWOaqaKvv3GlvdvBt-VQ.png', 'https://miro.medium.com/max/1200/1*ojM0Hdxkv89EXY3hDHRtJQ.jpeg', 'https://miro.medium.com/max/60/1*3DwNevk-ZgM06IKTurBZ1Q.jpeg?q=20', 'https://miro.medium.com/max/60/1*LpF5JcCuLsp9G30LO9e2fQ.png?q=20', 'https://miro.medium.com/max/2746/1*uS5BiQiKcQEbXvoe_al-WQ.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*5W4L4B-86yLz1E-7MKadOQ.jpeg?q=20'}",2020-03-05 00:15:01.180630,6.639789342880249
https://medium.com/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45,Intuitive Interpretation of Random Forest,"For someone who thinks that random forest is a black box algorithm, this post can offer a differing opinion. I am going to cover 4 interpretation methods that can help us get meaning out of a random forest model with intuitive explanations. I am also going to briefly discuss the pseudo code behind all these interpretation methods. I have learned about this in fast.ai ‘Introduction to Machine Learning’ course as MSAN student at USF.

1. How important are our features ?

It is pretty common to use model.feature_importances in sklearn random forest to study about the important features. Important features mean the features that are more closely related with dependent variable and contribute more for variation of the dependent variable. We generally feed as much features as we can to a random forest model and let the algorithm give back the list of features that it found to be most useful for prediction. But carefully choosing right features can make our target predictions more accurate .

The idea of calculating feature_importances is simple, but great.

Splitting down the idea into easy steps:

1. train random forest model (assuming with right hyper-parameters)

2. find prediction score of model (call it benchmark score)

3. find prediction scores p more times where p is number of features, each time randomly shuffling the column of i(th) feature

4. compare all p scores with benchmark score. If randomly shuffling some i(th) column is hurting the score, that means that our model is bad without that feature.

5. remove the features that do not hurt the benchmark score and retrain the model with reduced subset of features.

Spreadsheet example of calculating feature importance. (Shuffle F4, calculate prediction score and compare with benchmark score. This will give feature importance of F4 column)

Code to calculate feature importance:

Below code will give a dictionary of {feature, importance} for all the features.

Feature importance code from scratch:

Feature importance in random forest

Output:

importance = feat_imp(ens, X_train[cols], y_train); importance [('YearMade', -0.21947050888595573),

('Coupler_System', -0.21318328275792894),

('ProductSize', -0.18353291714217482),

('saleYear', -0.045706193607739254),

('Enclosure', -0.041566508577359523),

('MachineID', -0.01399141076436905),

('MachineHoursCurrentMeter', -1.9246700722952426e-05)] In above output, YearMade increases prediction RMSE most if it gets shuffled (proxy to getting removed from model). So it must be most important feature.

(above results correspond to data taken from a Kaggle competition. Here is the link - (above results correspond to data taken from a Kaggle competition. Here is the link - https://www.kaggle.com/c/bluebook-for-bulldozers

2. How confident are we about our predictions ?

Generally, when businesses want to predict something, their end goal is either to reduce costs or improve profits. Before taking big business decisions, businesses are interested to estimate the risk of taking that decision. But when the prediction results are presented without a confidence interval, rather than reducing the risk, we might inadvertently expose the business to more risk.

It is relatively easy to find the confidence level of our predictions when we use a linear model (in general models which are based on distribution assumptions). But when it comes to confidence interval for random forest, it is not very straightforward.

A. Graphical illustration of bias and variance

I guess, anyone who has taken a linear regression class must have seen this image (A). To find a best linear model, we look for model that finds best bias-variance tradeoff. The image here nicely illustrates the definition of bias and variance in our predictions. (Let these 4 images are darts thrown by 4 different persons)

If we have high bias and low variance (3rd person), we are hitting dart consistently away from bulls eye. On contrary, if we have high variance and low bias (2nd person), we are very inconsistent in hitting the dart. If one has to guess where the next dart will go when hit by the 2nd person, it can go either hit bulls eye or away from it. Now, let’s suppose catching a credit fraud in real life is analogous to hitting a bulls eye in above example. If the credit company has predictive model similar to 2nd person’s dart throwing behavior, the company might not catch fraud most of the times, even though on an average model is predicting right.

The takeaway is that rather than only mean predictions, we should also check confidence level of our point predictions.

How to do that in random forest ?

A random forest is made from multiple decision trees (as given by n_estimators). Each tree individually predicts for the new data and random forest spits out the mean prediction from those trees. The idea for confidence level of predictions is just to see how much predictions coming from different trees are varying for the new observations. Then to analyze further, we can seek some pattern (something like predictions corresponding to year 2011 have high variability) for observations which have highest variability of predictions.

The source code of prediction confidence based on tree variance:

Output of above code will look like following:

Confidence based on tree variance

Reading from this output, we can say that we are least confident about our prediction of validation observation at index 14.

3. What is the prediction path? (Tree Interpreter)

Feature importance (as in 1st section) is useful if we want to analyze which features are important for overall random forest model. But if we are interested in one particular observation, then the role of tree interpreter comes into play.

For example, there is a RF model which predicts — a patient X coming to hospital has high probability of readmission or not? For sake of simplicity, let’s consider we only have 3 features — patient's blood pressure data, patient's age and patient's sex . Now, if our model says that patient A has 80% chances of readmission, how can we know what is special in that person A that our model predicts he/she will be readmitted ? . In this case, tree interpreter tells the prediction path followed for that particular patient. Something like, because patient A is 65 years old male, that is why our model predicts that he will be readmitted. Another patient B who my model predicts to be readmitted might be because B has high blood pressure (not because of age or sex).

Basically, tree interpreter gives the sorted list of bias (mean of data at starting node) and individual node contributions for a given prediction.

B. Decision tree path (source: http://blog.datadive.net/interpreting-random-forests/)

C. Tree interpreter illustration (Final probability of readmission = 0.6)

The decision tree (depth: 3) for image (B) is based on Boston housing price data set. It shows the breakdown of decision path, in terms of prediction values from intermediate nodes and features that cause values to change. Contribution of a node is difference of value at that node from the value at the previous node.

This image (C) gives an example output of using tree interpreter for Patient A. It says that being 65 years old was highest contributor that model predicted high probability of readmission than mean.

D. Waterfall chart to visualize contributions

Visualization of spreadsheet output can also be done using Waterfall chart (D). I have made this using quick and easy waterfall chart from “waterfallcharts package”.

Code for above waterfall chart plot:

Just to be clear about terminology -

• Value (image B) means target value predicted by nodes. (just mean of target observations falling in that node).

• Contribution is value at present node minus value at previous node (this is what gives feature contribution for a path).

• Path is combination of all the feature splits taken by some observation in order to reach leaf node.

The function from treeinterpreter package is pretty straightforward for getting contributions from each node and can be explored here.

4. How is target variable related with important features? (Partial Dependence Plots)

Having found the most important features, next thing we might be interested in is to study the direct relationship between target variable and features of interest. An analogy of this from linear regression is model coefficients. For linear regression, coefficients are calculated in such a way that we can interpret them by saying: ”what would be change in Y with 1 unit change in X(j), keeping all other X(i’s) constant”.

Although we have feature importances from random forest, but they only give a relative change in Y with respect to change in X(i’s). We can not directly interpret them as how much change in Y is caused due to unit change in X(j), keeping all other features constant.

Luckily, we have partial dependence plots that can be viewed as graphical representation of linear model coefficients, but can be extended to seemingly black box models also. The idea is to isolate the changes made in predictions to solely come from a specific feature. It is different than scatter plot of X vs. Y as scatter plot does not isolate the direct relationship of X vs. Y and can be affected by indirect relationships with other variables on which both X and Y depend.

The steps to make PDP plot are as follows:

1. train a random forest model (let’s say F1…F4 are our features and Y is target variable. Suppose F1 is the most important feature).

2. we are interested to explore the direct relationship of Y and F1

3. replace column F1 with F1(A) and find new predictions for all observations. take mean of predictions. (call it base value)

4. repeat step 3 for F1(B) … F1(E), i.e. for all distinct values of feature F1.

5. PDP’s X-axis has distinct values of F1 and Y-axis is change in mean prediction for that F1 value from base value.

Spreadsheet illustration for logic behind PDP

Below (E)is how a partial dependence plot looks like. (done on kaggle bulldozer competition data). It shows the relationship of YearMade with SalesPrice.

E. Partial dependence plot (YearMade vs. Change in SalePrice)

And below (F) is how a line plot of SalePrice vs. YearMade would look like. We can see that scatter/line plot might not catch the direct impact of YearMade on SalesPrice as done by PDP.

F. Source of above 2 plots is rf interpretation notebook of fast.ai ml1 course.

End Notes:

In most of the cases random forests can beat linear models for prediction. An objection frequently raised for random forests is interpretation of results as compared to linear models. But one can address the misconceived objection using the discussed methodologies of interpretation.

Bio: I am currently studying Data Science (Analytics) as University of San Francisco and doing my intern at Manifold.ai. Previously, I have worked as Data Scientist at Capgemini and Sr. Business Analyst at Altisource.

Linkedin: https://www.linkedin.com/in/prince-grover-0562a946/","['forest', 'feature', 'intuitive', 'prediction', 'change', 'model', 'predictions', 'random', 'tree', 'data', 'features', 'interpretation']","For someone who thinks that random forest is a black box algorithm, this post can offer a differing opinion.
I am going to cover 4 interpretation methods that can help us get meaning out of a random forest model with intuitive explanations.
It is pretty common to use model.feature_importances in sklearn random forest to study about the important features.
But when it comes to confidence interval for random forest, it is not very straightforward.
A random forest is made from multiple decision trees (as given by n_estimators).",en,['Prince Grover'],2017-11-29 20:55:16.448000+00:00,"{'Random Forest', 'Machine Learning', 'Interpretation'}","{'https://miro.medium.com/max/60/1*5BxCUUyno69-0dWT7Xgh4A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*iBPQWjoV0vshKxI2BckyEg.png', 'https://miro.medium.com/max/2940/1*5BxCUUyno69-0dWT7Xgh4A.png', 'https://miro.medium.com/max/3200/1*hXtic8qATZNpjc7rhYtjCQ.png', 'https://miro.medium.com/max/1236/1*z6sqeSaTDCEb72BL6zZgHQ.png', 'https://miro.medium.com/max/928/1*3Tewu5G3sYHbTnEe3Byo4g.png', 'https://miro.medium.com/max/2156/1*ZLHP_7sCPJWaJBS-tJOf9w.png', 'https://miro.medium.com/max/2474/1*kVuDSb5_1LfRtBl_FpWhLw.png', 'https://miro.medium.com/max/60/1*z6sqeSaTDCEb72BL6zZgHQ.png?q=20', 'https://miro.medium.com/max/60/1*76kTF0lh4aMDYCYQZz8BeA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*iBPQWjoV0vshKxI2BckyEg.png', 'https://miro.medium.com/max/60/1*hXtic8qATZNpjc7rhYtjCQ.png?q=20', 'https://miro.medium.com/max/60/1*kVuDSb5_1LfRtBl_FpWhLw.png?q=20', 'https://miro.medium.com/max/60/1*N_a5ZTLGHjK5JvGYpxPkRg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*aHOoIFQy3971wshX-Nes7g.png', 'https://miro.medium.com/max/60/1*3Tewu5G3sYHbTnEe3Byo4g.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*8EYRvo9DcbvnrcBr54htsw.jpeg', 'https://miro.medium.com/max/1980/1*hREUgZQ1yj8NMGZsWgp-HQ.png', 'https://miro.medium.com/max/1856/1*3Tewu5G3sYHbTnEe3Byo4g.png', 'https://miro.medium.com/max/60/1*hREUgZQ1yj8NMGZsWgp-HQ.png?q=20', 'https://miro.medium.com/max/2756/1*N_a5ZTLGHjK5JvGYpxPkRg.png', 'https://miro.medium.com/fit/c/80/80/1*czLBAIH3EyO7s9kTPMWPDQ.jpeg', 'https://miro.medium.com/max/190/1*95YqGhNUaP_S8t5TJMkEmw.png', 'https://miro.medium.com/max/60/1*ZLHP_7sCPJWaJBS-tJOf9w.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*I2dp-wiFiD1M16yEJ-ixBA.jpeg', 'https://miro.medium.com/max/1800/1*76kTF0lh4aMDYCYQZz8BeA.png'}",2020-03-05 00:15:02.974751,1.7931060791015625
https://towardsdatascience.com/de-coding-random-forests-82d4dcbb91a1,De(Coding) Random Forests,"De(Coding) Random Forests

Building your Random Forest from scratch in Python and interpreting the Math behind the ‘Black Box’

Motivation: Random Forest Ensembles are widely used for real-world machine learning problems, Classification as well as Regression. Their popularity can be attributed to the fact that practitioners often get optimal results using a Random Forest algorithm with minimal data cleaning, and no feature scaling. For a better grasp of the underlying principles, I decided to code a Random Forest from scratch, and perform a few visualizations of different features in the data, and their role in deciding the final result. Widely believed to be a black box, a quick walk-through of the algorithm will prove it is actually quite interpretable, apart from being a powerful technique leveraging the ‘power of the majority vote’.

Introduction

Random Forest Ensembles are a divide-and-conquer approach used to improve performance of individually weak Decision Tree models. The main principle behind this is that a group of “weak learners” can come together to form a “strong learner”. Each classifier, individually, is a “weak learner,” while all the classifiers taken together are a “strong learner”.

Random Forest Ensemble

At the heart of the Random Forest concept is averaging the results from a number of Decision Trees. Decision Trees are often handy tools to explain the intuition behind a prediction to people unfamiliar with Machine Learning. But explaining how a Random Forest arrived at a prediction, and using which features or independent variables, can be quite a task. Random Forests are often misinterpreted as ‘Black Boxes’ or difficult to understand.

Random Forest Ensemble

Coding a Random Forest in Python

The following section is a walk-through to coding and optimizing a Random Forest from scratch, and gaining an in-depth understanding of the way Random Forests work. We also look at understanding how and why certain features are given more weightage than others when it comes to predicting the results.

Random Forest and Decision Tree classes

Using an Object oriented Programming approach in Python , a class ‘RandomForest’ is defined,comprised of ‘DecisionTree’ objects. The Random Forest is basically a collector and average calculator for each of the core functions performed by its Decision Trees.

The Random Forest class

The Decision Tree class

The Constructor ( __init__ ), takes the sample size or the number of training samples to consider for each tree, as well as the maximum features to split on for each tree, to as to inculcate randomness in each tree and remove bias in the predictions. It also takes a few other common hyper-parameters used in the Random Forest algorithm.

The fit method performs the key function of constructing the member trees of the Random Forest using the training data.

The predict method performs the function of averaging the test data predictions across each of the member trees, to decide on the final prediction results for the test data.

Logic Flow for Training the Model

The fit() method in Random Forest class is called for training the model, which calls function var_split() in the Decision Tree class, which calls find_better_split() in a loop. This constructs each of the member Decision Trees in the forest, and fits the training data for each tree.

var_split() function implementation in Decision Tree

find_better_split() function implementation

The Decision Tree recursively splits into the left and right sub-trees after finding a feature which will maximize the homogeniety at each split. This is done by a two-loop logic:

The first loop is through all features of the tree to determine which is the best feature for a split at a particular level. The function var_split() calls find_better_split() for each feature, to determine the feature yielding the purest leaf nodes.

In the inner loop function find_better_split() is called for each feature to calculate the optimum value of the feature at which the tree should be split.

This is done by sorting the values of that feature, looping across all distinct values of the feature and tracking the corresponding values of the dependent variable.

A score is calculated which is the information gain attained by splitting the tree for each feature at each distinct value. This score represents the variance of the sub-trees formed after the split, which needs to be minimized.

The combination of feature and feature value leading to lowest variance in the sub-trees , or maximum homogeniety , is chosen for the split.

Upon finding the best feature and the optimal split value, the tree splits into the left and right subtrees, and the leftTree and rightTree members of the class are populated.

Function var_split() is called recursively on the leftTree and rightTree members to find the best feature to split on and the best value of this feature. This continues until one of the conditions of a leaf node is satisfied, i.e. maximum depth of tree is reached or a minimum amount of data samples is reached.

The value assigned to this leaf is the mean value of the training samples contained in it, or a corresponding value which is collectively representative of the data in the leaf.

Making Predictions for the Test / Validation Dataset

predict() function for Random Forest

predict() implementation for a Decision Tree

The code for predicting results using the model is pretty self-explanatory.

Predictions for the Random Forest ensemble are done by averaging the prediction of each of the individual Decision Trees.

Note: Averaging the predictions is a good technique for a Random Forest Regressor, however for a Random Forest Classifier, considering the frequency of predictions of the trees and choosing the prediction value with the maximum frequency is the way to go.

Inside a Decision Tree, the prediction for each row is done by traversing through the tree, comparing the feature values with the values of the features to be split on at each level, and navigating to the left sub-tree if the feature value is lesser and right sub-tree if greater. This continues until a leaf node is reached.

The prediction for the row is then the value contained in the leaf node, which has been pre-assigned during the training phase.

Interpreting a Random Forest Model

We now have the basic structure of our own library for a Random Forest Ensemble. Let us add the super-useful enhancements of Feature Interaction and Tree Interpretation, which are important concepts to help interpret a Random Forest.

Intuition behind Feature Importances

Calculate feature importances for this model

To find how much say a particular feature has in deciding the predictions of the model, the following approach has been followed:

The evaluation metric (accuracy or error) for the predictions on a dataset is recorded.

Next, the values of the feature whose importance is to be determined, are randomly shuffled using the shuffle() method in the Numpy library.

The evaluation metric is again calculated by running the model to predict values for the dataset with the values of this particular feature column shuffled.

The difference between the resulting metric and the earlier metric without shuffling the feature values is calculated.

If the difference is huge with the resulting metric being much worse, then this particular feature definitely has a huge say in deciding the model predictions.

If randomly shuffling values for a particular feature brings the model performance drastically down, we can say that this feature is surely used in a split, comparatively higher up in the tree, and is important. This technique has been used for each feature by looping through the features, in my implementation.

Pretty visualizations such as Bar Charts can be plotted for features and their corresponding importances, to interpret the weightage given to different features by our Random Forest Model.

Feature Importance Chart

Partial Dependence Plots for Random Forest

Partial Dependence Plots are especially useful to understand how the result or target variable varies with a particular feature, and what is the actual relationship between the target and this feature. Is it monotonically increasing / decreasing or irregular ?

Univariate Plot vs Partial Dependence Plot

If we look at a typical Univariate plot for finding the effect of a feature on the target variable, we seldom see a continuous upward / downward curve or a straight line. Univariate plots are usually irregular.

As an example, consider you are working on a housing dataset and want to predict house prices, given a set of features including the year the house was built in, the locality and other features. If you plot a univariate plot of the price against the year built, you would expect a continuous upward curve because real estate prices usually increase with time. But the plot turns out to go downward between some years. This would falsely lead you to believe that those few years were bad years for house prices.

Univariate Plot for the log of House Prices varying with Year Built

Whereas, it could have been the case that the majority of houses bought during those years were in a locality which was relatively cheaper, and hence the decline in the curve.

A Partial Dependence Plot would solve this problem, and show the true relationship between the prices and the years , and you would find them to be continually increasing after all. This is due to the reason that in Partial Dependence Plots, we consider only the feature under consideration to be varying, ensuring all the other features remain constant. Thus we cut out the noise from other dependent variables, and get a better understanding of the true nature of interactions of each feature with the target variable.

Partial Dependence Plot would show the correct trend of prices against Year Built

Implementation of Partial Dependence Plot

My implementation of Partial Dependence Plot is in the following manner:

Partial Dependence Plot for a particular feature

Keeping all other columns (feature values) constant, keep looping through all the unique values of the feature to be considered, and run the model to predict results for each unique value of this feature.

Plot the resulting values to understand the trend behind the way the target variable varies with this particular feature.

Keeping other columns constant and varying the feature to calculate dependence on, is the key to solving the problem of noise from external sources creeping into the depiction of the relation between the feature and the target.

Tree Interpreters for Random Forest

A very useful technique of explaining the flow of Random Forest predictions is through using Tree Interpreters for each of the Decision Trees.

Tree Interpreter Class with the predict function

· The value at the root of the tree (which is simply an average value of all the targets in training data) is called ‘Bias’.

· The predict() function inside the Tree Interpreter class, calls the predict_row_for_ti() function inside a decision tree, which predicts the result for a ‘row’ or a single data sample.

· At each split, the contribution of the feature used in the split is calculated using the amount by which the average of samples would vary after splitting into the sub-trees.

· The feature contributions are stored in an array till a leaf is arrived at, and the results are then returned to the Tree Interpreter.

· At the leaf node, the prediction value for that data sample is returned, along with the bias and ‘contribs’, which are contributions of each feature towards the final result.

Function inside the Decision Tree, used by the Tree Interpreter

Waterfall Chart based on Tree Interpreter

The contributions provided by each feature for arriving at the leaf node (final result) can be visualized using a Waterfall Chart, using the recent ‘waterfall’ package in python.

A Waterfall Chart is one of the most effective ways to visualize the contributions of each feature, starting from the root of a tree, till the final result in the leaf node.

Waterfall Chart with the contributing features in X axis and resulting target values in the Y axis

Future Enhancements

A few areas in which further enrichment to the Random Forest library needs to be done are as follows:

· Building on the Tree Interpretation , work needs to be done on correlations of features in Random Forest, by finding groups of features which are commonly used in successive splits frequently, and hence might have higher correlations.

· This would be the second way of exploring feature interactions in Random Forests, the first way being the Partial Dependence plots which can be extended to more than one feature.

Note: Standard Machine Learning libraries in Python are highly optimized for speed and efficiency, due to which any code you write from scratch might need to be executed in Cython (underlying C implementation used by Python) to achieve faster results. For this, simply add ‘%%cython’ to the beginning of your code.

Final Thoughts and Key Take-Aways

This is a good starting point to coding Machine Learning Algorithms from scratch, and also understanding a few super useful libraries in Python, including Numpy for speedy mathematical calculations, and Matplotlib for Data Visualization.

The full implementation can be found in my Jupyter Notebook at https://github.com/SonaliDasgupta/MLandAIAlgorithmsFromScratch where I have tried to play around with ideas and visualizations using a dataset. The code has to be further refined and optimized, and I am working on it. Any suggestion or contributions are highly welcome.

This was a very enjoyable exercise for me and I hope I have helped you dive deeper mathematically and visually into the Random Forest implementation, and to understand that it is not as complex or ‘Black Box’ as it seems after all.

References:","['decoding', 'forest', 'feature', 'function', 'forests', 'decision', 'random', 'tree', 'features', 'values', 'value', 'dependence']","De(Coding) Random ForestsBuilding your Random Forest from scratch in Python and interpreting the Math behind the ‘Black Box’Motivation: Random Forest Ensembles are widely used for real-world machine learning problems, Classification as well as Regression.
Random Forest EnsembleAt the heart of the Random Forest concept is averaging the results from a number of Decision Trees.
Random Forest EnsembleCoding a Random Forest in PythonThe following section is a walk-through to coding and optimizing a Random Forest from scratch, and gaining an in-depth understanding of the way Random Forests work.
Random Forest and Decision Tree classesUsing an Object oriented Programming approach in Python , a class ‘RandomForest’ is defined,comprised of ‘DecisionTree’ objects.
Interpreting a Random Forest ModelWe now have the basic structure of our own library for a Random Forest Ensemble.",en,['Sonali Dasgupta'],2018-12-03 14:12:12.705000+00:00,"{'Data Visualization', 'Ensemble Learning', 'Random Forest', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*YyrVWEkCDiKeFZSO8_urcg.png?q=20', 'https://miro.medium.com/max/1608/1*kS7Tg9MFMWXbCXyqUXoGTQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*qyocJJEvOfKhHreB5zeswA.png?q=20', 'https://miro.medium.com/max/1608/1*NIA0RECz7rcNkZ7FTCPHOQ.png', 'https://miro.medium.com/max/1608/1*YyrVWEkCDiKeFZSO8_urcg.png', 'https://miro.medium.com/max/60/1*kS7Tg9MFMWXbCXyqUXoGTQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1098/1*yk2QHWvdOEc0Vp2nKZXlvA.png', 'https://miro.medium.com/fit/c/96/96/0*44IjME7iKqtRfKn7.', 'https://miro.medium.com/fit/c/160/160/0*44IjME7iKqtRfKn7.', 'https://miro.medium.com/max/60/1*yk2QHWvdOEc0Vp2nKZXlvA.png?q=20', 'https://miro.medium.com/max/60/1*ZaKaOncEGvP4CFLls-CXeA.png?q=20', 'https://miro.medium.com/max/60/1*NIA0RECz7rcNkZ7FTCPHOQ.png?q=20', 'https://miro.medium.com/max/806/1*ZaKaOncEGvP4CFLls-CXeA.png', 'https://miro.medium.com/max/1608/1*qyocJJEvOfKhHreB5zeswA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/804/1*kS7Tg9MFMWXbCXyqUXoGTQ.png'}",2020-03-05 00:15:09.804433,6.829681873321533
https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249,Random forests and decision trees from scratch in python,"2. Aggregation: The core concept that makes random forests better than decision trees is aggregating uncorrelated trees. The idea is to create several crappy model trees (low depth) and average them out to create a better random forest. Mean of some random errors is zero hence we can expect generalized predictive results from our forest. In case of regression we can average out the prediction of each tree (mean) while in case of classification problems we can simply take the majority of the class voted by each tree (mode).

Python code

To start coding our random forest from scratch, we will follow the top down approach. We will start with a single black box and further decompose it into several black boxes with decreased level of abstraction and greater details until we finally reach a point where nothing is abstracted anymore.

Random forest class

We are creating a random forest regressor, although the same code can be slightly modified to create a classifier. To start out, we need to know what our black box takes as input to yield the output (prediction) so we need to know the parameters that define our random forest :

x: independent variables of training set. To keep things minimal and simple I am not creating a separate fit method hence the base class constructor will accept the training set. y: the corresponding dependent variables necessary for supervised learning (Random forest is a supervised learning technique) n_trees : number of uncorrelated trees we ensemble to create the random forest. n_features: the number of features to sample and pass onto each tree, this is where feature bagging happens. It can either be sqrt, log2 or an integer. In case of sqrt, the number of features sampled to each tree is square root of total features and log base 2 of total features in case of log2. sample_size: the number of rows randomly selected and passed onto each tree. This is usually equal to total number of rows but can be reduced to increase performance and decrease correlation of trees in some cases (bagging of trees is a completely separate machine learning technique) depth: depth of each decision tree. Higher depth means more number of splits which increases the over fitting tendency of each tree but since we are aggregating several uncorrelated trees, over fitting of individual trees hardly bothers the whole forest. min_leaf: minimum number of rows required in a node to cause further split. Lower the min_leaf, higher the depth of the tree.

Let’s start defining our random forest class

__init__: constructor simply defining the random forest with help of our parameters and creating the required number of trees. create_tree: creates a new decision tree by calling the constructor of class DecisionTree which, for now has been assumed a black box. We will write it’s code later. Each tree receives a random subset of features (feature bagging) and a random set of rows (bagging trees although this is optional I’ve written it to show it’s possibility) predict: our random forest’s prediction is simply the average of all the decision tree predictions.

This is how easy it is to think of a random forest if we can magically create trees. Now we decrease the level of abstraction and write code to create a decision tree.

You can stop reading here if you know how to write a decision tree from scratch but if you don’t then read further.

Decision tree class

It will have the following parameters :-

indxs: this parameter exists to keep track of which indices of the original set goes to the right and which goes to the left tree. Hence every tree has this parameter “indxs” which stores the indices of the rows it contains. Prediction is made by averaging these rows. min_leaf: minimum row samples required at a leaf node to be able to cause a split. Every leaf node will have row samples less than min_leaf because they can no more split (ignoring the depth constraint). depth: Max depth or max number of splits possible within each tree.

Why are decision trees only binary?

We’re using the property decorator to make our code more concise.

__init__ : the decision tree constructor. It has several interesting snippets to look into:

a. if idxs is None: idxs=np.arange(len(y)) in case we don’t specify the indices of the rows in this particular tree’s calculations, simply take all the rows.

b. self.val = np.mean(y[idxs]) each decision tree predicts a value which is the average of all the rows it’s holding. The variable self.val holds the prediction for each node of the tree. For the root node, the value will simply be the average of all observations because it holds all the rows as we’ve made no split yet. I’ve used the term ‘node’ here because essentially a decision tree is just a node with a decision trees on it’s right and one on it’s left.

c. self.score = float(‘inf’) score of a node is calculated on the basis of how “well” it divides the original data set. We will define this “well” later, lets just assume for now that we have a way to measure such a quantity. Also, the score is set to infinity for our node because we haven’t made any splits yet thus our in-existent split is infinitely bad, indicating that any split will be better than no split.

d. self.find_varsplit() we make our first split!

2. find_varsplit: we use brute force approach to find the best split. This function loops through all the columns sequentially and finds the best split among them all. This function is still incomplete as it only makes a single split, later we extend this function to make left and right decision tree for every split until we reach the leaf node.

3. split_name: A property decorator to return the name of the column we’re splitting over. var_idx is the index of this column, we will calculate this index in the find_better_split function along with value of the column we split upon

4. split_col: A property decorator to return the column at index var_idx with elements at indices given by indxs variable. Basically, segregates a column with selected rows.

5. find_better_split: this functions finds the best possible split in a certain column, it’s a complicated so we’ve considered it as a black box in the above code. Lets define it later.

4. is_leaf: A leaf node is the one that has never made a split thus it has a score of infinite (as mentioned above) hence this functions is used to identify leaf nodes. Also in case we’ve crossed maximum depth i.e. self.depth <= 0, it’s a leaf node as we can’t go any deeper.

How to find the best split?

Decision trees train by splitting the data into two halves recursively based on certain conditions. If a test set has 10 columns with 10 data points (values) in each column, a total of 10x10 = 100 splits are possible, our task in hand is to find which of these splits is the best for our data.

We compare splits based on how well does it split our data into two halves. We make splits such that each of the two half has the most “similar” type of data. One way to increase this similarity is by reducing the variance or standard deviation of both halves. Thus we want to minimize the weighted average (score) of the standard deviation of two halves. We use the greedy approach to find the split by dividing the data into two halves for every value in the column and calculate weighted average (score) of standard deviation of the two halves to ultimately find the minimum. (greedy approach)

To speed things up, we can make a copy of the column and sort it to calculate the weighted average by splitting over value at n+1th index using the value of sum and sum of squares of values of two halves created by split over nth index. This is based on the following formula of standard deviation :

The following images pictorially showcase the process of score calculations, the last column in each image is a single number representing the score of split i.e weighted average of left and right standard deviations.

Proceed by sorting each column

Sort the column

Now we make splits index-sequentially

Start with index = 0

index = 1

index = 2 (best split)

index = 3

index = 4

index = 5

By simple greedy approach, we find that the split made over index = 2 is the best possible split since it has the least score. We perform same steps for all the columns later and compare them all to greedily find the best (minimum) score.

Following is the simple code for above pictorial representations :

The following snippet of code are interesting and need some explanations:

function std_agg calculates standard deviation using sum and sum of squares of values curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt the split score for each iteration is simply the weighted average of standard deviation of the two halves with number of rows in each half as their weights. Lower score facilitates lower variance, lower variance facilitates grouping of similar data which will lead to better predictions. if curr_score<self.score:

self.var_idx,self.score,self.split = var_idx,curr_score,xi whenever the current score is better (less than the currently saved score in self.score ) we will update the current score and store this column in the variable self.var_idx (remember this is the variable that helps selecting the column for two of our property decorators) and save the value upon which the split is made, in the variable self.split .

Now that we know how to find the best split for a chosen column, we need to recursively define splits for each of the further decision trees. For every tree we find the best column and it’s value to split upon then we recursively make two decision trees until we’ve reached the leaf node. To do this, we extend our incomplete function find_varsplit as follows:

for i in range(self.c): self.find_better_split(i) this loop goes through every single column and tries to find the best split in that column. By the time this loop ends we’ve exhausted all possible splits over all the columns and found the best possible split. The column to split upon is stored in self.var_idx variable and value of that column we split over is stored in self.split variable. Now we recursively form the two halves i.e. right and left decision trees until we’ve reached the leaf node. if self.is_leaf: return if we’ve reached the leaf node, we no more need to find better splits and simply end this tree. self.lhs holds the left decision tree. The rows it receive are held in lhs which are the indices of all such values in the selected column that are less than or equal to split value stored in split.value . The variable lf_indxs holds the sample of features (columns) the left tree receives (feature bagging). self.rhs holds the right decision tree. rhs stores the indices of rows passed onto the right decision tree. rhs is calculated in a way similar to lhs again, the variable rf_indxs holds the sample of features (columns) the right tree receives (feature bagging).

Conclusion

Here’s the complete code

The intention of this post is to make readers and myself more familiar with the general working of random forests for it’s better application and debugging in future. Random forests have many more parameters and associated complexities that could not be covered in a single post by me. To study a more robust and wholesome code I suggest you read the sklearn module’s random forest code which is open source.

I will like to extend my thanks to Jeremy howard and Rachel thomas for the fast.ai machine learning course which enabled me to write this post. In fact, all of my code here is created after small alterations to the original fast.ai course material.

If I made mistakes / was unclear in my explanations please let me know in the responses. Thank you for reading.","['rows', 'forest', 'column', 'forests', 'python', 'decision', 'trees', 'random', 'tree', 'node', 'score', 'scratch', 'split']","Aggregation: The core concept that makes random forests better than decision trees is aggregating uncorrelated trees.
Python codeTo start coding our random forest from scratch, we will follow the top down approach.
Random forest classWe are creating a random forest regressor, although the same code can be slightly modified to create a classifier.
y: the corresponding dependent variables necessary for supervised learning (Random forest is a supervised learning technique) n_trees : number of uncorrelated trees we ensemble to create the random forest.
Let’s start defining our random forest class__init__: constructor simply defining the random forest with help of our parameters and creating the required number of trees.",en,['Vaibhav Kumar'],2019-09-10 13:53:22.566000+00:00,"{'Python', 'Machine Learning', 'Random Forests', 'Decision Trees', 'Fastai'}","{'https://miro.medium.com/fit/c/96/96/1*qGmovAuVczAliG3yhSlx6g.jpeg', 'https://miro.medium.com/max/452/1*UsPs6CR1-ubnjxDgS8jEWw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1242/1*ArbOcF_4ZZ4m8wNUng-c_g.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*e18v-2-EabQTp_Bj92fkYA.png', 'https://miro.medium.com/max/60/1*MkbXFmPa8WhpH-kUz5JQZQ.png?q=20', 'https://miro.medium.com/max/50/1*UsPs6CR1-ubnjxDgS8jEWw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2402/1*R_VucwrVGU42IMTCkCk7OQ.png', 'https://miro.medium.com/max/60/1*vBu6qoRUvq7n-7g0fQKcTw.png?q=20', 'https://miro.medium.com/max/60/1*ArbOcF_4ZZ4m8wNUng-c_g.png?q=20', 'https://miro.medium.com/max/60/1*e18v-2-EabQTp_Bj92fkYA.png?q=20', 'https://miro.medium.com/max/60/1*ZMUrRqdNYbEkblqXyttOpg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*R_VucwrVGU42IMTCkCk7OQ.png?q=20', 'https://miro.medium.com/max/60/1*q2WKBJFKg2KwTOGedv3wMA.png?q=20', 'https://miro.medium.com/max/1244/1*kLUuCxxUujdxB6WirINs4w.png', 'https://miro.medium.com/max/1248/1*q2WKBJFKg2KwTOGedv3wMA.png', 'https://miro.medium.com/fit/c/160/160/1*qGmovAuVczAliG3yhSlx6g.jpeg', 'https://miro.medium.com/max/60/1*fEv4oADCjsJYZWFGIuCHkA.png?q=20', 'https://miro.medium.com/max/3808/1*Dt9cnTd5Tzo2dZmSCtzkRw.png', 'https://miro.medium.com/max/60/1*kLUuCxxUujdxB6WirINs4w.png?q=20', 'https://miro.medium.com/max/1244/1*MkbXFmPa8WhpH-kUz5JQZQ.png', 'https://miro.medium.com/max/60/1*Dt9cnTd5Tzo2dZmSCtzkRw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1254/1*ZMUrRqdNYbEkblqXyttOpg.png', 'https://miro.medium.com/max/1248/1*fEv4oADCjsJYZWFGIuCHkA.png', 'https://miro.medium.com/max/3784/1*vBu6qoRUvq7n-7g0fQKcTw.png', 'https://miro.medium.com/max/4300/1*e18v-2-EabQTp_Bj92fkYA.png'}",2020-03-05 00:15:12.743824,2.9383604526519775
https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3,"The Mathematics of Decision Trees, Random Forest and Feature Importance in Scikit-learn and Spark","Introduction

This post attempts to consolidate information on tree algorithms and their implementations in Scikit-learn and Spark. In particular, it was written to provide clarification on how feature importance is calculated.

There are many great resources online discussing how decision trees and random forests are created and this post is not intended to be that. Although it includes short definitions for context, it assumes the reader has a grasp on these concepts and wishes to know how the algorithms are implemented in Scikit-learn and Spark.

So, let’s start with…

Decision Trees

Decision trees learn how to best split the dataset into smaller and smaller subsets to predict the target value. The condition, or test, is represented as the “leaf” (node) and the possible outcomes as “branches” (edges). This splitting process continues until no further gain can be made or a preset rule is met, e.g. the maximum depth of the tree is reached.

Decision Tree Algorithms

There are multiple algorithms and the scikit-learn documentation provides an overview of a few of these (link)

So what do Scikit-learn and Spark use?

Scikit-learn documentation states it is using “an optimized version of the CART algorithm”. Whilst not explicitly mentioned in the documentation, it has been inferred that Spark is using ID3 with CART.

So let’s focus on these two — ID3 and CART.

The advantages and disadvantages have been taken from the paper Comparative Study ID3, CART and C4.5 Decision Tree Algorithm: A Survey. More thorough definitions can also be found there.

ID3

The algorithm creates a multi-way tree — each node can have two or more edges — finding the categorical feature that will maximize the information gain using the impurity criterion entropy. Not only can it not handle numerical features, it is only appropriate for classification problems.

Advantages

Understandable prediction rules are created from the training data

Builds the fastest tree

Builds a short tree

Only need enough attributes until all data is classified

Finding leaf nodes enable test data to be pruned, reducing number of tests

Whole dataset is searched to create tree

Disadvantages

Data may be over-fitted or over-classified, if a small sample is tested

Only one attribute at a time is tested for making a decision

Does not handle numeric attributes and missing values

CART

CART stands for Classification and Regression Trees. The algorithm creates a binary tree — each node has exactly two outgoing edges — finding the best numerical or categorical feature to split using an appropriate impurity criterion. For classification, Gini impurity or twoing criterion can be used. For regression, CART introduced variance reduction using least squares (mean square error).

Advantages

CART can easily handle both numerical and categorical variables

CART algorithm will itself identify the most significant variables and eliminate non-significant ones

CART can easily handle outliers

Disadvantages

CART may have unstable decision tree

CART splits by one by one variable

Node impurity / Impurity Criterion

Both Scikit-learn and Spark provide information in their documentation on the formulas used for impurity criterion. For classification, they both use Gini impurity by default but offer Entropy as an alternative. For regression, both calculate variance reduction using Mean Square Error. Additionally, variance reduction can be calculated with Mean Absolute Error in Scikit-learn.

Impurity Formulas used by Scikit-learn and Spark

Links to Documentation on Tree Algorithms

Information Gain

Another term worth noting is “Information Gain” which is used with splitting the data using entropy. It is calculated as the decrease in entropy after the dataset is split on an attribute:

Gain(T,X) = Entropy(T) — Entropy(T,X)

T = target variable

X = Feature to be split on

Entropy(T,X) = The entropy calculated after the data is split on feature X

Random Forests

Random forests (RF) construct many individual decision trees at training. Predictions from all trees are pooled to make the final prediction; the mode of the classes for classification or the mean prediction for regression. As they use a collection of results to make a final decision, they are referred to as Ensemble techniques.

Feature Importance

Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.

Implementation in Scikit-learn

For each decision tree, Scikit-learn calculates a nodes importance using Gini Importance, assuming only two child nodes (binary tree):

ni sub(j)= the importance of node j

w sub(j) = weighted number of samples reaching node j

C sub(j)= the impurity value of node j

left(j) = child node from left split on node j

right(j) = child node from right split on node j

sub() is being used as subscript isn’t available in Medium

See method compute_feature_importances in _tree.pyx

The importance for each feature on a decision tree is then calculated as:

fi sub(i)= the importance of feature i

ni sub(j)= the importance of node j

These can then be normalized to a value between 0 and 1 by dividing by the sum of all feature importance values:

The final feature importance, at the Random Forest level, is it’s average over all the trees. The sum of the feature’s importance value on each trees is calculated and divided by the total number of trees:

RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model

normfi sub(ij)= the normalized feature importance for i in tree j

T = total number of trees

See method feature_importances_ in forest.py

Notation was inspired by this StackExchange thread which I found incredible useful for this post.

Implementation in Spark

For each decision tree, Spark calculates a feature’s importance by summing the gain, scaled by the number of samples passing through the node:

fi sub(i) = the importance of feature i

s sub(j) = number of samples reaching node j

C sub(j) = the impurity value of node j

See method computeFeatureImportance in treeModels.scala

To calculate the final feature importance at the Random Forest level, first the feature importance for each tree is normalized in relation to the tree:

normfi sub(i) = the normalized importance of feature i

fi sub(i) = the importance of feature i

Then feature importance values from each tree are summed normalized:

RFfi sub(i)= the importance of feature i calculated from all trees in the Random Forest model

normfi sub(ij)= the normalized feature importance for i in tree j

See method featureImportances in treeModels.scala

Conclusion

This goal of this model was to explain how Scikit-Learn and Spark implement Decision Trees and calculate Feature Importance values.

Hopefully by reaching the end of this post you have a better understanding of the appropriate decision tree algorithms and impurity criterion, as well as the formulas used to determine the importance of each feature in the model.","['forest', 'feature', 'importance', 'decision', 'impurity', 'scikitlearn', 'trees', 'mathematics', 'random', 'tree', 'node', 'spark', 'using', 'calculated']","In particular, it was written to provide clarification on how feature importance is calculated.
There are many great resources online discussing how decision trees and random forests are created and this post is not intended to be that.
Decision Tree AlgorithmsThere are multiple algorithms and the scikit-learn documentation provides an overview of a few of these (link)So what do Scikit-learn and Spark use?
The advantages and disadvantages have been taken from the paper Comparative Study ID3, CART and C4.5 Decision Tree Algorithm: A Survey.
Feature ImportanceFeature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node.",en,['Stacey Ronaghan'],2019-11-01 20:07:18.028000+00:00,"{'Scikit Learn', 'Decision Tree', 'Random Forest', 'Machine Learning', 'Spark'}","{'https://miro.medium.com/fit/c/96/96/1*MqwGIgUvf2L3hCBdvOKTVw.png', 'https://miro.medium.com/max/3268/1*C-bkgMBs4drNVyBb1VJcEQ.png', 'https://miro.medium.com/max/3516/1*oar13be_cUsLR35MA_t6WQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*gK2tXtlbz12oMCdniAPPlg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*eES0Bh8jTB73P3ad_U2aCA.png?q=20', 'https://miro.medium.com/max/2364/1*gK2tXtlbz12oMCdniAPPlg.png', 'https://miro.medium.com/max/2132/1*uZPnQKYNmy7Tf3DvZ0e5tQ.png', 'https://miro.medium.com/max/2708/1*Uxq4-qZccT5xv5Hhe7_CeA.png', 'https://miro.medium.com/max/3936/1*1DuXNkB5k0fsE1Titghpog.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/964/1*Tllkia1vJ0E8A6Cso2W1hw.png', 'https://miro.medium.com/fit/c/160/160/1*MqwGIgUvf2L3hCBdvOKTVw.png', 'https://miro.medium.com/max/60/1*oar13be_cUsLR35MA_t6WQ.png?q=20', 'https://miro.medium.com/max/60/1*C-bkgMBs4drNVyBb1VJcEQ.png?q=20', 'https://miro.medium.com/max/60/1*uZPnQKYNmy7Tf3DvZ0e5tQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/5852/1*eES0Bh8jTB73P3ad_U2aCA.png', 'https://miro.medium.com/max/60/1*1DuXNkB5k0fsE1Titghpog.png?q=20', 'https://miro.medium.com/max/60/1*Tllkia1vJ0E8A6Cso2W1hw.png?q=20', 'https://miro.medium.com/max/1928/1*Tllkia1vJ0E8A6Cso2W1hw.png', 'https://miro.medium.com/max/60/1*Uxq4-qZccT5xv5Hhe7_CeA.png?q=20'}",2020-03-05 00:15:20.023535,7.278709411621094
https://medium.com/datadriveninvestor/decision-tree-and-random-forest-e174686dd9eb,Decision Tree and Random Forest,"In this article we will learn what is decision tree, how are decision trees built, advantages and disadvantages of decision tree. What is random forest, how they improve the predictive power over decision tree.

Prerequisites: Machine Learning

Decision Tree and Random Forest

What is Decision Tree?

Decision Tree is a supervised, non parametric machine learning algorithm. Used for both classification as well as regression problems.

It is a graphical representation of tree like structure with all possible solutions. It helps to reach a decision based on certain conditions.

sample decision tree

How are decision tree built?

Decision on how to split heavily impacts accuracy of decision tree. It splits nodes based on available input variables. Selects the input variable resulting in best homogenous dataset.

Let’s take a sample of 12 people and classify them as healthy or unhealthy based on input features .

Exercise for 30 min at least 5 time a week

Eat healthy

Active

Data for building decision tree

For this example we will use CART — Classification and Regression Tree which uses Gini Index(impurity measure) and Information Gain Index to build trees.

What is Gini Index?

Gini Index is a measure of node purity or impurity. It is a measure of how often a randomly chosen variable will be misclassified.

while building decision tree we want to know what will be the right question to start with. We split the dataset on people who exercise for more than 30 min 5 times a week. Next we randomly chose an instance and want to know the probability of often we will misclassify. This is Gini Index.

Gini Index

let’s understand by dividing the dataset where people exercised for 30 min or more for at least 5 times a week.

we have reordered the data such that all green rows are instances with Exercise column is yes. Yellow rows are instances with exercise column as no.

When we randomly pull an instance from green rows, we will never misclassify as we will always get the right answer that person is healthy.

Calculating the Gini for green rows

Gini = 1- [(2/2)*(2/2)]= 1–1=0

A value of 0 for Gini means all cases in the green dataset fall in the same class. we see that all green rows are falling in the healthy class. Data is said be pure so impurity is 0.

Calculating Gini for all the rows, we have half of the dataset healthy and other half unhealthy

Gini = 1- [(3/6) *(3/6) + (3/6)*(3/6)] = 1-[0.25 + 0.25]=1-.5=0.5

What is Information Gain?

Information Gain is the difference between uncertainty of the starting node and weighted impurity of the two child nodes. Information gain decides which feature should be used to split the data.

To construct a decision tree, we split the feature with the purest child nodes. Information gain helps us find the question that reduces uncertainty.

We will first split the feature with the highest information gain. This is a recursive process till all child nodes are pure or until the information gain is zero.

In our example, based on the highest information gain, the best question is “Are you Active?”. This will be the root of the decision tree.

information gain calculation

we recursively keep finding the best question, partitioning the data, find the impurity of the child nodes till all the child nodes are pure or Information gain is zero.

This is the final decision tree we constructed based on our data.

Final Decision Tree

Advantages of decision tree

Easy to interpret. Straightforward graphical visualization is intuitive makes it easy for anyone to understand

Useful to identify variables most helpful in prediction. It splits based on the attribute significance.

Handles both continuous and categorical targets attributes.

Performs well on large datasets

Not influenced by outliers and missing values to an extent.

Non parametric method- does make an assumption about the form of the mapping function. As they do not make any assumption about mapping function they are free to learn any functional form present in the data

Trees can handle qualitative input variables without the need to create dummy variables

Disadvantages of decision tree

Decision trees are prone to overfitting

Gives most optimal solution but not globally optimal solution

Decision trees do not have same predictive accuracy compared to other regression and classification models

We use another algorithm called Random Forest to overcome the disadvantages of decision tree.

What is Random Forest?

Let’s say you are asked to estimate how many candies are there in the jar. you need to do this without opening the jar.

You can ask different people to estimate for you and then if you take an average of all the different estimates, you will be very close the actual count.

This is exactly how random forest works. Random Forest increases predictive power of the algorithm and also helps prevent overfitting.

Random forest is the most simple and widely used algorithm. Used for both classification and regression. It is an ensemble of randomized decision trees. Each decision tree gives a vote for the prediction of target variable. Random forest choses the prediction that gets the most vote.

An ensemble learning model aggregates multiple machine learning models to give a better performance. In random forest we use multiple random decision trees for a better accuracy.

Random Forest is a ensemble bagging algorithm to achieve low prediction error. It reduces the variance of the individual decision trees by randomly selecting trees and then either average them or picking the class that gets the most vote.

Bagging is a method for generating multiple versions of a predictor to get an aggregated predictor

Advantages of Random Forest

High predictive accuracy.

Efficient on large datasets

Ability to handle multiple input features without need for feature deletion

Prediction is based on input features considered important for classification.

Works well with missing data still giving a better predictive accuracy

Disadvantages of random forest

Not easily interpretable

Random forest overfit with noisy classification or regression

References:

https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#overview","['information', 'forest', 'gain', 'nodes', 'decision', 'based', 'trees', 'tree', 'random', 'input']","In this article we will learn what is decision tree, how are decision trees built, advantages and disadvantages of decision tree.
What is random forest, how they improve the predictive power over decision tree.
Prerequisites: Machine LearningDecision Tree and Random ForestWhat is Decision Tree?
sample decision treeHow are decision tree built?
In random forest we use multiple random decision trees for a better accuracy.",en,['Renu Khandelwal'],2018-11-14 10:26:16.126000+00:00,"{'Bagging', 'Ensemble Learning', 'Decision Tree', 'Random Forest', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*NuQ3tk-xH6IOTQv8K4Zi5w.png?q=20', 'https://miro.medium.com/max/60/0*f7s_KNFwUEqYxQdr?q=20', 'https://miro.medium.com/max/1290/1*WkP2wu5RArOjrOYCQpDjuQ.png', 'https://miro.medium.com/max/60/1*HhGnXhNj99lJWiPpL7-MXg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*-pV8MMqetli5oQhovupPrg.jpeg', 'https://miro.medium.com/fit/c/80/80/0*ewcxmwIHC3RQwRqB.', 'https://miro.medium.com/max/60/1*WkP2wu5RArOjrOYCQpDjuQ.png?q=20', 'https://miro.medium.com/max/744/1*RbCOrEB6jpw0aO9Dw--Q-w.png', 'https://miro.medium.com/max/60/0*gpTGirn4qC6L87WF?q=20', 'https://miro.medium.com/max/1200/1*q_oTf__vOEWcQUhxxYGVEg.png', 'https://miro.medium.com/max/1690/1*VUN62Do2u9B3DaDNXHvPMg.png', 'https://miro.medium.com/max/2148/1*NuQ3tk-xH6IOTQv8K4Zi5w.png', 'https://miro.medium.com/fit/c/80/80/0*030HOaJEWhVa-6K_.', 'https://miro.medium.com/max/342/1*cSP1Sq2WYVd_4qGa2kU6IQ.png', 'https://miro.medium.com/max/1400/0*f7s_KNFwUEqYxQdr', 'https://miro.medium.com/max/4982/1*ZS6OKcOfL41pbdNNn0Pf1A.png', 'https://miro.medium.com/max/1400/0*gpTGirn4qC6L87WF', 'https://miro.medium.com/max/2460/1*q_oTf__vOEWcQUhxxYGVEg.png', 'https://miro.medium.com/max/686/1*Yn9Kp6IidHnybg51SBCG5g.png', 'https://miro.medium.com/max/52/1*cSP1Sq2WYVd_4qGa2kU6IQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*2mBCfRUpdSYRuf9EKnhTDQ.png', 'https://miro.medium.com/fit/c/96/96/1*O9kJeCvkRxESxQVwubW-Ww.jpeg', 'https://miro.medium.com/max/60/1*VUN62Do2u9B3DaDNXHvPMg.png?q=20', 'https://miro.medium.com/max/60/1*RbCOrEB6jpw0aO9Dw--Q-w.png?q=20', 'https://miro.medium.com/max/60/1*Yn9Kp6IidHnybg51SBCG5g.png?q=20', 'https://miro.medium.com/max/60/1*ZS6OKcOfL41pbdNNn0Pf1A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*O9kJeCvkRxESxQVwubW-Ww.jpeg', 'https://miro.medium.com/max/432/1*fr3BFYQ7ZiodLBbY1emz9g.png', 'https://miro.medium.com/max/60/1*q_oTf__vOEWcQUhxxYGVEg.png?q=20', 'https://miro.medium.com/max/1430/1*HhGnXhNj99lJWiPpL7-MXg.png'}",2020-03-05 00:15:22.299388,2.2758522033691406
https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0,A Feature Selection Tool for Machine Learning in Python,"On the left we have the plot_n most important features (plotted in terms of normalized importance where the total sums to 1). On the right we have the cumulative importance versus the number of features. The vertical line is drawn at threshold of the cumulative importance, in this case 99%.

Two notes are good to remember for the importance-based methods:

Training the gradient boosting machine is stochastic meaning the feature importances will change every time the model is run

This should not have a major impact (the most important features will not suddenly become the least) but it will change the ordering of some of the features. It also can affect the number of zero importance features identified. Don’t be surprised if the feature importances change every time!

To train the machine learning model, the features are first one-hot encoded. This means some of the features identified as having 0 importance might be one-hot encoded features added during modeling.

When we get to the feature removal stage, there is an option to remove any added one-hot encoded features. However, if we are doing machine learning after feature selection, we will have to one-hot encode the features anyway!

Low Importance Features

The next method builds on zero importance function, using the feature importances from the model for further selection. The function identify_low_importance finds the lowest importance features that do not contribute to a specified total importance.

For example, the call below finds the least important features that are not required for achieving 99% of the total importance:

fs.identify_low_importance(cumulative_importance = 0.99) 123 features required for cumulative importance of 0.99 after one hot encoding.

116 features do not contribute to cumulative importance of 0.99.

Based on the plot of cumulative importance and this information, the gradient boosting machine considers many of the features to be irrelevant for learning. Again, the results of this method will change on each training run.

To view all the feature importances in a dataframe:

fs.feature_importances.head(10)

The low_importance method borrows from one of the methods of using Principal Components Analysis (PCA) where it is common to keep only the PC needed to retain a certain percentage of the variance (such as 95%). The percentage of total importance accounted for is based on the same idea.

The feature importance based methods are really only applicable if we are going to use a tree-based model for making predictions. Besides being stochastic, the importance-based methods are a black-box approach in that we don’t really know why the model considers the features to be irrelevant. If using these methods, run them several times to see how the results change, and perhaps create multiple datasets with different parameters to test!

Single Unique Value Features

The final method is fairly basic: find any columns that have a single unique value. A feature with only one unique value cannot be useful for machine learning because this feature has zero variance. For example, a tree-based model can never make a split on a feature with only one value (since there are no groups to divide the observations into).

There are no parameters here to select, unlike the other methods:

fs.identify_single_unique() 4 features with a single unique value.

We can plot a histogram of the number of unique values in each category:

fs.plot_unique()

One point to remember is NaNs are dropped before calculating unique values in Pandas by default.

Removing Features

Once we’ve identified the features to discard, we have two options for removing them. All of the features to remove are stored in the ops dict of the FeatureSelector and we can use the lists to remove features manually. Another option is to use the remove built-in function.

For this method, we pass in the methods to use to remove features. If we want to use all the methods implemented, we just pass in methods = 'all' .

# Remove the features from all methods (returns a df)

train_removed = fs.remove(methods = 'all') ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run



Removed 140 features.

This method returns a dataframe with the features removed. To also remove the one-hot encoded features that are created during machine learning:

train_removed_all = fs.remove(methods = 'all', keep_one_hot=False) Removed 187 features including one-hot features.

It might be a good idea to check the features that will be removed before going ahead with the operation! The original dataset is stored in the data attribute of the FeatureSelector as a back-up!

Running all Methods at Once

Rather than using the methods individually, we can use all of them with identify_all . This takes a dictionary of the parameters for each method:

fs.identify_all(selection_params = {'missing_threshold': 0.6,

'correlation_threshold': 0.98,

'task': 'classification',

'eval_metric': 'auc',

'cumulative_importance': 0.99}) 151 total features out of 255 identified for removal after one-hot encoding.

Notice that the number of total features will change because we re-ran the model. The remove function can then be called to discard these features.

Conclusions

The Feature Selector class implements several common operations for removing features before training a machine learning model. It offers functions for identifying features for removal as well as visualizations. Methods can be run individually or all at once for efficient workflows.

The missing , collinear , and single_unique methods are deterministic while the feature importance-based methods will change with each run. Feature selection, much like the field of machine learning, is largely empirical and requires testing multiple combinations to find the optimal answer. It’s best practice to try several configurations in a pipeline, and the Feature Selector offers a way to rapidly evaluate parameters for feature selection.","['machine', 'feature', 'onehot', 'remove', 'change', 'importance', 'python', 'learning', 'selection', 'methods', 'model', 'unique', 'features', 'tool']","However, if we are doing machine learning after feature selection, we will have to one-hot encode the features anyway!
A feature with only one unique value cannot be useful for machine learning because this feature has zero variance.
All of the features to remove are stored in the ops dict of the FeatureSelector and we can use the lists to remove features manually.
To also remove the one-hot encoded features that are created during machine learning:train_removed_all = fs.remove(methods = 'all', keep_one_hot=False) Removed 187 features including one-hot features.
Feature selection, much like the field of machine learning, is largely empirical and requires testing multiple combinations to find the optimal answer.",en,['Will Koehrsen'],2018-06-22 20:36:12.547000+00:00,"{'Python', 'Machine Learning', 'Towards Data Science', 'Programming', 'Education'}","{'https://miro.medium.com/max/1516/1*fcLsRYskgzWxVoxj4npfvg.png', 'https://miro.medium.com/max/60/1*fcLsRYskgzWxVoxj4npfvg.png?q=20', 'https://miro.medium.com/max/2992/1*rqeNwjiakRS-SqsW9wCgrA.jpeg', 'https://miro.medium.com/max/622/1*fpLJQBGZWhQXPFG5FyA1kg.png', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*d1uRrw212LAmpjlszj7CFg.png?q=20', 'https://miro.medium.com/max/2180/1*W0qSMsheaWsXJBJ7i2pH4g.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1104/1*HJk89EkbcmriiWbxpV6Uew.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*hWCOAEWkH4z5BKKqkFAd1g.png?q=20', 'https://miro.medium.com/max/802/1*unCzyN2BgucGodbioUz-Kw.png', 'https://miro.medium.com/max/60/1*fpLJQBGZWhQXPFG5FyA1kg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*HJk89EkbcmriiWbxpV6Uew.png?q=20', 'https://miro.medium.com/max/60/1*F3BV5mUWG-GLP8gnS62Z6w.png?q=20', 'https://miro.medium.com/max/2152/1*hWCOAEWkH4z5BKKqkFAd1g.png', 'https://miro.medium.com/max/1200/1*rqeNwjiakRS-SqsW9wCgrA.jpeg', 'https://miro.medium.com/max/1816/1*_gK6g3YWylcgfL5Bz8JMUg.png', 'https://miro.medium.com/max/60/1*W0qSMsheaWsXJBJ7i2pH4g.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*0WBIKN83twXyWfyx9LG7Qg.png?q=20', 'https://miro.medium.com/max/1256/1*d1uRrw212LAmpjlszj7CFg.png', 'https://miro.medium.com/max/1212/1*0WBIKN83twXyWfyx9LG7Qg.png', 'https://miro.medium.com/max/60/1*_gK6g3YWylcgfL5Bz8JMUg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1228/1*F3BV5mUWG-GLP8gnS62Z6w.png', 'https://miro.medium.com/max/60/1*rqeNwjiakRS-SqsW9wCgrA.jpeg?q=20', 'https://miro.medium.com/max/60/1*unCzyN2BgucGodbioUz-Kw.png?q=20'}",2020-03-05 00:15:29.280528,6.981140375137329
https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420,A Complete Machine Learning Project Walk-Through in Python: Part One,"Problem Definition

The first step before we get coding is to understand the problem we are trying to solve and the available data. In this project, we will work with publicly available building energy data from New York City.

The objective is to use the energy data to build a model that can predict the Energy Star Score of a building and interpret the results to find the factors which influence the score.

The data includes the Energy Star Score, which makes this a supervised regression machine learning task:

Supervised: we have access to both the features and the target and our goal is to train a model that can learn a mapping between the two

we have access to both the features and the target and our goal is to train a model that can learn a mapping between the two Regression: The Energy Star score is a continuous variable

We want to develop a model that is both accurate — it can predict the Energy Star Score close to the true value — and interpretable — we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models.

Data Cleaning

Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or anomalies (looking at you mtcars and iris datasets). Real-world data is messy which means we need to clean and wrangle it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems.

First, we can load in the data as a Pandas DataFrame and take a look:

import pandas as pd

import numpy as np # Read in data into a dataframe

data = pd.read_csv('data/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2017__Data_for_Calendar_Year_2016_.csv') # Display top of dataframe

data.head()

What Actual Data Looks Like!

This is a subset of the full data which contains 60 columns. Already, we can see a couple issues: first, we know that we want to predict the ENERGY STAR Score but we don’t know what any of the columns mean. While this isn’t necessarily an issue — we can often make an accurate model without any knowledge of the variables — we want to focus on interpretability, and it might be important to understand at least some of the columns.

When I originally got the assignment from the start-up, I didn’t want to ask what all the column names meant, so I looked at the name of the file,

and decided to search for “Local Law 84”. That led me to this page which explains this is an NYC law requiring all buildings of a certain size to report their energy use. More searching brought me to all the definitions of the columns. Maybe looking at a file name is an obvious place to start, but for me this was a reminder to go slow so you don’t miss anything important!

We don’t need to study all of the columns, but we should at least understand the Energy Star Score, which is described as:

A 1-to-100 percentile ranking based on self-reported energy usage for the reporting year. The Energy Star score is a relative measure used for comparing the energy efficiency of buildings.

That clears up the first problem, but the second issue is that missing values are encoded as “Not Available”. This is a string in Python which means that even the columns with numbers will be stored as object datatypes because Pandas converts a column with any strings into a column of all strings. We can see the datatypes of the columns using the dataframe.info() method:

# See the column data types and non-missing values

data.info()

Sure enough, some of the columns that clearly contain numbers (such as ft²), are stored as objects. We can’t do numerical analysis on strings, so these will have to be converted to number (specifically float ) data types!

Here’s a little Python code that replaces all the “Not Available” entries with not a number ( np.nan ), which can be interpreted as numbers, and then converts the relevant columns to the float datatype:

Once the correct columns are numbers, we can start to investigate the data.

Missing Data and Outliers

In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let’s get a sense of how many missing values are in each column (see the notebook for code).

(To create this table, I used a function from this Stack Overflow Forum).

While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem (here is a discussion), and for this project, we will remove any columns with more than 50% missing values.

At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the definition of extreme outliers:

Below the first quartile − 3 ∗ interquartile range

Above the third quartile + 3 ∗ interquartile range

(For the code to remove the columns and the anomalies, see the notebook). At the end of the data cleaning and anomaly removal process, we are left with over 11,000 buildings and 49 features.","['machine', 'star', 'score', 'energy', 'column', 'project', 'python', 'columns', 'learning', 'problem', 'data', 'model', 'missing', 'complete', 'values', 'walkthrough']","In this project, we will work with publicly available building energy data from New York City.
The Energy Star score is a relative measure used for comparing the energy efficiency of buildings.
That clears up the first problem, but the second issue is that missing values are encoded as “Not Available”.
Missing Data and OutliersIn addition to incorrect datatypes, another common problem when dealing with real-world data is missing values.
First, let’s get a sense of how many missing values are in each column (see the notebook for code).",en,['Will Koehrsen'],2018-05-21 12:31:11.116000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/1266/1*1Vm0gHqc0cXoDAgH0eiP_w.png', 'https://miro.medium.com/max/58/1*eHKEmJk0LrIQc8yxUYWhIg.png?q=20', 'https://miro.medium.com/max/1648/1*5pIn2VmzWxL2mzY8Z433hg.png', 'https://miro.medium.com/max/2692/1*C8K7UQDrD9ASsKU-diTN_Q.png', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/1692/1*ZSY_DEr-Gi2uSLp7tPSFVQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1148/1*apjbGlP15KRg8blBwq_96A.png', 'https://miro.medium.com/max/60/1*IjshGINnTET1wffZoC2LyA.jpeg?q=20', 'https://miro.medium.com/max/1070/1*VBR172ne94Os6bnOIRGDmw.png', 'https://miro.medium.com/max/1678/1*eHKEmJk0LrIQc8yxUYWhIg.png', 'https://miro.medium.com/max/1148/1*1OWn-GlBnERW5xs0M83a5Q.png', 'https://miro.medium.com/max/60/1*1Vm0gHqc0cXoDAgH0eiP_w.png?q=20', 'https://miro.medium.com/max/1640/1*lBRxdBwvf5seFNwnJH7s8Q.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*lBRxdBwvf5seFNwnJH7s8Q.png?q=20', 'https://miro.medium.com/max/60/1*mNju0CER7cYrvbr-4vx-Fw.png?q=20', 'https://miro.medium.com/max/60/1*C8K7UQDrD9ASsKU-diTN_Q.png?q=20', 'https://miro.medium.com/max/1692/1*WGNYzKjgXSnahDwacrbdLw.png', 'https://miro.medium.com/max/60/1*5pIn2VmzWxL2mzY8Z433hg.png?q=20', 'https://miro.medium.com/max/60/1*sMBTwTO63m_mM-Y-6VFveA.png?q=20', 'https://miro.medium.com/max/60/1*eNySZpSyjMZ3qaFGm9Fx4w.png?q=20', 'https://miro.medium.com/max/60/1*WGNYzKjgXSnahDwacrbdLw.png?q=20', 'https://miro.medium.com/max/60/1*1OWn-GlBnERW5xs0M83a5Q.png?q=20', 'https://miro.medium.com/max/1200/1*IjshGINnTET1wffZoC2LyA.jpeg', 'https://miro.medium.com/max/60/1*VBR172ne94Os6bnOIRGDmw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*apjbGlP15KRg8blBwq_96A.png?q=20', 'https://miro.medium.com/max/1316/1*sMBTwTO63m_mM-Y-6VFveA.png', 'https://miro.medium.com/max/1118/1*mNju0CER7cYrvbr-4vx-Fw.png', 'https://miro.medium.com/max/1512/1*eNySZpSyjMZ3qaFGm9Fx4w.png', 'https://miro.medium.com/max/60/1*ZSY_DEr-Gi2uSLp7tPSFVQ.png?q=20', 'https://miro.medium.com/max/3200/1*IjshGINnTET1wffZoC2LyA.jpeg'}",2020-03-05 00:15:36.070270,6.788755416870117
https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and LambdaMART)","RankNet, LambdaRank and LambdaMART are all what we call Learning to Rank algorithms.

What is Learning to Rank?

Learning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:

Traditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.

LTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn’t care much about the exact score that each item gets, but cares more about the relative ordering among all the items.

The most common application of LTR is search engine ranking, but it’s useful anywhere you need to produce a ranked list of items.

The training data for a LTR model consists of a list of items and a “ground truth” score for each of those items. For search engine ranking, this translates to a list of results for a query and a relevance rating for each of those results with respect to the query. The most common way used by major search engines to generate these relevance ratings is to ask human raters to rate results for a set of queries. In case you are interested, I have written in detail on human rating systems here: Nikhil Dandekar’s answer to How does Google measure the quality of their search results?

For a more technical explanation of Learning to Rank check this paper by Microsoft Research: A Short Introduction to Learning to Rank

What is RankNet, LambdaRank and LambdaMART?

RankNet, LambdaRank and LambdaMART are all LTR algorithms developed by Chris Burges and his colleagues at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank and then LambdaMART.

In all three techniques, ranking is transformed into a pairwise classification or regression problem. That means you look at pairs of items at a time, come up with the optimal ordering for that pair of items, and then use it to come up with the final ranking for all the results.

Here are some high-level details for each of the algorithms:

RankNet

RankNet was originally developed using neural nets, but the underlying model can be different and is not constrained to just neural nets. The cost function for RankNet aims to minimize the number of inversions in ranking. Here an inversion means an incorrect order among a pair of results, i.e. when we rank a lower rated result above a higher rated result in a ranked list. RankNet optimizes the cost function using Stochastic Gradient Descent.

LambdaRank

Burgess et. al. found that during RankNet training procedure, you don’t need the costs, only need the gradients (λ) of the cost with respect to the model score. You can think of these gradients as little arrows attached to each document in the ranked list, indicating the direction we’d like those documents to move.

Further they found that scaling the gradients by the change in NDCG found by swapping each pair of documents gave good results. The core idea of LambdaRank is to use this new cost function for training a RankNet. On experimental datasets, this shows both speed and accuracy improvements over the original RankNet.

LambdaMART

LambdaMART combines LambdaRank and MART (Multiple Additive Regression Trees). While MART uses gradient boosted decision trees for prediction tasks, LambdaMART uses gradient boosted decision trees using a cost function derived from LambdaRank for solving a ranking task. On experimental datasets, LambdaMART has shown better results than LambdaRank and the original RankNet.

If you are interested, Chris Burges has a single paper that details the evolution from RankNet to LambdaRank to LambdaMART here: From RankNet to LambdaRank to LambdaMART: An Overview","['ranknet', 'cost', 'intuitive', 'lambdamart', 'ranking', 'lambdarank', 'ltr', 'list', 'learning', 'items', 'explanation', 'rank', 'results']","RankNet, LambdaRank and LambdaMART are all what we call Learning to Rank algorithms.
Learning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems.
For a more technical explanation of Learning to Rank check this paper by Microsoft Research: A Short Introduction to Learning to RankWhat is RankNet, LambdaRank and LambdaMART?
RankNet, LambdaRank and LambdaMART are all LTR algorithms developed by Chris Burges and his colleagues at Microsoft Research.
If you are interested, Chris Burges has a single paper that details the evolution from RankNet to LambdaRank to LambdaMART here: From RankNet to LambdaRank to LambdaMART: An Overview",en,['Nikhil Dandekar'],2016-01-14 01:56:56.657000+00:00,"{'Search', 'Machine Learning', 'Data Science'}","{'https://miro.medium.com/fit/c/96/96/0*rA2v1hVm9nVmlB4T.jpeg', 'https://miro.medium.com/max/60/0*tB2Y51Kpp7xNWVqP.?q=20', 'https://miro.medium.com/max/1956/1*7W4o-zTHSoXkw8qZVswwIQ.png', 'https://miro.medium.com/fit/c/80/80/1*avFJkOo4PJPouc9Hgi5cnQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*7YOEId4Diyau0V3wnkkKpA.jpeg', 'https://miro.medium.com/max/978/1*7W4o-zTHSoXkw8qZVswwIQ.png', 'https://miro.medium.com/max/548/0*tB2Y51Kpp7xNWVqP.', 'https://miro.medium.com/fit/c/160/160/0*rA2v1hVm9nVmlB4T.jpeg', 'https://miro.medium.com/fit/c/80/80/1*TteREGG4ihAY3nO7ShdetA.jpeg', 'https://miro.medium.com/max/60/1*7W4o-zTHSoXkw8qZVswwIQ.png?q=20'}",2020-03-05 00:15:36.891321,0.8210504055023193
https://medium.com/@kyleake/how-to-install-r-in-jupyter-with-irkernel-in-3-steps-917519326e41,How to Install R in Jupyter with IRKernel in 3 Steps?,"How to Install R in Jupyter with IRKernel in 3 Steps?

The detailed installation of the R kernel for the ‘Jupyter’ environment

Recently, my work in RStudio has been very slow and taking up too much resource on my laptop. Therefore, I search for the alternative IDE to run my R jobs and bump into IRKernel, which enables the R programming language in Jupyter Notebook. This article will illustrate how to set up R in Jupyter in details.

Requirements

Before you proceed, it’s important to pre-install:

Jupyter notebook. The best way is to install Anaconda distribution, and everything will be ready made for you.

R from CRAN-R (Regularly check the latest update from CRAN-R)

For those who want to manually do it without Anaconda distributions, please follow the guidelines here:

Install IRKernal in Anaconda Prompt

Open the Anaconda Prompt

Image 2: Locate where Anaconda Prompt is.

2. To install IRKernel with conda run:

conda install -c r r-irkernel

During the installation process, Anaconda might ask if you want to proceed ([y]/n)? Just type ‘y’

Image 3: IRKernel’s Installation Process

3. Proceed to ‘Anaconda Navigator’ and launch ‘Jupyter Notebook’. If you go to the ‘New’ tab pane, R should appear in the ‘Notebooks’ section

Image 4: Running in Jupyter Notebook, Source: https://irkernel.github.io/running/

Let’s Write Some Basic Computations for Testing","['r', 'install', 'anaconda', 'steps', 'irkernel', 'installation', 'yn', 'yimage', 'proceed', 'jupyter', 'notebook']","How to Install R in Jupyter with IRKernel in 3 Steps?
Therefore, I search for the alternative IDE to run my R jobs and bump into IRKernel, which enables the R programming language in Jupyter Notebook.
The best way is to install Anaconda distribution, and everything will be ready made for you.
To install IRKernel with conda run:conda install -c r r-irkernelDuring the installation process, Anaconda might ask if you want to proceed ([y]/n)?
If you go to the ‘New’ tab pane, R should appear in the ‘Notebooks’ sectionImage 4: Running in Jupyter Notebook, Source: https://irkernel.github.io/running/Let’s Write Some Basic Computations for Testing",en,"['Korkrid Akepanidtaworn', 'Kyle']",2018-11-17 01:01:01.974000+00:00,"{'R Language', 'Kernel', 'Jupyter Notebook', 'Anaconda', 'Ipython'}","{'https://miro.medium.com/fit/c/96/96/2*YwJvVlpzOwNLuw5TICnIKA.jpeg', 'https://miro.medium.com/max/46/0*a9q7ZoeCoBXXFd-m.jpg?q=20', 'https://miro.medium.com/max/60/1*OTYsgSjzJ3z2UDCGCJ_mHg.png?q=20', 'https://miro.medium.com/max/1006/0*gzJm3IBjivf8m6EK.png', 'https://miro.medium.com/max/60/0*gzJm3IBjivf8m6EK.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*pWtM1DBoqyuKqIl13sdhrA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*AiKFMI6FEyDWnNMtiPAr1A.jpeg', 'https://miro.medium.com/max/738/0*a9q7ZoeCoBXXFd-m.jpg', 'https://miro.medium.com/max/2216/1*OTYsgSjzJ3z2UDCGCJ_mHg.png', 'https://miro.medium.com/fit/c/80/80/1*UtThEsUH-4zPoIp_9T-hug@2x.jpeg', 'https://miro.medium.com/max/60/1*KQxlE6kG_sCeFHFtjDExig.png?q=20', 'https://miro.medium.com/max/2774/1*KQxlE6kG_sCeFHFtjDExig.png', 'https://miro.medium.com/max/2012/0*gzJm3IBjivf8m6EK.png', 'https://miro.medium.com/fit/c/160/160/2*YwJvVlpzOwNLuw5TICnIKA.jpeg'}",2020-03-05 00:15:38.738116,1.845823049545288
https://medium.com/mlreview/mastering-fast-gradient-boosting-on-google-colaboratory-with-free-gpu-51abbbf4b860,Mastering Fast Gradient Boosting on Google Colaboratory with free GPU,"Step 2. Select “GPU” as the hardware accelerator.

Importing CatBoost

The next step is to import CatBoost inside the environment. Colaboratory has built in libraries installed and most libraries can be installed quickly with a simple !pip install command.

Please ignore the warning message about already imported enum package. Furthermore take note that you need to re-import the library every time you start a new session of Colab.

!pip install catboost

The CatBoost library that you install from pypi has GPU support, so you can use it straight away. You only need to have a NVIDIA driver installed on your machine, and everything else will work out-of-the-box. This is also true for Windows, making it easier for Windows users who want to train their models on GPUs.

Download and prepare dataset

It’s time to code! Once we’ve configured our environment, the next step is to download and prepare the dataset. For GPU training, the bigger the dataset is, the bigger the speedup will be. It doesn’t make a lot of sense to use GPU training for objects of one thousand or less, but starting from around 10,000 you will get a good acceleration.

We require a large dataset to demonstrate the power of GPUs for GBDT tasks. We will use Epsilon, which has 500,000 documents and 2,000 features, and is included in catboost.datasets.

The code below does this task in approximately 10–15 minutes. Please be patient.

Training on CPU

To dispel the myth about GBDT not showing large speed gains on GPU, I would like to compare GBDT training time on CPU and GPU. Let’s start with CPU. The code below creates a model, trains it, and measures the execution time of the training. It uses default parameters as they provide a fairly good baseline in many cases.

We will first train all our models for 100 iterations (because it takes a really long time to train it on CPUs).

After you run this code, you can change it to a default of 1000 or more iterations to get better quality results.

CatBoost will require around 15 minutes to train on CPUs for 100 iterations.

Time to fit model on CPU: 862 sec

Training on GPUs

All previous code execution has been done on a CPU. It’s time to use a GPU now! To enable the GPU training you need to use task_type=’GPU’ parameter. Let’s rerun the experiment on GPU and see what will be the resulting time.

If Colab will show you the warning “GPU memory usage is close to the limit”, just press “Ignore”.

Time to fit model on GPU: 195 sec

GPU speedup over CPU: 4x

As you can see, the GPU is 4x times faster than the CPU. It takes just 3–4 minutes vs 14–15 with a CPU to fit the model. Moreover, the learning process is complete in just 30 seconds vs 12 minutes.

When we train 100 iterations, the bottleneck is preprocessing and not the training itself. But for thousands of iterations that are necessary to get the best quality on huge datasets, this bottleneck will be invisible. You can try training for 5,000 iterations on a CPU and a GPU and compare once again.

Code

All the code above you can find as a tutorial for Google Colaboratory at CatBoost repository.

Alternative GBDT libraries

To be fair there are at least two more popular open source GBDT libraries:

Both are available as pre-installed libraries on Colaboratory. Let’s train both of them on CPU / GPU and compare times. Spoiler: We had no luck at receiving results from them. Details are below.

XGBoost

For the start let’s try the CPU version of XGBoost. Need to stress that training parameters is the same as for CatBoost library.

The code is quite simple, but once we’ve run it with Epsilon dataset Colaboratory session has crashed.

Unfortunately the same error occurs with the GPU version. Kernel dies after you start the simple code above.

LightGBM

CPU version of LightGBM results in the same error message “Your session crashed after using all available RAM.” error.

Colaboratory pre-installed version of LightGBM doesn’t contain GPU out of the box. And the installation guide for GPU version unfortunately doesn’t work. See details below.

!pip install -U lightgbm — install-option= — gpu

It’s a pity, but we couldn’t compare CPU vs GPU execution times for other libraries, except CatBoost’s. Possibly it’ll work with smaller dataset.

Summary

GBDT algorithm works efficiently on a GPU.

CatBoost is a fast implementation of GBDT with GPU support out-of-the-box.

XGBoost and LightGBM don’t always work on Colaboratory with large datasets.

Google Colaboratory is useful tool with free GPU support.

Further reading

[1] V. Ershov, CatBoost Enables Fast Gradient Boosting on Decision Trees Using GPUs, NVIDIA blog post

[2] R. Mitchell, Gradient Boosting, Decision Trees and XGBoost with CUDA, NVIDIA blog post

[3] LightGBM GPU Tutorial

[4] CatBoost GitHub","['code', 'gbdt', 'fast', 'catboost', 'version', 'train', 'colaboratory', 'lightgbm', 'cpu', 'free', 'mastering', 'gpu', 'training', 'google', 'boosting', 'gradient']","For GPU training, the bigger the dataset is, the bigger the speedup will be.
It doesn’t make a lot of sense to use GPU training for objects of one thousand or less, but starting from around 10,000 you will get a good acceleration.
Training on CPUTo dispel the myth about GBDT not showing large speed gains on GPU, I would like to compare GBDT training time on CPU and GPU.
To enable the GPU training you need to use task_type=’GPU’ parameter.
Further reading[1] V. Ershov, CatBoost Enables Fast Gradient Boosting on Decision Trees Using GPUs, NVIDIA blog post[2] R. Mitchell, Gradient Boosting, Decision Trees and XGBoost with CUDA, NVIDIA blog post[3] LightGBM GPU Tutorial[4] CatBoost GitHub",en,['Sergey Brazhnik'],2019-02-27 20:20:38.654000+00:00,"{'Decision Tree', 'Machine Learning', 'Gpu', 'Gradient Boosting'}","{'https://miro.medium.com/max/956/0*_QwvEsyoQqfzZrov', 'https://miro.medium.com/fit/c/160/160/1*3yjjoZ--eh8E84rH1xRIhQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*R6303tLavDAf6jJAsMlaJQ.jpeg', 'https://miro.medium.com/max/160/1*0ykYC2ubehxNL5XrIIbscg.png', 'https://miro.medium.com/max/1014/0*r6F31LpBgJUy6pFB', 'https://miro.medium.com/max/60/0*wf4SZdzs8EEPgBZY?q=20', 'https://miro.medium.com/max/1118/0*-pLmpYsxBDsVN00n', 'https://miro.medium.com/max/60/0*t4hCh0RUdUV1JQb0?q=20', 'https://miro.medium.com/max/2854/0*t4hCh0RUdUV1JQb0', 'https://miro.medium.com/max/60/0*_QwvEsyoQqfzZrov?q=20', 'https://miro.medium.com/fit/c/96/96/0*JAI6AWapx2N6wRi_', 'https://miro.medium.com/max/625/1*qOD3yW7jyEgS5fyl81H2PA.png', 'https://miro.medium.com/fit/c/80/80/0*SFI8OFVkiCx18BqR.', 'https://miro.medium.com/max/60/0*C_5YGVm2_Rtspixq?q=20', 'https://miro.medium.com/max/60/0*nnuzTt3gk0N91jzX?q=20', 'https://miro.medium.com/max/60/0*-pLmpYsxBDsVN00n?q=20', 'https://miro.medium.com/max/60/1*UlhIXhJCA1bLNFunh2DRlw.png?q=20', 'https://miro.medium.com/max/3200/0*C_5YGVm2_Rtspixq', 'https://miro.medium.com/max/4320/1*UlhIXhJCA1bLNFunh2DRlw.png', 'https://miro.medium.com/fit/c/80/80/1*RvCgxD54qDce6hO_YzUNvA.jpeg', 'https://miro.medium.com/max/60/0*r6F31LpBgJUy6pFB?q=20', 'https://miro.medium.com/max/882/0*nnuzTt3gk0N91jzX', 'https://miro.medium.com/max/60/1*qOD3yW7jyEgS5fyl81H2PA.png?q=20', 'https://miro.medium.com/max/1728/0*wf4SZdzs8EEPgBZY', 'https://miro.medium.com/max/1250/1*qOD3yW7jyEgS5fyl81H2PA.png', 'https://miro.medium.com/fit/c/160/160/0*JAI6AWapx2N6wRi_'}",2020-03-05 00:15:39.686265,0.9471542835235596
https://medium.com/@cibiaananth/how-to-add-commit-and-push-to-git-using-one-command-on-windows-25d756f444b7,"How to add, commit and push to git using one command on Windows?","In case of fire: git commit, git push, leave the building. But how using one command? Because every second matters.

When someone says they use svn rather than git

Yes, we all are git-committed for life. Git commits and push has become an everyday routine for developers. But the question is how do we use these command every day.

After we started to use single command to commit and push finally we increased our development time.

So Let’s init.

The idea of using one simple command instead of three is to stop being too mainstream. Believe me, it is very simple.

First, let’s see the traditional way of git push.

git add .

git commit -m ""<your message here>""

git push

If you notice we are using three commands every day for the same task. What if told you, you can do the same with one single command?

Here it is.

Step 1

Open Notepad and create a new file.

Step 2

Write the commands in the following order.

git add .

git commit -m ""<your message here>""

git push

Remember, the order of command is more important.So first decide the workflow whether you are going to add, commit and push at a time or add it and later git commit and push. Based on your workflow you can write the commands here.

Step 3

The next step is to save the file with .bat extension.

Select File → Save → enter the file name with .bat extension. Now select Save as type and choose All Files and save it inside your project local git repo.

a batch file is a text file containing a series of commands intended to be executed by the command interpreter. And it will execute line by line starting from line 1.

Now if you go and click the batch file, you can see that the commands are executed from top to bottom. But this is not the perfect solution because the commit message varies for each commit. So let’s be more practical.

Open the batch file in Notepad and edit the second line like

git commit -m %1

%1 is used to pass the dynamic commit message from the command line. That’s all. Now we have to figure out how to run this file from terminal.

How to run a batch file from the terminal

To run a batch file from terminal cd/locate_your_file and then

<file_name>.bat

or simply

<file_name>

and hit ENTER. This will run your batch file.

Now come back to where we left. Here we have to pass our commit message to the batch file. In order to do that go to terminal and type

<file_name> ""<your commit message here> ""

In my case, the example would be,

push ""hello from batch file""

the %1 takes the first argument we pass after the file name. So the message we give here will replace the %1 in our batch file. We can pass up to 9 arguments corresponding to %9.

That’s it we are all set to push using one command.

BTW is it perfect solution?

Well as you noticed we need to have this batch file in our project repo which most of the developers would not like. Also we need to copy paste the batch file inside each local repo. So, in that case, we need to save this batch file in a separate folder and set the path variable. To do that,

First, save the batch file in a different folder and copy the path. Next, open the environment variable settings,

In Windows 10 and Windows 8

In Search, search for Environment Variables.

Click Edit system environment Variables and click environment Variables.

Windows 7

From the desktop, right-click the Computer icon.

icon. Choose Properties from the context menu.

from the context menu. Click the Advanced system settings link.

link. Click Environment Variables.

In the section System Variables, find the PATH environment variable and select it. Click Edit. If the PATH environment variable does not exist, click New .

In the Edit System Variable (or New System Variable) window, specify the path to the batch file and click OK and close all remaining windows by clicking OK.

Now open your terminal and enter the filename and commit message and hit ENTER it will run the commands no matter where the batch file is.

In my case the filename is push.bat

But remember as I told this completely depends on your git workflow and you can use the commands the way you want.

Happy coding :)","['file', 'batch', 'message', 'environment', 'click', 'push', 'variable', 'commands', 'commit', 'git', 'windows', 'command', 'add', 'using']","In case of fire: git commit, git push, leave the building.
git commit -m ""<your message here>""git pushIf you notice we are using three commands every day for the same task.
git commit -m ""<your message here>""git pushRemember, the order of command is more important.So first decide the workflow whether you are going to add, commit and push at a time or add it and later git commit and push.
But this is not the perfect solution because the commit message varies for each commit.
How to run a batch file from the terminalTo run a batch file from terminal cd/locate_your_file and then<file_name>.bator simply<file_name>and hit ENTER.",en,['Cibi Aananth'],2018-03-12 11:28:56.905000+00:00,"{'Bitbucket', 'Github', 'Terminal', 'Git', 'Cli'}","{'https://miro.medium.com/fit/c/80/80/1*r-KqDu-2P_8Owg9R_JV7dQ.jpeg', 'https://miro.medium.com/fit/c/160/160/2*wPEmqAO5srymqABtyMLNvQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*nnnuTbyq4l4NYZ5psYvAWw.jpeg', 'https://miro.medium.com/max/1200/1*ZTZ78mKvVANNPVdjwlI9YA.jpeg', 'https://miro.medium.com/proxy/1*zjDpQDpl1JySqK0hVyNemw.png', 'https://miro.medium.com/fit/c/80/80/1*xUACZKT3j9iLHi_O6gL0Yg.jpeg', 'https://miro.medium.com/proxy/1*ZTZ78mKvVANNPVdjwlI9YA.jpeg', 'https://miro.medium.com/proxy/1*AxqKHNiuzFbB8bhGvMtC6Q.gif', 'https://miro.medium.com/proxy/1*BO0-DZsDMIHCYZrm-x58yw.gif', 'https://miro.medium.com/fit/c/96/96/2*wPEmqAO5srymqABtyMLNvQ.jpeg'}",2020-03-05 00:15:46.526280,0.7998950481414795
https://medium.com/data-from-the-trenches/automatic-feature-engineering-an-event-driven-approach-b2ca09d166f,Automatic Feature Engineering: An Event-Driven Approach,"Feature Engineering: the Heart of Data Science

“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.” — Dr. Jason Brownlee

The groundwork for this field was laid long before the hype of Kaggle and Machine Learning, with KPI design.

Key Performance Indicators (KPIs) are crucial for companies of all sizes. They offer concrete metrics on business performance. Take the classic RFM (recency, frequency, monetary value) paradigm that is used by retailers to measure customer value. More recently, the Power User Curve is an interesting example of how thoughtful metrics help companies understand user engagement in a complex context.

We believe that a good feature engineering tool should not only generate sophisticated indicators, but should also keep them interpretable so that data scientists can use them either for the machine learning models or for the KPI dashboards.

The Need for Automated Feature Engineering

Imagine you are working for an e-commerce company. You have collected transactional data and are now almost ready to make some magic with machine learning.

The task at hand is churn prediction: you want to predict who might stop visiting your website next week so that the marketing team has enough time to react.

Before doing any feature engineering, you need to choose a reference date in the past, in this case it will be 2018–09–01. Only data before that date will be taken into account by the model, which predicts the churners of the week after 2018–09–01. This is a way to make sure that there is no data leaking: we are not looking at the future to predict the past.

As an experienced data scientist, you know that one important feature for this type of problem will be the recency of the client: if the timespan between two visits of a client is increasing, that’s an alert to potential!

You put on your SQL ninja hat and write the following PostgreSQL query:

This is fine, but now you want to go further: you want to add a time filter to capture long and short term signals, then you want to compute this feature for each type of activity the user does, and then you want to add some more statistics on top of these results, and then …

The list of features we would love to compute

You get the idea, the list of ideas keeps growing exponentially, and this is just for one feature!

EventsAggregator to the Rescue

Now let’s see how things change with EventsAggregator.

First, we need to instantiate the feature_aggregator object as follows:

We then apply the feature_aggregator to the input dataset:

Under the hood, feature_aggregator generates a set of SQL queries corresponding to our criteria.

For example, you can see below one of the generated queries for the Postgres database, where it considers only the 6 most recent months of history:","['machine', 'feature', 'type', 'eventdriven', 'automatic', 'approach', 'user', 'need', 'engineering', 'data', 'week', 'feature_aggregator', 'value']","Take the classic RFM (recency, frequency, monetary value) paradigm that is used by retailers to measure customer value.
More recently, the Power User Curve is an interesting example of how thoughtful metrics help companies understand user engagement in a complex context.
We believe that a good feature engineering tool should not only generate sophisticated indicators, but should also keep them interpretable so that data scientists can use them either for the machine learning models or for the KPI dashboards.
The Need for Automated Feature EngineeringImagine you are working for an e-commerce company.
Before doing any feature engineering, you need to choose a reference date in the past, in this case it will be 2018–09–01.",en,['Du Phan'],2018-10-08 14:55:44.581000+00:00,"{'Feature Engineering', 'Artificial Intelligence', 'Data Scien', 'Machine Learning', 'Data Analysis'}","{'https://miro.medium.com/fit/c/160/160/1*QG9CAzhMDB6P9wMsIjBIRQ.png', 'https://miro.medium.com/fit/c/160/160/0*NozUELhAXnIq92_b.', 'https://miro.medium.com/max/3328/1*LhxmulumaCDQxXNXoYD0Iw.png', 'https://miro.medium.com/fit/c/80/80/1*7h75Db8I0Yf4EqhKGoTRQw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*w2_c62k9MZhsN5vdHuxVJw.png', 'https://miro.medium.com/max/1000/1*JtTWgAcfVTWV8OTjT47Atg.jpeg', 'https://miro.medium.com/max/1722/1*U5jTAltMXjs7rKZpcU6KuQ.png', 'https://miro.medium.com/fit/c/80/80/2*xiPKBNij9wlyR9x8U8oFlA.png', 'https://miro.medium.com/max/800/1*68ASXTZ-sd5GwskImuMV5A.jpeg', 'https://miro.medium.com/max/60/1*U5jTAltMXjs7rKZpcU6KuQ.png?q=20', 'https://miro.medium.com/max/60/1*68ASXTZ-sd5GwskImuMV5A.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/0*NozUELhAXnIq92_b.', 'https://miro.medium.com/max/354/1*5GG2QGLTmzDILM9JOO8ygA.png', 'https://miro.medium.com/max/400/1*68ASXTZ-sd5GwskImuMV5A.jpeg', 'https://miro.medium.com/max/60/1*JtTWgAcfVTWV8OTjT47Atg.jpeg?q=20', 'https://miro.medium.com/max/60/1*LhxmulumaCDQxXNXoYD0Iw.png?q=20'}",2020-03-05 00:15:48.557516,2.031235694885254
https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96,Why Automated Feature Engineering Will Change the Way You Do Machine Learning,"DFS works using functions called “primitives” to aggregate and transform our data. These primitives can be as simple as taking a mean or a max of a column, or they can be complex and based on subject expertise because Featuretools allows us to define our own custom primitives.

Feature primitives include many operations we already would do manually by hand, but with Featuretools, instead of re-writing the code to apply these operations on different datasets, we can use the same exact syntax across any relational database. Moreover, the power of DFS comes when we stack primitives on each other to create deep features. (For more on DFS, take a look at this blog post by one of the inventors of the technique.)

Deep Feature Synthesis is flexible — allowing it to be applied to any data science problem — and powerful — revealing insights in our data by creating deep features.

I’ll spare you the few lines of code needed for the set-up, but the action of DFS happens in a single line. Here we make thousands of features for each client using all 7 tables in our dataset ( ft is the imported featuretools library) :

# Deep feature synthesis

feature_matrix, features = ft.dfs(entityset=es,

target_entity='clients',

agg_primitives = agg_primitives,

trans_primitives = trans_primitives)

Below are some of the 1820 features we automatically get from Featuretools:

The maximum total amount paid on previous loans by a client. This is created using a MAX and a SUM primitive across 3 tables.

and a primitive across 3 tables. The percentile ranking of a client in terms of average previous credit card debt. This uses a PERCENTILE and MEAN primitive across 2 tables.

and primitive across 2 tables. Whether or not a client turned in two documents during the application process. This uses a AND transform primitive and 1 table.

Each of these features is built using simple aggregations and hence is human-interpretable. Featuretools created many of the same features I did manually, but also thousands I never would have conceived — or had the time to implement. Not every single feature will be relevant to the problem, and some of the features are highly correlated, nonetheless, having too many features is a better problem than having too few!

After a little feature selection and model optimization, these features did slightly better in a predictive model compared to the manual features with an overall development time of 1 hour, a 10x reduction compared to the manual process. Featuretools is much faster both because it requires less domain knowledge and because there are considerably fewer lines of code to write.

I’ll admit that there is a slight time cost to learning Featuretools but it’s an investment that will pay off. After taking an hour or so to learn Featuretools, you can apply it to any machine learning problem.

The following graphs sum up my experience for the loan repayment problem:","['machine', 'way', 'feature', 'automated', 'change', 'primitive', 'primitives', 'dfs', 'learning', 'engineering', 'featuretools', 'client', 'problem', 'features', 'using', 'tables']","DFS works using functions called “primitives” to aggregate and transform our data.
Moreover, the power of DFS comes when we stack primitives on each other to create deep features.
Deep Feature Synthesis is flexible — allowing it to be applied to any data science problem — and powerful — revealing insights in our data by creating deep features.
I’ll admit that there is a slight time cost to learning Featuretools but it’s an investment that will pay off.
After taking an hour or so to learn Featuretools, you can apply it to any machine learning problem.",en,['Will Koehrsen'],2018-08-09 15:51:43.418000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/60/1*hwh1sOK9_GcKRYA4pg9vEA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/3182/1*hwh1sOK9_GcKRYA4pg9vEA.png', 'https://miro.medium.com/max/60/1*6B-QTXQZgeuf_YWrCCxMpw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*aEY3yRu-N4YJNDE6Pi0LRA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*kFW26Zy7Fuze197LSOl4Iw.jpeg', 'https://miro.medium.com/max/4496/1*kFW26Zy7Fuze197LSOl4Iw.jpeg', 'https://miro.medium.com/max/3154/1*j378-FXFsLyb7vsNOhcQOA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*wVzNbqJ5JHSqjf8x0uCzxw.png?q=20', 'https://miro.medium.com/max/60/1*nuRuzRMYJno7W8WckHT32A.png?q=20', 'https://miro.medium.com/max/60/1*j378-FXFsLyb7vsNOhcQOA.png?q=20', 'https://miro.medium.com/max/1414/1*nuRuzRMYJno7W8WckHT32A.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/6150/1*aEY3yRu-N4YJNDE6Pi0LRA.png', 'https://miro.medium.com/max/60/1*DoNn5kB0I1BTEjhO2D3yOA.png?q=20', 'https://miro.medium.com/max/60/1*kFW26Zy7Fuze197LSOl4Iw.jpeg?q=20', 'https://miro.medium.com/max/1600/1*ER9NQ7QQ36WNgEoHSaDduQ.png', 'https://miro.medium.com/max/60/1*4FLqyRQrCKK4fZ-ISNhAyg.png?q=20', 'https://miro.medium.com/max/1474/1*6B-QTXQZgeuf_YWrCCxMpw.png', 'https://miro.medium.com/max/1414/1*wRb-oxJyAg_pD--pH6Knlg.png', 'https://miro.medium.com/max/630/1*wVzNbqJ5JHSqjf8x0uCzxw.png', 'https://miro.medium.com/max/60/1*ER9NQ7QQ36WNgEoHSaDduQ.png?q=20', 'https://miro.medium.com/max/60/1*wRb-oxJyAg_pD--pH6Knlg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2016/1*4FLqyRQrCKK4fZ-ISNhAyg.png', 'https://miro.medium.com/max/3154/1*DoNn5kB0I1BTEjhO2D3yOA.png'}",2020-03-05 00:15:56.188862,7.631346702575684
https://towardsdatascience.com/why-every-data-scientist-should-use-dask-81b2b850e15b,Why every Data Scientist should use Dask?,"Dask is simply the most revolutionary tool for data processing that I have encountered. If you love Pandas and Numpy but were sometimes struggling with data that would not fit into RAM then Dask is definitely what you need. Dask supports the Pandas dataframe and Numpy array data structures and is able to either be run on your local computer or be scaled up to run on a cluster. Essentially you write code once and then choose to either run it locally or deploy to a multi-node cluster using a just normal Pythonic syntax. This is a great feature in itself, but it is not why I am writing this blog post and saying that every Data Scientist (at least the one’s using Python) should use Dask. The magic Dask feature for me, has been that with minimal code changes, I can run code code in parallel taking advantage of the processing power already on my laptop. Processing data in parallel, means less time to execute, less time to wait and more time to analyse! This blog post will talk about dask.delayed and how it fits into the data science workflow.

Photograph from https://pixabay.com/en/skiing-departure-wag-trace-curves-16263/

“Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love” — https://dask.pydata.org/en/latest/

Getting Familiar with Dask

As an introduction to Dask, I’ll start with a few examples just to give you an indication of its completely unobtrusive and natural syntax. The main take away here is that you can use what you already know without needing to learn a new big data tool like Hadoop or Spark.

Dask introduces 3 parallel collections that are able to store data that is larger than RAM, namely Dataframes, Bags and Arrays. Each of these collection types are able to use data partitioned between RAM and a hard disk as well distributed across multiple nodes in a cluster.

Dask DataFrame is made up of smaller split up Pandas dataframes and therefore allows a subset of Pandas query syntax. Here is example code that loads all csv files in 2018, parses the timestamp field and then runs a Pandas query:

Dask Dataframe example

A Dask Bag is able to store and process collections of Pythonic objects that are unable to fit into memory. Dask Bags are great for processing logs and collections of json documents. In this code example, all json files from 2018 are loaded into a Dask Bag data structure, each json record is parsed and users are filtered using a lambda function:

Dask Bag example

Dask Arrays support Numpy like slicing. In the following code example, an HDF5 dataset in chunked into (5000, 5000) dimension blocks:

Dask Array example

Parallel Processing with Dask

An alternate accurate name for this section would be “Death of the sequential loop”. A common pattern I encounter regularly involves looping over a list of items and executing a python method for each item with different input arguments. Common data processing scenarios include, calculating feature aggregates for each customer or performing log event aggregation for each student. Instead of executing a function for each item in the loop in a sequential manner, Dask Delayed allows multiple items to be processed in parallel. With Dask Delayed each function call is queued, added to an execution graph and scheduled.

Writing custom thread handling or using asyncio has always looked a bit tedious to me, so I’m not even going to bother with showing you comparative examples. With Dask, you don’t need to change your programming style or syntax! You just need to annotate or wrap the method that will be executed in parallel with @dask.delayed and call the compute method after the loop code.

Example Dask computation graph

In the example below, two methods have been annotated with @dask.delayed. Three numbers are stored in a list which must be squared and then collectively summed. Dask constructs a computation graph which ensures that the “square” method is run in parallel and that the output is collated as a list and then passed to the sum_list method. The computation graph can be printed out by calling .visualize(). Calling .compute() executes the computation graph. As you can see in the output, the the list items are not processing in order and are run in parallel.

Dask Delayed demonstration

The number of threads can be set (i.e., dask.set_options( pool=ThreadPool(10) ) and its also easy to swap to use processes on your laptop or personal desktop (i.e., dask.config.set( scheduler=’processes’ ).

I have illustrated that adding parallel processing to your data science workflow is trivial with Dask. I’ve used Dask recently to split user clickstream data into 40 minute sessions and build user aggregate features for clustering. Please share a description of how you are using Dask as a comment to this blog post. Happy Dasking…..

Additional Resources","['scientist', 'pandas', 'syntax', 'dask', 'using', 'data', 'method', 'run', 'parallel', 'processing', 'code']","Dask is simply the most revolutionary tool for data processing that I have encountered.
Processing data in parallel, means less time to execute, less time to wait and more time to analyse!
Common data processing scenarios include, calculating feature aggregates for each customer or performing log event aggregation for each student.
I have illustrated that adding parallel processing to your data science workflow is trivial with Dask.
Please share a description of how you are using Dask as a comment to this blog post.",en,['Aneesha Bakharia'],2018-06-06 13:17:03.261000+00:00,"{'Numpy', 'Dask', 'Pandas', 'Data Science'}","{'https://miro.medium.com/max/60/1*2-WB7QNzZigwadzFQW9uZw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/0*buwULpatMyE7WR74.png', 'https://miro.medium.com/max/2932/1*2-WB7QNzZigwadzFQW9uZw.png', 'https://miro.medium.com/max/54/1*cWs4VxYaAvwgKbvW1XatOg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/0*buwULpatMyE7WR74.png', 'https://miro.medium.com/max/1760/1*cWs4VxYaAvwgKbvW1XatOg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*2-WB7QNzZigwadzFQW9uZw.png'}",2020-03-05 00:15:58.331577,2.141706943511963
https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d,Feature Engineering: What Powers Machine Learning,"Implementation of Feature Engineering

Currently, the only open-source Python library for automated feature engineering using multiple tables is Featuretools, developed and maintained by Feature Labs. For the customer churn problem, we can use Featuretools to quickly build features for the label times that we created in prediction engineering. (Full code available in this Jupyter Notebook).

We have three tables of data: customer background info, transactions, and user listening logs. If we were using manual feature engineering, we’d brainstorm and build features by hand, such as the average value of a customer’s transactions, or her total spending on weekends in the previous year. For each feature, we’d first have to filter the data to before the cutoff time for the label. In contrast, in our framework, we make use of Featuretools to automatically build hundreds of relevant features in a few lines of code.

We won’t go through the details of Featuretools, but the heart of the library is an algorithm called Deep Feature Synthesis which stacks the feature engineering building blocks known as primitives (simple operations like “max” or finding the “weekday” of a transaction) to build “deep features”. The library also automatically filters data for features based on the cutoff time.

For more on automated feature engineering in Featuretools see:","['machine', 'feature', 'transactions', 'learning', 'engineering', 'featuretools', 'data', 'powers', 'build', 'features', 'library', 'using', 'tables']","Implementation of Feature EngineeringCurrently, the only open-source Python library for automated feature engineering using multiple tables is Featuretools, developed and maintained by Feature Labs.
For the customer churn problem, we can use Featuretools to quickly build features for the label times that we created in prediction engineering.
If we were using manual feature engineering, we’d brainstorm and build features by hand, such as the average value of a customer’s transactions, or her total spending on weekends in the previous year.
For each feature, we’d first have to filter the data to before the cutoff time for the label.
For more on automated feature engineering in Featuretools see:",en,['Will Koehrsen'],2018-11-15 14:36:20.853000+00:00,"{'Business', 'Data Science', 'Machine Learning', 'Education', 'Predictive Analytics'}","{'https://miro.medium.com/max/60/1*I0l4Pa2tyYTZHmNhoRm2TA.png?q=20', 'https://miro.medium.com/max/60/1*7Fns1F6xvVlY8JyAlYamNw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*K6ctE0RZme0cqMtknrxq8A.png?q=20', 'https://miro.medium.com/max/3038/1*xB0Y-0TrSCJcDq1YcrwSbA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*xB0Y-0TrSCJcDq1YcrwSbA.png?q=20', 'https://miro.medium.com/max/3154/1*s68vgToQvQaHpFE7p95xfQ.png', 'https://miro.medium.com/max/1364/1*7Fns1F6xvVlY8JyAlYamNw.png', 'https://miro.medium.com/max/60/0*W8UQXa0By4zNHChY?q=20', 'https://miro.medium.com/max/1676/1*I0l4Pa2tyYTZHmNhoRm2TA.png', 'https://miro.medium.com/max/60/0*xNv_hc81-IeL3MyA?q=20', 'https://miro.medium.com/max/60/1*s68vgToQvQaHpFE7p95xfQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*K6ctE0RZme0cqMtknrxq8A.png', 'https://miro.medium.com/max/2400/1*K6ctE0RZme0cqMtknrxq8A.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/2984/0*xNv_hc81-IeL3MyA', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3200/0*W8UQXa0By4zNHChY'}",2020-03-05 00:16:01.655232,3.322650909423828
https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219,Automated Feature Engineering in Python,"If we have a machine learning task, such as predicting whether a client will repay a future loan, we will want to combine all the information about clients into a single table. The tables are related (through the client_id and the loan_id variables) and we could use a series of transformations and aggregations to do this process by hand. However, we will shortly see that we can instead use featuretools to automate the process.

Entities and EntitySets

The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes.

We can create an empty entityset in featuretools using the following:

import featuretools as ft # Create new entityset

es = ft.EntitySet(id = 'clients')

Now we have to add entities. Each entity must have an index, which is a column with all unique elements. That is, each value in the index must appear in the table only once. The index in the clients dataframe is the client_id because each client has only one row in this dataframe. We add an entity with an existing index to an entityset using the following syntax:

The loans dataframe also has a unique index, loan_id and the syntax to add this to the entityset is the same as for clients . However, for the payments dataframe, there is no unique index. When we add this entity to the entityset, we need to pass in the parameter make_index = True and specify the name of the index. Also, although featuretools will automatically infer the data type of each column in an entity, we can override this by passing in a dictionary of column types to the parameter variable_types .

For this dataframe, even though missed is an integer, this is not a numeric variable since it can only take on 2 discrete values, so we tell featuretools to treat is as a categorical variable. After adding the dataframes to the entityset, we inspect any of them:

The column types have been correctly inferred with the modification we specified. Next, we need to specify how the tables in the entityset are related.

Table Relationships

The best way to think of a relationship between two tables is the analogy of parent to child. This is a one-to-many relationship: each parent can have multiple children. In the realm of tables, a parent table has one row for every parent, but the child table may have multiple rows corresponding to multiple children of the same parent.

For example, in our dataset, the clients dataframe is a parent of the loans dataframe. Each client has only one row in clients but may have multiple rows in loans . Likewise, loans is the parent of payments because each loan will have multiple payments. The parents are linked to their children by a shared variable. When we perform aggregations, we group the child table by the parent variable and calculate statistics across the children of each parent.

To formalize a relationship in featuretools, we only need to specify the variable that links two tables together. The clients and the loans table are linked via the client_id variable and loans and payments are linked with the loan_id . The syntax for creating a relationship and adding it to the entityset are shown below:

The entityset now contains the three entities (tables) and the relationships that link these entities together. After adding entities and formalizing relationships, our entityset is complete and we are ready to make features.

Feature Primitives

Before we can quite get to deep feature synthesis, we need to understand feature primitives. We already know what these are, but we have just been calling them by different names! These are simply the basic operations that we use to form new features:

Aggregations: operations completed across a parent-to-child (one-to-many) relationship that group by the parent and calculate stats for the children. An example is grouping the loan table by the client_id and finding the maximum loan amount for each client.

table by the and finding the maximum loan amount for each client. Transformations: operations done on a single table to one or more columns. An example is taking the difference between two columns in one table or taking the absolute value of a column.

New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives):

Feature Primitives

These primitives can be used by themselves or combined to create features. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). We pass in the entityset , the target_entity , which is the table where we want to add the features, the selected trans_primitives (transformations), and agg_primitives (aggregations):

The result is a dataframe of new features for each client (because we made clients the target_entity ). For example, we have the month each client joined which is a transformation feature primitive:

We also have a number of aggregation primitives such as the average payment amounts for each client:

Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives.

The complete dataframe has 793 columns of new features!

Deep Feature Synthesis

We now have all the pieces in place to understand deep feature synthesis (dfs). In fact, we already performed dfs in the previous function call! A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature.

For example, the MEAN(payments.payment_amount) column is a deep feature with a depth of 1 because it was created using a single aggregation. A feature with a depth of two is LAST(loans(MEAN(payments.payment_amount)) This is made by stacking two aggregations: LAST (most recent) on top of MEAN. This represents the average payment size of the most recent loan for each client.

We can stack features to any depth we want, but in practice, I have never gone beyond a depth of 2. After this point, the features are difficult to interpret, but I encourage anyone interested to try “going deeper”.","['table', 'feature', 'automated', 'primitives', 'python', 'multiple', 'featuretools', 'engineering', 'parent', 'features', 'entityset', 'dataframe', 'tables']","Feature PrimitivesBefore we can quite get to deep feature synthesis, we need to understand feature primitives.
New features are created in featuretools using these primitives either by themselves or stacking multiple primitives.
Deep Feature SynthesisWe now have all the pieces in place to understand deep feature synthesis (dfs).
A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features.
The depth of a deep feature is the number of primitives required to make the feature.",en,['Will Koehrsen'],2018-06-05 23:10:29.020000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/2212/1*tewxbRVcXb_weoy_g6EfkA.png', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/388/1*gEQkpyTDxXz21_gUPeNlMQ.png', 'https://miro.medium.com/max/1316/1*_p-HwN54IjLvmSSlkkazUQ.png', 'https://miro.medium.com/max/700/1*RbgNzspaiwq74aWU6W5LWQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*jHHOuEft93KDenbRpaFcnA.png?q=20', 'https://miro.medium.com/max/60/1*W_jS8Z4Ym5zAFTdjHki1ig.png?q=20', 'https://miro.medium.com/max/60/1*RbgNzspaiwq74aWU6W5LWQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1000/1*lg3OxWVYDsJFN-snBY7M5w.jpeg', 'https://miro.medium.com/max/1774/1*jHHOuEft93KDenbRpaFcnA.png', 'https://miro.medium.com/max/60/1*FHR7tlD4FuGKt8n5UHUpqw.png?q=20', 'https://miro.medium.com/max/60/1*QQGYN1PD06rNT-bJphNcBA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*7aOkE5N-WCNQHJi1qBcqjQ.png?q=20', 'https://miro.medium.com/max/60/1*DZ44KuggN_4jWKwuhrpCaw.png?q=20', 'https://miro.medium.com/max/632/1*7aOkE5N-WCNQHJi1qBcqjQ.png', 'https://miro.medium.com/max/710/1*W_jS8Z4Ym5zAFTdjHki1ig.png', 'https://miro.medium.com/max/2000/1*lg3OxWVYDsJFN-snBY7M5w.jpeg', 'https://miro.medium.com/max/1110/1*95c7QchQVM-9xUUA4ZB4XQ.png', 'https://miro.medium.com/max/1994/1*q24CTYC4x7fHj0YFwdusoQ.png', 'https://miro.medium.com/max/956/1*QQGYN1PD06rNT-bJphNcBA.png', 'https://miro.medium.com/max/786/1*y28-ibs-ZCpCvavVPmmZAw.png', 'https://miro.medium.com/max/60/1*95c7QchQVM-9xUUA4ZB4XQ.png?q=20', 'https://miro.medium.com/max/60/1*_p-HwN54IjLvmSSlkkazUQ.png?q=20', 'https://miro.medium.com/max/60/1*tewxbRVcXb_weoy_g6EfkA.png?q=20', 'https://miro.medium.com/max/58/1*gEQkpyTDxXz21_gUPeNlMQ.png?q=20', 'https://miro.medium.com/max/608/1*FHR7tlD4FuGKt8n5UHUpqw.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*y28-ibs-ZCpCvavVPmmZAw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/992/1*DZ44KuggN_4jWKwuhrpCaw.png', 'https://miro.medium.com/max/60/1*lg3OxWVYDsJFN-snBY7M5w.jpeg?q=20', 'https://miro.medium.com/max/60/1*q24CTYC4x7fHj0YFwdusoQ.png?q=20'}",2020-03-05 00:16:04.474063,2.8188319206237793
https://towardsdatascience.com/how-to-create-value-with-machine-learning-eb09585b332e,How to Create Value with Machine Learning,"Business Motivation: Make Sure You Solve the Right Problem

The most sophisticated machine learning pipeline will have no impact unless it creates value for a company. Therefore, the first step in framing a machine learning task is understanding the business requirement so you can determine the right problem to solve. Throughout this series, we’ll work through the common problem of addressing customer churn.

For subscription-based business models, predicting which customers will churn — stop paying for a service for a specified period of time — is crucial. Accurately predicting if and when customers will churn lets businesses engage with those who are at risk for unsubscribing or offer them reduced rates as an incentive to maintain a subscription. An effective churn prediction model allows a company to be proactive in growing the customer base.

For the customer churn problem the business need is:

increase the number of paying subscribers by reducing customer churn rates.

Traditional methods of reducing customer churn require forecasting which customers would churn with survival-analysis techniques, but, given the abundance of historical customer behavior data, this presents an ideal application of supervised machine learning.

We can address the business problem with machine learning by building a supervised algorithm that learns from past data to predict customer churn.

Stating the business goal and expressing it in terms of a machine learning-solvable task is the critical first step in the pipeline. Once we know what we want to have the model predict, we can move on to using the available data to develop and solve a supervised machine learning problem.

Next Steps

Over the next three articles, we’ll apply the prediction engineering, feature engineering, and modeling framework to solve the customer churn problem on a dataset from KKBOX, Asia’s largest subscription music streaming service.

Look for the following posts (or check out the GitHub repository):

We’ll see how to fill in the details with existing data science tools and how to change the prediction problem without rewriting the complete pipeline. By the end, we’ll have an effective model for predicting churn that is tuned to satisfy the business requirement.

Precision-recall curve for model tuned to business need.

Through these articles, we’ll see an approach to machine learning that lets us rapidly build solutions for multiple prediction problems. The next time your boss changes the problem parameters, you’ll be able to have a new solution up and running with only a few lines of changes to the code.","['machine', 'create', 'prediction', 'solve', 'churn', 'learning', 'customer', 'problem', 'model', 'data', 'business', 'value']","Business Motivation: Make Sure You Solve the Right ProblemThe most sophisticated machine learning pipeline will have no impact unless it creates value for a company.
Therefore, the first step in framing a machine learning task is understanding the business requirement so you can determine the right problem to solve.
For the customer churn problem the business need is:increase the number of paying subscribers by reducing customer churn rates.
We can address the business problem with machine learning by building a supervised algorithm that learns from past data to predict customer churn.
Through these articles, we’ll see an approach to machine learning that lets us rapidly build solutions for multiple prediction problems.",en,['Will Koehrsen'],2018-11-15 14:36:42.898000+00:00,"{'Data Science', 'Data Analytics', 'Machine Learning', 'Education', 'Predictive Analytics'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*3U7P2LcWYLC8xdZ42VFQYw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*3U7P2LcWYLC8xdZ42VFQYw.png', 'https://miro.medium.com/max/60/1*7Fns1F6xvVlY8JyAlYamNw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/1364/1*7Fns1F6xvVlY8JyAlYamNw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/3164/0*2URnrRp5Gp-oAwJD', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*2URnrRp5Gp-oAwJD?q=20', 'https://miro.medium.com/max/12158/1*3U7P2LcWYLC8xdZ42VFQYw.png'}",2020-03-05 00:16:10.925574,6.4515111446380615
https://towardsdatascience.com/prediction-engineering-how-to-set-up-your-machine-learning-problem-b3b8f622683b,Prediction Engineering: How to Set Up Your Machine Learning Problem,"The output of prediction engineering is a label times table: a set of labels with negative and positive examples made from past data along with an associated cutoff time indicating when we have to stop using data to make features for that label (more on this shortly).

For our use case we’ll work through in this series — customer churn — we defined the business problem as increasing monthly active subscribers by reducing rates of churn. The machine learning problem is building a model to predict which customers will churn using historical data. The first step in this task is making a set of labels of past examples of customer churn.

The parameters for what constitutes a churn and how often we want to make predictions will vary depending on the business need, but in this example, let’s say we want to make predictions on the first of each month for which customers will churn one month out from the time of prediction. Churn will be defined as going more than 31 days without an active membership.

It’s important to remember this is only one definition of churn corresponding to one business problem. When we write functions to make labels, they should take in parameters so they can be quickly changed to different prediction problems.

Our goal for prediction engineering is a label times table as follows:

Example of label times table

The labels correspond to whether a customer churned or not based on historical data. Each customer is used as a training example multiple times because they have multiple months of data. Even if we didn’t use customers many times, because this is a time-dependent problem, we have to correctly implement the concept of cutoff times.

Cutoff Times: How to Ensure Your Features are Valid

The labels are not complete without the cutoff time which represents when we have to stop using data to make features for a label. Since we are making predictions about customer churn on the first of each month, we can’t use any data after the first to make features for that label. Our cutoff times are therefore all on the first of the month as shown in the label times table above.

All the features for each label must use data from before this time to prevent the problem of data leakage. Cutoff times are a crucial part of building successful solutions to time-series problems that many companies do not account for. Using invalid data to make features leads to models that do well in development but fail in deployment.

Imagine we did not limit our features to data that occurred before the first of the month for each label. Our model would figure out that customers who had a paid transaction during the month could not have churned in that month and would thus record high metrics. However, when it came time to deploy the model and make predictions for a future month, we do not have access to the future transactions and our model would perform poorly. It’s like a student who does great on homework because she has the answer key but then is lost on the test without the same information.","['machine', 'set', 'prediction', 'learning', 'engineering', 'churn', 'customer', 'data', 'month', 'labels', 'times', 'problem', 'features', 'label', 'cutoff']","The machine learning problem is building a model to predict which customers will churn using historical data.
The first step in this task is making a set of labels of past examples of customer churn.
Our goal for prediction engineering is a label times table as follows:Example of label times tableThe labels correspond to whether a customer churned or not based on historical data.
Our cutoff times are therefore all on the first of the month as shown in the label times table above.
Cutoff times are a crucial part of building successful solutions to time-series problems that many companies do not account for.",en,['Will Koehrsen'],2018-11-15 14:36:59.651000+00:00,"{'Data Science', 'Machine Learning', 'Engineering', 'Education', 'Predictive Analytics'}","{'https://miro.medium.com/max/60/1*P_UAY9ZJHZMF7S1sq_0t8w.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/834/0*QbjuCcl5F5AD0a40', 'https://miro.medium.com/max/2648/1*30DPomKK-OusQqrZ6d_d4g.png', 'https://miro.medium.com/max/60/0*s7d_Jy9K5q-wf6mu?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1300/0*ONGyVXrlMv68mUB7', 'https://miro.medium.com/max/60/1*30DPomKK-OusQqrZ6d_d4g.png?q=20', 'https://miro.medium.com/max/1200/1*P_UAY9ZJHZMF7S1sq_0t8w.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/0*NEMHLorkAHXsWF9M?q=20', 'https://miro.medium.com/max/418/0*pc8fb1ucXB3fHeUx', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*_1L27dwJfDmIy9BcW3WjSg.png?q=20', 'https://miro.medium.com/max/60/0*2o7xf1t3PJKuvwbu?q=20', 'https://miro.medium.com/max/46/0*QbjuCcl5F5AD0a40?q=20', 'https://miro.medium.com/max/60/0*ONGyVXrlMv68mUB7?q=20', 'https://miro.medium.com/max/1440/0*NEMHLorkAHXsWF9M', 'https://miro.medium.com/max/1800/0*s7d_Jy9K5q-wf6mu', 'https://miro.medium.com/max/60/0*pc8fb1ucXB3fHeUx?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2524/1*P_UAY9ZJHZMF7S1sq_0t8w.png', 'https://miro.medium.com/max/3200/0*2o7xf1t3PJKuvwbu', 'https://miro.medium.com/max/3816/1*_1L27dwJfDmIy9BcW3WjSg.png'}",2020-03-05 00:16:14.060910,3.1353352069854736
https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27,Interpretable Machine Learning with XGBoost,"This is a story about the danger of interpreting your machine learning model incorrectly, and the value of interpreting it correctly. If you have found the robust accuracy of ensemble tree models such as gradient boosting machines or random forests attractive, but also need to interpret them, then I hope you find this informative and helpful.

Imagine we are tasked with predicting a person’s financial status for a bank. The more accurate our model, the more money the bank makes, but since this prediction is used for loan applications we are also legally required to provide an explanation for why a prediction was made. After experimenting with several model types, we find that gradient boosted trees as implemented in XGBoost give the best accuracy. Unfortunately, explaining why XGBoost made a prediction seems hard, so we are left with the choice of retreating to a linear model, or figuring out how to interpret our XGBoost model. No data scientist wants to give up on accuracy…so we decide to attempt the latter, and interpret the complex XGBoost model (which happens to have 1,247 depth 6 trees).

Classic global feature importance measures

The first obvious choice is to use the plot_importance() method in the Python XGBoost interface. It gives an attractively simple bar-chart representing the importance of each feature in our dataset: (code to reproduce this article is in a Jupyter notebook)

Results of running xgboost.plot_importance(model) for a model trained to predict if people will report over $50k of income from the classic “adult” census dataset (using a logistic loss).

If we look at the feature importances returned by XGBoost we see that age dominates the other features, clearly standing out as the most important predictor of income. We could stop here and report to our manager the intuitively satisfying answer that age is the most important feature, followed by hours worked per week and education level. But being good data scientists…we take a look at the docs and see there are three options for measuring feature importance in XGBoost:

Weight. The number of times a feature is used to split the data across all trees. Cover. The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits. Gain. The average training loss reduction gained when using a feature for splitting.

These are typical importance measures that we might find in any tree-based modeling package. Weight was the default option so we decide to give the other two approaches a try to see if they make a difference:

Results of running xgboost.plot_importance with both importance_type=”cover” and importance_type=”gain”.

To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost! For the cover method it seems like the capital gain feature is most predictive of income, while for the gain method the relationship status feature dominates all the others. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best.

What makes a measure of feature importance good or bad?

It not obvious how to compare one feature attribution method to another. We could measure end-user performance for each method on tasks such as data-cleaning, bias detection, etc. But these tasks are only indirect measures of the quality of a feature attribution method. Here, we will instead define two properties that we think any good feature attribution method should follow:

Consistency. Whenever we change a model such that it relies more on a feature, then the attributed importance for that feature should not decrease. Accuracy. The sum of all the feature importances should sum up to the total importance of the model. (For example if importance is measured by the R² value then the attribution to each feature should sum to the R² of the full model)

If consistency fails to hold, then we can’t compare the attributed feature importances between any two models, because then having a higher assigned attribution doesn’t mean the model actually relies more on that feature.

If accuracy fails to hold then we don’t know how the attributions of each feature combine to represent the output of the whole model. We can’t just normalize the attributions after the method is done since this might break the consistency of the method.

Are current attribution methods consistent and accurate?

Back to our work as bank data scientists…we realize that consistency and accuracy are important to us. In fact if a method is not consistent we have no guarantee that the feature with the highest attribution is actually the most important. So we decide to the check the consistency of each method using two very simple tree models that are unrelated to our task at the bank:

Simple tree models over two features. Cough is clearly more important in model B than model A.

The output of the models is a risk score based on a person’s symptoms. Model A is just a simple “and” function for the binary features fever and cough. Model B is the same function but with +10 whenever cough is yes. To check consistency we must define “importance”. Here we will define importance two ways: 1) as the change in the model’s expected accuracy when we remove a set of features. 2) as the change in the model’s expected output when we remove a set of features.

The first definition of importance measures the global impact of features on the model. While the second definition measures the individualized impact of features on a single prediction. In our simple tree models the cough feature is clearly more important in model B, both for global importance and for the importance of the individual prediction when both fever and cough are yes.

The weight, cover, and gain methods above are all global feature attribution methods. But when we deploy our model in the bank we will also need individualized explanations for each customer. To check for consistency we run five different feature attribution methods on our simple tree models:

Tree SHAP. A new individualized method we are proposing. Saabas. An individualized heuristic feature attribution method. mean(|Tree SHAP|). A global attribution method based on the average magnitude of the individualized Tree SHAP attributions. Gain. The same method used above in XGBoost, and also equivalent to the Gini importance measure used in scikit-learn tree models. Split count. Represents both the closely related “weight” and “cover” methods in XGBoost, but is computed using the “weight” method. Permutation. The resulting drop in accuracy of the model when a single feature is randomly permuted in the test data set.

Feature attributions for model A and model B using six different methods. As far we can tell, these methods represent all the tree-specific feature attribution methods in the literature.

All the previous methods other than feature permutation are inconsistent! This is because they assign less importance to cough in model B than in model A. Inconsistent methods cannot be trusted to correctly assign more importance to the most influential features. The astute reader will notice that this inconsistency was already on display earlier when the classic feature attribution methods we examined contradicted each other on the same model. What about the accuracy property? It turns out Tree SHAP, Sabaas, and Gain are all accurate as defined earlier, while feature permutation and split count are not.

It is perhaps surprising that such a widely used method as gain (gini importance) can lead to such clear inconsistency results. To better understand why this happens let’s examine how gain gets computed for model A and model B. To make this simple we will assume that 25% of our data set falls into each leaf, and that the datasets for each model have labels that exactly match the output of the models.

If we consider mean squared error (MSE) as our loss function, then we start with an MSE of 1200 before doing any splits in model A. This is the error from the constant mean prediction of 20. After splitting on fever in model A the MSE drops to 800, so the gain method attributes this drop of 400 to the fever feature. Splitting again on the cough feature then leads to an MSE of 0, and the gain method attributes this drop of 800 to the cough feature. In model B the same process leads to an importance of 800 assigned to the fever feature and 625 to the cough feature:

Computation of the gain (aka. Gini importance) scores for model A and model B.

Typically we expect features near the root of the tree to be more important than features split on near the leaves (since trees are constructed greedily). Yet the gain method is biased to attribute more importance to lower splits. This bias leads to an inconsistency, where when cough becomes more important (and it hence is split on at the root) its attributed importance actually drops. The individualized Saabas method (used by the treeinterpreter package) calculates differences in predictions as we descend the tree, and so it also suffers from the same bias towards splits lower in the tree. As trees get deeper, this bias only grows. In contrast the Tree SHAP method is mathematically equivalent to averaging differences in predictions over all possible orderings of the features, rather than just the ordering specified by their position in the tree.

It is not a coincidence that only Tree SHAP is both consistent and accurate. Given that we want a method that is both consistent and accurate, it turns out there is only one way to allocate feature importances. The details are in our recent NIPS paper, but the summary is that a proof from game theory on the fair allocation of profits leads to a uniqueness result for feature attribution methods in machine learning. These unique values are called Shapley values, after Lloyd Shapley who derived them in the 1950’s. The SHAP values we use here result from a unification of several individualized model interpretation methods connected to Shapley values. Tree SHAP is a fast algorithm that can exactly compute SHAP values for trees in polynomial time instead of the classical exponential runtime (see arXiv).

Interpreting our model with confidence

The combination of a solid theoretical justification and a fast practical algorithm makes SHAP values a powerful tool for confidently interpreting tree models such as XGBoost’s gradient boosting machines. Armed with this new approach we return to the task of interpreting our bank XGBoost model:

The global mean(|Tree SHAP|) method applied to the income prediction model. The x-axis is essentially the average magnitude change in model output when a feature is “hidden” from the model (for this model the output has log-odds units). See papers for details, but “hidden” means integrating the variable out of the model. Since the impact of hiding a feature changes depending on what other features are also hidden, Shapley values are used to enforce consistency and accuracy.

We can see that the relationship feature is actually the most important, followed by the age feature. Since SHAP values have guaranteed consistency we don’t need to worry about the kinds of contradictions we found before using the gain, or split count methods. However, since we now have individualized explanations for every person, we can do more than just make a bar chart. We can plot the feature importance for every customer in our data set. The shap Python package makes this easy. We first call shap.TreeExplainer(model).shap_values(X) to explain every prediction, then call shap.summary_plot(shap_values, X) to plot these explanations:

Every customer has one dot on each row. The x position of the dot is the impact of that feature on the model’s prediction for the customer, and the color of the dot represents the value of that feature for the customer. Dots that don’t fit on the row pile up to show density (there are 32,561 customers in this example). Since the XGBoost model has a logistic loss the x-axis has units of log-odds (Tree SHAP explains the change in the margin output of the model).

The features are sorted by mean(|Tree SHAP|) and so we again see the relationship feature as the strongest predictor of making over $50K annually. By plotting the impact of a feature on every sample we can also see important outlier effects. For example, while capital gain is not the most important feature globally, it is by far the most important feature for a subset of customers. The coloring by feature value shows us patterns such as how being younger lowers your chance of making over $50K, while higher education increases your chance of making over $50K.

We could stop here and show this plot to our boss, but let’s instead dig a bit deeper into some of these features. We can do that for the age feature by plotting the age SHAP values (changes in log odds) vs. the age feature values:

The y-axis is how much the age feature changes the log odds of making over $50K annually. The x-axis is the age of the customer. Each dot represents a single customer from the data set.

Here we see the clear impact of age on earning potential as captured by the XGBoost model. Note that unlike traditional partial dependence plots (which show the average model output when changing a feature’s value) these SHAP dependence plots show interaction effects. Even though many people in the data set are 20 years old, how much their age impacts their prediction differs as shown by the vertical dispersion of dots at age 20. This means other features are impacting the importance of age. To see what feature might be part of this effect we color the dots by the number of years of education and see that a high level of education lowers the effect of age in your 20’s, but raises it in your 30's:

The y-axis is how much the age feature changes the log odds of making over $50K annually. The x-axis is the age of the customer. Education-Num is the number of years of education the customer has completed.

If we make another dependence plot for the number of hours worked per week we see that the benefit of working more plateaus at about 50 hrs/week, and working extra is less likely to indicate high earnings if you are married:

Hours worked per week vs. the impact of the number of hours worked on earning potential.

Interpreting your own model

This simple walk-through was meant to mirror the process you might go through when designing and deploying your own models. The shap package is easy to install through pip, and we hope it helps you explore your models with confidence. It includes more than what this article touched on, including SHAP interaction values, model agnostic SHAP value estimation, and additional visualizations. Notebooks are available that illustrate all these features on various interesting datasets. For example you can check out the top reasons you will die based on your health checkup in a notebook explaining an XGBoost model of mortality. For languages other than Python, Tree SHAP has also been merged directly into the core XGBoost and LightGBM packages.","['machine', 'feature', 'interpretable', 'importance', 'xgboost', 'learning', 'methods', 'model', 'method', 'shap', 'tree', 'features', 'age']","Unfortunately, explaining why XGBoost made a prediction seems hard, so we are left with the choice of retreating to a linear model, or figuring out how to interpret our XGBoost model.
To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost!
We can do that for the age feature by plotting the age SHAP values (changes in log odds) vs. the age feature values:The y-axis is how much the age feature changes the log odds of making over $50K annually.
For example you can check out the top reasons you will die based on your health checkup in a notebook explaining an XGBoost model of mortality.
For languages other than Python, Tree SHAP has also been merged directly into the core XGBoost and LightGBM packages.",en,['Scott Lundberg'],2019-08-13 20:56:08.192000+00:00,"{'Xgboost', 'Data Science', 'Data Visualization', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/1*FYaIhrCMXw_D10KOyQHYXA.png?q=20', 'https://miro.medium.com/max/60/1*VY7Lag3AjbEawjUuteftdg.png?q=20', 'https://miro.medium.com/max/1200/1*T7XtvWyesmYAXM0Y9Jv2Gw.jpeg', 'https://miro.medium.com/max/1800/1*FYaIhrCMXw_D10KOyQHYXA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*UEQiHKTnjHJ-swIjcAkRnA.png?q=20', 'https://miro.medium.com/max/1946/1*2ObHhjeC1Yj58R7Ofkv55Q.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*CPyNiwei9CSdFkZBcUyqWQ.png?q=20', 'https://miro.medium.com/max/3944/1*T7XtvWyesmYAXM0Y9Jv2Gw.jpeg', 'https://miro.medium.com/max/60/1*T7XtvWyesmYAXM0Y9Jv2Gw.jpeg?q=20', 'https://miro.medium.com/max/2000/1*T15dqSQRJFMxU3wgN4Rkhw.png', 'https://miro.medium.com/fit/c/96/96/1*SD3hMVq2yKA5BoxDIOp-AA.jpeg', 'https://miro.medium.com/max/60/1*IWBGb4PC7F2q0fszK-Iplw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2170/1*EO5ltTPLoqufclS_UNPnyA.png', 'https://miro.medium.com/max/3618/1*v4r9KuDjJBs2TIB7zw2E4A.png', 'https://miro.medium.com/max/1746/1*Fc0q-o56b1KW8rzMq0bQnQ.png', 'https://miro.medium.com/fit/c/160/160/1*SD3hMVq2yKA5BoxDIOp-AA.jpeg', 'https://miro.medium.com/max/60/1*EO5ltTPLoqufclS_UNPnyA.png?q=20', 'https://miro.medium.com/max/60/1*2ObHhjeC1Yj58R7Ofkv55Q.png?q=20', 'https://miro.medium.com/max/2918/1*CPyNiwei9CSdFkZBcUyqWQ.png', 'https://miro.medium.com/max/2198/1*VY7Lag3AjbEawjUuteftdg.png', 'https://miro.medium.com/max/60/1*Fc0q-o56b1KW8rzMq0bQnQ.png?q=20', 'https://miro.medium.com/max/3668/1*UEQiHKTnjHJ-swIjcAkRnA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/4000/1*IWBGb4PC7F2q0fszK-Iplw.png', 'https://miro.medium.com/max/60/1*v4r9KuDjJBs2TIB7zw2E4A.png?q=20', 'https://miro.medium.com/max/60/1*T15dqSQRJFMxU3wgN4Rkhw.png?q=20'}",2020-03-05 00:16:19.847085,5.78617525100708
https://towardsdatascience.com/10-python-pandas-tricks-that-make-your-work-more-efficient-2e8e483808ba,10 Python Pandas tricks that make your work more efficient,"10 Python Pandas tricks that make your work more efficient

Some commands you may know already but may not know they can be used this way

Photo from https://unsplash.com/

Pandas is a widely used Python package for structured data. There’re many nice tutorials of it, but here I’d still like to introduce a few cool tricks the readers may not know before and I believe they’re useful.

read_csv

Everyone knows this command. But the data you’re trying to read is large, try adding this argument: nrows = 5 to only read in a tiny portion of the table before actually loading the whole table. Then you could avoid the mistake by choosing wrong delimiter (it may not always be comma separated).

(Or, you can use ‘head’ command in linux to check out the first 5 rows (say) in any text file: head -n 5 data.txt (Thanks Ilya Levinson for pointing out a typo here))

Then, you can extract the column list by using df.columns.tolist() to extract all columns, and then add usecols = [‘c1’, ‘c2’, …] argument to load the columns you need. Also, if you know the data types of a few specific columns, you can add the argument dtype = {‘c1’: str, ‘c2’: int, …} so it would load faster. Another advantage of this argument that if you have a column which contains both strings and numbers, it’s a good practice to declare its type to be string, so you won’t get errors while trying to merge tables using this column as a key.

select_dtypes

If data preprocessing has to be done in Python, then this command would save you some time. After reading in a table, the default data types for each column could be bool, int64, float64, object, category, timedelta64, or datetime64. You can first check the distribution by

df.dtypes.value_counts()

to know all possible data types of your dataframe, then do

df.select_dtypes(include=['float64', 'int64'])

to select a sub-dataframe with only numerical features.

copy

This is an important command if you haven’t heard of it already. If you do the following commands:

import pandas as pd

df1 = pd.DataFrame({ 'a':[0,0,0], 'b': [1,1,1]})

df2 = df1

df2['a'] = df2['a'] + 1

df1.head()

You’ll find that df1 is changed. This is because df2 = df1 is not making a copy of df1 and assign it to df2, but setting up a pointer pointing to df1. So any changes in df2 would result in changes in df1. To fix this, you can do either

df2 = df1.copy()

or

from copy import deepcopy

df2 = deepcopy(df1)

map

This is a cool command to do easy data transformations. You first define a dictionary with ‘keys’ being the old values and ‘values’ being the new values.

level_map = {1: 'high', 2: 'medium', 3: 'low'}

df['c_level'] = df['c'].map(level_map)

Some examples: True, False to 1, 0 (for modeling); defining levels; user defined lexical encodings.

apply or not apply?

If we’d like to create a new column with a few other columns as inputs, apply function would be quite useful sometimes.

def rule(x, y):

if x == 'high' and y > 10:

return 1

else:

return 0 df = pd.DataFrame({ 'c1':[ 'high' ,'high', 'low', 'low'], 'c2': [0, 23, 17, 4]})

df['new'] = df.apply(lambda x: rule(x['c1'], x['c2']), axis = 1)

df.head()

In the codes above, we define a function with two input variables, and use the apply function to apply it to columns ‘c1’ and ‘c2’.

but the problem of ‘apply’ is that it’s sometimes too slow. Say if you’d like to calculate the maximum of two columns ‘c1’ and ‘c2’, of course you can do

df['maximum'] = df.apply(lambda x: max(x['c1'], x['c2']), axis = 1)

but you’ll find it much slower than this command:

df['maximum'] = df[['c1','c2']].max(axis =1)

Takeaway: Don’t use apply if you can get the same work done with other built-in functions (they’re often faster). For example, if you want to round column ‘c’ to integers, do round(df[‘c’], 0) or df[‘c’].round(0) instead of using the apply function: df.apply(lambda x: round(x['c'], 0), axis = 1) .

value counts

This is a command to check value distributions. For example, if you’d like to check what are the possible values and the frequency for each individual value in column ‘c’ you can do

df['c'].value_counts()

There’re some useful tricks / arguments of it:

A. normalize = True: if you want to check the frequency instead of counts.

B. dropna = False: if you also want to include missing values in the stats.

C. df['c'].value_counts().reset_index() : if you want to convert the stats table into a pandas dataframe and manipulate it

D. df['c'].value_counts().reset_index().sort_values(by='index') : show the stats sorted by distinct values in column ‘c’ instead of counts.

(Update 2019.4.18 — for D. above, Hao Yang points out a simpler way without .reset_index(): df['c'].value_counts().sort_index() )

number of missing values

When building models, you might want to exclude the row with too many missing values / the rows with all missing values. You can use .isnull() and .sum() to count the number of missing values within the specified columns.

import pandas as pd

import numpy as np df = pd.DataFrame({ 'id': [1,2,3], 'c1':[0,0,np.nan], 'c2': [np.nan,1,1]})

df = df[['id', 'c1', 'c2']]

df['num_nulls'] = df[['c1', 'c2']].isnull().sum(axis=1)

df.head()

select rows with specific IDs

In SQL we can do this using SELECT * FROM … WHERE ID in (‘A001’, ‘C022’, …) to get records with specific IDs. If you want to do the same thing with pandas, you can do

df_filter = df['ID'].isin(['A001','C022',...])

df[df_filter]

Percentile groups

You have a numerical column, and would like to classify the values in that column into groups, say top 5% into group 1, 5–20% into group 2, 20%-50% into group 3, bottom 50% into group 4. Of course, you can do it with pandas.cut, but I’d like to provide another option here:

import numpy as np

cut_points = [np.percentile(df['c'], i) for i in [50, 80, 95]]

df['group'] = 1

for i in range(3):

df['group'] = df['group'] + (df['c'] < cut_points[i])

# or <= cut_points[i]

which is fast to run (no apply function used).

to_csv

Again this is a command that everyone would use. I’d like to point out two tricks here. The first one is

print(df[:5].to_csv())

You can use this command to print out the first five rows of what are going to be written into the file exactly.

Another trick is dealing with integers and missing values mixed together. If a column contains both missing values and integers, the data type would still be float instead of int. When you export the table, you can add float_format=‘%.0f’ to round all the floats to integers. Use this trick if you only want integer outputs for all columns — you’ll get rid of all annoying ‘.0’s.","['table', 'column', 'pandas', 'c1', 'efficient', 'python', 'columns', 'apply', 'data', 'work', 'command', 'tricks', 'values', 'missing']","10 Python Pandas tricks that make your work more efficientSome commands you may know already but may not know they can be used this wayPhoto from https://unsplash.com/Pandas is a widely used Python package for structured data.
B. dropna = False: if you also want to include missing values in the stats.
(Update 2019.4.18 — for D. above, Hao Yang points out a simpler way without .reset_index(): df['c'].value_counts().sort_index() )number of missing valuesWhen building models, you might want to exclude the row with too many missing values / the rows with all missing values.
Another trick is dealing with integers and missing values mixed together.
If a column contains both missing values and integers, the data type would still be float instead of int.",en,['Shiu-Tang Li'],2019-05-20 07:27:45.097000+00:00,"{'Pandas', 'Data Science', 'Pandas Dataframe', 'Python', 'Dataframes'}","{'https://miro.medium.com/max/60/1*1zC8lx7tciV1hM6nKrMS2A.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/12000/1*1zC8lx7tciV1hM6nKrMS2A.jpeg', 'https://miro.medium.com/max/1200/1*1zC8lx7tciV1hM6nKrMS2A.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/0*yRqBomqb3QicwL8R', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/0*yRqBomqb3QicwL8R'}",2020-03-05 00:16:22.257183,2.410097599029541
https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a,A Complete Exploratory Data Analysis and Visualization for Text Data,"It worked!

Univariate visualization with Plotly

Single-variable or univariate visualization is the simplest type of visualization which consists of observations on only a single characteristic or attribute. Univariate visualization includes histogram, bar plots and line charts.

The distribution of review sentiment polarity score

df['polarity'].iplot(

kind='hist',

bins=50,

xTitle='polarity',

linecolor='black',

yTitle='count',

title='Sentiment Polarity Distribution')

Figure 4

Vast majority of the sentiment polarity scores are greater than zero, means most of them are pretty positive.

The distribution of review ratings

df['Rating'].iplot(

kind='hist',

xTitle='rating',

linecolor='black',

yTitle='count',

title='Review Rating Distribution')

Figure 5

The ratings are in align with the polarity score, that is, most of the ratings are pretty high at 4 or 5 ranges.

The distribution of reviewers age

df['Age'].iplot(

kind='hist',

bins=50,

xTitle='age',

linecolor='black',

yTitle='count',

title='Reviewers Age Distribution')

Figure 6

Most reviewers are in their 30s to 40s.

The distribution review text lengths

df['review_len'].iplot(

kind='hist',

bins=100,

xTitle='review length',

linecolor='black',

yTitle='count',

title='Review Text Length Distribution')

Figure 7

The distribution of review word count

df['word_count'].iplot(

kind='hist',

bins=100,

xTitle='word count',

linecolor='black',

yTitle='count',

title='Review Text Word Count Distribution')

Figure 8

There were quite number of people like to leave long reviews.

For categorical features, we simply use bar chart to present the frequency.

The distribution of division

df.groupby('Division Name').count()['Clothing ID'].iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8,

title='Bar chart of Division Name', xTitle='Division Name')

Figure 9

General division has the most number of reviews, and Initmates division has the least number of reviews.

The distribution of department

df.groupby('Department Name').count()['Clothing ID'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8,

title='Bar chart of Department Name', xTitle='Department Name')

Figure 10

When comes to department, Tops department has the most reviews and Trend department has the least number of reviews.

The distribution of class

df.groupby('Class Name').count()['Clothing ID'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8,

title='Bar chart of Class Name', xTitle='Class Name')

Figure 11

Now we come to “Review Text” feature, before explore this feature, we need to extract N-Gram features. N-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. In order to do this, we use scikit-learn’s CountVectorizer function.

First, it would be interesting to compare unigrams before and after removing stop words.

The distribution of top unigrams before removing stop words

top_unigram.py

Figure 12

The distribution of top unigrams after removing stop words

top_unigram_no_stopwords.py

Figure 13

Second, we want to compare bigrams before and after removing stop words.

The distribution of top bigrams before removing stop words

top_bigram.py

Figure 14

The distribution of top bigrams after removing stop words

top_bigram_no_stopwords.py

Figure 15

Last, we compare trigrams before and after removing stop words.

The distribution of Top trigrams before removing stop words

top_trigram.py

Figure 16

The distribution of Top trigrams after removing stop words

top_trigram_no_stopwords.py

Figure 17

Part-Of-Speech Tagging (POS) is a process of assigning parts of speech to each word, such as noun, verb, adjective, etc

We use a simple TextBlob API to dive into POS of our “Review Text” feature in our data set, and visualize these tags.

The distribution of top part-of-speech tags of review corpus

POS.py

Figure 18

Box plot is used to compare the sentiment polarity score, rating, review text lengths of each department or division of the e-commerce store.

What do the departments tell about Sentiment polarity

department_polarity.py

Figure 19

The highest sentiment polarity score was achieved by all of the six departments except Trend department, and the lowest sentiment polarity score was collected by Tops department. And the Trend department has the lowest median polarity score. If you remember, the Trend department has the least number of reviews. This explains why it does not have as wide variety of score distribution as the other departments.

What do the departments tell about rating

rating_division.py

Figure 20

Except Trend department, all the other departments’ median rating were 5. Overall, the ratings are high and sentiment are positive in this review data set.

Review length by department

length_department.py

Figure 21

The median review length of Tops & Intimate departments are relative lower than those of the other departments.

Bivariate visualization with Plotly

Bivariate visualization is a type of visualization that consists two features at a time. It describes association or relationship between two features.

Distribution of sentiment polarity score by recommendations

polarity_recommendation.py

Figure 22

It is obvious that reviews have higher polarity score are more likely to be recommended.

Distribution of ratings by recommendations

rating_recommendation.py

Figure 23

Recommended reviews have higher ratings than those of not recommended ones.

Distribution of review lengths by recommendations

review_length_recommend.py

Figure 24

Recommended reviews tend to be lengthier than those of not recommended reviews.

2D Density jointplot of sentiment polarity vs. rating

sentiment_polarity_rating.py

Figure 24

2D Density jointplot of age and sentiment polarity

age_polarity.py

Figure 25

There were few people are very positive or very negative. People who give neutral to positive reviews are more likely to be in their 30s. Probably people at these age are likely to be more active.

Finding characteristic terms and their associations

Sometimes we want to analyzes words used by different categories and outputs some notable term associations. We will use scattertext and spaCy libraries to accomplish these.

First, we need to turn the data frame into a Scattertext Corpus. To look for differences in department name, set the category_col parameter to 'Department Names' , and use the review present in the Review Text column, to analyze by setting the text col parameter. Finally, pass a spaCy model in to the nlp argument and call build() to construct the corpus.

Following are the terms that differentiate the review text from a general English corpus.","['analysis', 'removing', 'exploratory', 'sentiment', 'review', 'stop', 'polarity', 'data', 'visualization', 'department', 'distribution', 'score', 'text', 'complete']","The distribution of review sentiment polarity scoredf['polarity'].iplot(kind='hist',bins=50,xTitle='polarity',linecolor='black',yTitle='count',title='Sentiment Polarity Distribution')Figure 4Vast majority of the sentiment polarity scores are greater than zero, means most of them are pretty positive.
The distribution review text lengthsdf['review_len'].iplot(kind='hist',bins=100,xTitle='review length',linecolor='black',yTitle='count',title='Review Text Length Distribution')Figure 7The distribution of review word countdf['word_count'].iplot(kind='hist',bins=100,xTitle='word count',linecolor='black',yTitle='count',title='Review Text Word Count Distribution')Figure 8There were quite number of people like to leave long reviews.
The distribution of top unigrams before removing stop wordstop_unigram.pyFigure 12The distribution of top unigrams after removing stop wordstop_unigram_no_stopwords.pyFigure 13Second, we want to compare bigrams before and after removing stop words.
The distribution of top bigrams before removing stop wordstop_bigram.pyFigure 14The distribution of top bigrams after removing stop wordstop_bigram_no_stopwords.pyFigure 15Last, we compare trigrams before and after removing stop words.
Distribution of sentiment polarity score by recommendationspolarity_recommendation.pyFigure 22It is obvious that reviews have higher polarity score are more likely to be recommended.",en,['Susan Li'],2019-04-27 18:25:54.104000+00:00,"{'Data Science', 'Python', 'Plotly', 'NLP', 'Visualization'}","{'https://miro.medium.com/max/60/1*aJEp998491GN212bfizaUQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*tvAJ4q8qldsPARCsPCedhg.png?q=20', 'https://miro.medium.com/max/60/1*CccrIoMfxRb74oYr8E6LhA.png?q=20', 'https://miro.medium.com/max/1118/1*fJyNhWujbPjMluWyY6bnlw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*fJyNhWujbPjMluWyY6bnlw.png?q=20', 'https://miro.medium.com/max/60/1*mgw9XY-UdwtZMG4lRDuv5w.png?q=20', 'https://miro.medium.com/max/2560/1*aJEp998491GN212bfizaUQ.jpeg', 'https://miro.medium.com/max/3068/1*tvAJ4q8qldsPARCsPCedhg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2328/1*11SFKxPE7nWgFyROgqAdQA.png', 'https://miro.medium.com/max/60/1*yxyOZISLc7sLZVcRtBzDOw.png?q=20', 'https://miro.medium.com/max/3004/1*-UYbEjFfnpzyeBgAmugT6Q.png', 'https://miro.medium.com/max/60/1*-UYbEjFfnpzyeBgAmugT6Q.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*TK40esWITCAcFu6QzXFbnQ.png?q=20', 'https://miro.medium.com/max/550/1*9qEJlNDGBFF65LTptdecCg.png', 'https://miro.medium.com/max/664/1*mgw9XY-UdwtZMG4lRDuv5w.png', 'https://miro.medium.com/max/60/1*9qEJlNDGBFF65LTptdecCg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/2440/1*CccrIoMfxRb74oYr8E6LhA.png', 'https://miro.medium.com/max/60/1*anQqprZzdRECJCqEo8zPqA.png?q=20', 'https://miro.medium.com/max/2384/1*-KdN_PTCOURWrBAV6kpO6g.png', 'https://miro.medium.com/max/60/1*11SFKxPE7nWgFyROgqAdQA.png?q=20', 'https://miro.medium.com/max/2800/1*XGXvYtu6LdyydE9Lh0FXzw.png', 'https://miro.medium.com/max/2390/1*anQqprZzdRECJCqEo8zPqA.png', 'https://miro.medium.com/max/2740/1*yxyOZISLc7sLZVcRtBzDOw.png', 'https://miro.medium.com/max/2506/1*TK40esWITCAcFu6QzXFbnQ.png', 'https://miro.medium.com/max/60/1*XGXvYtu6LdyydE9Lh0FXzw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/60/1*-KdN_PTCOURWrBAV6kpO6g.png?q=20', 'https://miro.medium.com/max/60/1*1E-zIJXMas05676qvuWSzw.png?q=20', 'https://miro.medium.com/max/2990/1*1E-zIJXMas05676qvuWSzw.png', 'https://miro.medium.com/max/1200/1*aJEp998491GN212bfizaUQ.jpeg'}",2020-03-05 00:16:29.045303,6.7871222496032715
https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc,"What is LightGBM, How to implement it? How to fine tune the parameters?","Hello,

Machine Learning is the fastest growing field in the world. Everyday there will be a launch of bunch of new algorithms, some of those fails and some achieve the peak of success. Today, I am touching one of the most successful machine learning algorithm, Light GBM.

What motivated me to write a blog on LightGBM?

While working on kaggle data science competition I came across multiple powerful algorithms. LightGBM is one of those. LightGBM is a relatively new algorithm and it doesn’t have a lot of reading resources on the internet except its documentation. It becomes difficult for a beginner to choose parameters from the long list given in the documentation. Simply to help new geeks, I am coming up with this beautiful blog.

I will try my best to keep this blog small and simple as adding hundreds of pages of irrelevant information will confuse you.

What is Light GBM?

Light GBM is a gradient boosting framework that uses tree based learning algorithm.

How it differs from other tree based algorithm?

Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.

Below diagrams explain the implementation of LightGBM and other boosting algorithms.

Explains how LightGBM works

How other boosting algorithm works

Why Light GBM is gaining extreme popularity?

The size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as ‘Light’ because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.

Can we use Light GBM everywhere?

No, it is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows.

We briefly discussed the concept of Light GBM, now what about it’s implementation?

Implementation of Light GBM is easy, the only complicated thing is parameter tuning. Light GBM covers more than 100 parameters but don’t worry, you don’t need to learn all.

It is very important for an implementer to know atleast some basic parameters of Light GBM. If you carefully go through following parameters of LGBM, I bet you will find this powerful algorithm a piece of cake.

Let’s start discussing parameters.

Parameters

Control Parameters

max_depth: It describes the maximum depth of tree. This parameter is used to handle model overfitting. Any time you feel that your model is overfitted, my first advice will be to lower max_depth.

min_data_in_leaf: It is the minimum number of the records a leaf may have. The default value is 20, optimum value. It is also used to deal over fitting

feature_fraction: Used when your boosting(discussed later) is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.

bagging_fraction: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting.

early_stopping_round: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. This will reduce excessive iterations.

lambda: lambda specifies regularization. Typical value ranges from 0 to 1.

min_gain_to_split: This parameter will describe the minimum gain to make a split. It can used to control number of useful splits in tree.

max_cat_group: When the number of category is large, finding the split point on it is easily over-fitting. So LightGBM merges them into ‘max_cat_group’ groups, and finds the split points on the group boundaries, default:64

Core Parameters

Task: It specifies the task you want to perform on data. It may be either train or predict.

application: This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.

regression: for regression

binary: for binary classification

multiclass: for multiclass classification problem

boosting: defines the type of algorithm you want to run, default=gdbt

gbdt: traditional Gradient Boosting Decision Tree

rf: random forest

dart: Dropouts meet Multiple Additive Regression Trees

goss: Gradient-based One-Side Sampling

num_boost_round: Number of boosting iterations, typically 100+

learning_rate: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003…

num_leaves: number of leaves in full tree, default: 31

device: default: cpu, can also pass gpu

Metric parameter

metric: again one of the important parameter as it specifies loss for model building. Below are few general losses for regression and classification.

mae: mean absolute error

mse: mean squared error

binary_logloss: loss for binary classification

multi_logloss: loss for multi classification

IO parameter

max_bin: it denotes the maximum number of bin that feature value will bucket in.

categorical_feature: It denotes the index of categorical features. If categorical_features=0,1,2 then column 0, column 1 and column 2 are categorical variables.

ignore_column: same as categorical_features just instead of considering specific columns as categorical, it will completely ignore them.

save_binary: If you are really dealing with the memory size of your data file then specify this parameter as ‘True’. Specifying parameter true will save the dataset to binary file, this binary file will speed your data reading time for the next time.

Knowing and using above parameters will definitely help you implement the model. Remember I said that implementation of LightGBM is easy but parameter tuning is difficult. So let’s first start with implementation and then I will give idea about the parameter tuning.

Implementation

Installating LGBM:

Installing LightGBM is a crucial task. I found this as the best resource which will guide you in LightGBM installation.

I am using Anaconda and installing LightGBM on anaconda is a clinch. Just run the following command on your Anaconda command prompt and whoosh, LightGBM is on your PC.

conda install -c conda-forge lightgbm

Dataset:

This data is very small just 400 rows and 5 columns (specially used for learning purpose). This is a classification problem where we have to predict whether a customer will buy the product from advertise given on the website. I am not explaining dataset as dataset is self-explanatory. You can download dataset from my drive.

Note: The dataset is clean and has no missing value. The main aim behind choosing this much smaller data is to keep the things simpler and understandable.

I am assuming that you all know basics of python. Go through data preprocessing steps, they are fairly easy but if you have any doubt then ask me in the comment, I will get back to you asap.

Data preprocessing:

import numpy as np

import matplotlib.pyplot as plt

import pandas as pd # Importing the dataset

dataset = pd.read_csv('...input\\Social_Network_Ads.csv')

X = dataset.iloc[:, [2, 3]].values

y = dataset.iloc[:, 4].values # Splitting the dataset into the Training set and Test set

from sklearn.cross_validation import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) # Feature Scaling

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train = sc.fit_transform(x_train)

x_test = sc.transform(x_test)

Model building and training:

We need to convert our training data into LightGBM dataset format(this is mandatory for LightGBM training).

After creating a converting dataset, I created a python dictionary with parameters and their values. Accuracy of your model totally depends on the values you provide to parameters.

In the end block of code, I simply trained model with 100 iterations.

import lightgbm as lgb d_train = lgb.Dataset(x_train, label=y_train) params = {}

params['learning_rate'] = 0.003

params['boosting_type'] = 'gbdt'

params['objective'] = 'binary'

params['metric'] = 'binary_logloss'

params['sub_feature'] = 0.5

params['num_leaves'] = 10

params['min_data'] = 50

params['max_depth'] = 10 clf = lgb.train(params, d_train, 100)

Few things to notice in parameters:

Used ‘binary’ as objective(remember this is classification problem)

Used ‘binary_logloss’ as metric(same reason, binary classification problem)

‘num_leaves’=10 (as it is small data)

‘boosting type’ is gbdt, we are implementing gradient boosting(you can try random forest)

Model prediction:

we just need to write a line for predictions.

Output will be a list of probabilities. I converted probabilities to binary prediction keeping threshold=0.5

#Prediction

y_pred=clf.predict(x_test) #convert into binary values for i in range(0,99):

if y_pred[i]>=.5: # setting threshold to .5

y_pred[i]=1

else:

y_pred[i]=0

Results:

We can check results either using confusion matrix or directly calculating accuracy

Code:

#Confusion matrix from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred) #Accuracy from sklearn.metrics import accuracy_score

accuracy=accuracy_score(y_pred,y_test)

Screenshots of result:

Confusion Matrix

Accuracy Score

Many of you must be thinking that I used smaller dataset and still my model has 92% accuracy. Why there is no overfitting? The simple reason is I fine tuned model parameters.

So now let’s jump into parameter fine tuning.

Parameter Tuning:

Data scientists always struggle in deciding when to use which parameter? and what should be the ideal value of that parameter?

Following set of practices can be used to improve your model efficiency.

num_leaves: This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting. min_data_in_leaf: Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset. max_depth: You also can use max_depth to limit the tree depth explicitly.

For Faster Speed:

Use bagging by setting bagging_fraction and bagging_freq

and Use feature sub-sampling by setting feature_fraction

Use small max_bin

Use save_binary to speed up data loading in future learning

to speed up data loading in future learning Use parallel learning, refer to parallel learning guide.

For better accuracy:

Use large max_bin (may be slower)

(may be slower) Use small learning_rate with large num_iterations

with large Use large num_leaves (may cause over-fitting)

(may cause over-fitting) Use bigger training data

Try dart

Try to use categorical feature directly

To deal with over-fitting:

Use small max_bin

Use small num_leaves

Use min_data_in_leaf and min_sum_hessian_in_leaf

and Use bagging by set bagging_fraction and bagging_freq

and Use feature sub-sampling by set feature_fraction

Use bigger training data

Try lambda_l1 , lambda_l2 and min_gain_to_split to regularization

, and to regularization Try max_depth to avoid growing deep tree

Conclusion:

I implemented LightGBM on multiple datasets and found that its accuracy challenged other boosting algorithms. From my experience, I will always recommend you to try this algorithm at least once.

I hope you guys enjoyed this blog and it was useful to all. I would request you you to give suggestion which will help to improve this blog.

Source: Microsoft LightGBM Documentation

Thanks,

Pushkar Mandot","['tune', 'used', 'fine', 'dataset', 'lightgbm', 'gbm', 'data', 'implement', 'light', 'model', 'parameter', 'tree', 'parameters', 'value']","Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.
Explains how LightGBM worksHow other boosting algorithm worksWhy Light GBM is gaining extreme popularity?
Light GBM is prefixed as ‘Light’ because of its high speed.
Light GBM covers more than 100 parameters but don’t worry, you don’t need to learn all.
It is very important for an implementer to know atleast some basic parameters of Light GBM.",en,['Pushkar Mandot'],2018-12-01 09:10:32.360000+00:00,"{'Xgboost', 'Parameter Tuning', 'Parameters', 'Machine Learning', 'Lightgbm'}","{'https://miro.medium.com/max/1200/1*AZsSoXb8lc5N6mnhqX5JCg.png', 'https://miro.medium.com/max/982/1*p2CbUUWlTKI4MheKLCXbbA.png', 'https://miro.medium.com/max/1026/1*7fNBkJxF6ZU3lktUML319Q.png', 'https://miro.medium.com/fit/c/80/80/1*RUtnaV9XF1xtfYj9Pjit-w.jpeg', 'https://miro.medium.com/max/60/1*7fNBkJxF6ZU3lktUML319Q.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*NXT3Mow_MRFaL68T5Cq2HA.png', 'https://miro.medium.com/fit/c/160/160/0*1GbkXp0V7oY_M8sw.', 'https://miro.medium.com/max/60/1*AZsSoXb8lc5N6mnhqX5JCg.png?q=20', 'https://miro.medium.com/max/2422/1*AZsSoXb8lc5N6mnhqX5JCg.png', 'https://miro.medium.com/max/60/1*whSa8rY4sgFQj1rEcWr8Ag.png?q=20', 'https://miro.medium.com/max/1966/1*whSa8rY4sgFQj1rEcWr8Ag.png', 'https://miro.medium.com/fit/c/80/80/1*SD3hMVq2yKA5BoxDIOp-AA.jpeg', 'https://miro.medium.com/max/60/1*p2CbUUWlTKI4MheKLCXbbA.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*1GbkXp0V7oY_M8sw.'}",2020-03-05 00:16:30.893257,1.8479537963867188
https://towardsdatascience.com/price-elasticity-data-understanding-and-data-exploration-first-of-all-ae4661da2ecb,Price Elasticity: Data Understanding and Data Exploration First Of All!,"In one of my previous post here I described how to evaluate regressions, using the most used metrics and plots. I took an experiment about modeling price elasticity as an example and, after analyzing the model with residual plots, it turned out there’s a problem after the 1st of September in the test data set:

fig. 1 — The calendar_date variable vs residuals plot shows that something strange happens after the 1st of September

Tools in R for a better data exploration will be shown in this post, showing a good way to prepare the data for a high performing predictive modeling. It’s also the only way to try to give an explanation of the problem emerged previously.

If you want to start from data sets without using the above suggested experiment, you can get them here.

Never Put All Into The Pot As Is

Opening the price elasticity experiment in Azure Machine Learning Studio we can see that, after the join between three different data set, there are just very few basic data transformations before all the data is transferred into the modeling phase:

fig.2 — Join and Transformation phases in the price elasticity experiment

It seems data hasn’t been analyzed and understood very well, because this experiment was just for demo purpose. But, in general, it’s important to become confident with the data to be used in a predictive model, because, as Techopedia says in its “Data Preprocessing” section:

Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors.

In general, the problem with almost every information system is that “rubbish in, rubbish out”. Since a data set may contain a wealth of potential pitfalls as said before, a detailed data exploration has to be applied before any modeling phase in a machine learning experiment.

What’s Behind The Data

The three data sets used as the starting point of the experiment come from the transactions of a Burger Cafè into the Microsoft building in China. The pressure on costs makes it necessary to revisit prices in excess. It’s important for Microsoft to know how the burgers’ demand of the store reacts to a price increment. In other words, it’s essential to know the price elasticity (of demand) of the burgers in the store in order to know how much the price can be increased. In fact the price elasticity is the degree with which the price of a product affects the its demand.

fig. 3 — Price elasticity of demand

It’s clear from the previous picture that if the price is set at $3.000, the products sold are 4 millions; if instead the price is increased to $4.000, the products sold are only 2 millions.

Depending on its slope, the demand curve can be elastic or inelastic.

fig. 4 — Elastic and inelastic demand curve

With the elastic demand in the picture on the left, if the price is increased from 50p to 60p, the quantity decreases of a huge amount of items (this is the worst case for the seller and the best one for the buyer); if instead the demand is inelastic as in the picture on the right, increasing the price by 40% will lead a small reduction of quantity (in this case the seller can play on the price, because the buyer needs that good).

fig. 5 — Buyer behavior in case of a good that follows an inelastic demand and that undergoes an increase in price

Now, it’s quite simple to regress a demand curve having only the two variables Price and Quantity. It becomes more complex when more variables are involved (such as selling burgers in combo with other products). Thanks to machine learning, we’ll gather all the information needed to understand how much the price can be increased without causing a drastic fall in revenue.

Data Exploration To The Rescue

The first thing to do is try to have the best understanding of the data. This is gained by exploring it using some specific statistical tools.

A comprehensive guide to Data Exploration is out of the scope of this post. A very good one by Analytics Vidhya can be found here. From this guide we can get the steps involved to get a reasonable feeling with data:

Variable Identification Univariate Analysis Bi-variate Analysis Missing values treatment Outlier treatment Variable transformation Variable / Feature creation

Let’s start with the first step.

Variable Identification

There are three data set to analyze. Let’s see the variables contained in each one.

“Cafè — Transaction — Store 1” Data Set

The variables that make up this data set can be analyzed directly in Azure Machine Learning Studio (clicking on “Visualize” from the output pin of the data set) and they are the ones showed in the following picture.

fig. 6 — Variables in the “Cafè — Transaction — Store 1” data set

Each row is an aggregative transaction that sums the quantities sold in a specific day for a specific product identified by SELL_ID. It’s supposed the PRICE for that product in that day will not vary.

In details:

STORE : a categorical variable, containing in this case only the value “1” that identifies the Store 1

: a categorical variable, containing in this case only the value “1” that identifies the Store 1 CALENDAR_DATE : a date/time variable, having the time always set to 00:00 AM

: a date/time variable, having the time always set to 00:00 AM PRICE : a numeric variable, associated with the price of the product identified by the SELL_ID

: a numeric variable, associated with the price of the product identified by the SELL_ID QUANTITY : a numeric variable, associated with the quantity of the product sold, identified by the SELL_ID

: a numeric variable, associated with the quantity of the product sold, identified by the SELL_ID SELL_ID : a categorical variable, identifier of the product sold

: a categorical variable, identifier of the product sold SELL_CATEGORY: a categorical variable, category of the product sold

“Cafè — Sell Meta Data” Data Set

Here are the variables of this data set.

fig. 7 — Variables in the “Cafè — Sell Meta Data” data set

In details:

SELL_ID : the same identifier of the product sold shown in the previous data set

: the same identifier of the product sold shown in the previous data set SELL_CATEGORY : the same category of the product sold shown in the previous data set. Looking at the picture is now clear that the category “0” identifies single products; the category “2” identifies the combo ones

: the same category of the product sold shown in the previous data set. Looking at the picture is now clear that the category “0” identifies single products; the category “2” identifies the combo ones ITEM_ID : a categorical variable, identifier of the item that is contained in the product

: a categorical variable, identifier of the item that is contained in the product ITEM_NAME: a categorical variable, identifying the name of the item

As highlighted in the previous picture, the presence of multiple instances of the same SELL_ID in correspondence with distinct ITEM_IDs, makes us think that SELL_ID is the identifier of products that can be single items or a combo of items. For example, the combo with SELL_ID = 2053 is made by the items BURGER, COKE and COFFEE. The products with SELL_ID 1070, 3028, 3055 and 3067 are instead single items.

“DateInfo” Data Set

This data set contains the calendar info with some external data.

fig. 8 — Variables in the “DateInfo” data set

In details:

CALENDAR_DATE : a date/time variable, indicating the date of a calendar

: a date/time variable, indicating the date of a calendar YEAR : a numeric variable, indicating the year of the corresponding CALENDAR_DATE

: a numeric variable, indicating the year of the corresponding CALENDAR_DATE HOLIDAY : a categorical variable, indicating if the corresponding CALENDAR_DATE is a holiday

: a categorical variable, indicating if the corresponding CALENDAR_DATE is a holiday IS_WEEKEND : a categorical variable, indicating if the corresponding CALENDAR_DATE is a weekend

: a categorical variable, indicating if the corresponding CALENDAR_DATE is a weekend IS_SCHOOLBREAK : a categorical variable, indicating if the corresponding CALENDAR_DATE is a school break

: a categorical variable, indicating if the corresponding CALENDAR_DATE is a school break AVERAGE_TEMPERATURE : a numeric variable, indicating the average temperature in Fahrenheit of the corresponding CALENDAR_DATE day

: a numeric variable, indicating the average temperature in Fahrenheit of the corresponding CALENDAR_DATE day IS_OUTDOOR: a categorical variable, whose meaning is unknown to me. It might be a flag indicating if the temperature is the one measured outside (value 1) or inside the building (value 0), since it’s part of the external data info and it’s near to the AVERAGE_TEMPERATURE variable

Join All The Data Sets Together

Some of these variables can be manipulated and then joined as a unique data set, to be used as the source of our machine learning experiment. Here the operations to perform:

fig. 9 — Operations to perform in order to have a unique data set

Since the transactions data contains the SELL_ID variable as reference, I need to have all the meta data information for a SELL_ID product in a row in order to join with it. Therefore we have to transform the data set from a long format to a wide one. This can be obtained by pivoting the “Sell Meta Data” information.

fig. 10 — Pivoting of a data set from a long format to a wide one

This task is implemented in the following simple (and quite arcane) R code snippet, into the Execute R Script module in fig.9:

For any detail about this function, refer to this link. Sincerely I’d have preferred the tidy way to do the same transformation, just a little bit more verbose, but understandable:

From the “Join by SELL_ID” module there are some variables to filter out.

fig. 11 — Variables to filter out from the first Join Data module

The variables STORE and BURGER are zero-variance predictors (their values are always 1), so they don’t bring any meaningful signal. Then these variables are removed thanks to the Select Columns in Dataset module. The SELL_ID variable is a product identifier, a label identifying the product sold in the transaction. Since it’s an ID, it’s easy to think to drop it from the data set, but we keep it, because it may be important to relate PRICE and QUANTITY to the product sold. The SELL_CATEGORY variable, coming from the transactions, is kept in too (in the original experiment this variable was filtered out). The output is the following one:

fig. 12 — Variables kept after the Select Columns in Dataset module

Then all the information about dates and external factors are joined by the CALENDAR_DATE variable to the “Join by SELL_ID” data set, and the final result is the following:

fig. 13 — Final data set after the Join Phase

The resulting data set can be downloaded here in CSV format. You can use the file for the following analysis if you don’t have an Azure Machine Learning Studio account.

Univariate Analysis

Once we have the input data set ready, every single variable of it has to be explored. According to the nature of the variable, different statistical methods (central tendency, measure of dispersion) and plots are required to do the job. At the end, this job is always repetitive for a data scientist. Since we don’t like monkey work, a great open source tool by Microsoft comes to the rescue: Interactive Data Exploratory Analysis and Reporting (IDEAR). The tool provides three releases, one for each of the following platforms:

Microsoft Machine Learning R Server

Python

R

In this post I’m using the release for R. In particular, I’m using it with R 3.4.3 in RStudio (to date, IDEAR has some issues with R ≥3.5.0). The complete instruction about how to install and use it are here.

First of all, we need to export the joined data set in a CSV format. This can be done thanks to the Convert to CSV module:

fig. 14 — Export the joined data set to CSV

Then it’s necessary to write a yaml file containing the meta data information of variables in the CSV file, according to the IDEAR conventions mentioned in the instructions. The content of the yaml file is the following:

After opening the Run-IDEAR.R file in RStudio and after executing it, a dialog asking to click ok to proceed will be prompted (sometimes the dialog icon will appear, but the dialog itself remains hidden in background; just click on the icon). Then open the yaml file just created in the Open dialog and the game is done: a fantastic Shiny app will appear with all the statistics we need.

fig. 15 — IDEAR Data Quality Report

Since we have defined the CALENDAR_DATE variable as date/time in the yaml file, IDEAR has automatically generated other date/time components (year, month, week, day, week day), adding the “autogen_<var>” suffix to their name. These new variables will help us to have better insights during the multi-variate analysis.

After the “Data Summary” section, which contains information about the variables, their type and their values, there is the “Dive deeper into each individual variable” section, where we can do the univariate analysis.

Analyze the Target Variable

First of all the “More detailed statistics of each variable” subsection gives us a fast glimpse to few basic statistics for each variable.

Then the “Visualize the target variable” subsection contains four plots that help to understand how the target variable (in our case QUANTITY) behaves.

fig. 16 — Behavior of target variable

The QUANTITY variable seems to be bimodal (it has two main bumps in the density plot) and it has few outliers for high values (check the boxplot), due to the right-skewness of the distribution.

Analyze Numeric Variables

It’s possible to investigate all the numeric variables thanks to the “Visualize the numerical variables” subsection: PRICE, QUANTITY (just analyzed) and AVERAGE_TEMPERATURE.

fig. 17 — Behavior of the PRICE variable

The PRICE variable seems to be bimodal too and just a little bit right-skewed. The boxplot shows no outliers.

The AVERAGE_TEMPERATURE is bimodal too. This time the distribution is a little bit left-skewed

fig. 18 — Behavior of the AVERAGE_TEMPERATURE variable

Later we’ll apply some transformations to these variables in order to adjust their skewness as possible, trying to mitigate the effect of outliers without removing them.

Analyze Categorical Variables

Thanks to the “Visualize categorical variables” subsection, we can explore the categorical variables too. The auto-generated variables from the CALENDAR_DATE one are categorical too. For example, it’s interesting to see that the number of transactions goes down in the last quarter of the year.

fig. 19 — Exploration of the CALENDAR_DATE_autogen-month variable

Looking at the SELL_ID categorical variable, we have these plots:

fig. 20 — Exploration of the SELL_ID variable

Only four products are sold (the single burger, 1070; the burger+coke combo, 2051; the burger+lemonade combo, 2052; the burger+coke+coffee combo, 2053). What’s strange is that all these products are sold exactly in the same quantity. May be the data set of transactions has been derived from a more complete one.

Multi-variate Analysis

Things become interesting when exploring the interaction of two variables.

Rank variables

First of all, IDEAR provides us the “Rank variables” subsection that helps to understand which variables are the most strongly associated to the selected one. Different metrics are used for categorical and numerical variables respectively.

The associations between categorical and numerical variables are computed using the eta-squared metric.

The associations between categorical variables are computed using the Cramer’ V metric.

After choosing 6 as the number of top categorical variables associated to QUANTITY, the result is the following:

fig. 21 — Strength of association between variables in the data set

The variable labels on the x axis are a little bit truncated. For the sake of clarity, the lists of numerical and categorical variables ordered for the most strongly associated first will follow.

Numerical variables

PRICE TEMPERATURE

Categorical variables

SELL_ID SELL_CATEGORY LEMONADE COKE CALENDAR_DATE_autogen_wday IS_WEEKEND

It’s interesting to note the strength of SELL_ID and SELL_CATEGORY variables, that were unexpectedly not included in the training data set of the original experiment. Moreover, the association between the number of the day of week (from 1 to 7) with the target variable is an unexpected insight.

Interaction between two categorical variables

IDEAR uses mosaic plots to compare two categorical variables. At first sight it might be a little bit confusing for people not aware of some statistical concepts. For example, the following is the mosaic plot for the variables SELL_CATEGORY and COKE:

fig. 22 — Mosaic plot for SELL_CATEGORY vs COKE

Thanks to this plot we can say that combo products (SELL_CATEGORY = 2) will almost always contain coke (COKE = 1). In fact, combo products (SELL_CATEGORY = 2) not containing coke (COKE = 0) are less present in transactions comparing to the case of independence between them (red color). At the same time it’s confident to say that single products (SELL_CATEGORY = 0) will not contain coke.

For a more detailed explanation about how to draw a mosaic plot, refer to this post.

Interaction between two numerical variables

The most used plot to compare two numerical variables is the scatter plot. It’s interesting to compare the variables PRICE and QUANTITY in IDEAR:

fig. 23 — Scatter plot for PRICE vs QUANTITY

There are two lines to help you to see any relationship between variables: the simple regression model line (linear model, lm) in red, and the Locally Weighted Scatterplot Smoothing (LOWESS) line in blue. While the former is a parametric way to fit the curve (you assume in advance the data fits some type of distribution), the latter is a non-parametric strategy that tries to find a curve of best fit without assuming the data must fit some distribution shape.

The correlation value shown upon the plot is the Pearson correlation between the two variables.

It seems QUANTITY has a different behavior for PRICEs equal or greater than $14. May be there is another variable that makes this happen. We’ll investigate this later.

If we want to study the behavior of PRICE and QUANTITY versus the AVERAGE_TEMPERATURE we’ll see there is no particular associations:

fig. 24 — Scatter plot for AVERAGE_TEMPERATURE vs PRICE

fig. 25 — Scatter plot for AVERAGE_TEMPERATURE vs QUANTITY

Correlations between numerical variables

This section in IDEAR allows to calculate correlations between two numeric variables according to Pearson, Kendall and Spearman. Several configurations can be chosen (order, shape, layout) for the plot.

Compared with the standard Pearson correlation, Kendall’s τ is much more robust to extreme observations and to non-linearity. So let’s see if the AVERAGE_TEMPERATURE variable is associated to the other numerical variables in a non-linear way:

fig. 26 — Kendall’s correlation for numeric variables

An unexpected little association between AVERAGE_TEMPERATURE and PRICE emerged. The one with QUANTITY is negligible.

Interactions between numerical and categorical variables

IDEAR visualizes associations between numerical and categorical variables via boxplots. This is a common way to check if the distribution of the numerical variable is significantly different at different levels of the categorical variable. At the same time, the null hypothesis that the numeric variable has the same mean values across the different levels of the categorical variable is tested. A one-way analysis of variance (ANOVA) between the two variables is done. The p-value returned by the F-value in the ANOVA test helps us to accept (the means of boxplots have the same value) or reject (the means of boxplots have different values) the null hypothesis.

For example, let’s check if the PRICE variable distribution varies at different levels of the SELL_ID variable (the sell product identifier):

fig. 27 — Interaction between PRICE and SELL_ID

From the picture above it’s clear that the single burger (SELL_ID = 1070) determines a neat price increase (the null hypothesis is rejected). Some outliers have emerged that deserve attention.

The QUANTITY distribution significantly varies with the SELL_ID levels too:

fig. 28 — Interaction between QUANTITY and SELL_ID

From the picture above some evident outliers for the burger+coke combo product (SELL_ID = 2051) emerged.

Other interesting outliers come to light when analyzing the SELL_CATEGORY and COKE variables versus the PRICE:

fig. 29 — Interaction between PRICE and SELL_ID

fig. 30 — Interaction between PRICE and COKE

Missing values treatment

We’re lucky with our data set: we have no missing values, as you can see from the “Check the data quality” section in IDEAR, choosing all the 18 variables to analyze:

fig. 31 — Missing values analysis

This plot is a smart use of the levelplot() in R. As a similar example, take a look at this link.

Outlier treatment

We sow from the univariate analysis that single variables don’t have so evident outliers. Things changes during the multi-variate analysis, when comparing numerical and categorical variables.

From fig. 27 we saw that there are transactions where single burger (SELL_ID = 1070) was sold at less than $14. Let’s go deeper to this case. You’ll find comments into the code.

We were lucky to find a data issue just checking for outliers. Removing the wrong transaction rows we have also removed all the extreme outliers showed before. So, we have just killed multiple birds with one stone!

Variable transformation

Sometimes outliers in a variable distribution aren’t wrong measures, they may be inherent in the nature of the variable itself, especially if it’s skewed. Cutting away these values from a distribution can remove useful information for a successful predictive modeling. So, it’s preferable to transform the variable to mitigate the skewness, trying to make its distribution much similar to a normal one. Reducing non-normality often reduces non-linearity as well and even if the transformed distribution is not exactly normal, it will be usually symmetric.

When a distribution is right-skewed, a log transformation is often used. But what if it’s left-skewed? Fortunately there is a generic way to transform non-normal distribution: the Box-Cox transformation. It’ll be applied to PRICE and QUANTITY only, because AVERAGE_TEMPERATURE is already symmetric (a Box-Cox transformation upon it doesn’t change its distribution, since it cannot be transformed in a normal one).

The var_distribution function gives us four plots, the same as the IDEAR use in the “Visualize the numerical variables” section (just press “Export” to get the code into the price-elasticity-joined.log.r file, declared into the YAML one).

The boxcox_transf function returns a list containing the transformed data and the value of lambda calculated by the Box-Cox transformation.

Here follow the plots for the original PRICE variable and for the Box-Cox transformed variable (T-PRICE).

fig. 32 — Distribution plots for PRICE and transformed PRICE (T-PRICE)

As you can see, the transformed density plot shows a more symmetric curve and the transformed Q-Q plot is closer to the normal line. The lambda value used to transform the PRICE variable is -1.1.

Here are the same plots for the QUANTITY variable.

fig. 33 — Distribution plots for QUANTITY and transformed PRICE (T-QUANTITY)

The lambda value used to transform the QUANTITY variable is -0.1. In this case, as you can see from the T-QUANTITY boxplot, the outliers have disappeared without removing them thanks to the transformation.

Variable creation

This is the most creative phase of a Data Science project and it’s also known as Feature Engineering. In few words (from this blog post):

It’s the process of using domain knowledge of the data to create features that make machine learning algorithms work. If feature engineering is done correctly, it increases the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process. Feature Engineering is an art.

New Date/Time variables

The first example of feature engineering is the creation of multiple variables from a date variable. As we have seen at the beginning of this post, IDEAR does automatically this transformation for us in order to better analyze our data set.

It’s also useful to extract a new variable representing the counter of days since the first day we can find in the data set. In general, date/time variable are cyclical (e.g. number of month goes from 1 to 12; number of day goes from 1 to 31). A machine learning algorithm doesn’t take into account the cyclicity of variables. So, a counter of days that represents the passing of the time is a good variable to add, because it can help the algorithm to catch any sales growth since the beginning of the activity.

Here the R code to extract these variables from the CALENDAR_DATE variable (it was automatically parsed in a date column by the read_csv function).

New Categorical variables

Let’s look at this plot:

fig. 34 — Interaction between QUANTITY and HOLIDAY

It seems that in business days (HOLIDAY = “NULL”) the quantities sold are significantly greater then during the holidays. So, a new dummy variable called IS_HOLIDAY (1 if the transactions happen in holidays; 0 otherwise) might help machine learning algorithms to perform in a better way.

There are also some factor columns that have to be converted to one-hot encoded variables (as you can see, we already have some of them in our data set: COFFEE, LEMONADE, …). One-hot encoded variables (or dummy variables) are numeric variables, since they represent a characteristic that exists (1) or not exists (0). In our analysis they’re often transformed in categorical variables to facilitate the graphical representations. In R one-hot encoded variables can be achieved thanks to the model.matix function.

(Here you can find the reason for that “-1”)

New Numerical variables

Having a variable with the number of items in a combo product could be an important feature to determine a price variation. So, just after the “Long format to wide format” Execute Script module you can see in fig. 2, the data set is the one in fig. 10. Here it’s possible to add the variable NO_ITEMS defined as:

Further analysis after cleaning data

Holidays analysis

Before dropping the HOLIDAY column, as commented in the previous code, it’s interesting to check how each holiday is represented in our data set for each year. How to do that? The pirateplot comes to the rescue. Before using this plot, for a better visual representation, we’ll add a new variable (HOLIDAY_ABBR) with abbreviated labels for each holiday.

fig. 35 — Pirateplot for QUANTITY versus SELL_ID and YEAR

As you can see, a few holidays are missing for some year:

Mid-Autumn Day is missing in 2015

National Day is missing in 2015

WWII Celebration (the end of the Second World War) is missing for years 2012, 2013 and 2014

It’s easy to justify the lack of the first two holidays in the year 2015. The registered transactions we have in the data set have a maximum date of September the 10th for the year 2015. So these holidays fell later than the maximum date.

The WWII Celebration is missing in years before the 2015. Since the data set is made by transactions of a Burger Cafè in Microsoft China, searching a little bit on Internet I found this post. It seems China decided to celebrate the end of WWII just since 2015. So, the reason of lack in years before 2015 is explained.

Impact of SELL_ID to the demand curve

We’d like to know if the burger sales follows a linear demand. In fig. 23 we supposed there were another variable that broke what could be a linear behavior between QUANTITY and PRICE. Let’s go deeper in the analysis to answer this question.

The third variable to consider is SELL_ID (a categorical variable). It’s legitimate to suppose different product are sold at different price in different quantities. From fig. 23 it’s clear that a different behavior is followed by products sold at a price of $14 or more. Let’s consider a new variable IS_PRICE_HIGH set to 1 if the price is $14 or more, 0 otherwise. So, now we need to plot a numeric variable (QUANTITY) versus two categorical ones(SELL_ID and IS_PRICE_HIGH). The pirateplot will help us again.

fig. 36 — Pirateplot for QUANTITY versus SELL_ID and IS_PRICE_HIGH

Looking at the plot, the presence of only the SELL_ID 1070 into the IS_PRICE_HIGH area corresponding to prices equal of greater of $14, and the inference bands around the means that aren’t overlapping, it seems confirmed that each product identified by SELL_ID is sold at their mean quantity with 9% of confidence and that high prices identify specific SELL_ID.

If we plot the PRICE distributions per SELL_ID, also prices are quite distinct for each SELL_ID.

fig. 37 — Pirateplot for PRICE versus SELL_ID

Mean confidence bars for SELL_IDs 2052 and 2053 are overlapping, so they are hardly distinguishable by price.

Let’s check how the demand curves behave for each SELL_ID.

fig. 38 — Scatter plots between PRICE and QUANTITY for each SELL_ID value

It would seem that we can actually shape the demand curves with a linear regression.

Conclusion

It’s totally wrong to face a machine learning problem without first carefully analyzing the input data set. Data has to be really understood and properly “munged” so that it can show all its insights. Only after a complete understanding of the data, the Data Scientist can transform and create new variables useful to perform well with a machine learning algorithm.

In this post we have deepened the knowledge of the Burger Cafè transactions data set. In the next post we’ll apply this knowledge to implement a new machine learning model in Azure Machine Learning Studio and we’ll check if:","['set', 'quantity', 'variables', 'elasticity', 'price', 'variable', 'sold', 'data', 'categorical', 'product', 'exploration', 'understanding', 'sell_id']","7 — Variables in the “Cafè — Sell Meta Data” data setIn details:SELL_ID : the same identifier of the product sold shown in the previous data set: the same identifier of the product sold shown in the previous data set SELL_CATEGORY : the same category of the product sold shown in the previous data set.
“DateInfo” Data SetThis data set contains the calendar info with some external data.
13 — Final data set after the Join PhaseThe resulting data set can be downloaded here in CSV format.
17 — Behavior of the PRICE variableThe PRICE variable seems to be bimodal too and just a little bit right-skewed.
Analyze Categorical VariablesThanks to the “Visualize categorical variables” subsection, we can explore the categorical variables too.",en,['Luca Zavarella'],2019-01-08 17:03:51.247000+00:00,"{'Data Science', 'Machine Learning', 'Rstats', 'Data Exploration', 'Statistics'}","{'https://miro.medium.com/max/60/1*CUEy0edhUmHXlBluzQtQfQ.png?q=20', 'https://miro.medium.com/max/1978/1*5d68Zes3UR3OIxYvKYruqw.png', 'https://miro.medium.com/max/1598/1*QgywGi_8plFmZU3iNig3Ng.png', 'https://miro.medium.com/max/60/1*WjS8jvPn3xDPldgiqt4ceg.png?q=20', 'https://miro.medium.com/max/60/1*pKGU8X6qWv2ZI9qS3MKteA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1354/1*jcwQ_GoTALMQw8Tc0DvtJQ.png', 'https://miro.medium.com/max/1718/1*deRkCiwHO1Fe6OP8sVCV_Q.png', 'https://miro.medium.com/max/1000/1*DtrGZG3J1I7SP7mYS3fQfQ.jpeg', 'https://miro.medium.com/max/2340/1*CUEy0edhUmHXlBluzQtQfQ.png', 'https://miro.medium.com/max/1550/1*ItuXG-w--T_YOSGdomH70Q.png', 'https://miro.medium.com/max/60/1*YZ-9AEMN44IxUZ_1W-vyuQ.png?q=20', 'https://miro.medium.com/max/36/1*S_FabAop9a7492It225lEg.png?q=20', 'https://miro.medium.com/max/60/1*2GdwiU4di_Quya-Fc_-pCQ.jpeg?q=20', 'https://miro.medium.com/max/974/1*2GdwiU4di_Quya-Fc_-pCQ.jpeg', 'https://miro.medium.com/max/60/1*j76XOSBpxLIDrqjO1JhZGQ.png?q=20', 'https://miro.medium.com/max/1228/1*S_FabAop9a7492It225lEg.png', 'https://miro.medium.com/max/56/1*LdZzRp5ODSPD6zvT5n_xFA.png?q=20', 'https://miro.medium.com/max/2020/1*OkQ31fZZt9wUBlhHe7APZg.png', 'https://miro.medium.com/fit/c/160/160/1*dKM1AAB0aRS4a_r_gwPPEg.jpeg', 'https://miro.medium.com/max/1544/1*G-XsZReuG9xrE5pFJkPnQA.png', 'https://miro.medium.com/max/1610/1*bCFj7GMQSOJwujLOtbZa1Q.png', 'https://miro.medium.com/max/60/1*DtrGZG3J1I7SP7mYS3fQfQ.jpeg?q=20', 'https://miro.medium.com/max/2400/1*pSPK4_68MbEJGW7poJtF_g.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1650/1*MOlvATgRJ1vqJjXjXo8ifQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*P4E0JVPS0aufMueGehcXCw.png?q=20', 'https://miro.medium.com/max/60/1*gabo-3UVJ_UqCz32c9uDKw.png?q=20', 'https://miro.medium.com/max/60/1*CkkRB17xxNz8iaXUPOljvA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*dKM1AAB0aRS4a_r_gwPPEg.jpeg', 'https://miro.medium.com/max/1766/1*gabo-3UVJ_UqCz32c9uDKw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*F6M70vPXOqDx-3KBxTAwNA.png?q=20', 'https://miro.medium.com/max/2400/1*WjS8jvPn3xDPldgiqt4ceg.png', 'https://miro.medium.com/max/1524/1*mkbnWmRGgO6QmVmwPveqSg.png', 'https://miro.medium.com/max/60/1*jcwQ_GoTALMQw8Tc0DvtJQ.png?q=20', 'https://miro.medium.com/max/3050/1*P4E0JVPS0aufMueGehcXCw.png', 'https://miro.medium.com/max/960/1*ToJXbcAe-2UiYmNS9JUFVw.png', 'https://miro.medium.com/max/1812/1*j76XOSBpxLIDrqjO1JhZGQ.png', 'https://miro.medium.com/max/1574/1*8zTlP9dFqSLJCKcCt79nuA.png', 'https://miro.medium.com/max/60/1*p0lFdO8I3jYdJzFeo1NpuQ.png?q=20', 'https://miro.medium.com/max/60/1*ToJXbcAe-2UiYmNS9JUFVw.png?q=20', 'https://miro.medium.com/max/60/1*5d68Zes3UR3OIxYvKYruqw.png?q=20', 'https://miro.medium.com/max/1814/1*R9PewnpC61UBp8PB1edx-Q.png', 'https://miro.medium.com/max/60/1*bCFj7GMQSOJwujLOtbZa1Q.png?q=20', 'https://miro.medium.com/max/60/1*tEBf3iDCtN26XJ_YgQNjjg.png?q=20', 'https://miro.medium.com/max/60/1*QgywGi_8plFmZU3iNig3Ng.png?q=20', 'https://miro.medium.com/max/1228/1*w8ZGZutyeijpHGdBkY_ukw.png', 'https://miro.medium.com/max/60/1*25e92_OUkAtYluvZ5wrvPg.png?q=20', 'https://miro.medium.com/max/60/1*mkbnWmRGgO6QmVmwPveqSg.png?q=20', 'https://miro.medium.com/max/60/1*ATygIjJsy2otPlAQB-N8_A.png?q=20', 'https://miro.medium.com/max/2256/1*sF6GAj3ub_zZv6Giw_NDwQ.png', 'https://miro.medium.com/max/60/1*ItuXG-w--T_YOSGdomH70Q.png?q=20', 'https://miro.medium.com/max/1576/1*ATygIjJsy2otPlAQB-N8_A.png', 'https://miro.medium.com/max/1498/1*UFr3eaDIdkihbxkI8N6lKQ.png', 'https://miro.medium.com/max/3000/1*zYdXu52wnhJKhVr7fB8hrw.png', 'https://miro.medium.com/max/1534/1*25e92_OUkAtYluvZ5wrvPg.png', 'https://miro.medium.com/max/60/1*uoe641315kyvN6T0S7Gs8A.png?q=20', 'https://miro.medium.com/max/60/1*1KPgHzPqQUozCOjRVEGmHA.png?q=20', 'https://miro.medium.com/max/60/1*R9PewnpC61UBp8PB1edx-Q.png?q=20', 'https://miro.medium.com/max/60/1*vfwAhb-XCmnMEu5wFzTP8Q.png?q=20', 'https://miro.medium.com/max/60/1*OkQ31fZZt9wUBlhHe7APZg.png?q=20', 'https://miro.medium.com/max/60/1*8zTlP9dFqSLJCKcCt79nuA.png?q=20', 'https://miro.medium.com/max/60/1*G-XsZReuG9xrE5pFJkPnQA.png?q=20', 'https://miro.medium.com/max/2000/1*DtrGZG3J1I7SP7mYS3fQfQ.jpeg', 'https://miro.medium.com/max/2400/1*pKGU8X6qWv2ZI9qS3MKteA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1536/1*YZ-9AEMN44IxUZ_1W-vyuQ.png', 'https://miro.medium.com/max/60/1*pSPK4_68MbEJGW7poJtF_g.png?q=20', 'https://miro.medium.com/max/2218/1*uoe641315kyvN6T0S7Gs8A.png', 'https://miro.medium.com/max/60/1*nv90gz11EBOeWELfmLkyuw.png?q=20', 'https://miro.medium.com/max/60/1*zYdXu52wnhJKhVr7fB8hrw.png?q=20', 'https://miro.medium.com/max/2316/1*tEBf3iDCtN26XJ_YgQNjjg.png', 'https://miro.medium.com/max/60/1*UFr3eaDIdkihbxkI8N6lKQ.png?q=20', 'https://miro.medium.com/max/36/1*w8ZGZutyeijpHGdBkY_ukw.png?q=20', 'https://miro.medium.com/max/60/1*deRkCiwHO1Fe6OP8sVCV_Q.png?q=20', 'https://miro.medium.com/max/58/1*MOlvATgRJ1vqJjXjXo8ifQ.png?q=20', 'https://miro.medium.com/max/60/1*sF6GAj3ub_zZv6Giw_NDwQ.png?q=20', 'https://miro.medium.com/max/1546/1*1KPgHzPqQUozCOjRVEGmHA.png', 'https://miro.medium.com/max/928/1*LdZzRp5ODSPD6zvT5n_xFA.png', 'https://miro.medium.com/max/1568/1*F6M70vPXOqDx-3KBxTAwNA.png', 'https://miro.medium.com/max/1830/1*p0lFdO8I3jYdJzFeo1NpuQ.png', 'https://miro.medium.com/max/1024/1*vfwAhb-XCmnMEu5wFzTP8Q.png', 'https://miro.medium.com/max/1580/1*nv90gz11EBOeWELfmLkyuw.png', 'https://miro.medium.com/max/1558/1*CkkRB17xxNz8iaXUPOljvA.png'}",2020-03-05 00:16:37.931483,7.037263870239258
https://medium.com/microsoftazure/how-to-better-evaluate-the-goodness-of-fit-of-regressions-990dbf1c0091,How to Better Evaluate the Goodness-of-Fit of Regressions,"Most of the content of this post is platform-agnostic. Since in these days I’m using Azure Machine Learning, I take it as a starting point of my studies.

It’s quite simple for an Azure Machine Learning average user to create a regression experiment, make the data flow in it and get the predicted values. It’s also easy to have some metrics to evaluate the implemented model. Once you get them, the following questions arise:

How can I interpret these numbers?

Are these metrics enough to assess the goodness-of-fit of the model?

This post wants to provide you with the statistical foundation behind these metrics and with some additional tools that will help you to better understand how the model has fitted. These tools are implemented in a R script you can simply copy&paste into an Execute R Script module.

The Facts

Anyone who modeled a Regression in Azure Machine Learning has come across the default Evaluate Model module to evaluate the performances of the developed model. Let’s use the “Modeling Price Elasticity — Part 3: Price Elasticity with Combo and External Factors” experiment as an example. You can get it from the Cortana Intelligence Gallery through this link:

http://gallery.cortanaintelligence.com/Experiment/Modeling-Price-Elasticity-Part-3-Price-Elasticity-with-Combo-and-External-Factors-1

Once the experiment is successfully executed, the Evaluate Model module gives these results:

fig. 1 — The Evaluate Model module results

Let’s take a deep dive into these numbers and this plot.

Understanding the Metrics

Five metrics give us some hints about the goodness-of-fit of our model. The first two metrics, the Mean Absolute Error and the Root Mean Squared Error (also called Standard Error of the Regression), have the same unit as the original data. In fact, given ŷ the prediction, y the actual value and n the size of the sample, their definitions are the following:

These metrics can be used to compare models having the error e measured in the same units. MAE is simpler to understand, since it describes the average error. RMSE is not as intuitive as the other metric. It should be used when large errors are not allowed, because they are squared and then averaged.

Both these metrics can range from 0 to ∞.

An interesting relation between them is given by the following inequations:

So RMSE gets bigger than MAE as the sample size increases.

The other two metrics are the Relative Absolute Error and the Relative Squared Error, defined as following:

where y bar is the mean of the actual values y. They are derived from the first two metrics with the difference that there is a division by the variation of y. Because of that they are named “relative” and they can range from 0 to 1.

Since they are “relative”, these metrics can be used to compare the accuracy between models having errors measured in different units.

All the above mentioned metrics (MAE, RAE, RMSE, RSE) are insensitive to the direction of errors (the signs of errors are removed by the absolute value and by squaring them). For all of these metrics lower values are better.

The last metric we have is the Coefficient of Determination or R Squared (R²). It is defined as following:

The second addendum in the definition of R² can be seen as following:

where FVU is the Fraction of Variance Unexplained. The first equation compares the unexplained variance (variance of the model’s residuals, VARres) with the total variance (of the actual values, VARtot). Since FVU ranges from 0 to 1 (VARres cannot be higher than VARtot), R² is what remains after subtracting the measure of unexplained (FVU) from the whole. So R² measures something related to the explained variance.

Taking a look at fig. 1, we have that R² = 0.888678. This means that 89% of the variability between the two variables has been accounted for (the “explained” part), and the remaining 11% of the variability is still unaccounted for. For wide classes of linear models, the balance equation between accounted and unaccounted variability can be expressed as following:

That said, we can rewrite the definition of R² as following:

In this form:

R² can be seen as the percentage of the prediction variable’s variation that is explained by a linear model.

In other words, it’s a measure of how close the data is to the fitted regression line:

fig. 2 — Evidence of the R² value in relation to the goodness-of-fitting

So if R² = 0.888678, then 89% of the total variation in y can be explained by the linear relationship between features and y.

This metric usually ranges from 0 to 1 and is unitless. But it can also be negative when the predictions are not obtained by linear regression. Contrary to other metrics, the closer R² is to 1, the better the model explains all the variability of the target variable around its mean.

The Right Way to Evaluate the Goodness-Of-Fit for Every Regression

Can R² tell us always the truth about the goodness-of-fit of our model? As you can imagine, the short answer is: no!

R² cannot tell us if the predictions are biased and sometimes it leads you to make bad decisions:

R² can be low even if the model is good (the data contains an high amount of unexplainable variability)

R² can be high even if the model is not good (the regressed function fits quite well but the resulting residuals are not randomly distributed, see later)

To make matters worst, all the assumptions done in the previous paragraph for R² take for granted the regression is linear.

If you are dealing with a nonlinear regression, R² alone can lead to wrong conclusions. Only 28–43% of the models tuned using R² are correct.

Specifically, for nonlinear regressions:

R² tends to be high for both very bad and very good models (even if you consider the adjusted-R² defined later).

R² do not always increases for better nonlinear models.

In case of nonlinear regression, it’s better to use the Residual Standard Error (or Standard Error of Estimate or Standard Error of Regression). It measures the average distance of the actual values from the regression line and it is conceptually similar to the standard deviation, with the difference that the standard deviation measures the average distance of the actual values from the mean. The Residual Standard Error (S) is defined below:

where k refers to the number of predictors (parameters to be estimated using the regression), not including the intercept (it’s accounted by the “- 1” in the df formula). Going back to our Azure Machine Learning experiment, we can easily find the number for k visualizing the Train Model module:

fig. 3 — Visualizing the results of the Train Model module

As you can see, each feature weight is made explicit. Counting the number of the weights, excluding the bias (the intercept), we’ll estimate k. In this case, k is 23. Did you notice you have 23 parameters versus 11 features in input to the Train Model module? This happens when there are categorical features that are automatically converted in dummy variables (or indicator values) by the Linear Regression algorithm in Azure ML.

Just as an example, I wrote a script in R to clarify some statistics on residuals using a default data frame (the results are added as comments):

You may notice the similarity between the values of the Root Mean Squared Error and the Residual Standard Error (remember they have the same unit). The only difference in their formulas is given by the usage of the degrees of freedom instead of n. In a very large sample the difference in values becomes irrelevant.

The real power of the Residual Standard Error is that it allows us to approximately know where most of the observations will fall.

We can say that approximately 95% of the observations should fall within +/- 2·S from the regression line.

Using the Residual Standard Error found in the R script (S = 2.63), we can say that about 95% of the number of murders should fall within +/- 5.26 from the fitted line. If we stated we can have an error of 5% at most, then must be S ≤ 2.5. In that case our model needs a little improvement.

Additional Metrics: MAPE, NRMSE and R²adj

There are other metrics that sometimes are less used in some domains. Here are a some of them.

MAPE (Mean Absolute Percent Error)

It’s the acronym of and it measures the size of the error in percentage terms. It is also called Mean Absolute Percentage Deviation (MAPD) and it is defined as following:

If MAPE is 3, then we can say that, on average, the forecast is off by 3%.

Be aware that this metric may lead misleading results:

It’s not defined for observation measuring zero (sometimes the zero value belong to the domain of analysis, e.g. 0° Celsius for the Temperature variable).

If used to compare the accuracy of different models, consider it’s strongly biased when forecast is less than the actual; in this case it reports lower errors than than the case when the forecast is more than the actual.

That’s why the Symmetric Mean Absolute Percent Error (sMAPE) or the Weighted Mean Absolute Percent Error (wMAPE) are sometimes preferred.

NRMSE (Normalized Root Mean Square Error)

As well as RAE and RSE, the Normalized Root Mean Square Error is useful to compare models with different scale. Since we’re considering a normalization, there are more way to normalize an existing metric. For RMSE we can find these kinds of normalization:

where s(ŷ) is the standard deviation of the predictions.

R²adj (adjusted-R²)

The adjusted-R² is a modified version of R² and it takes account of the number k of predictors upon defined for the degrees of freedom. One of the issues of R² is that it always increases (and it never decreases) as you add a variable to the model, even if the new variable is not related to the target one. The adjusted-R² will increase only if the new variable will add some explained variance to the model. It’s definition is the following:

The following inequality is always true:

Understanding the Error Histogram in the Evaluate Model Module

If the prediction is the result of a valid linear regression model, it can be decomposed in three main components:

If the model is good, the first two components (in parenthesis) are the only part of the prediction that is deterministic. If the deterministic part describes so well all those factors that can be predicted, then the remaining part can only be explained by random and unpredictable events.

In our case the error component is represented by the residuals. In a good model they should not be either systematically high or low, they should be centered on zero (the more the prediction is “near” the actual value, the more we’ll have residuals equal to zero) and they should not follow a particular pattern (if there is a pattern, there’ll be a deterministic predictor we’re missing in the model). That’s why random errors are assumed to produce residuals that are normally distributed.

The Evaluate Model module contains also the distribution of residuals. We expect them to be roughly normal with a mean of zero and some constant variance. So we expect a bell shaped distribution. But if we look at the first picture of this post(fig. 1), the histogram presents only positive residuals. The reason is that it’s the histogram of the absolute value of residuals. If we use this histogram for our analysis, it’s quite impossible to understand if the distribution is symmetric around the zero or if it’s skewed. It’s better to have the histogram of the residuals as-is, so that it’s possible to notice asymmetries at a glance:

fig. 4 — Histogram of the residuals of the regression

Now it’s clear the distribution of residuals is right skewed.

There are other graphical representations of residuals that will help us to see if there are unwanted patterns in them, rather than the randomness that you want to see: the predicted vs actual plot, the normal q-q plot and the predicted vs residual plot (residual plot).

Useful Plots to Check the Residuals Health

Let’s now present the new weapons to check the fitting of our regression.

Predicted vs Actual Plot

The Predicted vs Actual plot is a scatter plot and it’s one of the most used data visualization to asses the goodness-of-fit of a regression at a glance. For each predicted value on the x axis we draw a point corresponding to the actual value on the y axis. The result is the following:

fig. 5 — The predicted vs actual plot of the price model

If for each element of the sample every prediction corresponds to the actual value, we’ll have the hypothetical line y = x (the dashed one in the following picture) that represents the perfect model. So, the more the points are close to the dashed line, the more the model is good.

In fig. 5 there are some points far from the dashed line, meaning that may be the model needs to be improved.

Predicted vs Residual Plot (also called “Residual Plot”)

The most useful way to represent the residuals is with the Residual Plot. It puts the residuals in relation with the corresponding predictions in the Cartesian plane. The Residual Plot for the Price Elasticity model is the following:

fig. 6 — Residual plot of the price model

Standardized residuals are often used, so that more than one models with different unit can be compared. Their definition is the following:

The farther the distance from the line at 0, the worse the prediction. An optimal residual plot should be like the following:

fig. 7 — An optimal residual plot

where the random distribution of the residuals are clear. As you can see, the Price Elasticity model’s Residual Plot in fig. 6 is not so optimal.

Depending on the configuration the points may have in the residual plot, an appropriate diagnosys can be made. A list of them can be found in this interesting blog post.

Normal Q-Q Plot

The Q-Q plot, or quantile-quantile plot, helps us to asses if a given distribution is comparable with some theoretical distribution such as a Normal or exponential. It’s a scatter plot made up by two sets of quantiles against one another. If the two distributions being compared are identical, the Q-Q plot follows the line y = x (the dashed one in the following picture). In our case we want to compare our residuals distribution with the theoretical Normal one to check the randomness of it. The Standard Normal distribution is the following one:

fig. 8 — Probability density function of the Standard Normal Distribution

Just remember that in a Standard Normal distribution, the 0.5 quantile, or 50th percentile, is 0. Half the data lies below 0. That’s the peak of the bell. The 0.95 quantile, or 95th percentile, is about 1.64. 95 percent of the data lies below 1.64. In our case, the Normal Q-Q plot of the residuals is the following:

fig. 9 — Normal Q-Q Plot for Residuals

It’s clear that we have a problem above the 1.5 theoretical quantile, the same problem we noticed thanks to the histogram in fig. 4.

Did you notice the grey band over the dotted line? It’s the confidence band and you can find a good definition of it here.

(Feature | Label | External Feature) vs Residual Plot

Plotting the residuals versus the label and every feature (included and not included into the model) helps a lot in diagnostic residuals. Not included features are the ones who were dropped before the training of the model, due to multicollinearity or overfitting reasons. A structure appearing in the residuals might be explained by a not included variable. In that case you may include this variable in a more complex model.

All these plots may highlight if an existing pattern is related to one of the features. They can be scatter plots or box plots in case of categorical (factor in R) variable. Here some examples:

fig. 10 — The calendar_date variable vs residuals plot shows that something strange happen after the 1st of September

fig. 11 — The “holiday” variable is categorical, so the resulting residual plot is a box plot by groups

Enriching the Azure ML Evaluate Model Module with a R Script

Ok, it’s the time to provide the upon mentioned tools to the Azure ML user who wants to measure the performances of his regression model. It’s enough to add an Execute R Script module just after the Score Model module. Then:

link its left input port with the Score Model output port;

link its right input port with the hold-out test data.

fig. 12 — The new Execute R Script module

Just be aware to eventually remove the unused variable after the holding-out has happened (after the split), so that all the external feature are given as input to the Execute R Script module through its right input port.

Now just copy and past the following script in it:

Keep in mind you only have to modify the “Input Parameters” section according to your needs. In details:

target_variable_name is the name of the label column selected in the Train Model module.

is the name of the label column selected in the Train Model module. predicted_values_variable_name is the name of the predictions column added by the Score Model module. Be aware that its name changes according to the regression algorithm chosen. For example, the name is “Scored Label Mean” for a Decision Forest Regression.

is the name of the predictions column added by the Score Model module. Be aware that its name changes according to the regression algorithm chosen. For example, the name is “Scored Label Mean” for a Decision Forest Regression. k is the number of parameters to be estimated by the regression, used in the degrees of freedom and in the adjusted-R² calculations.

Then, after successfully executed the experiment, we can see all the residual plots introduced earlier visualizing the rightmost output port results. Here you can see just few of them:

fig. 13 — Some of the new plots available from the new Execute R Script module

The leftmost output port gives the following metrics:

fig. 14 — The new available metrics highlighted

As you can see, there are also some metrics already available in the default Evaluate Model module just for convenience. The NRMSE metric is calculated using the first definition above mentioned (max - min).

Conclusions

This post wants to give an in-depth vision of the theory behind the metrics an Azure ML user can easily find just dragging an Evaluate Model module into the experiment. This knowledge is essential to make the right choices about the final model. Additional metrics and plots are provided in order to refine the goodness-to-fit of the model.

Given that, what’s the best strategy to find a good model?

The R² and S metrics in conjunction with other model statistics and the residual plots are the way to refine the model. A profound knowledge in the areas under analysis is also the keystone for the final tuning of the model.

The application of these principles will be the topic of the next post.","['residual', 'regressions', 'goodnessoffit', 'regression', 'error', 'r²', 'module', 'evaluate', 'better', 'model', 'standard', 'plot', 'residuals', 'metrics']","The FactsAnyone who modeled a Regression in Azure Machine Learning has come across the default Evaluate Model module to evaluate the performances of the developed model.
You can get it from the Cortana Intelligence Gallery through this link:http://gallery.cortanaintelligence.com/Experiment/Modeling-Price-Elasticity-Part-3-Price-Elasticity-with-Combo-and-External-Factors-1Once the experiment is successfully executed, the Evaluate Model module gives these results:fig.
The Right Way to Evaluate the Goodness-Of-Fit for Every RegressionCan R² tell us always the truth about the goodness-of-fit of our model?
In case of nonlinear regression, it’s better to use the Residual Standard Error (or Standard Error of Estimate or Standard Error of Regression).
Predicted vs Residual Plot (also called “Residual Plot”)The most useful way to represent the residuals is with the Residual Plot.",en,['Luca Zavarella'],2019-02-05 19:10:45.598000+00:00,"{'Regression', 'Azure', 'Machine Learning', 'Rstats', 'Statistics'}","{'https://miro.medium.com/max/1792/1*tv4JCzOQLe6EULhjS0trCA.png', 'https://miro.medium.com/max/60/1*a0lNcBAmjFD0R84PNn02_Q.png?q=20', 'https://miro.medium.com/max/960/1*91ZlkqHRrrmTusakv_RTAw.png', 'https://miro.medium.com/max/60/1*BoSebqWqLwO-ztq776JqVA.png?q=20', 'https://miro.medium.com/max/60/1*XKv2zc0cM6ocA-oZkKOzIw.png?q=20', 'https://miro.medium.com/max/1000/1*uaWFO7oTgXjfI2AK-bx1Uw.png', 'https://miro.medium.com/max/60/1*XVIQS7llnP7hEY2AbHzeOQ.png?q=20', 'https://miro.medium.com/max/60/1*eGdSnun2CHESGCE9SS9bWA.png?q=20', 'https://miro.medium.com/max/886/1*RvgIf0UNVCsKk20cgJ7pfA.png', 'https://miro.medium.com/max/60/1*0agqSvzPbuo9VouTu3o53A.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*XOSQvn7aelKLC76u', 'https://miro.medium.com/max/250/1*6et9YS9bNbTN9qYsdjB0dw.png', 'https://miro.medium.com/max/960/1*cBa3zpTCw85TOyuCdBRYbQ.png', 'https://miro.medium.com/max/556/1*UdTCATNltVSEufu4ISCATA.png', 'https://miro.medium.com/max/960/1*okQc2DLNdtQBWAjYtLM6PQ.png', 'https://miro.medium.com/max/60/1*YTTcqKISaFBvHxCYX-6r4g.png?q=20', 'https://miro.medium.com/max/1144/1*eGdSnun2CHESGCE9SS9bWA.png', 'https://miro.medium.com/max/1308/1*h1o7KY5Xnt_f1YqbRMZdUw.png', 'https://miro.medium.com/max/60/1*cBa3zpTCw85TOyuCdBRYbQ.png?q=20', 'https://miro.medium.com/max/1204/1*0agqSvzPbuo9VouTu3o53A.png', 'https://miro.medium.com/max/1090/1*zr3ytSeIjNeh280Jmu6rwQ.png', 'https://miro.medium.com/max/60/1*h1o7KY5Xnt_f1YqbRMZdUw.png?q=20', 'https://miro.medium.com/max/60/1*TLih28msxoZmKQ4j-V045A.png?q=20', 'https://miro.medium.com/max/646/1*XKv2zc0cM6ocA-oZkKOzIw.png', 'https://miro.medium.com/max/450/1*OscN4QUQIdB9b4xJQg3cPg.png', 'https://miro.medium.com/max/360/1*11-u7d4d_6-CWMz_a1qJZA.png', 'https://miro.medium.com/fit/c/80/80/2*xlRLw9utv7CGS9cdoAo4HA.jpeg', 'https://miro.medium.com/max/60/1*3JhCQr6fhLcYCzOawDGzPQ.png?q=20', 'https://miro.medium.com/max/1114/1*J_Tq2AiG4RdefXsErbuv2w.png', 'https://miro.medium.com/max/732/1*T9-Ww7kDd4hjKNn6NecrlA.png', 'https://miro.medium.com/max/60/1*Vql22I7PvTSfIS17VeNf3w.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*dKM1AAB0aRS4a_r_gwPPEg.jpeg', 'https://miro.medium.com/max/720/1*3JhCQr6fhLcYCzOawDGzPQ.png', 'https://miro.medium.com/max/818/1*Gcmw3-J3kQo7ex8bPzc2gQ.png', 'https://miro.medium.com/max/60/1*427fdFpPMhnGdq_fjGGGoQ.png?q=20', 'https://miro.medium.com/max/60/1*UdTCATNltVSEufu4ISCATA.png?q=20', 'https://miro.medium.com/max/220/1*WdQCNAsFCY3bFUL2CySjbw.png', 'https://miro.medium.com/max/60/1*okQc2DLNdtQBWAjYtLM6PQ.png?q=20', 'https://miro.medium.com/max/60/1*J_Tq2AiG4RdefXsErbuv2w.png?q=20', 'https://miro.medium.com/max/60/1*uaWFO7oTgXjfI2AK-bx1Uw.png?q=20', 'https://miro.medium.com/max/60/1*tv4JCzOQLe6EULhjS0trCA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*qv8EcKW19IsdLkKFOIxQJw.jpeg', 'https://miro.medium.com/max/42/1*Wtalq4UvaI8WH0uwGQZajg.png?q=20', 'https://miro.medium.com/max/60/1*OscN4QUQIdB9b4xJQg3cPg.png?q=20', 'https://miro.medium.com/max/60/1*91ZlkqHRrrmTusakv_RTAw.png?q=20', 'https://miro.medium.com/max/60/1*WdQCNAsFCY3bFUL2CySjbw.png?q=20', 'https://miro.medium.com/max/960/1*Vql22I7PvTSfIS17VeNf3w.png', 'https://miro.medium.com/max/60/1*LHaOV-qX-fKiX4w1puVQSw.png?q=20', 'https://miro.medium.com/max/60/1*h9Um3mVBmz8qk5Wqg-jRfQ.png?q=20', 'https://miro.medium.com/max/1114/1*YTTcqKISaFBvHxCYX-6r4g.png', 'https://miro.medium.com/max/52/1*T9-Ww7kDd4hjKNn6NecrlA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*dKM1AAB0aRS4a_r_gwPPEg.jpeg', 'https://miro.medium.com/max/960/1*a0lNcBAmjFD0R84PNn02_Q.png', 'https://miro.medium.com/max/826/1*427fdFpPMhnGdq_fjGGGoQ.png', 'https://miro.medium.com/max/480/1*XVIQS7llnP7hEY2AbHzeOQ.png', 'https://miro.medium.com/fit/c/160/160/1*unP3QO2tsX2f8Fr4C8t8tQ.png', 'https://miro.medium.com/max/730/1*BoSebqWqLwO-ztq776JqVA.png', 'https://miro.medium.com/max/900/1*Wtalq4UvaI8WH0uwGQZajg.png', 'https://miro.medium.com/max/960/1*h9Um3mVBmz8qk5Wqg-jRfQ.png', 'https://miro.medium.com/max/960/1*XVIQS7llnP7hEY2AbHzeOQ.png', 'https://miro.medium.com/max/14/1*zr3ytSeIjNeh280Jmu6rwQ.png?q=20', 'https://miro.medium.com/max/36/1*11-u7d4d_6-CWMz_a1qJZA.png?q=20', 'https://miro.medium.com/max/60/1*Gcmw3-J3kQo7ex8bPzc2gQ.png?q=20', 'https://miro.medium.com/max/596/1*TLih28msxoZmKQ4j-V045A.png', 'https://miro.medium.com/max/60/1*RvgIf0UNVCsKk20cgJ7pfA.png?q=20', 'https://miro.medium.com/max/1792/1*LHaOV-qX-fKiX4w1puVQSw.png'}",2020-03-05 00:16:40.808617,2.8751325607299805
https://towardsdatascience.com/calculating-price-elasticity-of-demand-statistical-modeling-with-python-6adb2fa7824d,"Price Elasticity of Demand, Statistical Modeling with Python","Price Elasticity of Demand, Statistical Modeling with Python

How to maximize profit

Price elasticity of demand (PED) is a measure used in economics to show the responsiveness, or elasticity, of the quantity demanded of a good or service to a change in its price when nothing but the price changes. More precisely, it gives the percentage change in quantity demanded in response to a one percent change in price.

In economics, elasticity is a measure of how sensitive demand or supply is to price.

In marketing, it is how sensitive consumers are to a change in price of a product.

It gives answers to questions such as:

“If I lower the price of a product, how much more will sell?”

“If I raise the price of one product, how will that affect sales of the other products?”

“If the market price of a product goes down, how much will that affect the amount that firms will be willing to supply to the market?”

We will build a linear regression model to estimate PED, and we will use Python’s Statsmodels to estimate our models as well as conduct statistical tests, and data exploration. Let’s get started!

The data

We will work with the beef price and demand data that can be downloaded from here.

%matplotlib inline from __future__ import print_function

from statsmodels.compat import lzip

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import statsmodels.api as sm

from statsmodels.formula.api import ols beef = pd.read_csv('beef.csv')

beef.head(10)

Figure 1

Regression Analysis

Ordinary Least Squares (OLS) Estimation

beef_model = ols(""Quantity ~ Price"", data=beef).fit()

print(beef_model.summary())

Figure 2

Observations:

The small P values indicate that we can reject the null hypothesis that Price has no effect on Quantity. Hight R-squared indicates that our model explains a lot of the response variability. In regression analysis, we’d like our regression model to have significant variables and to produce a high R-squared value. We will show graphs to help interpret regression analysis results more intuitively.","['demand', 'regression', 'import', 'quantity', 'change', 'supply', 'modeling', 'python', 'elasticity', 'price', 'statistical', 'model', 'product']","Price Elasticity of Demand, Statistical Modeling with PythonHow to maximize profitPrice elasticity of demand (PED) is a measure used in economics to show the responsiveness, or elasticity, of the quantity demanded of a good or service to a change in its price when nothing but the price changes.
More precisely, it gives the percentage change in quantity demanded in response to a one percent change in price.
In economics, elasticity is a measure of how sensitive demand or supply is to price.
The dataWe will work with the beef price and demand data that can be downloaded from here.
In regression analysis, we’d like our regression model to have significant variables and to produce a high R-squared value.",en,['Susan Li'],2018-12-06 03:09:58.280000+00:00,"{'Data Science', 'Python', 'Dynamic Pricing', 'Machine', 'Statistical Analysis'}","{'https://miro.medium.com/max/2326/1*-TUARtS0oJr1cNwcMdQYvA.png', 'https://miro.medium.com/max/60/1*Rky-tEjpyNuTS9wsqUf8Vg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*yhl_E_fI-4IifYhLV-3KDg.jpeg', 'https://miro.medium.com/max/60/1*J5fgwq3PGwJaG4thvfnolQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/52/1*wWh5czghgLBMFTavSZmIqw.png?q=20', 'https://miro.medium.com/max/2168/1*GheccPx1HiYnYyomvJbHiA.png', 'https://miro.medium.com/max/60/1*YQuG4waOoV5xnZnLtlVz4g.png?q=20', 'https://miro.medium.com/max/60/1*-Vspv3sGlPb9DYZ33LYqqA.png?q=20', 'https://miro.medium.com/max/60/1*GheccPx1HiYnYyomvJbHiA.png?q=20', 'https://miro.medium.com/max/2186/1*J5fgwq3PGwJaG4thvfnolQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2184/1*Ctb8IiMB90QHDPnTCGYegw.png', 'https://miro.medium.com/max/60/1*yhl_E_fI-4IifYhLV-3KDg.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/852/1*wWh5czghgLBMFTavSZmIqw.png', 'https://miro.medium.com/max/2592/1*YQuG4waOoV5xnZnLtlVz4g.png', 'https://miro.medium.com/max/60/1*Ctb8IiMB90QHDPnTCGYegw.png?q=20', 'https://miro.medium.com/max/58/1*Lc-Z9rCmiR-MQs9hJabDiA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/846/1*Lc-Z9rCmiR-MQs9hJabDiA.png', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/1820/1*Rky-tEjpyNuTS9wsqUf8Vg.png', 'https://miro.medium.com/max/60/1*-TUARtS0oJr1cNwcMdQYvA.png?q=20', 'https://miro.medium.com/max/1778/1*-Vspv3sGlPb9DYZ33LYqqA.png', 'https://miro.medium.com/max/2560/1*yhl_E_fI-4IifYhLV-3KDg.jpeg'}",2020-03-05 00:16:48.091606,7.281987905502319
https://towardsdatascience.com/get-faster-pandas-with-modin-even-on-your-laptops-b527a2eeda74,"Get faster pandas with Modin, even on your laptops.","Modin

Modin is an early-stage project at UC Berkeley’s RISELab designed to facilitate the use of distributed computing for Data Science. It is a multiprocess Dataframe library with an identical API to pandas that allows users to speed up their Pandas workflows.

Modin accelerates Pandas queries by 4x on an 8-core machine, only requiring users to change a single line of code in their notebooks. The system has been designed for existing Pandas users who would like their programs to run faster and scale better without significant code changes. The ultimate goal of this work is to be able to use Pandas in a cloud setting.

Installation

Modin is completely open-source and can be found on GitHub: https://github.com/modin-project/modin

Modin can be installed from PyPI:

pip install modin

For Windows, one of the dependencies is Ray. Ray is not yet supported natively on Windows, so in order to install it, one needs to use the WSL(Windows Subsystem for Linux).","['wslwindows', 'pandas', 'install', 'users', 'modin', 'ray', 'windows', 'workflowsmodin', 'laptops', 'work', 'faster', 'designed', 'code']","ModinModin is an early-stage project at UC Berkeley’s RISELab designed to facilitate the use of distributed computing for Data Science.
It is a multiprocess Dataframe library with an identical API to pandas that allows users to speed up their Pandas workflows.
Modin accelerates Pandas queries by 4x on an 8-core machine, only requiring users to change a single line of code in their notebooks.
The system has been designed for existing Pandas users who would like their programs to run faster and scale better without significant code changes.
The ultimate goal of this work is to be able to use Pandas in a cloud setting.",en,['Parul Pandey'],2019-04-22 04:38:09.882000+00:00,"{'Pandas', 'Data Science', 'Machine Learning', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/1248/1*FvGd5XHaoaTBF-_GDv-UtQ.png', 'https://miro.medium.com/max/60/0*cOcYOJ3Lib0_04Ji.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/1260/1*PI1k03Q6KVGZmVTUU4p5eQ.png', 'https://miro.medium.com/max/60/1*WEVQ9dB2ew8SUHONgiZtAA.png?q=20', 'https://miro.medium.com/max/2000/0*GYnWlFf7QyueGWEm.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/886/1*WEVQ9dB2ew8SUHONgiZtAA.png', 'https://miro.medium.com/max/982/1*0JxGIZXjtd0i9InYkXF-mg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*wLCHEekWiQAj-Q-Fg_8zcg.jpeg?q=20', 'https://miro.medium.com/max/60/0*GYnWlFf7QyueGWEm.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*PI1k03Q6KVGZmVTUU4p5eQ.png?q=20', 'https://miro.medium.com/max/60/1*dR8v0vX2-da2rb6IAq7a5A.png?q=20', 'https://miro.medium.com/max/1200/1*wLCHEekWiQAj-Q-Fg_8zcg.jpeg', 'https://miro.medium.com/max/60/1*FvGd5XHaoaTBF-_GDv-UtQ.png?q=20', 'https://miro.medium.com/max/1266/1*-6XBixD_78WkBhoH18apLg.png', 'https://miro.medium.com/max/60/1*aYabjeuzxk08lq5JxzX2vQ.png?q=20', 'https://miro.medium.com/max/60/1*-VPZYVqkQF9daglo-xdtiw.png?q=20', 'https://miro.medium.com/max/60/1*-6XBixD_78WkBhoH18apLg.png?q=20', 'https://miro.medium.com/max/60/1*0JxGIZXjtd0i9InYkXF-mg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/1370/1*-VPZYVqkQF9daglo-xdtiw.png', 'https://miro.medium.com/max/1648/1*dR8v0vX2-da2rb6IAq7a5A.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1254/1*aYabjeuzxk08lq5JxzX2vQ.png', 'https://miro.medium.com/max/3840/1*wLCHEekWiQAj-Q-Fg_8zcg.jpeg', 'https://miro.medium.com/max/1506/0*cOcYOJ3Lib0_04Ji.png'}",2020-03-05 00:16:54.643330,6.550748825073242
https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80,Demystifying Black-Box Models with SHAP Value Analysis,"As an Applied Data Scientist at Civis, I implement the latest data science research to solve real-world problems. We recently worked with a global tool manufacturing company to reduce churn among their most loyal customers. A newly proposed tool, called SHAP (SHapley Additive exPlanation) values, allowed us to build a complex time-series XGBoost model capable of making highly accurate predictions for which customers were at risk, while still allowing for an individual-level interpretation of the factors that made each of these customers more or less likely to churn.

To understand why this is important, we need to take a closer look at the concepts of model accuracy and interpretability. Until recently, we always had to choose between an accurate model that was hard to interpret, or a simple model that was easy to explain but sacrificed some accuracy. Classic methods like logistic regression or a simple decision tree make it easy to explain why we assign a person to the positive or negative class, but there is only so much predictive power we can squeeze out of these basic models. To improve accuracy, more complex models may use thousands of these decision trees and then combine their results with yet another model or ensemble rule (e.g. majority vote). On the other end of the complexity spectrum, deep learning uses neural networks with multiple interconnected layers, each layer looking at a higher level of abstraction of the underlying data. This added complexity gives these models more flexibility, allowing them to reach high accuracy levels that cannot be obtained by simple models, but at the expense of our ability to comprehend why the model made the predictions it did. Even the people who designed and trained the model can no longer explain what led one person to get assigned to one class over another. For the work we do at Civis (where our models have to generate insights and recommendations for actions), getting the trade-off between accuracy and interpretability just right can be a difficult balancing act. With SHAP values, we are finally able to get both!

The SHAP values technique was proposed in recent papers by Scott M. Lundberg from the University of Washington [1, 2]. It is based on Shapley values, a technique used in game theory to determine how much each player in a collaborative game has contributed to its success. In our case, each SHAP value measures how much each feature in our model contributes, either positively or negatively, to a customer’s predicted churn risk score (see Figure 1). This is a similar idea to feature importance in logistic regression, where we can determine the impact of each feature by looking at the magnitude of its coefficient. However, SHAP values offer two important benefits. First, SHAP values can be calculated for any tree-based model, so instead of being restricted to simple, linear — and therefore less accurate — logistic regression models, we can build complex, non-linear and more accurate models. Second, each individual customer will have their own set of SHAP values. Traditional feature importance algorithms will tell us which features are most important across the entire population, but this one-size-fits-all approach doesn’t always apply to each individual customer. A factor that is an important driver for one customer may be a non-factor for another. By looking only at the global trends, these individual variations can get lost, with only the most common denominators remaining. With individual-level SHAP values, we can pinpoint which factors are most impactful for each customer, allowing us to customize our next actions accordingly.

Figure 1. SHAP values measure the impact of each variable on a customer’s Engagement score (measuring their likelihood to remain a loyal customer in the next month or year). For each individual customer, this allows us to identify the biggest risk factors (red arrows) and protective factors (blue arrows), and recommend a tailored intervention plan.

While SHAP values can be a great tool, they do have shortcomings (although they are common in calculating feature importance using observational data). For one, SHAP values are sensitive to high correlations among different features. When features are correlated, their impact on the model score can be split among them in an infinite number of ways. This means that the SHAP values will be lower than if all but one of the correlated feature(s) had been removed from the model. The risk is that dividing impacts this way makes them look less important than if their impacts remained undivided. To be fair, all known feature importance methods have this problem. A second shortcoming is that SHAP values represent a descriptive approximation of the predictive model. For example, SHAP values can tell us that for a given customer, a low number of sales visits has the largest negative impact on their risk score, so we may decide to schedule more sales visits in the upcoming month. However, we cannot determine based on the SHAP values alone what the impact of this intervention will be. Again, this is a fundamental limitation to data science. There is only so much we can do with observational data. To accurately estimate the impact of different churn prevention techniques, we will need to conduct a randomized controlled trial (RCT).

We think there’s a lot of promise in SHAP values. Instead of having to choose between accuracy and interpretability, we finally have a tool that lets us push the envelope in terms of model complexity and accuracy, while still allowing us to derive intuitive explanations for each individual prediction. SHAP values have been added to the XGBoost library in Python, so the tool is available to anyone. Scott Lundberg, the author of the SHAP values method, has expressed interest in expanding the method to a broader selection of models, beyond tree-based algorithms. As we continue to test this out further, we’ll report back with our experience!

References:","['demystifying', 'blackbox', 'models', 'analysis', 'feature', 'tool', 'accuracy', 'customer', 'model', 'impact', 'data', 'shap', 'values', 'value']","To understand why this is important, we need to take a closer look at the concepts of model accuracy and interpretability.
With SHAP values, we are finally able to get both!
The SHAP values technique was proposed in recent papers by Scott M. Lundberg from the University of Washington [1, 2].
SHAP values have been added to the XGBoost library in Python, so the tool is available to anyone.
Scott Lundberg, the author of the SHAP values method, has expressed interest in expanding the method to a broader selection of models, beyond tree-based algorithms.",en,['Civis Analytics'],2018-06-25 20:37:07.451000+00:00,"{'Data Science', 'Predictive Modeling', 'Data Science Engineering', 'Machine Learning', 'Predictive Analytics'}","{'https://miro.medium.com/max/2574/1*7kH8RQHxZK5qElP_DbfGpQ.png', 'https://miro.medium.com/max/60/1*7kH8RQHxZK5qElP_DbfGpQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*R8tOP_Jj8SBC6LVHjXElKQ.png', 'https://miro.medium.com/fit/c/80/80/1*R8tOP_Jj8SBC6LVHjXElKQ.png', 'https://miro.medium.com/fit/c/160/160/1*W4FDr_NX0-ND7adf7cvkxA.png', 'https://miro.medium.com/max/2574/1*aoKjT5tksmoZBV2tMJcsOg.png', 'https://miro.medium.com/fit/c/96/96/1*R8tOP_Jj8SBC6LVHjXElKQ.png', 'https://miro.medium.com/max/60/1*aoKjT5tksmoZBV2tMJcsOg.png?q=20', 'https://miro.medium.com/max/1200/1*7kH8RQHxZK5qElP_DbfGpQ.png'}",2020-03-05 00:16:56.400762,1.7574312686920166
https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83,Interpreting complex models with SHAP values,"Note: This post was originally published on the Canopy Labs website, and describes work I’ve been lucky to do as a data scientist there.

An important question in the field of machine learning is why an algorithm made a certain decision.

This is important for a variety of reasons. As an end user, I am more likely to trust a recommendation if I understand why it was exposed to me. As an organization, understanding that customers made a purchase because this campaign was particularly effective can allow me to tailor my future outreach efforts.

However, this is a challenging and still developing field in machine learning. In this post, I am going to discuss exactly what it means to interpret a model, and explore a novel technique called SHAP (https://github.com/slundberg/shap) which is particularly effective at allowing us to take the hood off complex algorithms.

What does it mean to interpret a model (and why is it so hard)?

Let’s start by defining exactly what it means to interpret a model. At a very high level, I want to understand what motivated a certain prediction.

For instance, lets reuse the problem from the XGBoost documentation, where given the age, gender and occupation of an individual, I want to predict whether or not they will like computer games:

In this case, my input features are age, gender and occupation. I want to know how these features impacted the model’s prediction that someone would like computer games.

However, there are two different ways to interpret this:

On a global level. Looking at the entire dataset, which features did the algorithm find most predictive? XGBoost’s get_score() function - which counts how many times a feature was used to split the data – is an example of considering global feature importance, since it looks at what was learned from all the data. On a local level. Maybe, across all individuals, age was the most important feature, and younger people are much more likely to like computer games. But if Frank is a 50-year-old who works as a video game tester, it’s likely that his occupation is going to be much more significant than his age in determining whether he likes computer games. Identifying which features were most important for Frank specifically involves finding feature importances on a ‘local’ – individual – level.

With this definition out of the way, let’s move on to one of the big challenges in model interpretability:

Trading off between interpretablity and complexity

Let’s consider a very simple model: a linear regression. The output of the model is

In the linear regression model above, I assign each of my features x_i a coefficient ϕ_i, and add everything up to get my output. In the case of my computer games problem, my input features would be (x_Age, x_Gender, x_Job).

In this case, its super easy to find the importance of a feature; if ϕ_i has a large absolute value, then feature xi had a big impact on the final outcome (e.g. if ∣ϕ_Age∣ is large, then age was an important feature). However, there is also a drawback, which is that this model is so simple that it can only uncover linear relationships.

For instance, maybe age is an important feature, and if you’re between 12 and 18 you’re much more likely to like computer games than at any other age; since this is a non-linear relationship, a linear regression wouldn’t be able to uncover it.

In order to uncover this more complicated relationship, I’ll need a more complicated model.

However, as soon as I start using more complicated models, I lose the ease of interpretability which I got with this linear model. In fact, as soon as I try to start uncovering non-linear, or even interwoven relationships — e.g. what if age is important depending on your gender? — then it becomes very tricky to interpret the model.

This decision — between an easy to interpret model which can only uncover simple relationships, or complex models which can find very interesting patterns that may be difficult to interpret — is the trade off between interpretability and complexity.

This is additionally complicated by the fact that I might be interpreting a model because I’m hoping to learn something new and interesting about the data. If this is the case, a linear model may not cut it, since I may already be familiar with the relationships it would uncover.

The ideal case would therefore be to have a complex model which I can also interpret.

How can we interpret complex models?

Thinking about linear regressions has yielded a good way of thinking about model interpretations:

I’ll assign to each feature x_i a coefficient ϕ_i which describes — linearly — how the feature affects the output of the model. We’ve already discussed the shortcomings of this model, but bear with me:

Across many data points, the coefficients ϕ will fail to capture complex relationships. But on an individual level, then they’ll do fine, since for a single prediction, each variable will truly have impacted the model’s prediction by a constant value.

For instance, consider the case of Frank, the 50-year-old video game tester who loves computer games. For him, ϕ_Job will be high and ϕ_Age will be low.

But then, for Bobby, a 14-year-old, ϕ_Age will be high since the model has see that 14-year olds tend love computer games because they are 14 years old.

What we’ve done here is take a complex model, which has learnt non-linear patterns in the data, and broken it down into lots of linear models which describe individual data points. Its important to note that these explanation coefficients ϕ are not the output of the model, but rather what we are using to interpret this model. By aggregating all of these simple, individual models together, we can understand how the model behaves across all the customers.

So, to sum up:

Instead of trying to explain the whole complex model, I am just going to try and explain how the complex model behaved for one data point. I’ll do this using a linear explanation model; let’s call it g.

In addition, to further simplify my simple model, I won’t multiply the coefficients ϕ by the original feature value, x. Instead, I’ll multiply it by 1 if the feature is present, and 0 if it is not.

In the case of predicting who loves computer games, what I therefore get is the following:

where g_Frank=p_Frank, the original prediction of the model for Frank.

Note that the coefficients apply only to Frank; if I want to find how the model behaved for Bobby, I’ll need to find a new set of coefficients. In addition, since Bobby doesn’t have a job, I multiplied ϕ_Bobby Job by 0 (since there isn’t an x_Bobby Job). His simple model will therefore be

I’ll do this for all the data points and aggregate it to get an idea of how my model worked globally.

Now that I have this framework within which to interpret complex models, I need to think about exactly what properties I want ϕ to capture to be useful.

Shapley values (or, how can I calculate ϕ?)

The solution to finding the values of ϕ predates machine learning. In fact, it has its foundations in game theory.

Consider the following scenario: a group of people are playing a game. As a result of playing this game, they receive a certain reward; how can they divide this reward between themselves in a way which reflects each of their contributions?

There are a few things which everyone can agree on; meeting the following conditions will mean the game is ‘fair’ according to Shapley values:

The sum of what everyone receives should equal the total reward If two people contributed the same value, then they should receive the same amount from the reward Someone who contributed no value should receive nothing If the group plays two games, then an individual’s reward from both games should equal their reward from their first game plus their reward from the second game

These are fairly intuitive rules to have when dividing a reward, and they translate nicely to the machine learning problem we are trying to solve. In a machine learning problem, the reward is the final prediction of the complex model, and the participants in the game are features. Translating these rules into our previous notation:

g_Frank should be equal to p_Frank, the probability the complex model assigned to Frank of liking computer games.

2. If two features x contributed the same value to the final prediction, then their coefficients ϕ should have the same value

3. If a feature contributed nothing to the final prediction (or if it is missing), then its contribution to g should be 0

4. If I add up g_(Frank+Bobby) then this should be equal to g_Frank+g_Bobby

It’s worth noting that so far, our simple model by default respects rules 3 and 4.

It turns out that there is only one method of calculating ϕ so that it will also respect rules 1 and 2. Lloyd Shapley introduced this method in 1953 (which is why values of ϕ calculated in this way are known as Shapley values).

The Shapley value for a certain feature i (out of n total features), given a prediction p (this is the prediction by the complex model) is

There’s a bit to unpack here, but this is also much more intuitive than it looks. At a very high level, what this equation does is calculate what the prediction of the model would be without feature i, calculate the prediction of the model with feature i, and then calculate the difference:

This is intuitive; I can just add features and see how the model’s prediction changes as it sees new features. The change in the model’s prediction is essentially the effect of the feature.

However, the order in which you add features is important to how you assign their values. Let’s consider Bobby’s example to understand why; it’s the fact that he is both 14 and male that means he has a high chance of liking computer games. This means that whichever feature we add second will get a disproportionately high weighting, since the model will see that Bobby is a really likely candidate for liking computer games only when it has both pieces of information.

To better illustrate this, lets imagine that we are trying to assign feature values to the decision tree from the XGBoost documentation. Different implementations of decision trees have different ways of dealing with missing values, but for this toy example, lets say that if a value the tree splits on is missing, it calculates the average of the leaves below it.

As a reminder, here is the decision tree (with Bobby labelled):

First, we’ll see Bobby’s age, and then his gender.

When the model sees Bobby’s age, it will take him left on the first split. Then, since it doesn’t have a gender yet, it will assign him the average of the leaves below, or (2 + 0.1) / 2 = 1.05. So the effect of the age feature is 1.05.

Then, when the model learns he is male, it will give him a score of 2. The effect of the gender feature is therefore 2−1.05=0.95.

So in this scenario, ϕ_Age Bobby=1.05 and ϕ_Gender Bobby=0.95.

Next, lets say we see his gender, and then his age.

In the case where we only have a gender, the model doesn’t have an age to split on. It therefore has to take an average of all the leaves below the root.

First, the average of the depth 2 leaves: (2 + 0.1) / 2 = 1.05. This result is then averaged with the other depth 1 leaf: (1.05 + (-1)) / 2 = 0.025. So, the effect of the gender feature is 0.025.

Then, when the model learns he is 14, it gives him a score of 2. The effect of the age feature is then (2–0.025)=1.975.

So in this scenario, ϕ_Age Bobby=1.975 and ϕ_Gender Bobby=0.025.

Which value should we assign ϕ_Age Bobby? If we assign ϕ_Age Bobby a value of 1.975, does this mean we assign ϕ_Gender Bobby a value of 0.025 (since, by rule 1 of Shapley fairness, the total coefficients must equal the final prediction of the model for Bobby, in this case 2)?

This is far from ideal, since it ignores the first sequence, in which ϕ_Gender Bobby would get 0.95 and ϕ_Age Bobby would get 1.05.

What a Shapley value does is consider both values, calculating a weighted sum to find the final value. This is why the equation for ϕ_i(p) must permute over all possible sets of S of feature groupings (minus the feature i we are interested in). This is described in S⊆N/i below the summation, where N is all the features.

How are the weights assigned to each component of the sum? It basically considers how many different permutations of the sets exist, considering both the features which are in the set S (this is done by the ∣S∣!) as well as the features which have yet to be added (this is done by the (n−∣S∣−1)!. Finally, everything is normalized by the features we have in total.

Calculating a Shapley value

For Bobby, what would the Shapley value be for his age?

First, I need to construct my sets S. These are all possible combinations of Bobby’s features, excluding his age. Since he only has one other feature — his gender — this yields two sets: {x_Gender}, and an empty set {}.

Next, I need to calculate ϕ_i(p) or each of these sets, S. Note than as I have 2 features, n=2.

In the case where S = {}:

The prediction of the model when it sees no features is the average of all the leaves, which we have calculated to be 0.025. We’ve also calculated that when it sees only the age, it is 1.05, so

This yields

In the case where S = {x_Gender}:

We’ve calculated that the prediction of the model with only the gender is 0.025, and then when it sees both his age and his gender is 2, so

So

Adding these two values together yields

Note that this value makes sense; its right in the middle of what we calculated when we calculated feature importance just by adding features one by one.

In summary, Shapley values calculate the importance of a feature by comparing what a model predicts with and without the feature. However, since the order in which a model sees features can affect its predictions, this is done in every possible order, so that the features are fairly compared.

Shap values

Unfortunately, going through all possible combinations of features quickly becomes computationally unfeasible.

Luckily, the SHAP library introduces optimizations which allow Shapley values to be used in practice. It does this by developing model specific algorithms, which take advantage of different model’s structures. For instance, SHAP’s integration with gradient boosted decision trees takes advantage of the hierarchy in a decision tree’s features to calculate the SHAP values.

This allows the SHAP library to calculate Shapley values significantly faster than if a model prediction had to be calculated for every possible combination of features.

Conclusion

Shapley values, and the SHAP library, are powerful tools to uncovering the patterns a machine learning algorithm has identified.

In particular, by considering the effects of features in individual datapoints, instead of on the whole dataset (and then aggregating the results), the interplay of combinations of features can be uncovered. This allows far more powerful insights to be generated than with global feature importance methods.

Sources

S. Lundberg, S Lee, A Unified Approach to Interpreting Model Predictions, 2017","['models', 'interpreting', 'feature', 'prediction', 'complex', 'model', 'age', 'shap', 'bobby', 'features', 'values', 'value', 'computer']","The ideal case would therefore be to have a complex model which I can also interpret.
What we’ve done here is take a complex model, which has learnt non-linear patterns in the data, and broken it down into lots of linear models which describe individual data points.
So, to sum up:Instead of trying to explain the whole complex model, I am just going to try and explain how the complex model behaved for one data point.
To better illustrate this, lets imagine that we are trying to assign feature values to the decision tree from the XGBoost documentation.
This allows the SHAP library to calculate Shapley values significantly faster than if a model prediction had to be calculated for every possible combination of features.",en,['Gabriel Tseng'],2018-06-21 00:49:06.996000+00:00,"{'Shap', 'Xgboost', 'Machine Learning', 'Data Science'}","{'https://miro.medium.com/max/60/1*hzAZXuIihO2t5fQ63_87gw@2x.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*4HNqEv64Kcwodip4.', 'https://miro.medium.com/fit/c/80/80/2*2-krp3v1vEvt3y4O5YE6Fw.jpeg', 'https://miro.medium.com/max/1900/1*geF_00XqR6uajh1-gFTLbw@2x.png', 'https://miro.medium.com/max/60/1*geF_00XqR6uajh1-gFTLbw@2x.png?q=20', 'https://miro.medium.com/max/60/1*JKAYttMODx8UasMPPX6i9g@2x.png?q=20', 'https://miro.medium.com/max/1996/1*hzAZXuIihO2t5fQ63_87gw@2x.png', 'https://miro.medium.com/max/60/1*sUn4qN-zcdkD55CGwtyxRA@2x.png?q=20', 'https://miro.medium.com/max/1804/1*Ts8kTUMdcLFDotRCWauASQ@2x.png', 'https://miro.medium.com/max/3000/1*GVzbGLWhEIUUrnT92QBzZA.png', 'https://miro.medium.com/max/60/1*O5nUAoV8tGkyjQsrNRMvVg@2x.png?q=20', 'https://miro.medium.com/max/60/1*6ARpT9VnIO8_ZWjp_eHWsg@2x.png?q=20', 'https://miro.medium.com/max/60/1*W6BDI-u_kzNGOVGscsGTsA@2x.png?q=20', 'https://miro.medium.com/max/60/1*p9QfGRiZIbaBC7gl8didqQ@2x.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*4HNqEv64Kcwodip4.', 'https://miro.medium.com/max/1656/1*JKAYttMODx8UasMPPX6i9g@2x.png', 'https://miro.medium.com/max/1460/1*sUn4qN-zcdkD55CGwtyxRA@2x.png', 'https://miro.medium.com/max/1824/1*TebQuJsPc7upto5dvURjSA.png', 'https://miro.medium.com/max/60/1*Ts8kTUMdcLFDotRCWauASQ@2x.png?q=20', 'https://miro.medium.com/max/1856/1*P80HZgIAXir21BTWz7Xi6g@2x.png', 'https://miro.medium.com/max/912/1*TebQuJsPc7upto5dvURjSA.png', 'https://miro.medium.com/max/1644/1*O5nUAoV8tGkyjQsrNRMvVg@2x.png', 'https://miro.medium.com/max/892/1*W6BDI-u_kzNGOVGscsGTsA@2x.png', 'https://miro.medium.com/fit/c/80/80/0*4HNqEv64Kcwodip4.', 'https://miro.medium.com/max/60/1*Z7Z7Ff7wbo8XV2hQELislw@2x.png?q=20', 'https://miro.medium.com/max/60/1*P80HZgIAXir21BTWz7Xi6g@2x.png?q=20', 'https://miro.medium.com/max/1756/1*Z7Z7Ff7wbo8XV2hQELislw@2x.png', 'https://miro.medium.com/fit/c/80/80/1*FKXx4OYUkHpNvcvTx4WA_w.jpeg', 'https://miro.medium.com/max/856/1*6ARpT9VnIO8_ZWjp_eHWsg@2x.png', 'https://miro.medium.com/max/60/1*GVzbGLWhEIUUrnT92QBzZA.png?q=20', 'https://miro.medium.com/max/60/1*TebQuJsPc7upto5dvURjSA.png?q=20', 'https://miro.medium.com/max/2864/1*S8mMBghBAMwdi4SSpP6YFA@2x.png', 'https://miro.medium.com/max/2844/1*p9QfGRiZIbaBC7gl8didqQ@2x.png', 'https://miro.medium.com/max/60/1*S8mMBghBAMwdi4SSpP6YFA@2x.png?q=20'}",2020-03-05 00:16:57.493274,1.092512845993042
https://medium.com/@techstreams/git-google-drive-simple-git-host-3a84db4fc1fd,Git + Google Drive = Simple Git Host,"Configure Google Drive for Personal Git Hosting

STEP 1: Download and install Google Drive for desktop and ensure Git is installed on your system.

STEP 2: Open a command-line interface and create a Git repository for your project.

STEP 3: From inside the project folder, create a ‘bare’ Git clone of the repository on Google Drive.

git clone --bare . PATH_TO_GOOGLE_DRIVE_SYNC_FOLDER/ANY_SUBFOLDER_PATH/PROJECT_NAME.git



Example: git clone --bare . ~/GoogleDrive/Git/addon.git

STEP 4: Configure a Git remote.

git remote add REMOTE_NAME PATH_TO_GOOGLE_DRIVE_SYNC_FOLDER/ANY_SUBFOLDER_PATH/PROJECT_NAME.git



Example: git remote add gdrive ~/GoogleDrive/Git/addon.git

STEP 5: Push/Pull project changes to/from the remote … changes will sync with Google Drive.

Example: git push gdrive master

OR","['googledrivegitaddongitstep', 'path_to_google_drive_sync_folderany_subfolder_pathproject_namegitexample', 'project', 'gdrive', 'simple', 'clone', 'drive', 'bare', 'repository', 'git', 'remote', 'host', 'google']","Configure Google Drive for Personal Git HostingSTEP 1: Download and install Google Drive for desktop and ensure Git is installed on your system.
STEP 3: From inside the project folder, create a ‘bare’ Git clone of the repository on Google Drive.
git clone --bare .
PATH_TO_GOOGLE_DRIVE_SYNC_FOLDER/ANY_SUBFOLDER_PATH/PROJECT_NAME.gitExample: git clone --bare .
git remote add REMOTE_NAME PATH_TO_GOOGLE_DRIVE_SYNC_FOLDER/ANY_SUBFOLDER_PATH/PROJECT_NAME.gitExample: git remote add gdrive ~/GoogleDrive/Git/addon.gitSTEP 5: Push/Pull project changes to/from the remote … changes will sync with Google Drive.",en,['Laura Taylor'],2017-10-18 16:20:01.939000+00:00,"{'Developer', 'Google Drive', 'Productivity', 'Workflow', 'Git'}","{'https://miro.medium.com/fit/c/80/80/0*vQVQ89qxBOkBf1Pk.', 'https://miro.medium.com/max/1145/1*5SzVp57oD621rFYd2yN8AA.png', 'https://miro.medium.com/fit/c/160/160/1*v_2I21U5byM9JYvnrgv84g.png', 'https://miro.medium.com/fit/c/80/80/2*ZlnF-63UkDdgRayEwRyYEA.png', 'https://miro.medium.com/fit/c/80/80/1*JZouvSesHZCsv-WsMFf9KA.jpeg', 'https://miro.medium.com/max/60/1*5SzVp57oD621rFYd2yN8AA.png?q=20', 'https://miro.medium.com/max/2290/1*5SzVp57oD621rFYd2yN8AA.png', 'https://miro.medium.com/fit/c/96/96/1*v_2I21U5byM9JYvnrgv84g.png'}",2020-03-05 00:16:58.335300,0.8410243988037109
https://medium.com/@mike.p.moritz/using-docker-compose-to-deploy-a-lightweight-python-rest-api-with-a-job-queue-37e6072a209b,Using Docker Compose to deploy a lightweight Python REST API with a job queue,"Docker is a great platform for deploying large scale distributed enterprise applications, and the same benefits can be applied for simple use cases. However, even at small scale there is often a need to coordinate multiple containers, which is where Docker Compose comes in.

A common example is standing up a REST API with a job queue backend to handle longer running tasks. For this demo Python project, two technologies will be chosen with an eye towards simplicity and speed.

FastAPI — Alternative to Flask, Django

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.

RQ — Alternative to Celery

RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. It should be integrated in your web stack easily.

REST API

We will start by building the base Docker container with the API functionality, and for the ASGI server we will use uvicorn.

Create a new project structure like so, with two empty files:

├── myproj_app

│ ├── api.py

│ └── Dockerfile

In api.py create a simple test endpoint:

from fastapi import FastAPI app = FastAPI()

def hello():

""""""Test endpoint"""""" @app .get('/hello')def hello():""""""Test endpoint"""""" return {'hello': 'world'}

Before building out the Docker container, run the application locally. The below commands assume you already have pyenv and pyenv-virtualenv installed.

pyenv virtualenv 3.6.5 myproj

pyenv activate myproj pip install fastapi, uvicorn

pip freeze > requirements.txt uvicorn api:app --host 0.0.0.0 --port 5057

You should now be able to navigate to http://localhost:5057/hello and see world in the response.

Dockerfile

In order to run a Docker container we first need to populate the Dockerfile. The FROM line defines the base image, which in this case will be Python 3.6 running on alpine (a slim Linux distribution from Docker).

We will create a new user myproj for the project and use the home directory to run the application.

Note that here the requirements are installed directly instead of using a virtualenv, since this simplifies some of the steps.

FROM python:3.6-alpine RUN adduser -D myproj WORKDIR /home/myproj COPY requirements.txt requirements.txt

RUN pip install -r requirements.txt COPY api.py ./ RUN chown -R myproj:myproj ./

USER myproj CMD uvicorn api:app --host 0.0.0.0 --port 5057

Assuming that you already have Docker installed, you can then run the following two commands to build the project (tagged latest ) and then run it. The -p 5057:5057 portion will map the port inside the container to your actual host. The --rm flag is added to automatically cleanup the file system after the container exits, but can removed for debugging (or other) purposes.

docker build -t myproj:latest .

docker run --name myproj -p 5057:5057 --rm myproj:latest

After running the above you should once again be able to navigate to http://localhost:5057/hello and see hello in the response.

Welcome to your first Docker app! Celebrate, you’ve earned it.

Task Queue

To round out the demo application we need to add a worker to process tasks, and the Redis backend the manage the queue.

First install the required additional libraries:

pip install rq, redis

pip freeze > requirements.txt

Next we will build out api.py .

The Redis connection is pretty standard, but note the odd hostname my_redis . We will come back to that later.

In the next line you setup the RQ queue, with a custom name my_queue . You will use this when you want to send a new task to your worker.

Create a new endpoint /groups/ that will accept a {group_name} . Note that the default status code is 201 since this will be generating a background task.

If you are accustomed to Flask, the Group class may seem unfamiliar. With FastAPI the POST body is expected to confirm to a model class (defined earlier in the file using the pydantic BaseModel).

Also different is how error codes are returned, which are handled through HTTPExceptions. The example below checks if the group name exists via a dummy list, and then returns a 404 if not.

We actually add the task by calling the enqueue method with the worker function that will receive the task, runTask , and the variables required.

from fastapi import FastAPI, HTTPException

from pydantic import BaseModel

from redis import Redis

from rq import Queue from worker import runTask app = FastAPI() redis_conn = Redis(host='my_redis', port=6379, db=0)

q = Queue('my_queue', connection=redis_conn) # Request body classes

class Group(BaseModel):

owner: str

description: str = None

def hello():

""""""Test endpoint"""""" @app .get('/hello')def hello():""""""Test endpoint"""""" return {'hello': 'world'}

def addTask(group_name: str, group: Group):

""""""

Adds tasks to worker queue.

Expects body as dictionary matching the Group class.

"""""" @app .post('/groups/{group_name}', status_code=201)def addTask(group_name: str, group: Group):""""""Adds tasks to worker queue.Expects body as dictionary matching the Group class."""""" if group_name not in ('group1', 'group2'):

raise HTTPException(

status_code=404, detail='Group not found'

) job = q.enqueue(

runTask,

group_name, group.owner, group.description

) return {'job': job}

To process the tasks we will create a new file worker.py . Very simply the runTask function simulates a long-running task with time.sleep() . For actual task handling this would be the core of the functionality.

import time def runTask(group_name, group_owner, group_description):

print('starting runTask') # in place of actual logging



time.sleep(5) # simulate long running task print('finished runTask') return {group_name: 'task complete'}

Your project structure should now look like the below. Note the addition of __init__.py so we can import the worker function into api.py . We also slipped in a dockerignore file which you should populate before deploying.

├── myproj_app

│ ├── api.py

│ └── worker.py

│ └── __init__.py

│ └── requirements.txt

│ └── Dockerfile

│ └── .dockerignore

The key now is that the worker is its own application and should run in its own container. To fully leverage Docker we should also run Redis in a container instead of own the host itself. Docker Compose gives us an easy way to accomplish this, and allows us to stand-up all of these containers on one host. This structure be used for testing purposes or even deploying lightweight apps.

Docker Compose

From https://docs.docker.com/compose/overview/:

Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. Compose works in all environments: production, staging, development, testing, as well as CI workflows.

To get started we will create a new configuration file docker-compose.yml .

The version field defines the version of the docker-compose configuration file you will use, and the services section is where the containers are defined. In general you can view each of the service sections as a YAML representation of what would be in the Dockerfile and docker run command.

The first service is an out-of-the-box Redis container running on port 6379. There is also a volume added so that data is persisted, and this will create a new redis folder wherever you startup the services from.

The next two sections use the same image from our project to stand-up the API and the RQ queue as defined by command . The depends_on section aligns startup order and the links section allows for cross-container communication. As you will remember from the redis_conn variable, this allows us to refer to the Redis host by its service name myproj_redis .

version: '3' services:

myproj_redis:

image: redis:4.0.6-alpine

ports:

- ""6379:6379""

volumes:

- ./redis:/data myproj_api:

image: myproj:latest

command: uvicorn api:app --host 0.0.0.0 --port 5057

ports:

- ""5057:5057""

depends_on:

- myproj_redis

links:

- myproj_redis myproj_worker:

image: myproj:latest

command: rq worker --url redis://myproj_redis:6379 my_queue

depends_on:

- myproj_redis

links:

- myproj_redis

Your project structure should now look like:

├── myproj_app

│ ├── api.py

│ └── worker.py

│ └── __init__.py

│ └── Dockerfile

│ └── .dockerignore

│ └── docker-compose.yml

Before running Docker Compose, re-build the project. Note that you can also incorporate this step in the docker-compose file.

The next command will deploy all of the containers, including downloading any images that are not already available locally. The -f flag is not actually required since we are using the filename, but is shown here in case you would like to create different configurations for different deployment options.

docker build -t myproj:latest .

docker-compose -f docker-compose.yml up

You should see that the service is up by navigating to http://localhost:5057/hello

And you can send a new task to the queue with the following curl command:

Since we ran compose in the foreground you will see logs from all of the different services, and the progression of the task receiving and processing.

That’s it!

There is obviously a lot more you can do, and many things you would want to clean-up before deploying for consumption, but hopefully this can get you started.","['deploy', 'job', 'host', 'worker', 'redis', 'import', 'docker', 'python', 'queue', 'compose', 'rest', 'api', 'running', 'using', 'task', 'project', 'container', 'lightweight', 'run']","REST APIWe will start by building the base Docker container with the API functionality, and for the ASGI server we will use uvicorn.
DockerfileIn order to run a Docker container we first need to populate the Dockerfile.
In general you can view each of the service sections as a YAML representation of what would be in the Dockerfile and docker run command.
The first service is an out-of-the-box Redis container running on port 6379.
As you will remember from the redis_conn variable, this allows us to refer to the Redis host by its service name myproj_redis .",en,['Mike Moritz'],2019-09-17 17:53:40.778000+00:00,"{'Apps', 'Python', 'Docker Compose', 'Software Engineering', 'API', 'Docker'}","{'https://miro.medium.com/max/9792/0*1rtdYR-mtigr5Zcx', 'https://miro.medium.com/fit/c/80/80/1*cOIZquRu_m8TQgDF1clYVw.jpeg', 'https://miro.medium.com/max/8000/0*GaACYQSUtFjIrvuZ', 'https://miro.medium.com/fit/c/80/80/2*1to1GyfcsA9EBjt0u0fpjg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*cOIZquRu_m8TQgDF1clYVw.jpeg', 'https://miro.medium.com/max/1200/0*1rtdYR-mtigr5Zcx', 'https://miro.medium.com/max/60/0*1rtdYR-mtigr5Zcx?q=20', 'https://miro.medium.com/fit/c/96/96/1*cOIZquRu_m8TQgDF1clYVw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*_z9UPkFFq_NxVxmubiUwtg.jpeg', 'https://miro.medium.com/max/40/0*GaACYQSUtFjIrvuZ?q=20'}",2020-03-05 00:16:59.771847,1.436546802520752
https://medium.com/google-cloud/node-python-and-go-microservices-in-a-single-google-app-engine-project-54aac3d60225,"Node, Python and Go microservices in a single Google App Engine project!","Node Microservice

Node microservice uses Express to serve HTTP requests.

Step 2.1: Change the directory to gae-node-python-go

ode . We will execute all commands for Node microservice from this directory.

Step 2.2: Create hello.js file and punch in below code. Note the code from lines 11 to 14. The code has the server listen to the port specified by the “process.env.PORT” variable. This is an environment variable set by the GAE runtime and if your server does not listen to this port, it will not be able to receive requests. It’s value is always set to “8080” by the GAE runtime.

Step 2.3: Run npm init and punch in default values to create package.json file.

Step 2.4: Run npm install --save express to install Express .

Step 2.5: We need to specify the start script, the Node version and the NPM version to GAE in package.json file like below.

""scripts"": {

""start"": ""node hello.js""

} ""engines"": {

""node"": "">=8.11.2"",

""npm"": ""5.x""

}

In above code, we have instructed GAE to:

Run node hello.js upon start.

upon start. Use any Node release greater than or equal to 8.11.2 (latest Node.js LTS release as of 6th June 2018).

(latest Node.js LTS release as of 6th June 2018). Use the latest NPM 5 release.

Your final package.json file should look similar to below.

Step 2.6: Create node-app.yaml file and punch in below code. The runtime param lets GAE know that we want to use the Node runtime, the env param lets GAE know that we want to use the flexible environment and the service param lets GAE know the name of the service, default in this case. The service param is optional for the default service but we have specified it anyway.

Furthermore, to reduce costs during development, we have specified the use of bare minimum computing resources possible on GAE. But you would obviously want to increase their values and look into auto scaling for production workloads.

Step 2.7: Run gcloud app deploy node-app.yaml to deploy Node microservice. This should take a few minutes.","['file', 'start', 'specified', 'runtime', 'single', 'project', 'param', 'python', 'npm', 'engine', 'gae', 'node', 'service', 'google', 'microservices', 'app', 'code']","This is an environment variable set by the GAE runtime and if your server does not listen to this port, it will not be able to receive requests.
It’s value is always set to “8080” by the GAE runtime.
Step 2.5: We need to specify the start script, the Node version and the NPM version to GAE in package.json file like below.
""scripts"": {""start"": ""node hello.js""} ""engines"": {""node"": "">=8.11.2"",""npm"": ""5.x""}In above code, we have instructed GAE to:Run node hello.js upon start.
The service param is optional for the default service but we have specified it anyway.",en,['Raj Chaudhary'],2018-06-16 11:26:23.560000+00:00,"{'Google Cloud Platform', 'Google App Engine', 'Nodejs', 'Python', 'Golang'}","{'https://miro.medium.com/max/5760/1*UR1wfeu_QSoGyig7RulYrQ.png', 'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/5760/1*4TJmJQmEmui7y3cKRlhjWA.png', 'https://miro.medium.com/max/5760/1*zdz5Od6cW1hf1lZJr0vb5w.png', 'https://miro.medium.com/max/1200/1*4TJmJQmEmui7y3cKRlhjWA.png', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/60/1*5t6B11nnnOlgQowVRd3g1w.png?q=20', 'https://miro.medium.com/max/60/1*4TJmJQmEmui7y3cKRlhjWA.png?q=20', 'https://miro.medium.com/max/5760/1*5t6B11nnnOlgQowVRd3g1w.png', 'https://miro.medium.com/fit/c/160/160/1*dmbNkD5D-u45r44go_cf0g.png', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/max/60/1*UR1wfeu_QSoGyig7RulYrQ.png?q=20', 'https://miro.medium.com/max/60/1*zdz5Od6cW1hf1lZJr0vb5w.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*dmbNkD5D-u45r44go_cf0g.png', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png'}",2020-03-05 00:17:03.085292,3.312519073486328
https://medium.com/@dwdraju/simplifying-docker-in-dev-using-network-in-docker-compose-e13edaf28de0,Simplifying Docker in Microservice Dev. using Network in Docker Compose,"It might be handy to use docker in some random independent application but when we need to connect multiple docker containers and they are inter-related, that needs a well designed approach.

C ontext

At Pagevamp, we have many application services, some of them are api, builder, pagevamp, app. Common stateful services the services use are mysql, mongo, redis. We preferred docker compose file for each application services so that it will be more manageable and independent instead of one file for all. And yes, the stateful services: database and rest also on docker.

Trying links and depends_on

A simple docker-compose file with — links :

ERROR: Service ‘api’ has a link to service ‘mysql’ which is undefined.

So, we need to create each service which it depends prior to docker-compose up for api . You might be thinking --links is deprecated, let’s try depends_on but NOOO !!!

ERROR: Service 'api' depends on service 'db' which is undefined.

Well, these both expect order in which we need to start docker containers. If the dependency is only on the stateful services, that might be fine. Just start the stateful containers and then other. But when we have many services and need to discover services by one another, there should be a way to figure this out. Simplifying what I am trying to say, in our case pagevamp should discover api and its not feasible to take the ip address of api each time but we need certain stable naming for each service and discoverable using the same. Why wait to start two services when you need one?

Here Comes Docker Network

Docker Network is here to make you feel height of flexibility. One stop answer:

Keep all services on one network

But how?

Let’s create a docker network first

docker network create pagevamp

Docker compose file for api:

Similar for another service:

And for stateful services:

Now, each service on the network is discoverable from each other using the container name. For eg. pagevamp.pv can discover api using url api.pagevamp.pv and can connect to database using host name mysql and mongo .

Bonus: Nginx Proxy

When you start a application service, there has to be a way to be accessible from browser.

Hard way:

Find docker container name: docker ps Get IP address of the container: docker inspect [container name] | grep IPAddress Go to the IP address

Easy way:

In the above example, there is environment variable VIRTUAL_HOST in docker-compose.yml file with easy host name for each. Here comes jwilder/nginx-proxy which eases managing virtual hosts. We just need to add the host names on /etc/hosts file in the format:

127.0.0.1 pagevamp.pv

127.0.0.1 api.pagevamp.pv

Now, we need not to play with ip address and each services on the network can be discovered using respective name.

Do you have even more efficient solution? Feel free to share.","['file', 'network', 'services', 'start', 'stateful', 'docker', 'need', 'dev', 'api', 'service', 'compose', 'simplifying', 'microservice', 'using']","Common stateful services the services use are mysql, mongo, redis.
Trying links and depends_onA simple docker-compose file with — links :ERROR: Service ‘api’ has a link to service ‘mysql’ which is undefined.
ERROR: Service 'api' depends on service 'db' which is undefined.
Let’s create a docker network firstdocker network create pagevampDocker compose file for api:Similar for another service:And for stateful services:Now, each service on the network is discoverable from each other using the container name.
pagevamp.pv can discover api using url api.pagevamp.pv and can connect to database using host name mysql and mongo .",en,['Raju Dawadi'],2018-10-11 04:19:44.042000+00:00,"{'Docker Network', 'Docker Compose', 'Docker'}","{'https://miro.medium.com/fit/c/96/96/1*r7hrULSjglXP2M63J8Txug.png', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/160/160/1*r7hrULSjglXP2M63J8Txug.png', 'https://miro.medium.com/fit/c/80/80/1*FnGEmZk3Ny4uV7HAFVOulg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*Pcw-PTvfNRJzUOjFepPtfw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*r7hrULSjglXP2M63J8Txug.png'}",2020-03-05 00:17:04.215261,1.1299693584442139
https://medium.com/@ericsalesdeandrade/how-to-call-rest-apis-with-pandas-and-store-the-results-in-redshift-2b35f40aa98f,How to use APIs with Pandas and store the results in Redshift,"Here is an easy tutorial to help understand how you can use Pandas to get data from a RESTFUL API and store into a database in AWS Redshift.

Some basic understanding of Python (with Requests, Pandas and JSON libraries), REST APIs, Jupyter Notebook, AWS S3 and Redshift would be useful.

The goal of the tutorial is to use the geographical coordinates (longitude and latitude) provided in a CSV file to call an external API and reverse geocode the coordinates (i.e. get location details) and lastly store the response data in a Redshift table. Please note you may require caching approval from the API data supplier.

We will load the CSV with Pandas, use the Requests library to call the API, store the response into a Pandas Series and then a CSV, upload it to a S3 Bucket and copy the final data into a Redshift Table.The steps mentioned above are by no means the only way to approach this and the task can be performed by many different ways.

The dataset we will use contains country population as measured by the World Bank in 2013 and can be found on the below website.

Let’s begin.

Step 1 — Download the dataset

Links to column explanation and dataset

https://developer.here.com/documentation/geovisualization/topics/sample-datasets.html

http://js.cit.datalens.api.here.com/datasets/starter_pack/Global_country_populations_2013.csv

The first thing is to download the CSV file from the above website.

Step 2 — Start Jupyter Notebook and load the dataset into memory with Python

Install Jupyter Notebook (with Anaconda or otherwise) and fire it up by typing the following commands in the terminal while in the directory you wish to save the notebook.

Launch Jupyter Notebook

Assuming you have installed the required libraries let’s load the CSV file into memory.

import pandas as pd

import requests

import json

import time

from pandas.io.json import json_normalize



df = pd.read_csv('Global_country_populations_2013.csv')

df = df[['CountryName','lat','lon']]

df.head()

Load data into Pandas DataFrame

We have also imported other libraries as we will need them later on. The ‘read_csv()’ method will read your CSV file into a Pandas DataFrame. Please note you need to specify the path to file here if its not stored in the same directory. We then truncate the DataFrame to keep only the columns we need mainly [“CountryName”, “lat”, “lon”].

Step 3 — Define function to call API

The API we will use for the reverse geocoding is LocationIQ (from Unwired Labs) which offers free non commercial use with a rate of 60 calls/min and 10,000 calls per day.



try:

YOUR_API_KEY = 'xxxxxxxxxxx'

url = '



response = (requests.get(url).text)

response_json = json.loads(response)

time.sleep(0.5)

return response_json



except Exception as e:

raise e def get_reverse_geocode_data(row):try:YOUR_API_KEY = 'xxxxxxxxxxx'url = ' https://eu1.locationiq.org/v1/reverse.php?key=' + YOUR_API_KEY + '&lat=' + str(row['lat']) + '&lon=' + str(row['lon']) + '&format=json'response = (requests.get(url).text)response_json = json.loads(response)time.sleep(0.5)return response_jsonexcept Exception as e:raise e

In the above code we have defined a function — get_reverse_geocode_data(row). Once you sign up, you will get an API key which you need to include here along with the endpoint or URL and required parameters which you can obtain from the documentation. The parameter ‘row’ refers to each row of columns ‘lat’ and ‘lon’ of the Pandas DataFrame that will be passed as input to the API. The Requests library is used to make a HTTPS GET request to the url and receive the response using the ‘.text’ method.

You can use ‘json.loads()’ to convert the response from a JSON string into a JSON object which is easy to work with. The ‘time.sleep(0.5)’ parameter is used to control the calls to the API due to limitations set by the free tier plan (60 calls per min). For commercial high volume plans this won’t be necessary.

Step 4 — Call the function using the DataFrame Columns as parameters

df['API_response'] = df.apply(get_reverse_geocode_data,axis=1)

df['API_response'].head()

Call the function with DataFrame Columns as input

You can use the ‘df.apply()’ method to apply the function to the entire DataFrame. The ‘axis=1’ parameter is used to apply the function across the columns. The ‘row’ parameter we used earlier enables us to reference any column of the DataFrame we wish to use as input and takes the column value at that row as input for that particular execution.

Step 5 — Normalise or Flatten the JSON response

Now that you have successfully received the JSON response from the API, its time to flatten it into columns and pick out the fields you wish to keep.

new_df = json_normalize(df['API_response'])

new_df = new_df[['lat','lon','display_name']]

new_df

The ‘json_normalize()’ function is great for this. You can pass in the Pandas Series you wish to normalize as argument and it returns a new Pandas DataFrame with the columns flattened. You can then join this new DataFrame to your old one by using the Foreign Key or in this case we will only use the new DataFrame. Let’s only keep the ‘display_name’ field (from the API_response) along with ‘lat’ and ‘lon’. Below is the new DataFrame

Normalize the JSON to get new DataFrame

Let’s also add a Unique Identifier for each row

import uuid

new_df['id'] = pd.Series([uuid.uuid1() for i in range(len(new_df))])

new_df

Adding Unique Identifier

Step 6 — Generate CSV file and Upload to S3 Bucket

The following code creates a CSV file from Pandas DataFrame into the specified directory.

new_df.to_csv(path_or_buf=file_name,index=False)

The ‘to_csv’ method from Pandas automatically creates an Index column so we can avoid that by setting ‘index=False’.The CSV is now created and we can upload it to S3.

In this tutorial we will use TinyS3 <https://github.com/smore-inc/tinys3> which is a very easy to use library for working with S3. You can also use Boto3 if you wish.

import tinys3

import os

access_key = 'xxxxxxxxx'

secret_key = 'xxxxxxxxx'

endpoint = 'xxxxxxxx'

Bucket_name = 'xxxxxxxx' conn = tinys3.Connection(access_key, secret_key, tls=False, endpoint) f = open(file_name,'rb') conn.upload(file_name, f, Bucket_name) f.close()

os.remove(file_name)

The documentation is very self explanatory and basically says to add your AWS access key, secret access key and bucket name. You can then create a connection to S3 and upload the relevant file. We then delete the file from the drive by using ‘os.remove(file_name)’.

Step 7— Create Redshift Table and Copy Data into it

You will need to create a Redshift table before you can copy data into it. This can be done with standard SQL commands for PostgreSQL databases executed using Psycopg2 which is a PostgreSQL library for Python.

Create Table

import psycopg2 my_db = 'xxxxxxx'

my_host = 'xxxxxxx'

my_port = 'xxxx'

my_user = 'xxxxxxxx'

my_password = 'xxxxxxx' con = psycopg2.connect(dbname=my_db,host=my_host,port=my_port,user=my_user,password=my_password) cur = con.cursor() sql_query = ""CREATE TABLE reverse_geocode_location (lat varchar(255),lon varchar(255),display_name varchar(255),id varchar(255),PRIMARY KEY (id));"" cur.execute(sql_query)

con.commit() cur.close()

con.close()

Using Psycopg2 its very easy to execute SQL commands in Redshift or any other PostgreSQL engine database via Python. We first need to create a connection, then a cursor and lastly execute our SQL query. Don’t forget to close the connection to the database after the SQL query has been successfully executed.

Copy Data From S3 to Redshift

We are now ready for the last and final step in our Tutorial — Copy CSV file from S3 to Redshift. The reason we use COPY instead of using SQL Alchemy or other SQL clients because Redshift is optimised for columnar storage and this method is really fast to load data into it instead of loading the data row by row. We can use Psycopg2 once again for this.

Its very important to note that the column datatypes between the CSV file and Redshift table has to be the same and in the same order or the COPY command will fail. You can check any LOAD errors by reading from the STL_LOAD_ERRORS table.

import psycopg2 my_db = 'xxxxxxx'

my_host = 'xxxxxxx'

my_port = 'xxxx'

my_user = 'xxxxxxxx'

my_password = 'xxxxxxx' con = psycopg2.connect(dbname=my_db,host=my_host,port=my_port,user=my_user,password=my_password) cur = con.cursor() sql_query = """"copy reverse_geocode_location from 's3://YOUR_BUCKET_NAME/YOUR_FILE_NAME' credentials 'aws_access_key_id=YOUR_ACCESS_KEY;aws_secret_access_key=YOUR_SECRET_ACCESS_KEY' csv IGNOREHEADER 1 NULL 'NaN' ACCEPTINVCHARS;"""" cur.execute(sql_query)

con.commit() cur.close()

con.close()

In the COPY command above we need to specify the bucket name, file name, security keys and a few flags. An explanation of the flags used can be found here:

Please Note: You need to grant correct IAM Role permissions in order to copy data from S3 into Redshift.

Step 8— Read data from your table to verify

Once you have successfully followed the above steps, you should now have the data copied into your Redshift table. You can verify it by reading the data using Psycopg2 in Python or any other SQL client.

select * from reverse_geocode_location

Conclusion and Next Steps

This tutorial covers some basics about using a Pandas Series as input to call a REST API and store the result in AWS Redshift. However if the data is scaled considerably its important to:

Subscribe to high performance, high volume handling API. Reduce the in memory processing by writing to disk repeatedly or carry out parallel processing using libraries like Dask. Use Apache Spark or other similar technologies to handle very large data processing. Choose the database technology correctly based on performance requirement.

I hope this tutorial has been somewhat helpful, if you have any questions please get in touch via the comments.","['file', 'csv', 'redshift', 'pandas', 'copy', 'store', 'data', 'api', 's3', 'results', 'dataframe', 'apis', 'using']","Some basic understanding of Python (with Requests, Pandas and JSON libraries), REST APIs, Jupyter Notebook, AWS S3 and Redshift would be useful.
The ‘read_csv()’ method will read your CSV file into a Pandas DataFrame.
You can pass in the Pandas Series you wish to normalize as argument and it returns a new Pandas DataFrame with the columns flattened.
Step 7— Create Redshift Table and Copy Data into itYou will need to create a Redshift table before you can copy data into it.
Copy Data From S3 to RedshiftWe are now ready for the last and final step in our Tutorial — Copy CSV file from S3 to Redshift.",en,['Eric Sales De Andrade'],2019-08-15 09:36:48.342000+00:00,"{'AWS', 'Pandas', 'Python', 'Redshift', 'Programming', 'API'}","{'https://miro.medium.com/max/2892/1*TivJ4WxucRupgMUOeO9HMw.png', 'https://miro.medium.com/max/60/1*zk_yZlmGH1s4pVVG6rI2ZA.png?q=20', 'https://miro.medium.com/max/60/1*-Rlkdm4hil51MZ7jWUuyCg.png?q=20', 'https://miro.medium.com/max/2424/1*zk_yZlmGH1s4pVVG6rI2ZA.png', 'https://miro.medium.com/max/60/1*ckA1BMurM47OfC7qhGK3Fw.png?q=20', 'https://miro.medium.com/max/3800/1*ckA1BMurM47OfC7qhGK3Fw.png', 'https://miro.medium.com/max/60/1*4AMxjuhLf5pQdYmKZ9t5FQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*wOr5vZOyM4LGVsMIsm994g.jpeg', 'https://miro.medium.com/max/2280/1*vqJjKkJAGhWZ45GiUvySUw.png', 'https://miro.medium.com/max/1140/1*vqJjKkJAGhWZ45GiUvySUw.png', 'https://miro.medium.com/fit/c/80/80/1*4-DIjB5ZOAVpLPbCXVVO1Q.jpeg', 'https://miro.medium.com/max/2336/1*-Rlkdm4hil51MZ7jWUuyCg.png', 'https://miro.medium.com/fit/c/80/80/1*pAY7sM2cgqkkgcmLDSAHWw.jpeg', 'https://miro.medium.com/max/2992/1*4AMxjuhLf5pQdYmKZ9t5FQ.png', 'https://miro.medium.com/fit/c/80/80/0*glKtlTuDQ3jIl2Nz.jpg', 'https://miro.medium.com/max/60/1*TivJ4WxucRupgMUOeO9HMw.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*wOr5vZOyM4LGVsMIsm994g.jpeg', 'https://miro.medium.com/max/60/1*vqJjKkJAGhWZ45GiUvySUw.png?q=20'}",2020-03-05 00:17:05.972221,1.7559661865234375
https://towardsdatascience.com/how-to-setup-your-jupyterlab-project-environment-74909dade29b,How to Setup Your JupyterLab Project Environment,"Create and Customize Your Containerized and Script-controlled JupyterLab Project Environment in a minute.

TL;DR:

The JupyterLab-Configurator lets you easily create your JupyterLab configuration that runs JupyterLab in a container and automates the whole setup using scripts. A container is a separated environment that encapsulates the libraries you install in it without affecting your host computer. Scripts automate executing all the commands you would normally need to run manually. For you can review and edit scripts, you get full control of your configuration at any time.

In this post, you’ll see how this JupyterLab configuration works and how you can customize it to cater to your needs.

I am a Data Scientist, not a DevOps Engineer

Dammit Jim. I’m a Data Scientist, not a DevOps Engineer

The list of requirements of a Data Scientist is very long. It contains math and statistics, programming and databases, communication and visualization, domain knowledge, and many more.

”Please, don’t add DevOps to the list,” you think? Ok! How do these processes sound to you?

Create your JupyterLab configuration:

The JupyterLab-Configurator lets you easily create your custom configuration Download and unzip your configuration Customize it to your needs (optional)

The following picture shows the JupyterLab configuration in action. Use it with two simple steps:

Execute sh {path_to_your_project}/run.sh Open localhost:8888 in a browser

Using the JupyterLab configuration

And what if I am interested in how this configuration works? How much time does it take to explain it?

Certainly, some hours, Sir. But ya don’t have some hours, so I’ll do it for ya in a few minutes.

The remainder of this post gives you an overview of how this JupyterLab configuration works, conceptually. It explains the building blocks and enables you to customize the configuration to your needs, e.g.

add software packages

add your own Python modules

customize the Jupyter notebook server

Why do I need a JupyterLab-Configuration anyway?

In 2018, Project Jupyter launched JupyterLab — an interactive development environment for working with notebooks, code, and data. JupyterLab has full support for Jupyter notebooks and enables you to use text editors, terminals, data file viewers, and other custom components side by side with notebooks in a tabbed work area.

Provided you run a Unix-based operating system (MacOs or Linux), you can install and start JupyterLab with two simple commands:

python -m pip install jupyterlab

jupyter lab

But wait! As simple as the manual setup of JupyterLab may look at first sight as likely it is to not cater to all the things you need to do in your data science project. You may also need:

Jupyter-kernels (e.g. bash, Javascript, R, …)

File converters (e.g. Pandoc, Markdown, …)

Libraries (e.g. NumPy, SciPy, TensorFlow, PyTorch, …)

Supporting software (Git, NbSphinx, …)

Installing these dependencies directly on your computer is not a good idea because you would have a hard time ensuring to keep your computer clean.

What if you had different projects that require different versions of a library? Would you uninstall the old version and install the correct version everytime you switch between the projects?

What if you do not need a library anymore? Would you remove it right away and reinstall it, if you discover that you need it after all? Or would you wait until you forgot removing this library at all?

Installing these dependencies manually is not a good idea, either. You would have no control over all the things you installed.

What if you wanted to work on this project on another computer? How much time and work would it require you to set up the project again?

What if someone asked you for all the third-party-libraries you are using? Among all the libraries you installed on your host computer, how would you identify those you are using in this project?

A Containerized Configuration

A container is a virtual environment that is separated from the host computer. It creates its own runtime environment that can adapt to your specific project needs. It interacts with its host only via specified ways. Any change of the container does not affect your host computer or vice versa. Docker is one of the most prominent and widely used platforms for virtualization of project environments.

The following picture depicts the Docker process that contains two steps: (1) build an image from the Dockerfile and (2) run the image in a container.

The Docker process

Our configuration automates this process in the run.sh -script. This is a shell script ( sh or bash ) that runs on the host computer. Likewise, this script is your entry point to start your JupyterLab project. Simply open a terminal and run:

sh {path_to_your_project}/run.sh

The Dockerfile is the script that tells Docker how to configure the system within the container. During the docker build -step, Docker creates an image of this system. An image is an executable package that includes everything needed to run an application — the code, a runtime environment, libraries, environment variables, and configuration files.

While Docker supports building up systems from the scratch, it is best practice to start from an existing image, e.g. an image containing an operating system or even a full configuration.

The configuration starts from an existing image. You can find the corresponding Dockerfile in this GitHub-Repository. This image contains the following software and libraries:

Ubuntu 18.04

Python 3.7.0

Pip

Jupyter and JupyterLab

Bash and Jupyter Bash-Kernel

Document (pdf) tools ( pandoc , texlive-xetex )

, ) Build tools (e.g., build-essential , python3-setuptools , checkinstall )

, , ) Communication tools ( openssl , wget , requests , curl )

, , , ) Various Python development libraries

If you require further software libraries, the Dockerfile is your place to go. Just add a new line after the FROM statement. This new line needs to start with RUN and contains any shell command you may want to execute, usually something like apt-get install or pip install . For example, you can use pip to install some major data science packages with the following statements:

RUN pip install numpy

RUN pip install scipy

RUN pip install pandas

Changes in the Dockerfile become effective during the build -step. If you already started the container, you’ll need to stop it (e.g. use ctrl+c in your terminal) and restart it ( sh {path_to_your_project}/run.sh ). When you edited your Dockerfile, the build -step may take some time. For Docker tries to reuse existing images, it is very fast in subsequent starts when you did not change anything.

If you remove commands from your Dockerfile and rerun the run.sh -script, Docker creates a new image of the system. You do not need to uninstall anything from the system. Because the removed command has never been part of this resulting system. This keeps your configuration clean at all times. You can experimentally install libraries without worrying. If you don’t need them, just remove them. You will get a system image that never installed them in the first place.

The following image depicts how the Dockerfile configures the system: it installs the software as specified in its RUN -commands.

The Dockerfile specifies the configuration of the system

The docker run -command executes this image in a container. Further, it defines how the system running within the container connects to the outside world, i.e. the host computer.

There are two main types of connections: volumes and ports. A volume is a link between a directory at the host computer and one in the container. These directories synchronize, i.e. any change in the host-directory will affect the directory in the container and vice versa. A port-mapping lets Docker forward any request (e.g. HTTP-requests) made to the host computer’s port to the mapped port of the container.

The following image depicts our configuration thus far. The run.sh -script takes care of the Docker build and run steps. Once you execute the script, it creates a running container that connects with your host computer via a file system volume and a port mapping.

The run.sh script automates the Docker process

File system

When you download the files from the Git-Hub-Repository, you will get the following file structure in the .zip file:

{path_to_your_project}/

├─ config/

│ ├─ {projectname}.Dockerfile

│ ├─ jupyter_notebook_configuration.py

│ └─ run_jupyter.sh

├─ libs/

│ └─ nbimport.py

├─ notebooks/

│ └─ …

└─ run.sh

The config -folder contains the configuration files of your JupyterLab project. These files configure the Docker-container, install the software packages, and configure the JupyterLab environment.

-folder contains the configuration files of your JupyterLab project. These files configure the Docker-container, install the software packages, and configure the JupyterLab environment. The libs -folder contains the software libraries that are not installed as packages but that you add as files, e.g. Python-modules that you wrote yourself in other projects.

-folder contains the software libraries that are not installed as packages but that you add as files, e.g. Python-modules that you wrote yourself in other projects. The notebooks -folder is the directory where we put the Jupyter-Notebooks.

In the Dockerfile, we set environment variables that point to these directories. For the scripts in the configuration uses these environment variables, you can edit them if you like. Just make sure that the path in the variable matches the actual path.

ENV MAIN_PATH=/usr/local/bin/{projectname}

ENV LIBS_PATH=${MAIN_PATH}/libs

ENV CONFIG_PATH=${MAIN_PATH}/config

ENV NOTEBOOK_PATH=${MAIN_PATH}/notebooks

In the configuration, we map the current working directory ( {path_to_your_project} ) to the ${MAIN_PATH} -folder in the container. So, any file you put into this directory is available in your JupyterLab project. Vice versa, any file you add or change within JupyterLab (e.g. Jupyter notebooks) will appear on your host computer.

Further, in the EXPOSE command of the Dockerfile, we specify that the configuration provides the JupyterLab port 8888 . This port inside the container is mapped to the port of your host computer.

The following image depicts how the container connects its file system and port to the host computer.

Connect the file system and the port

JupyterLab-specific Configuration

The final command in our Dockerfile is the CMD -command. It tells Docker that this instruction is something you want to execute whenever you start the container. In the configuration, we execute the run_jupyter.sh -script. This script allows us to do some last minute preparations, like:

put the jupyter_notebook_configuration.py file at the location where JupyterLab expects it

file at the location where JupyterLab expects it configure a custom Jupyter-Kernel that automatically loads the nbimport.py Python module

The jupyter_notebook_configuration.py lets you configure the Jupyter notebook server, e.g. setting a password to use for web authentication. A list of available options can be found here.

The custom Python kernel adds the ${LIBS_PATH} to your Python sys.path . This allows you to import any Python module from the ${LIBS_PATH} -folder, e.g. import libs.nbimport . This nbimport.py -module further enables you to import Jupyter-notebooks that are located in the ${NOTEBOOK_PATH} -folder. Whenever you start a Jupyter notebook with a Python kernel, the system does these things automatically for you.

Finally, the run_jupyter.sh -script starts JupyterLab. You can now open localhost:8888 in a browser, where 8888 is the port you specified.

The following image depicts the complete JupyterLab configuration.

The complete JupyterLab configuration

Summary

The JupyterLab-Configurator lets you easily create your custom configuration. This JupyterLab-Configuration runs JupyterLab in a container. It separates the environment JupyterLab runs in from the host environment. Thus, you can change the JupyterLab environment (e.g. un-/installing packages) without affecting your host computer or any other project.

This JupyterLab-Configuration automates the whole setup using scripts. These scripts:

Enable you running JupyterLab with a single command (e.g. sh run.sh )

(e.g. ) Make your project portable: just move or copy the directory to another host computer

Reveal what is part of your configuration and allow you to review and edit your configuration

Make your configuration part of your sources. You can version-control them like you can version-control your code

The GitHub-repository provides the whole source code.

Using the JupyterLab configuration is very easy:

Execute sh {path_to_your_project}/run.sh Open localhost:8888 in a browser

Using the JupyterLab configuration","['system', 'install', 'project', 'environment', 'configuration', 'docker', 'image', 'setup', 'jupyterlab', 'container', 'host']","Create and Customize Your Containerized and Script-controlled JupyterLab Project Environment in a minute.
TL;DR:The JupyterLab-Configurator lets you easily create your JupyterLab configuration that runs JupyterLab in a container and automates the whole setup using scripts.
So, any file you put into this directory is available in your JupyterLab project.
It separates the environment JupyterLab runs in from the host environment.
Using the JupyterLab configuration is very easy:Execute sh {path_to_your_project}/run.sh Open localhost:8888 in a browserUsing the JupyterLab configuration",en,['Frank Zickert'],2019-07-23 13:17:00.328000+00:00,"{'Jupyterlab', 'Jupyter', 'Script', 'Docker', 'DevOps'}","{'https://miro.medium.com/max/360/1*ApWGqL4Swec5Toyi-bL5CQ.png', 'https://miro.medium.com/max/720/1*ApWGqL4Swec5Toyi-bL5CQ.png', 'https://miro.medium.com/max/2016/1*4vuRtKkp_tZAYb5l7utLHA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*mzE5-eODSmx9IMMRVoR6cw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1686/1*2f3HOznZ7tkoy5w1C2RZHw.png', 'https://miro.medium.com/max/2484/1*DTmT3cd8zsXlXb3ckITOqQ.png', 'https://miro.medium.com/max/1506/1*JdkSX4HNn7UaKIfGfxE3KQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*4vuRtKkp_tZAYb5l7utLHA.png?q=20', 'https://miro.medium.com/max/1400/1*yQlphH3ujG3Pon_T7hk7Jg.jpeg', 'https://miro.medium.com/max/60/1*JdkSX4HNn7UaKIfGfxE3KQ.png?q=20', 'https://miro.medium.com/max/60/1*yQlphH3ujG3Pon_T7hk7Jg.jpeg?q=20', 'https://miro.medium.com/max/60/1*ApWGqL4Swec5Toyi-bL5CQ.png?q=20', 'https://miro.medium.com/max/60/1*DTmT3cd8zsXlXb3ckITOqQ.png?q=20', 'https://miro.medium.com/max/2890/1*oZCK6yMowP8TTRhSivhPqQ.png', 'https://miro.medium.com/fit/c/96/96/0*PLoI1E1voolwUPnB', 'https://miro.medium.com/fit/c/160/160/0*PLoI1E1voolwUPnB', 'https://miro.medium.com/max/1528/1*mzE5-eODSmx9IMMRVoR6cw.png', 'https://miro.medium.com/max/60/1*2f3HOznZ7tkoy5w1C2RZHw.png?q=20', 'https://miro.medium.com/max/60/1*Mq2ISzHP3W2a1_VeBXuatQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*oZCK6yMowP8TTRhSivhPqQ.png?q=20', 'https://miro.medium.com/max/1760/1*Mq2ISzHP3W2a1_VeBXuatQ.jpeg'}",2020-03-05 00:17:13.592443,7.620222568511963
https://towardsdatascience.com/using-what-if-tool-to-investigate-machine-learning-models-913c7d4118f,Using the ‘What-If Tool’ to investigate Machine Learning models.,"The left panel contains three tabs called Datapoint Editor, Performance & Fairness ; and Features.

1. Datapoint Editor Tab

The Datapoint Editor helps to perform data analysis through:

Viewing and Editing details of Datapoints

It allows diving into a selected data point which gets highlighted in yellow on the right panel. Let’s try changing the age from 53 to 58 and clicking the “Run inference” button to see what effect it has on the model’s performance.

By simply changing the age of this person, the model now predicts that the person belongs to high-income category. For this data point, earlier the inference score for the positive (high income) class was 0.473, and the score for negative (low income) class was 0.529. However, by changing the age, the positive class score became 0.503.

Finding Nearest Counterfactuals

Another way to understand the model’s behaviour is to look at what small set of changes can cause the model to flip its decision which is called counterfactuals. With one click we can see the most similar counterfactual, which is highlighted in green, to our selected data point. In the data point editor tab we now also see the feature values for the counterfactual next to the feature values for our original data point. The green text represents features where the two data points differ. WIT uses L1 and L2 distances to calculate the similarity between the data points.

In this case, the nearest counterfactual is slightly older and has a different occupation and capital gain, but is otherwise identical.

We can also see the similarity between the selected points and others using the “show similarity to selected datapoint” button. WIT measures the distance from the selected point to every other datapoint. Let’s change our X-axis scatter to show the L1 distance to the selected datapoint.

Analysing partial dependence plots

The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model( J. H. Friedman 2001).

The PDPs for age and Education for a data point are as follows:

The plot above shows that:

The model has learned a positive correlation between age and income

More advanced degrees give the model more confidence in higher income.

High capital gains is a very strong indicator of high income, much more than any other single feature.

2. Performance & Fairness Tab

This tab allows us to look at the overall model performance using confusion matrices and ROC curves.

Model Performance Analysis

To measure the model’s performance, we need to tell the tool what is the ground truth feature i.e the feature that the model is trying to predict which in this case is “Over-50K”.

We can see that at the default threshold level of 0.5, our model is incorrect about 15% of the time, with about 5% of the time being false positives and 10% of the time being false negatives. Change the threshold values to see its impact on the model’s accuracy.

There is also a setting for “cost ratio” and an “optimize threshold” button which can also be tweaked.

ML Fairness

Fairness in Machine Learning is as important as model building and predicting an outcome. Any bias in the training data will be reflected in the trained model and if such a model is deployed, the resultant outputs will also be biased. The WIT can help investigate fairness concerns in a few different ways. We can set an input feature (or set of features) with which to slice the data. For example, let’s see the effect of gender on model performance.

Effect of gender on Model’s performance

We can see that the model is more accurate on females than males. Also, the model predicts high income for females much less than it does for males (9.3% of the time for females vs 28.6% of the time for males). One probable reason might be due to the under-representation of females in the dataset which we shall explore in the next section.

Additionally, the tool can optimally set the decision threshold for the two subsets while taking into account any of a number of constraints related to algorithmic fairness such as demographic parity or equal opportunity.

3. Features Tab

The features tab gives the summary statistics of each of the features in the dataset including histograms, quantile charts, bar charts etc. The tab also enables to look into the distribution of values for each feature in the dataset. For instance, let us explore the sex, capital gain and race features.","['machine', 'investigate', 'models', 'feature', 'point', 'datapoint', 'learning', 'performance', 'model', 'whatif', 'data', 'selected', 'age', 'features', 'tool', 'using']","For this data point, earlier the inference score for the positive (high income) class was 0.473, and the score for negative (low income) class was 0.529.
With one click we can see the most similar counterfactual, which is highlighted in green, to our selected data point.
In the data point editor tab we now also see the feature values for the counterfactual next to the feature values for our original data point.
We can also see the similarity between the selected points and others using the “show similarity to selected datapoint” button.
Performance & Fairness TabThis tab allows us to look at the overall model performance using confusion matrices and ROC curves.",en,['Parul Pandey'],2019-05-03 13:40:31.603000+00:00,"{'Data Science', 'Artificial Intelligence', 'Google', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/60/0*bUExfjdJSB0BqCpt.png?q=20', 'https://miro.medium.com/max/1972/1*NO4eJz9J0GYn60W0UkpuhA.gif', 'https://miro.medium.com/max/1200/1*NBwHkeXoOu4fwA4dFdpyKw.jpeg', 'https://miro.medium.com/max/2966/1*BhlfFvlDiLC4FYyi_WTRZA.png', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*131qNOeuhboNCGVTh9y6sw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2008/1*H7S9oSQgPP7H56NSFXg_hg.png', 'https://miro.medium.com/max/60/1*Fpeb_UkmNv53Wo55nQ9O0A.png?q=20', 'https://miro.medium.com/max/2000/0*RgV_ffd8S28l2xuQ.png', 'https://miro.medium.com/max/60/1*lSHybyMux8FdsWlO7HasgA.png?q=20', 'https://miro.medium.com/max/1932/1*Go_5BeraltIgPnfaW6xA0g.gif', 'https://miro.medium.com/max/1448/1*_V4de4Q2lJAEnMWKI-maOQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1410/1*5FmxAvQhvgNASSwoco5NlA.png', 'https://miro.medium.com/freeze/max/60/1*NO4eJz9J0GYn60W0UkpuhA.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*Go_5BeraltIgPnfaW6xA0g.gif?q=20', 'https://miro.medium.com/max/60/1*pSN720U3hG54Zrkv5UpNag.png?q=20', 'https://miro.medium.com/max/54/1*QlLrTAdwfi1t9rwonhUu9A.png?q=20', 'https://miro.medium.com/max/60/1*BhlfFvlDiLC4FYyi_WTRZA.png?q=20', 'https://miro.medium.com/max/1356/1*dFWgN4zuEQz6e-qRuV_p3g.png', 'https://miro.medium.com/max/1614/1*QlLrTAdwfi1t9rwonhUu9A.png', 'https://miro.medium.com/max/60/1*H7S9oSQgPP7H56NSFXg_hg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Al4bw950-mIt4D_CWVIE5Q.png?q=20', 'https://miro.medium.com/max/2000/1*Al4bw950-mIt4D_CWVIE5Q.png', 'https://miro.medium.com/max/60/1*5FmxAvQhvgNASSwoco5NlA.png?q=20', 'https://miro.medium.com/max/5772/1*NBwHkeXoOu4fwA4dFdpyKw.jpeg', 'https://miro.medium.com/max/1664/1*34aQWjQZC_Q0gCG4_YNF_g.png', 'https://miro.medium.com/max/60/1*dFWgN4zuEQz6e-qRuV_p3g.png?q=20', 'https://miro.medium.com/max/582/1*lSHybyMux8FdsWlO7HasgA.png', 'https://miro.medium.com/max/60/1*_V4de4Q2lJAEnMWKI-maOQ.png?q=20', 'https://miro.medium.com/max/60/1*NBwHkeXoOu4fwA4dFdpyKw.jpeg?q=20', 'https://miro.medium.com/max/1724/1*pSN720U3hG54Zrkv5UpNag.png', 'https://miro.medium.com/max/2210/1*Fpeb_UkmNv53Wo55nQ9O0A.png', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/60/0*RgV_ffd8S28l2xuQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3210/1*131qNOeuhboNCGVTh9y6sw.png', 'https://miro.medium.com/max/1500/0*bUExfjdJSB0BqCpt.png', 'https://miro.medium.com/max/56/1*34aQWjQZC_Q0gCG4_YNF_g.png?q=20'}",2020-03-05 00:17:15.758212,2.16576886177063
https://towardsdatascience.com/why-git-and-git-lfs-is-not-enough-to-solve-the-machine-learning-reproducibility-crisis-f733b49e96e8,Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis,"Some claim the machine learning field is in a crisis due to software tooling that’s insufficient to ensure repeatable processes. The crisis is about difficulty in reproducing results such as machine learning models. The crisis could be solved with better software tools for machine learning practitioners.

The reproducibility issue is so important that the annual NeurIPS conference plans to make this a major topic of discussion at NeurIPS 2019. The “Call for Papers” announcement has more information https://medium.com/@NeurIPSConf/call-for-papers-689294418f43

The so-called crisis is because of the difficulty in replicating the work of co-workers or fellow scientists, threatening their ability to build on each other’s work or to share it with clients or to deploy production services. Since machine learning, and other forms of artificial intelligence software, are so widely used across both academic and corporate research, replicability or reproducibility is a critical problem.

We might think this can be solved with typical software engineering tools, since machine learning development is similar to regular software engineering. In both cases we generate some sort of compiled software asset for execution on computer hardware hoping to get accurate results. Why can’t we tap into the rich tradition of software tools, and best practices for software quality, to build repeatable processes for machine learning teams?

Unfortunately traditional software engineering tools do not fit well with the needs of machine learning researchers.

A key issue is the training data. Often this is a large amount of data, such as images, videos, or texts, that are fed into machine learning tools to train an ML model. Often the training data is not under any kind of source control mechanism, if only because systems like Git do not deal well with large data files, and source control management systems designed to generate delta’s for text files do not deal well with changes to large binary files. Any experienced software engineer will tell you that a team without source control will be in a state of barely managed chaos. Changes won’t always be recorded and team members might forget what was done.

At the end of the day that means a model trained against the training data cannot be replicated because the training data set will have changed in unknown-able ways. If there is no software system to remember the state of the data set on any given day, then what mechanism is there to remember what happened when?

Git-LFS is your solution, right?

The first response might be to simply use Git-LFS (Git Large File Storage) because it, as the name implies, deals with large files while building on Git. The pitch is that Git-LFS “replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise.” One can just imagine a harried machine learning team saying “sounds great, let’s go for it”. It handles multi-gigabyte files, speeds up checkout from remote repositories, and uses the same comfortable workflow. That sure ticks a lot of boxes, doesn’t it?

Not so fast, didn’t your manager instruct you to evaluate carefully before jumping in with both feet? Another life lesson to recall is to look both ways before crossing the street.

The first thing your evaluation should turn up is that Git-LFS requires an LFS server, and that server is not available through every Git hosting service. The big three (Github, Gitlab and Atlassian) all support Git-LFS, but maybe you have a DIY bone in your body. Instead of using a 3rd party Git hosting service, you might prefer to host your own Git service. Gogs, for example, is a competent Git service you can easily run on your own hardware, but it does not have built-in support for Git-LFS.

Depending on your data needs this next could be a killer: Git LFS lets you store files up to 2 GB in size. That is a Github limitation rather than Git-LFS limitation, however all Git-LFS implementations seem to come with various limitations. Gitlab and Atlassian both have their own lists of Git-LFS limitations. Consider this 2GB limit from Github: One of the use-cases in the Git-LFS pitch is storing video files, but isn’t it common for videos to be way beyond 2GB in size? Therefore GIt-LFS on Github is probably unsuitable for machine learning datasets.

It’s not just the 2GB file size limit, but Github places such a tight limit on the free tier of Git-LFS use that one must purchase a data plan covering both data and bandwidth usage.

An issue related to bandwidth is that when using a hosted Git-LFS solution, your training data is stored in a remote server and must be downloaded over the Internet. The time to download training data is a serious user experience problem.

Another issue is the ease of placing data files on a cloud storage system (AWS, GCP, etc) as is often required when to run cloud-based AI software. This is not supported, since the main Git-LFS offerings from the big 3 Git services store your LFS files on their server. There is a DIY Git-LFS server that does store files on AWS S3 at https://github.com/meltingice/git-lfs-s3 But setting up a custom Git-LFS server of course requires additional work. And, what if you need the files to be on GCP instead of AWS infrastructure? Is there a Git-LFS server which stores data on the cloud storage platform of your choice? Is there a Git-LFS server that utilizes a simple SSH server? In other words, GIt-LFS limits your choices of where the data is stored.

Does using Git-LFS solve the so-called Machine Learning Reproducibility Crisis?

With Git-LFS your team has better control over the data, because it is now version controlled. Does that mean the problem is solved?

Earlier we said the “key issue is the training data”, but that was a lie. Sort of. Yes keeping the data under version control is a big improvement. But is the lack of version control of the data files the entire problem? No.

What determines the results of training a model or other activities? The determining factors include the following, and perhaps more:

Training data — the image database or whatever data source is used in training the model

The scripts used in training the model

The libraries used by the training scripts

The scripts used in processing data

The libraries or other tools used in processing data

The operating system and CPU/GPU hardware

Production system code

Libraries used by production system code

Obviously the result of training a model depends on a variety of conditions. Since there are so many variables to this, it is hard to be precise, but the general problem is a lack of what’s now called Configuration Management. Software engineers have come to recognize the importance of being able to specify the precise system configuration used in deploying systems.

Solutions to machine learning reproducibility

Humans are an inventive lot, and there are many possible solutions to this “crisis”.

Environments like R Studio or Jupyter Notebook offer a kind of interactive Markdown document which can be configured to execute data science or machine learning workflows. This is useful for documenting machine learning work, and specifying which scripts and libraries are used. But these systems do not offer a solution to managing data sets.

Likewise, Makefiles and similar workflow scripting tools offer a method to repeatedly execute a series of commands. The executed commands are determined through file-system time stamps. These tools offer no solution for data management.

At the other end of the scale are companies like Domino Data Labs or C3 IoT offering hosted platforms for data science and machine learning. Both package together an offering built upon a wide swath of data science tools. In some cases, like C3 IoT, users are coding in a proprietary language and storing their data in a proprietary data store. It can be enticing to use a one-stop-shopping service, but will it offer the needed flexibility?

In the rest of this article we’ll discuss DVC. It was designed to closely match Git functionality, to leverage the familiarity most of us have with Git, but with features making it work well for both workflow and data management in the machine learning context.

DVC (https://dvc.org) takes on and solves a larger slice of the machine learning reproducibility problem than does Git-LFS or several other potential solutions. It does this by managing the code (scripts and programs), alongside large data files, in a hybrid between DVC and a source code management (SCM) system like Git. In addition DVC manages the workflow required for processing files used in machine learning experiments. The data files and commands-to-execute are described in DVC files which we’ll learn about in the following sections. Finally, with DVC it is easy to store data on many storage systems from the local disk, to an SSH server, or to cloud systems (S3, GCP, etc). Data managed by DVC can be easily shared with others using this storage system.

Image courtesy dvc.org

DVC uses a similar command structure to Git. As we see here, just like git push and git pull are used for sharing code and configuration with collaborators, dvc push and dvc pull is used for sharing data. All this is covered in more detail in the coming sections, or if you want to skip right to learning about DVC see the tutorial at https://dvc.org/doc/tutorial.

DVC remembers precisely which files were used at what point of time

At the core of DVC is a data store (the DVC cache) optimized for storing and versioning large files. The team chooses which files to store in the SCM (like Git) and which to store in DVC. Files managed by DVC are stored such that DVC can maintain multiple versions of each file, and to use file-system links to quickly change which version of each file is being used.

Conceptually the SCM (like Git) and DVC both have repositories holding multiple versions of each file. One can check out “version N” and the corresponding files will appear in the working directory, then later check out “version N+1” and the files will change around to match.

Image courtesy dvc.org

On the DVC side, this is handled in the DVC cache. Files stored in the cache are indexed by a checksum (MD5 hash) of the content. As the individual files managed by DVC change, their checksum will of course change, and corresponding cache entries are created. The cache holds all instances of each file.

For efficiency, DVC uses several linking methods (depending on file system support) to insert files into the workspace without copying. This way DVC can quickly update the working directory when requested.

DVC uses what are called “DVC files” to describe both the data files and the workflow steps. Each workspace will have multiple DVC files, with each describing one or more data files with the corresponding checksum, and each describing a command to execute in the workflow.

cmd: python src/prepare.py data/data.xml

deps:

- md5: b4801c88a83f3bf5024c19a942993a48

path: src/prepare.py

- md5: a304afb96060aad90176268345e10355

path: data/data.xml

md5: c3a73109be6c186b9d72e714bcedaddb

outs:

- cache: true

md5: 6836f797f3924fb46fcfd6b9f6aa6416.dir

metric: false

path: data/prepared

wdir: .

This example DVC file comes from the DVC Getting Started example (https://github.com/iterative/example-get-started) and shows the initial step of a workflow. We’ll talk more about workflows in the next section. For now, note that this command has two dependencies, src/prepare.py and data/data.xml , and an output data directory named data/prepared . Everything has an MD5 hash, and as these files change the MD5 hash will change and a new instance of changed data files are stored in the DVC cache.

DVC files are checked into the SCM managed (Git) repository. As commits are made to the SCM repository each DVC file is updated (if appropriate) with new checksums of each file. Therefore with DVC one can recreate exactly the data set present for each commit, and the team can exactly recreate each development step of the project.

DVC files are roughly similar to the “pointer” files used in Git-LFS.

The DVC team recommends using different SCM tags or branches for each experiment. Therefore accessing the data files, and code, and configuration, appropriate to that experiment is as simple as switching branches. The SCM will update the code and configuration files, and DVC will update the data files, automatically.

This means there is no more scratching your head trying to remember which data files were used for what experiment. DVC tracks all that for you.

DVC remembers the exact sequence of commands used at what point of time

The DVC files remember not only the files used in a particular execution stage, but the command that is executed in that stage.

Reproducing a machine learning result requires not only using the precise same data files, but the same processing steps and the same code/configuration. Consider a typical step in creating a model, of preparing sample data to use in later steps. You might have a Python script, prepare.py, to perform that split, and you might have input data in an XML file named data/data.xml .

$ dvc run -d data/data.xml -d code/prepare.py \

-o data/prepared \

python code/prepare.py

This is how we use DVC to record that processing step. The DVC “run” command creates a DVC file based on the command-line options.

The -d option defines dependencies, and in this case we see an input file in XML format, and a Python script. The -o option records output files, in this case there is an output data directory listed. Finally, the executed command is a Python script. Hence, we have input data, code and configuration, and output data, all dutifully recorded in the resulting DVC file, which corresponds to the DVC file shown in the previous section.

If prepare.py is changed from one commit to the next, the SCM will automatically track the change. Likewise any change to data.xml results in a new instance in the DVC cache, which DVC will automatically track. The resulting data directory will also be tracked by DVC if they change.

A DVC file can also simply refer to a file, like so:

md5: 99775a801a1553aae41358eafc2759a9

outs:

- cache: true

md5: ce68b98d82545628782c66192c96f2d2

metric: false

path: data/Posts.xml.zip

persist: false

wdir: ..

This results from the “ dvc add file ” command, which is used when you simply have a data file, and it is not the result of another command. For example in https://dvc.org/doc/tutorial/define-ml-pipeline this is shown, which results in the immediately preceeding DVC file:

$ wget -P data https://dvc.org/s3/so/100K/Posts.xml.zip

$ dvc add data/Posts.xml.zip

The file Posts.xml.zip is then the data source for a sequence of steps shown in the tutorial that derive information from this data.

Take a step back and recognize these are individual steps in a larger workflow, or what DVC calls a pipeline. With “ dvc add ” and “ dvc run ” you can string together several Stages, each being created with a “ dvc run ” command, and each being described by a DVC file. For a complete working example, see https://github.com/iterative/example-get-started and https://dvc.org/doc/tutorial

This means that each working directory will have several DVC files, one for each stage in the pipeline used in that project. DVC scans the DVC files to build up a Directed Acyclic Graph (DAG) of the commands required to reproduce the output(s) of the pipeline. Each stage is like a mini-Makefile in that DVC executes the command only if the dependencies have changed. It is also different because DVC does not consider only the file-system timestamps, like Make does, but whether the file content has changed, as determined by the checksum in the DVC file versus the current state of the file.

Bottom line is that this means there is no more scratching your head trying to remember which version of what script was used for each experiment. DVC tracks all of that for you.

Image courtesy dvc.org

DVC makes it easy to share data and code between team members

A machine learning researcher is probably working with colleagues, and needs to share data and code and configuration. Or the researcher may need to deploy data to remote systems, for example to run software on a cloud computing system (AWS, GCP, etc), which often means uploading data to the corresponding cloud storage service (S3, GCP, etc).

The code and configuration side of a DVC workspace is stored in the SCM (like Git). Using normal SCM commands (like “ git clone ”) one can easily share it with colleagues. But how about sharing the data with colleagues?

DVC has the concept of remote storage. A DVC workspace can push data to, or pull data from, remote storage. The remote storage pool can exist on any of the cloud storage platforms (S3, GCP, etc) as well as an SSH server.

Therefore to share code, configuration and data with a colleague, you first define a remote storage pool. The configuration file holding remote storage definitions is tracked by the SCM. You next push the SCM repository to a shared server, which carries with it the DVC configuration file. When your colleague clones the repository, they can immediately pull the data from the remote cache.

This means your colleagues no longer have to scratch their head wondering how to run your code. They can easily replicate the exact steps, and the exact data, used to produce the results.

Image courtesy dvc.org

Conclusion

The key to repeatable results is using good practices, to keep proper versioning of not only their data but the code and configuration files, and to automate processing steps. Successful projects sometimes requires collaboration with colleagues, which is made easier through cloud storage systems. Some jobs require AI software running on cloud computing platforms, requiring data files to be stored on cloud storage platforms.

With DVC a machine learning research team can ensure their data, configuration and code are in sync with each other. It is an easy-to-use system which efficiently manages shared data repositories alongside an SCM system (like Git) to store the configuration and code.

Resources

Back in 2014 Jason Brownlee wrote a checklist he claimed would encourage reproducible machine learning results, by default: https://machinelearningmastery.com/reproducible-machine-learning-results-by-default/

A Practical Taxonomy of Reproducibility for Machine Learning Research — A research paper by staff of Kaggle and the U of Washington http://www.rctatman.com/files/2018-7-14-MLReproducability.pdf

A researcher at McGill Univ, Joelle Pineau, has another checklist for Machine Learning reproducibility https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

She made a presentation at the NeurIPS 2018 conference: https://videoken.com/embed/jH0AgVcwIBc (start at about 6 minutes)

The 12 Factor Application is a take on reproducibility or reliability of web services https://12factor.net/

A survey of scientists by the journal Nature noted over 50% of scientists agree there is a crisis in reproducing results https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970","['machine', 'file', 'gitlfs', 'crisis', 'reproducibility', 'files', 'software', 'used', 'solve', 'learning', 'data', 'git', 'dvc']","Some claim the machine learning field is in a crisis due to software tooling that’s insufficient to ensure repeatable processes.
The crisis is about difficulty in reproducing results such as machine learning models.
Does using Git-LFS solve the so-called Machine Learning Reproducibility Crisis?
Solutions to machine learning reproducibilityHumans are an inventive lot, and there are many possible solutions to this “crisis”.
DVC (https://dvc.org) takes on and solves a larger slice of the machine learning reproducibility problem than does Git-LFS or several other potential solutions.",en,['David Herron'],2019-04-30 22:05:24.086000+00:00,"{'Data Science', 'Artificial Intelligence', 'Science', 'Machine Learning', 'Git'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3088/0*T5hWTf7Zp-9qVjby', 'https://miro.medium.com/max/3200/0*4pXXBqyOKj7K5Uyv', 'https://miro.medium.com/max/3200/0*GCgyqbXvSmbm-Njq', 'https://miro.medium.com/max/60/0*4FC6JgUem-yjSJ6O?q=20', 'https://miro.medium.com/max/60/0*4pXXBqyOKj7K5Uyv?q=20', 'https://miro.medium.com/max/3200/0*4FC6JgUem-yjSJ6O', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*Ce5cFf326yK54vN1?q=20', 'https://miro.medium.com/fit/c/160/160/1*LluZujATfWWdeJNVoaShvA.jpeg', 'https://miro.medium.com/max/1200/0*4pXXBqyOKj7K5Uyv', 'https://miro.medium.com/max/60/0*GCgyqbXvSmbm-Njq?q=20', 'https://miro.medium.com/max/60/0*T5hWTf7Zp-9qVjby?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*LluZujATfWWdeJNVoaShvA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3200/0*Ce5cFf326yK54vN1'}",2020-03-05 00:17:22.129419,6.370206594467163
https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205,"Ensemble methods: bagging, boosting and stacking","What are ensemble methods?

Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.

Single weak learner

In machine learning, no matter if we are facing a classification or a regression problem, the choice of the model is extremely important to have any chance to obtain good results. This choice can depend on many variables of the problem: quantity of data, dimensionality of the space, distribution hypothesis…

A low bias and a low variance, although they most often vary in opposite directions, are the two most fundamental features expected for a model. Indeed, to be able to “solve” a problem, we want our model to have enough degrees of freedom to resolve the underlying complexity of the data we are working with, but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. This is the well known bias-variance tradeoff.

Illustration of the bias-variance tradeoff.

In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them. Most of the time, these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a strong learner (or ensemble model) that achieves better performances.

Combine weak learners

In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. The ensemble model we obtain is then said to be “homogeneous”. However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an “heterogeneous ensembles model”.

One important point is that our choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance whereas if we choose base models with low variance but high bias, it should be with an aggregating method that tends to reduce bias.

This brings us to the question of how to combine these models. We can mention three major kinds of meta-algorithms that aims at combining weak learners:

bagging , that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process

, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process boosting , that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy

, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy stacking, that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions

Very roughly, we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).

In the following sections, we will present in details bagging and boosting (that are a bit more widely used than stacking and will allow us to discuss some key notions of ensemble learning) before giving a brief overview of stacking.","['models', 'homogeneous', 'low', 'bagging', 'learners', 'learning', 'methods', 'model', 'variance', 'weak', 'stacking', 'boosting', 'base', 'ensemble']","The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.
In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them.
Combine weak learnersIn order to set up an ensemble learning method, we first need to select our base models to be aggregated.
Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways.
In the following sections, we will present in details bagging and boosting (that are a bit more widely used than stacking and will allow us to discuss some key notions of ensemble learning) before giving a brief overview of stacking.",en,['Joseph Rocca'],2019-05-05 12:13:40.499000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/2520/1*kISLC1Udq0m6g5kwHhMuJg@2x.png', 'https://miro.medium.com/max/3630/1*_B5HX2whbTs3DS8M6YBD_w@2x.png', 'https://miro.medium.com/max/60/1*7XVde-bMixpKf8mj61qhJQ@2x.png?q=20', 'https://miro.medium.com/max/60/1*nu96mPOtrXosJYgWA4Rvbw@2x.png?q=20', 'https://miro.medium.com/max/60/1*5pA6iY-qDP2JIsLoyfje-Q@2x.png?q=20', 'https://miro.medium.com/max/60/1*avYNzmLUeqKr1zWPkn6cwg@2x.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*6JbndZ2zY2c4QqS73HQ47g@2x.png?q=20', 'https://miro.medium.com/max/3630/1*6JbndZ2zY2c4QqS73HQ47g@2x.png', 'https://miro.medium.com/max/3630/1*DK2iShmkQKibMz-mNcMwng@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png?q=20', 'https://miro.medium.com/max/60/1*kISLC1Udq0m6g5kwHhMuJg@2x.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*Ede150EkH-LDgY-m-SrmLg.png', 'https://miro.medium.com/max/60/1*Dn6v09t5_L5cvADxJHJzHQ@2x.png?q=20', 'https://miro.medium.com/max/2522/1*ZucZsXkOwrpY2XaPh6teRw@2x.png', 'https://miro.medium.com/max/60/1*lWnm3eJVe3uo95OcSg5jUA@2x.png?q=20', 'https://miro.medium.com/max/2520/1*lWnm3eJVe3uo95OcSg5jUA@2x.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/708/1*Dn6v09t5_L5cvADxJHJzHQ@2x.png', 'https://miro.medium.com/max/3096/1*jEbEHwvfUzAUI00muEAVGw@2x.png', 'https://miro.medium.com/max/60/1*YUJJ5nDbhBi0SkFeccsTxQ@2x.png?q=20', 'https://miro.medium.com/max/3630/1*7XVde-bMixpKf8mj61qhJQ@2x.png', 'https://miro.medium.com/max/3660/1*nu96mPOtrXosJYgWA4Rvbw@2x.png', 'https://miro.medium.com/max/3630/1*5pA6iY-qDP2JIsLoyfje-Q@2x.png', 'https://miro.medium.com/max/1200/1*q6x_dETZ3wZjpcrYFntmJQ.jpeg', 'https://miro.medium.com/max/3630/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png', 'https://miro.medium.com/max/60/1*7wz2AIdH0pZSIUAxveLlIg@2x.png?q=20', 'https://miro.medium.com/max/2804/1*7wz2AIdH0pZSIUAxveLlIg@2x.png', 'https://miro.medium.com/max/3630/1*zAMhmZ78a6V9W878zfk5eA@2x.png', 'https://miro.medium.com/max/60/1*ZucZsXkOwrpY2XaPh6teRw@2x.png?q=20', 'https://miro.medium.com/max/60/1*DK2iShmkQKibMz-mNcMwng@2x.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*Ede150EkH-LDgY-m-SrmLg.png', 'https://miro.medium.com/max/600/1*PdXCJcy5kOeJAVAB2lH6CQ@2x.png', 'https://miro.medium.com/max/3630/1*avYNzmLUeqKr1zWPkn6cwg@2x.png', 'https://miro.medium.com/max/1272/1*dRk0faxd3IphTn0wzm43gg@2x.png', 'https://miro.medium.com/max/60/1*jEbEHwvfUzAUI00muEAVGw@2x.png?q=20', 'https://miro.medium.com/max/2860/1*4Ytrff-V4Xnh0_FKN_t1PA@2x.png', 'https://miro.medium.com/max/892/1*YUJJ5nDbhBi0SkFeccsTxQ@2x.png', 'https://miro.medium.com/max/60/1*4Ytrff-V4Xnh0_FKN_t1PA@2x.png?q=20', 'https://miro.medium.com/max/60/1*_B5HX2whbTs3DS8M6YBD_w@2x.png?q=20', 'https://miro.medium.com/max/60/1*PdXCJcy5kOeJAVAB2lH6CQ@2x.png?q=20', 'https://miro.medium.com/max/60/1*dRk0faxd3IphTn0wzm43gg@2x.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*q6x_dETZ3wZjpcrYFntmJQ.jpeg?q=20', 'https://miro.medium.com/max/3840/1*q6x_dETZ3wZjpcrYFntmJQ.jpeg', 'https://miro.medium.com/max/60/1*zAMhmZ78a6V9W878zfk5eA@2x.png?q=20'}",2020-03-05 00:17:29.074092,6.94467306137085
https://towardsdatascience.com/how-to-version-control-your-machine-learning-task-cad74dce44c4,How to Version Control your Machine Learning task — I,"What is Version Control ?

A component of software configuration management, version control, also known as revision control or source control,[1] is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the “revision number”, “revision level”, or simply “revision”. For example, an initial set of files is “revision 1”. When the first change is made, the resulting set is “revision 2”, and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.

Why Version Control ?

An important question: Why do we need Version Control ? I am doing task on my local computer/cloud and I am deploying it at my server once the model is ready and only if I am done testing it. So why do I need version control ?

Now let’s look at a scenario: I am working for a company like Botsupply and I have clients. I am the AI guy. I made a question answering search using TF-IDF based model. I deployed it on my server. In the next phase, I made some changes to it and on my dummy data my accuracy increases. I deployed it on the server. Now due to the complexity in the test data, the performance decreases. Now I want to go back to the previous version.

One way is to deploy the previous version again. Second, or the better solution is version control and revert to the previous version.","['machine', 'server', 'task', 'version', 'set', 'previous', 'control', 'revision', 'changes', 'learning', 'number', 'need', 'question']","What is Version Control ?
Changes are usually identified by a number or letter code, termed the “revision number”, “revision level”, or simply “revision”.
Why Version Control ?
So why do I need version control ?
Second, or the better solution is version control and revert to the previous version.",en,['Kumar Shridhar'],2018-10-06 07:31:58.150000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Version Control'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*Ne-Pf2CmqoJ9CeRb5XfZhw.png?q=20', 'https://miro.medium.com/max/60/1*ekGRk4XNNEMqr2PHPQqhIg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/max/60/1*A4l0MO6Qi98hRHhTQA-WRg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2210/1*ekGRk4XNNEMqr2PHPQqhIg.png', 'https://miro.medium.com/max/857/1*A4l0MO6Qi98hRHhTQA-WRg.png', 'https://miro.medium.com/max/2990/1*Ne-Pf2CmqoJ9CeRb5XfZhw.png', 'https://miro.medium.com/max/1714/1*A4l0MO6Qi98hRHhTQA-WRg.png', 'https://miro.medium.com/fit/c/160/160/1*m5fhlOhPpSDnLc9jPMtxOg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*v6vO1fMPEg6sPWA6k_d13w.png?q=20', 'https://miro.medium.com/max/1856/1*v6vO1fMPEg6sPWA6k_d13w.png'}",2020-03-05 00:17:35.374057,6.298953056335449
https://medium.com/@mtngt/docker-flask-a-simple-tutorial-bbcb2f4110b5,Docker + Flask | A Simple Tutorial,"Setup Steps

Create a folder to hold the project. We will operate out of here for the most part. Use the mkdir command to create a folder.

$ mkdir hello_docker_flask

Navigate to that directory with cd .

$ cd hello_docker_flask

Make sure you have docker installed, the version is not particularly important as these basic commands are just about the same in all versions.

$ docker -v

Docker version 17.12.0-ce, build c97c6d6

Don’t have it installed? Here is the link to the docker official site, however you can use whatever method you like to install it.

Now that docker is ready lets see if you have any running containers.

$ docker ps

CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

<I do not have have any running right now>

If you are just getting started, there shouldn’t be any here. Either way it won’t hurt to have another one running at the same time.

Have some currently running and want to kill them?

$ docker kill <CONTAINER ID>

You can also check to see if you have any containers even if they are not running.

$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE

mtngt/angular_docker latest ec5a8c5f01f1 2 hours ago 17MB

Again these won’t hurt, but a good way to check to see what you already have (commands like this will come in handy later).

Want to clear out all the not running stuff as well?

$ docker system prune -a

That will delete everything you have in your local docker instance. So be careful.

Okay now that we know the basics, lets get started.","['mkdir', 'way', 'version', 'lets', 'docker', 'simple', 'flask', 'kill', 'tutorial', 'started', 'wont', 'installed', 'running']","$ cd hello_docker_flaskMake sure you have docker installed, the version is not particularly important as these basic commands are just about the same in all versions.
$ docker -vDocker version 17.12.0-ce, build c97c6d6Don’t have it installed?
Here is the link to the docker official site, however you can use whatever method you like to install it.
$ docker kill <CONTAINER ID>You can also check to see if you have any containers even if they are not running.
$ docker system prune -aThat will delete everything you have in your local docker instance.",en,[],2019-12-10 02:32:20.026000+00:00,"{'API', 'Flask', 'Docker', 'Python'}","{'https://miro.medium.com/max/1200/1*ERwd3Sq1zPsR7EARxMgiMg.png', 'https://miro.medium.com/max/2560/1*ERwd3Sq1zPsR7EARxMgiMg.png', 'https://miro.medium.com/fit/c/80/80/1*R7bsiWBhxYS8oyX68bn7kQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*EywFJyDCX0m3PYAxl_ypGg.png', 'https://miro.medium.com/fit/c/80/80/2*t6XbaDd1obxzoJ_HCS7lbg.jpeg', 'https://miro.medium.com/fit/c/96/96/1*mUZMRX12pp0zkex2fFphSA.jpeg', 'https://miro.medium.com/max/60/1*ERwd3Sq1zPsR7EARxMgiMg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*mUZMRX12pp0zkex2fFphSA.jpeg'}",2020-03-05 00:17:37.603322,2.229264974594116
https://medium.com/google-cloud,Community – Medium,One of the main benefits of using an all-in-one observability suite like Stackdriver is that it provides all of the capabilities you may…,"['observability', 'community', 'suite', 'stackdriver', 'allinone', 'capabilities', 'benefits', 'provides', 'main', 'using', 'medium']",One of the main benefits of using an all-in-one observability suite like Stackdriver is that it provides all of the capabilities you may…,,[],,set(),"{'https://cdn-images-1.medium.com/fit/c/72/72/2*V7nK7qwiaK2D_Xrgdkv8Rw.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/2*Kwoif53T33mPC9SolZO-RA.png', 'https://cdn-images-1.medium.com/fit/c/72/72/0*iE31uTCXvbLjEK0y', 'https://cdn-images-1.medium.com/max/184/1*B8ovMiQN38kbeu-4yK0-VA@2x.png', 'https://cdn-images-1.medium.com/fit/c/72/72/1*d3GOsC8ahsIh8KqjZs6PvQ.png', 'https://cdn-images-1.medium.com/fit/c/72/72/0*pPeZ4hKE7KR07g89.', 'https://cdn-images-1.medium.com/fit/c/72/72/1*61kkW_qRtaxnvkbXPPe7JA.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/1*rQp6ogqZ27BcRY0vTx2rww.jpeg', 'https://cdn-images-1.medium.com/max/1200/1*FUjLiCANvATKeaJEeg20Rw.png'}",2020-03-05 00:17:38.275086,0.6707637310028076
https://towardsdatascience.com/neural-networks-why-do-they-work-so-well-part-i-22f0d3690511,Neural Networks: Why do they work so well?,"Let’s talk about neural nets! But before we do, please review your linear algebra and calculus.

If you’d like to find out how I created the visualizations in this artcle, check out my machine learning models repository on GitHub.

Prerequisites

This article requires knowledge of a few key mathematical objects and techniques. These requirements are very useful to know in general, so I recommend you look into them if you aren’t familiar. Click on any of the links for my favorite resource on the topic.

Linear Algebra

Calculus

Alright, with that out of the way, let’s get to it!

The Basics: input & output

Note: if you are already familiar with the basics of statistical models, feel free to skip this section.

At its core, a neural network is just a function. More specifically, it’s a function that makes predictions. This means that it takes in some input and produces an output. The output predicts some true underlying value that may or may not be known.

Let’s add variable names to keep track of everything:

x: an input that is fed into the neural network

an input that is fed into the neural network y-hat: the output from the neural network that predicts y

the output from the neural network that predicts y: the true value that the neural network makes a prediction for

To get a better grasp of these definitions, let’s look at a concrete example.

Let’s say we want to predict if you have heart disease. We’ll do so based on a set of features we know about you. We have your weight, height, age, resting heart rate, and cholesterol level.

To start, we can stack your features into a vector x like so:

To get a neural network’s prediction for the probability that you have heart disease, we feed x in into the model. This outputs y-hat — the model’s prediction for p(heart disease). We can write this like so (where N is the neural network):

Here, y-hat is a continuous value between 0 and 1 as it is a probability. However, the true value y is binary (i.e. either 1 or 0) because you either have heart disease (1) or you don’t (0).

Ok! You now have a general idea of what a neural network does and why it might be useful. But, you still have no idea how a neural network makes a prediction for y. Let’s dig a little deeper.

Layers: 🍰

Note: in this section, I use an underscore followed by enclosing brackets to denote in-text subscript characters that aren’t available in Unicode. Here’s an example:

As previously stated, a neural network is a function. But, more specifically, it is a composition of functions. In deep learning speak, these functions are called layers. Later we’ll talk more about how these layers are composed but for now, let’s just focus on just one layer.

Because a layer is a function, it simply takes in an input and produces an output. We will call this function f_[ℓ] for layer ℓ. By convention, f_[ℓ] takes in an input a_[ℓ-1] and produces an output a_[ℓ]. Here, a_[ℓ] is called the activation for layer ℓ.

We’ve labeled f_[ℓ]’s inputs and outputs, but what specifically does the layer f_[ℓ] do? Well, f_[ℓ] maps a_[ℓ-1] → a_[ℓ] in three stages. Here is what happens in each:

First, a weight matrix W applies a linear transformation → W a_[ℓ-1] Then, a bias vector b is added → W a_[ℓ-1] + b Finally, a nonlinear activation function σ is applied → σ(W a_[ℓ-1] + b)

This sequence is summarized by the equation below:

Ok, we made some mathematical notation for what a layer does. But I’m not sure that we’ve truly understood what a layer is doing. Let’s try to get a better feel for this with a visualization of how f_[ℓ] transforms space.

In order to visualize f_[ℓ], we’re going to need to work with low (1, 2, or 3) dimensional inputs and outputs. For this visualization, we’ll have a 2D input and a 2D output. This means that the input a_[ℓ-1] ∈ ℝ² and the output a_[ℓ] ∈ ℝ². We can also say that f_[ℓ] maps ℝ² → ℝ². Visually, this means that every point in a 2D space is mapped to a new point in 2D space by f_[ℓ].

To plot how f_[ℓ] maneuvers each point in 2D space, we need to choose a function for σ, a matrix for W, and a vector for b. For this visualization, we’ll choose σ, W, and b like so:

Now that we have this concrete function, we can show how it affects every point in a 2D space. In order to see what’s happening, we’ll only show points that lie on a grid. With that in mind, let’s produce a visualization:

A visualization of a layer f_[ℓ] mapping ℝ² → ℝ².

Notice how the layer f_[ℓ] takes every point in ℝ² through the three different transformations. First, W stretches 2D space while keeping all lines parallel. Then, b shifts space away from the origin. And finally, σ smooshes space with no regard for keeping grid lines parallel. When f_[ℓ] completes its transformations, each input activation a_[ℓ-1] will have moved from its original position to its corresponding output activation a_[ℓ] according to f_[ℓ].

Great, but again, why is this useful? Well, on its own, a single layer isn’t necessarily useful at all. Layers really only become useful when we compose them. Let’s talk more about how this is done.

Composition: y = f(f(f(x)))

As previously mentioned, we can construct a neural network by composing layers. Composing layers means that we feed the output activation of one layer into the input of the next layer.

Once we’ve wired up each layer, we can feed in an input x (also called a_[0]) into the first layer f_[1]. The first layer f_[1] feeds its output activation a_[1] into f_[2] which feeds its activation a_[2] into f_[3] and so on. Until finally the last layer of the network f_[L] is reached and an output y-hat (also called a_[L]) is produced as a prediction for y.

As an example of what this process looks like mathematically, here is a three-layer neural network N :

Great! So that sums up what a neural network is. But, it’s still not clear why a series of composed layers are useful for making predictions. To get a better intuition, let’s make another visualization.

Like the last visualization, we are going to work with inputs in ℝ² and outputs in ℝ². The key difference here is that instead of showing how one layer f_[ℓ] transforms ℝ², we will show how an entire neural network N transforms ℝ². To ensure that the whole process can be visualized, every layer of N (f_[1], f_[2], f_[3], …, f_[L]) will also map ℝ² → ℝ². This allows us to see what’s happening at every step of the neural network’s transformation.

For this visualization, I’m also going to plot two spirals belonging to either an orange or blue class. These spirals will help demonstrate the network’s usefulness. With all that in mind, here is what we come up with:

A visualization of a neural network separating 2 spirals by mapping ℝ² → ℝ².

Cool, right? Let’s unpack this a bit more.

First, notice how each layer is applied one after the other. The first layer f_[1] maps a_[0] → a_[1]. The second layer f_[2] maps a_[1] → a_[2]. The third layer f_[3] maps a_[2] → a_[3] and so on. Until finally f_[L] maps a_[L-1] → a_[L] (i.e y-hat). This just follows the basic definition of a neural network.

Next, notice how each layer of the network separates the orange and blue spirals little by little. At each stage, the current transformation builds on the progress of the previous transformation. Through this process, the network is forming its prediction for each of the orange and blue points. When the mapping finally ends, the input points x land on the model’s prediction for x’s class — either orange (-1, 1) or blue (1, 1). Here, the model seems to be doing a pretty decent job.

But the reason why a neural network is useful has nothing to do with this specific example. The power of the model lies in its flexibility. Because each layer ℓ can have its very own σ_[ℓ], W_[ℓ], and b_[ℓ], there are theoretically infinite configurations that the model can take on.

This means that a neural network can do a lot more than separate these orange and blue spirals. In fact, it can separate tons of different points in all different configurations. And it can do more than separate points too. In fact, it can approximate almost any transformation (i.e. function). This makes the model applicable to tons and tons of problems.

Here’s another instance of a neural network. This one separates three spirals.

A visualization of a neural network separating 3 spirals by mapping ℝ² → ℝ².

But hold on just a second. How in the world does a model know how to approximate a given function? Of course, it uses a sequence of layers. But how do the layers know how to make themselves useful for a given problem? More specifically, how does each layer in the neural net figure out how to set its parameters σ, W, and b to accomplish something useful?

I’ll save that question for the next article where we’ll begin to explore how a neural network learns its σs, Ws, and bs.","['network', 'f_ℓ', 'lets', 'function', 'neural', 'networks', 'visualization', 'work', 'output', 'input', 'layer', 'ℝ²']","Let’s add variable names to keep track of everything:x: an input that is fed into the neural networkan input that is fed into the neural network y-hat: the output from the neural network that predicts ythe output from the neural network that predicts y: the true value that the neural network makes a prediction forTo get a better grasp of these definitions, let’s look at a concrete example.
You now have a general idea of what a neural network does and why it might be useful.
As an example of what this process looks like mathematically, here is a three-layer neural network N :Great!
A visualization of a neural network separating 3 spirals by mapping ℝ² → ℝ².
I’ll save that question for the next article where we’ll begin to explore how a neural network learns its σs, Ws, and bs.",en,['Isaiah Nields'],2020-02-24 00:51:24.264000+00:00,set(),"{'https://miro.medium.com/max/7718/1*plK_zeZXqhV4LWGYPfAmZg.png', 'https://miro.medium.com/max/2000/1*U4V-Ygbe_qCfAnBQOf8G5A@2x.png', 'https://miro.medium.com/max/1920/1*KJa4OQZkcSsvutz4hruvdw@2x.gif', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/freeze/max/60/1*8qAh0fYcXMrfw3tljEBSDA@2x.gif?q=20', 'https://miro.medium.com/fit/c/96/96/1*o4XAR8Gisku5Cl4TvqSPKA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2000/1*lcMkfP9txA8mwkioeMM9Ow@2x.png', 'https://miro.medium.com/max/1920/1*8qAh0fYcXMrfw3tljEBSDA@2x.gif', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1920/1*WsyqBdBHI4kIQmkubCLPEg@2x.gif', 'https://miro.medium.com/freeze/max/60/1*WsyqBdBHI4kIQmkubCLPEg@2x.gif?q=20', 'https://miro.medium.com/max/2000/1*iJyLdgdqzrtGDJ087_s1jA@2x.png', 'https://miro.medium.com/max/60/1*2Gd0zuFIeuCsTztghHbLcw@2x.png?q=20', 'https://miro.medium.com/max/60/1*iJyLdgdqzrtGDJ087_s1jA@2x.png?q=20', 'https://miro.medium.com/max/60/1*plK_zeZXqhV4LWGYPfAmZg.png?q=20', 'https://miro.medium.com/max/60/1*lcMkfP9txA8mwkioeMM9Ow@2x.png?q=20', 'https://miro.medium.com/max/2000/1*2Gd0zuFIeuCsTztghHbLcw@2x.png', 'https://miro.medium.com/max/2000/1*j9Tk1PmPIS1R3dlz59tRbQ@2x.png', 'https://miro.medium.com/max/60/1*fB4BpR9czApuA1SPQPWNXg@2x.png?q=20', 'https://miro.medium.com/max/60/1*U4V-Ygbe_qCfAnBQOf8G5A@2x.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*o4XAR8Gisku5Cl4TvqSPKA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2000/1*fB4BpR9czApuA1SPQPWNXg@2x.png', 'https://miro.medium.com/max/60/1*j9Tk1PmPIS1R3dlz59tRbQ@2x.png?q=20', 'https://miro.medium.com/max/1200/1*plK_zeZXqhV4LWGYPfAmZg.png', 'https://miro.medium.com/freeze/max/60/1*KJa4OQZkcSsvutz4hruvdw@2x.gif?q=20'}",2020-03-05 00:17:44.925299,6.650213241577148
https://medium.com/@acowpy/scraping-files-images-using-scrapy-scrapinghub-and-google-cloud-storage-c7da9f9ac302,"Scraping files & images using scrapy, scrapinghub and Google Cloud Storage","Recently I was looking for a simple solution for processing files and images captured during our web scrapes - primarily PDFs and product image files.

We use scrapy cloud for all of our automated web scrapes (highly recommend), and they recently added support for Amazon S3 and Google Cloud Storage.

We went with Google Cloud because a lot of the rest of our stack is with Google, and they have a 12/mth $300 free trial.

Scrapinghub has an article on support for downloading and processing images, and scrapy docs also has some information, but it took me a while to figure out how to authenticate with Google Cloud from a scrape deployed to scrapinghub, so I decided to publish my solution here as I don’t feel it was adequately covered in the aforementioned docs (they just bump you to the generic google cloud authentication docs).

Google Cloud authentication relies on a JSON file containing key information. I found the easiest way to authenticate was to build a simple subclass of the generic FilesPipeline and GCSFilesStore classes defined with the scrapy library.

First, if you have not create a Google Cloud Storage bucket and service account, see the documentation here to setup and download the credentials JSON file. Once downloaded, open the JSON file and copy the contents into the CREDENTIALS variable as shown below.

Then. within pipelines.py within your main project folder add the following:



CREDENTIALS = {

""type"": ""service_account"",

""project_id"": ""COPY FROM CREDENTIALS FILE"",

""private_key_id"": ""COPY FROM CREDENTIALS FILE"",

""private_key"": ""COPY FROM CREDENTIALS FILE"",

""client_email"": ""COPY FROM CREDENTIALS FILE"",

""client_id"": ""COPY FROM CREDENTIALS FILE"",

""auth_uri"": ""

""token_uri"": ""

""auth_provider_x509_cert_url"":

""

""client_x509_cert_url"": ""COPY FROM CREDENTIALS FILE""

} class GCSFilesStoreJSON(GCSFilesStore):CREDENTIALS = {""type"": ""service_account"",""project_id"": ""COPY FROM CREDENTIALS FILE"",""private_key_id"": ""COPY FROM CREDENTIALS FILE"",""private_key"": ""COPY FROM CREDENTIALS FILE"",""client_email"": ""COPY FROM CREDENTIALS FILE"",""client_id"": ""COPY FROM CREDENTIALS FILE"",""auth_uri"": "" https://accounts.google.com/o/oauth2/auth "",""token_uri"": "" https://accounts.google.com/o/oauth2/token "",""auth_provider_x509_cert_url"": https://www.googleapis.com/oauth2/v1/certs "",""client_x509_cert_url"": ""COPY FROM CREDENTIALS FILE"" def __init__(self, uri):

from google.cloud import storage

client =

storage.Client.from_service_account_info(self.CREDENTIALS)

bucket, prefix = uri[5:].split('/', 1)

self.bucket = client.bucket(bucket)

self.prefix = prefix class GCSFilePipeline(FilesPipeline):

def __init__(self, store_uri, download_func=None, settings=None):

super(GCSFilePipeline, self).__init__(store_uri,download_func,settings)

Next, enable in your custom Item Pipeline in settings.py:

ITEM_PIPELINES = {

'myproject.pipelines.GCSFilePipeline': 1,

} FILES_STORE = 'gs://some_bucket_name/'

IMAGES_STORE = 'gs://some_bucket_name/'

GCS_PROJECT_ID = 'some_project_id'

Then all you need to do is save urls to a file_urls field within your scrape and the contents will automatically be uploaded to the specified bucket, e.g (downloads the google logo file):





class GoogleSpider(scrapy.Spider):

name = 'google_logo'

allowed_domains = ['google.com.au']

start_urls = [' import scrapyclass GoogleSpider(scrapy.Spider):name = 'google_logo'allowed_domains = ['google.com.au']start_urls = [' https://www.google.com.au'

item = {}

item['file_urls'] =

[""{}{}"".format(""

response.xpath(""//img[

.extract_first())]

yield item def parse(self, response):item = {}item['file_urls'] =[""{}{}"".format("" https://www.google.com.au "",response.xpath(""//img[ @id ='hplogo']/@src"").extract_first())]yield item

Note: For image (as opposed to file) processing, replace ‘File’ with ‘Image’ everywhere above and it should work perfectly. File and Image pipelines are very similar but (from scrapy docs), the Images Pipeline has a few extra functions for processing images:","['file', 'files', 'scrapy', 'credentials', 'processing', 'cloud', 'copy', 'image', 'json', 'scraping', 'storage', 'images', 'scrapinghub', 'google', 'using']","We use scrapy cloud for all of our automated web scrapes (highly recommend), and they recently added support for Amazon S3 and Google Cloud Storage.
We went with Google Cloud because a lot of the rest of our stack is with Google, and they have a 12/mth $300 free trial.
Google Cloud authentication relies on a JSON file containing key information.
First, if you have not create a Google Cloud Storage bucket and service account, see the documentation here to setup and download the credentials JSON file.
File and Image pipelines are very similar but (from scrapy docs), the Images Pipeline has a few extra functions for processing images:",en,['Aaron Cowper'],2018-05-18 12:50:39.535000+00:00,"{'Scrapy', 'Google Cloud Platform', 'Python', 'Web Scraping', 'Image Processing'}","{'https://miro.medium.com/fit/c/80/80/0*4eB7pSz-dl5t1RG7.', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/80/80/2*Py0ptoWNHy-Jz4V5q1dA9g.jpeg', 'https://miro.medium.com/fit/c/96/96/1*gmQlvl2x9x8YGsFUIyCK1w.png', 'https://miro.medium.com/fit/c/160/160/1*gmQlvl2x9x8YGsFUIyCK1w.png', 'https://miro.medium.com/fit/c/80/80/2*31IClG0J3w2Q-Rk5KU4-FA.jpeg'}",2020-03-05 00:17:45.859358,0.934058666229248
https://medium.com/google-cloud/app-engine-flex-container-engine-946fbc2fe00a,App Engine Flex || Kubernetes Engine — ?,"App Engine Flex || Kubernetes Engine — ?

Deploying containerized apps 2 ways on GCP

Customers ask for our help in determining whether App Engine Flex(ible Environment) or Google Kubernetes Engine (GKE) is best-suited to their needs.

There is no universal answer and our close ties with our customers helps us determine the best answer for them for their needs. This post summarizes one, good approach that will help anyone gain evidence for an answer: to try both.

In this post, I will use an exemplar as a solution, deploy it to Flex and to GKE, and load-test both solutions. Thanks to the consistency provided by containers, we’ll have high confidence that the different experience with each platform is due to the platform and not our solution.

Let’s get started!

An Exemplar

Something web-y using a NoSQL store? That sounds about right to me. Fortunately, the App Engine Flex documentation includes a sample app that we can use (GitHub here). You can pick your language flavor; I’m going with Golang because I’ve been writing in Python and Java recently. We’ll use Cloud Datastore (and possibly another later on) for persistence.

Setup

You can get started for free with Google Cloud Platform (GCP). I’m using a Linux (Debian) machine and will show bash commands here. Everything will work from a Mac or Windows machine but your-mileage-may-vary as you’ll need to do some work to convert the commands.

export PROJECT=[YOUR-PROJECT-ID]

export REGION=[YOUR-PREFERRED-REGION]

export BILLING=[YOUR-BILLING-ID]

export GITHUB=[YOUR-GITHUB-PROFILE] mkdir -p ${HOME}/Projects/${PROJECT}

cd ${HOME}/Projects/${PROJECT} gcloud projects create $PROJECT gcloud alpha billing projects link $PROJECT \

--billing-account=$BILLING # Enable Datastore

gcloud services enable datastore.googleapis.com \

--project=$PROJECT # Enable Kubernetes Engine

gcloud services enable container.googleapis.com \

--project=$PROJECT

App Engine Flex

Let’s create an App Engine Flex application in our project. You may choose a GCP region that’s most convenient for you with the following command, the Cloud SDK will prompt you to select a region in which App Engine Flex is available:

gcloud app create --project=$PROJECT

If you know your preferred region already, you may specify it here:

gcloud app create --region=$REGION --project=$PROJECT

GCP will then provision the application for you:



cannot be changed. More information about regions is at

< WARNING: Creating an App Engine application for a project is irreversible and the regioncannot be changed. More information about regions is at https://cloud.google.com/appengine/docs/locations >. Creating App Engine application in project [${PROJECT}] and region [${REGION}]....done.

Success! The app is now created. Please use `gcloud app deploy` to deploy your first app.

If you follow the instructions to clone the GitHub repo containing the sample, you should find yourself in a directory containing two files: app.yaml and datastore.go.

NB app.yaml is a configuration file for App Engine. This file is not used by Kubernetes Engine.

I’m a little pernickety and I prefer to create everything cleanly my way:

mkdir -p $HOME/Projects/$PROJECT/go/src/github.com/$GITHUB/aeoke

cd $HOME/Projects/$PROJECT/go/src/github.com/$GITHUB/aeoke

I’ll call out app.yaml. As mentioned, this provides config guidance to the App Engine service:

runtime: go

env: flex automatic_scaling:

min_num_instances: 1 #[START env_variables]

env_variables:

GCLOUD_DATASET_ID: $PROJECT

#[END env_variables]

NB Replace $PROJECT with your project ID. NB GCLOUD_DATASET_ID is passed as an environment variable to the Golang runtime and accessed with os.Getenv(“GCLOUD_DATASET_ID”). This is a best practice for passing config to containerized apps.

Don’t forget to create datastore.go too and pull the dependencies:

go get ./...

All being well, you should then be able to deploy the app. You’ll need to deploy the app to benefit from the full majesty of it:

gcloud app deploy --project=$PROJECT ...

Successfully built b4efec18970b

Successfully tagged us.gcr.io/${PROJECT}/appengine/default.20171010t153004:latest

PUSH

Pushing us.gcr.io/${PROJECT}/appengine/default.20171010t153004:latest

The push refers to a repository [us.gcr.io/${PROJECT}/appengine/default.20171010t153004]

bf419b41a797: Preparing

...

bf419b41a797: Pushed

...

Deployed service [default] to [ Updating service [default]...done.Deployed service [default] to [ https://${PROJECT}.appspot.com You can stream logs from the command line by running:

$ gcloud app logs tail -s default To view your application in the web browser run:

$ gcloud app browse

I’ve included some of the deployment details because you will see from the above that the deployment pushes a container to a repository. Specifically to usr.gcr.io/${PROJECT}/appengine… This URL refers to GCP’s hosted container registry called Google Container Registry (GCR). We will reuse the image from this repository when we deploy to Kubernetes Engine.

You may wish to check Cloud Console to monitor the status of the app. Don’t forget to replace ${PROJECT} with your Project ID:

You should see something similar to this:

Cloud Console: App Engine “Services”

Once the app is deployed, you may access it by clicking the “default” service from Cloud Console, accessing the service directly via it’s URL (replace $PROJECT with your Project ID), or with the command ‘gcloud app browse’:

The Exemplar deployed to App Engine Flex

You should explore the Console.

You may be interested to see the Instances that support our app. We explicitly set min_num_instances to be “1” in the app.yaml and so, with insignificant load, there’s a singular instance supporting our app:

Cloud Console: App Engine “Instances”

Refreshing the page several times ensures there’s a goodly amount of data persisted in Cloud Datastore:

Cloud Console: Datastore

NB Every page refresh (GET) on our app will add another entity to the Datastore “Visit” Kind. The Golang (queryVisits) function queries and displays the 10 most-recent entries only.

As mentioned previously, the Flex deployment created a (Docker) container and persisted this using Google Container Registry. Let’s look at the Container Registry page of our project:

Cloud Console: Container Registry

I’ve drilled down into the Registry to show more details. The image name is “appengine/default.[DEPLOYMENT-TIME]” and it has been given the “latest” tag. It is possible to more explicitly reference this image by its digest which includes a sha256 hash.

You may also find the image with a Cloud SDK command. We’ll use this image again in the Kubernetes Engine deployment so it may be useful to remember this:

gcloud container images list \

--repository=us.gcr.io/${PROJECT}/appengine \

--project=$PROJECT

For the curious, Google Container Registry uses Google Cloud Storage (GCS) to store the image layers. You may investigate here:

Google Kubernetes Engine (GKE)

Let’s start by creating a cluster on which we can deploy the Exemplar app.

For consistency, we’re going to use custom machine-types with 1 vCPU and 1GB of RAM as this is what App Engine Flex is using. We’ll start with 1 (worker) node. GKE manages the master node for us but the master is not used to run our containers. I recommend also using the same region (and preferably the same zone) as App Engine. In this case, I’m using us-east4 and App Engine used zone ‘c’. As with App Engine, I’m going to enable GKE to auto-scale BUT we’re (usefully) required to provide a maximum number of nodes as an upper-bound on the auto-scaler. I’ve chosen 10 nodes here but you may wish to use a lower number.

export CLUSTER=[YOUR-CLUSTER-ID]

export ZONE=${REGION}-c gcloud container clusters create $CLUSTER \

--enable-kubernetes-alpha \

--project=$PROJECT \

--zone=$ZONE \

--machine-type=custom-1-1024 \

--image-type=COS \

--num-nodes=1 \

--enable-autoscaling \

--max-nodes=10 \

--quiet

NB GKE provides two flavors of auto-scaling. The first is an intrinsic feature of Kubernetes and permits more pods to be created as load on a service is increased. The number of nodes forming the cluster remains fixed. The second (specified by — enabled-autoscaling) uses Cluster Autoscaler. As the name suggests, we now permit the cluster to grow (and to shrink) as demand upon it changes. This results in additional nodes being added to the cluster to grow it and nodes being removed from the cluster to shrink it. With more nodes, there’s more capacity to run a greater number of pods.

Once the cluster is created, you may observe it from within Cloud Console:

In order to control the cluster from the command-line, we need authenticate to it. This is facilitated with a Cloud SDK (gcloud) convenience command:

gcloud container clusters get-credentials $CLUSTER \

--zone=$ZONE \

--project=$PROJECT

To check that everything’s working correctly:

kubectl get nodes NAME STATUS AGE

gke-cluster-01-default-pool-001b0e59-8dk8 Ready 56s

Optional: You may control the cluster using the Kubernetes Dashboard. GKE deploys the Dashboard with clusters. To access the Dashboard, configure a proxy to the API server and then open the URL in your browser. I use port=0 to gain an available port at random. In this case, the port chosen was 42545. You should use whichever port is provided to you when you run the command:

kubectl proxy --port=0 & Starting to serve on 127.0.0.1:42545

Once the proxy is running, you can access the API server on the root (“/”) and the UI Dashboard on “/ui”:

http://localhost:42545/ui

There’s a small issue for me presently where the UI does not render correctly so I’m going to provide examples with Cloud Console and from the command-line :-(

Update: the UI issue is being addressed. You should be able to access the UI by explicitly finalizing the URL after redirection with a “/” so: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/ And…

Kubernetes UI Happiness!

I’ll add some sample screenshots from this UI to the end of this post.

From the App Engine Flex deployment, we have an existing image in GCR that we can reuse. The easiest way to spin this into a service on GKE is to reference the image from a Deployment, expose the Deployment as a Service and then have GCP create an HTTP/S Load-Balancer.

Please review your App Engine Flex deployment to determine the name of the container image that App Engine Flex created for you. You will need to reference the image in the Deployment YAML file:

us.gcr.io/$PROJECT/appengine/default/YYMMDDtHHMMSS:latest

Alternatively you may find the image name with the following command. The version tagged “latest” will be the one we’ll use. So please don’t forget to append “:latest” to the image name when we create the Deployment config:

gcloud container images list \

--repository=us.gcr.io/${PROJECT}/appengine

Before we can create the Deployment, we need to create a service account and a key, and assign it permission to access Cloud Datastore. We must then upload the key as a secret to GKE. This way, the Exemplar app’s pods may access the key when they need to access Cloud Datastore.

It sounds complex (and is more complex than it ought be) but there’s a straightforward pattern that’s documented here for Cloud Pub/Sub. I’ve tweaked this only slightly for Cloud Datastore:

export ROBOT=""gke-datastore"" gcloud iam service-accounts create $ROBOT \

--display-name=$ROBOT \

--project=$PROJECT gcloud iam service-accounts keys create ./key.json \

--iam-account=${ROBOT}@${PROJECT}.iam.gserviceaccount.com \

--project=$PROJECT gcloud projects add-iam-policy-binding $PROJECT \

--member=serviceAccount:${ROBOT}@${PROJECT}.iam.gserviceaccount.com \

--role=roles/datastore.user kubectl create secret generic datastore-key \

--from-file=key.json=./key.json



We can now define a Deployment combining the GCR image name, the service account key created in this previous step and the environment variables sufficient to define the Exemplar app.

Create a file (I’m using datastore-deployment.yaml). Replace ${IMAGE} with the path to your image (us.gcr.io/$PROJECT/appengine….) and replace ${PROJECT} with your Project ID.

The only real complexity in this configuration is that it exposes the Secret created in the previous step as a file that can be referenced by the pod through an environment variable (GOOGLE_APPLICATION_CREDENTIALS). This powerful mechanism is called Application Default Credentials:

apiVersion: apps/v1beta1

kind: Deployment

metadata:

name: datastore

spec:

replicas: 1

template:

metadata:

labels:

app: datastore

spec:

volumes:

- name: google-cloud-key

secret:

secretName: datastore-key

containers:

- name: datastore

image: ${IMAGE}

ports:

- name: http

containerPort: 8080

protocol: TCP

volumeMounts:

- name: google-cloud-key

mountPath: /var/secrets/google

env:

- name: GOOGLE_APPLICATION_CREDENTIALS

value: /var/secrets/google/key.json

- name: GCLOUD_DATASET_ID

value: ${PROJECT}

We can now create the Deployment:

kubectl create --filename=datastore-deployment.yaml

All being well, you should be told:

deployment ""datastore"" created

Next, let’s add a (Horizontal) Pod Autoscaler to the Deployment to permit it to autoscale the number of pods when a CPU threshold (80%) is reached:

kubectl autoscale deployment/datastore --max=20 --cpu-percent=80

You should be able to:

kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE

datastore 1 1 1 0 2m kubectl get replicasets NAME DESIRED CURRENT READY AGE

datastore-3517606568 1 1 0 2m kubectl get pods NAME READY STATUS RESTARTS

datastore-3517606568-8ffn2 1/1 Running 0

You may also observe this Deployment using Cloud Console:

Cloud Console: Kubernetes Engine “Workloads”

We now need to add a Service ‘veneer’ to this deployment and expose the result using an Ingress on an HTTP/S Load Balancer. We can do this simply from the command-line:

kubectl expose deployment/datastore \

--type=NodePort \

--port=9999 \

--target-port=8080

Finally, we can create an Ingress. This will create an HTTP/S Load-Balancer on GCP that points to our service and … all being well… should permit us to access our former Flex-only service as a newly-deployed GKE-service.

Create a file (I’m using datastore-ingress.yaml):

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

name: datastore

spec:

backend:

serviceName: datastore

servicePort: 9999

and then create the Ingress:

kubectl create --filename=datastore-ingress.yaml

Once the Ingress reports an external address, you should be able to access the service using it. In my case (yours will be different) the public IP address is 107.178.252.137:

kubectl get ingress/datastore NAME HOSTS ADDRESS PORTS AGE

datastore * 107.178.252.137 80 6m

You can view the Ingress multiple ways:

Cloud Console: Kubernetes Engine “Load Balancing”

You can also check “Network Services” where you will see (probably) 2 HTTP/S Load-Balancers created. One was created by App Engine Flex (and customarily called “aef-um”. The second was created by GKE by the Ingress (and customarily called something “k8s-um-default….”):

Cloud Console: Network Services “Load Balancing”

NB the IP:Port defined here matches (as you would expect) the IP address provided by describing the datastore Ingress. Your IP address and other details will be different but your IP address is the you should use:

http://107.178.252.137/

Working!

We took the image created by the App Engine Flex deployment and reused it in a Deployment to GKE. Once deployed, we exposed the Deployment as an HTTP/S Load-Balancer using GKE’s Ingress.

We now have 2 deployments of the same container and can load-test them to see how each service performs under load.

Aside App Engine Flex deployed the container image from Container Registry to Compute Engine VMs auto-scaled by a Managed Instance Group and exposed through an HTTP/S Load-Balancer. Kubernetes Engine deployed the container image from Container Registry to Compute Engine VMs auto-scaled by a Managed Instance Group and exposed through an HTTP/S Load-Balancer. The underlying resources (correctly) are the same for both services. Both services use declarative (intentional) configuration. An important difference between the services is that App Engine Flex biases automation to Google’s control whereas Kubernetes Engine requires more oversight by the customer. Kubernetes Engine is evolving more rapidly and is adding more powerful automation. A subtle difference is that Flex uses containers as a means to an end. Customarily, users of Flex could ignore that containers are being employed because this is done behind the scenes. Kubernetes Engine — as the name suggests — is predicated on containers and is explicitly designed as a tool that facilitates the management of services built from containers. With Flex, a service is always n-containers of one type. With Kubernetes Engine, a service comprises m-pods and the pods may themselves comprise p-containers.

Load-testing

The more astute than I among you, will realize that, as we consider load-testing, I’ve introduced a discrepancy. While App Engine Flex is behind TLS, the Kubernetes Engine App is (currently) not. Let’s fix that!

There are many ways to achieve this goal but this approach is the easiest. We will need to create a certificate, upload this as a Secret to GKE and then revise the Ingress to reference it. I assume you have a domain that you may use. I will use Cloud DNS.

Let’s start by deciding upon a name for the GKE app. I will use “gke.dazwilkin.com” and I alias this to the IP address of the HTTP/S Load-Balancer created by the GKE Ingress:

Cloud Console: Network Services “Cloud DNS”

export NAME=[YOUR-DNS-NAME] // Mine is gke.dazwilkin.com mkdir -p $HOME/Projects/$PROJECT/certs

cd $HOME/Projects/$PROJECT/certs

If you’re using Google Cloud DNS, your DNS changes will be most quickly accessible through Google’s Public DNS and you may query it with:

nslookup ${NAME} 8.8.8.8 Server: 8.8.8.8

Address: 8.8.8.8#53 Non-authoritative answer:

Name: ${NAME}

Address: ${IP} // The IP address of the HTTP/S Load-Balancer

Now that we have an DNS name, we can use openssl to generate a certificate to test. This is *not* what you should do in production. I recommend Let’s Encrypt or other cert authority.

openssl req \

-x509 \

-nodes \

-days 365 \

-newkey rsa:2048 \

-keyout ${NAME}.key \

-out ${NAME}.crt \

-subj '/CN=${NAME}'

Then we can use this bash goodness to base64 encode and then upload the ‘key’ and ‘crt’ files as a GKE Secret named as a our DNS name:

echo ""

apiVersion: v1

kind: Secret

metadata:

name: ${NAME}

data:

tls.crt: `base64 --wrap 0 ./${NAME}.crt`

tls.key: `base64 --wrap 0 ./${NAME}.key`

"" | kubectl create --filename -

And, lastly, we need to tweak the Ingress to include the certificate by referencing the Secret:

Open your Ingress config (I’m using ‘datastore-ingress.yaml’), replace $NAME with your DNS name, save it:

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

name: datastore

spec:

tls:

- secretName: ${NAME}

backend:

serviceName: datastore

servicePort: 9999

And then — you’ll get a warning but you may ignore it —

kubectl apply --filename=datastore-ingress.yaml

If you then refresh the Cloud Console page showing the Load-Balancers. You should see an “HTTPS” frontend added to the GKE Load-Balancer:

Cloud Console: Network Services “Load Balancing”

All being well, you should now be able to access the Exemplar solution on GKE via TLS:

curl --insecure https://${NAME}

OK. Let’s put some load on each of these services and see what happens. You may use Apache’s benchmarking tool “ab” but, I’m going to use ‘wrk’ (link):



git clone

cd wrk && make cd $HOME/Projects/$PROJECT/git clone https://github.com/wg/wrk.git cd wrk && make ./wrk

Usage: wrk <options> <url>

Options:

-c, --connections <N> Connections to keep open

-d, --duration <T> Duration of test

-t, --threads <N> Number of threads to use



-s, --script <S> Load Lua script file

-H, --header <H> Add header to request

--latency Print latency statistics

--timeout <T> Socket/request timeout

-v, --version Print version details



Numeric arguments may include a SI unit (1k, 1M, 1G)

Time arguments may include a time unit (2s, 2m, 2h)

Let’s start with App Engine Flex:

As with any load-test, it pays to run the same test several times. Here’s my first set of results. The top-line is (for this run) 650 RPS (μ=380ms δ=77ms)



10 threads and 250 connections

Thread Stats Avg Stdev Max +/- Stdev

Latency 380.96ms 76.89ms 1.03s 83.37%

Req/Sec 66.71 31.44 240.00 64.34%

39336 requests in 1.00m, 71.94MB read

Requests/sec: 654.64

Transfer/sec: 1.20MB Running 1m test @ https://${PROJECT}.appspot.com/ 10 threads and 250 connectionsThread Stats Avg Stdev Max +/- StdevLatency 380.96ms 76.89ms 1.03s 83.37%Req/Sec 66.71 31.44 240.00 64.34%39336 requests in 1.00m, 71.94MB readRequests/sec: 654.64Transfer/sec: 1.20MB

And then, the only difference in the command for GKE is to use ${NAME}:

And the results. The top-line is (for this run) 1420 RPS (μ=175ms δ=20ms):



10 threads and 250 connections

Thread Stats Avg Stdev Max +/- Stdev

Latency 175.39ms 19.73ms 1.17s 82.62%

Req/Sec 143.15 27.74 232.00 68.37%

85459 requests in 1.00m, 62.99MB read

Requests/sec: 1422.64

Transfer/sec: 1.05MB Running 1m test @ https://flex-or-gke.dazwilkin.com/ 10 threads and 250 connectionsThread Stats Avg Stdev Max +/- StdevLatency 175.39ms 19.73ms 1.17s 82.62%Req/Sec 143.15 27.74 232.00 68.37%85459 requests in 1.00m, 62.99MB readRequests/sec: 1422.64Transfer/sec: 1.05MB

For this first run, GKE has double the throughput of Flex (half the latency) and a much (5x) tighter distribution of latency.

Ran the tests a second time and grabbed monitoring…10 minutes, 25 threads and 250 connections…

App Engine Flex achieved 1740 RPS (μ=150ms δ=80ms)



--threads=25 \

--connections=250 \

--duration=600s \

https://${PROJECT}.appspot.com/ ./wrk \--threads=25 \--connections=250 \--duration=600s \

25 threads and 250 connections

Thread Stats Avg Stdev Max +/- Stdev

Latency 148.86ms 82.94ms 1.98s 82.34%

Req/Sec 70.52 29.78 130.00 59.61%

1045679 requests in 10.00m, 1.87GB read

Requests/sec: 1742.51

Transfer/sec: 3.19MB Running 10m test @ https://{$PROJECT}.appspot.com/ 25 threads and 250 connectionsThread Stats Avg Stdev Max +/- StdevLatency 148.86ms 82.94ms 1.98s 82.34%Req/Sec 70.52 29.78 130.00 59.61%1045679 requests in 10.00m, 1.87GB readRequests/sec: 1742.51Transfer/sec: 3.19MB

Stackdriver Monitoring: App Engine

It’s non-trivial (for me?) to produce equivalent metrics for GKE but, here’s my best effort. GKE achieved 1350 RPS (μ=180ms δ=20ms):



--threads=25 \

--connections=250 \

--duration=600s \

https://${NAME}/ ./wrk \--threads=25 \--connections=250 \--duration=600s \

25 threads and 250 connections

Thread Stats Avg Stdev Max +/- Stdev

Latency 184.78ms 22.99ms 1.27s 86.09%

Req/Sec 54.26 16.20 101.00 78.38%

812839 requests in 10.00m, 599.13MB read

Socket errors: connect 0, read 1, write 0, timeout 0

Requests/sec: 1354.51

Transfer/sec: 1.00MB Running 10m test @ https://${NAME}/ 25 threads and 250 connectionsThread Stats Avg Stdev Max +/- StdevLatency 184.78ms 22.99ms 1.27s 86.09%Req/Sec 54.26 16.20 101.00 78.38%812839 requests in 10.00m, 599.13MB readSocket errors: connect 0, read 1, write 0, timeout 0Requests/sec: 1354.51Transfer/sec: 1.00MB

Cloud Console: Network Services “Load Balancing”

Stackdriver custom dashboard

Cloud Console: Compute Engine “VM Instances”

Cloud Console: Kubernetes Engine “Workloads”

With GKE I’m receiving notifications from the Load-Balancer that “Usage is at capacity” which surprises me. GKE is not adding nodes to the pool (which I think it should) … ah, I’m just impatient… bumping VMs to 3 and Pods to 8:

Conclusions

It is practical to migrate an App Engine Flex deployment to GKE

In this case (!) Flex achieved greater throughput than GKE.

The increased velocity appears due to the rapidity with which App Engine is able to signal auto-scaling events; GKE scales pods promptly within an existing cluster of nodes but slightly more slowly to scale up the number of nodes.

App Engine and GKE share fundamental GCP resources including the HTTP/S Load-Balancer service and Managed Infrastructure Groups auto-scaling.

For the same load, using the same VM size (1 vCPU and 1GB RAM): App Engine Flex scaled to 6 containers on 6 instances VMs (1 instance/VM ); GKE scaled to 10 pods (1 container/pod) on 3 VMs (50%).

I’m still working on better ways to provide comparable monitoring.

Kubernetes UI

There’s an interim hack to access the Kubernetes Dashboard. Add a final “/” to the URL that you’re redirected to by the proxy. Then:

Kubernetes Dashboard

The Dashboard provides a Kubernetes-specific UI and I’m a fan.

Here you can see the Cluster is putting pressure on GKE to autoscale… 6/8 pods and blocking on CPU:

Kubernetes Dashboard: Waiting for Cluster Autoscaling

Kubernetes Dashboard: Scaled

In this second snapshot the Cluster has scaled (now at 3 GCE VMs) and able to sustain the load with 8 pods.

I’m going to investigate but — I assume — this mean 80% of the cluster’s (aggregate) CPU which corresponds to the (Horizontal) Autoscale requirement to scale on 80% CPU:

Kubernetes Dashboard: CPU usage

And:

Kubernetes Dashboard: Memory usage

Stackdriver

Last word: I’m trying to find an equivalent way to present the measurement of the GKE service:","['kubernetes', 'create', 'gke', 'deployment', 'cloud', 'flex', 'image', 'engine', 'service', 'app']","App Engine Flex || Kubernetes Engine — ?
Fortunately, the App Engine Flex documentation includes a sample app that we can use (GitHub here).
Please review your App Engine Flex deployment to determine the name of the container image that App Engine Flex created for you.
An important difference between the services is that App Engine Flex biases automation to Google’s control whereas Kubernetes Engine requires more oversight by the customer.
While App Engine Flex is behind TLS, the Kubernetes Engine App is (currently) not.",en,['Daz Wilkin'],2018-04-12 19:16:24.026000+00:00,"{'Google Container Engine', 'Google Cloud Platform', 'Google App Engine'}","{'https://miro.medium.com/max/2042/1*RKUMoaHG9WmNrihZ6m1CMA.png', 'https://miro.medium.com/max/60/1*hWHNUEjSHTqlmUa8zEn75g.png?q=20', 'https://miro.medium.com/max/1200/1*WNlcZaxUQ_gLnmvSOFDGIQ.png', 'https://miro.medium.com/max/60/1*PhR7sIMQ7ZV3IZ2qqVQCug.png?q=20', 'https://miro.medium.com/max/2950/1*CYIXDol_ENAXQdJlvOOIiA.png', 'https://miro.medium.com/max/60/1*KnmlRfnyRlxwaVMxVJ2b4A.png?q=20', 'https://miro.medium.com/max/60/1*CYIXDol_ENAXQdJlvOOIiA.png?q=20', 'https://miro.medium.com/max/2946/1*x32hzAs7aGmBsR2PltOA1Q.png', 'https://miro.medium.com/max/1982/1*s0T_aWcYprlWFp3UlrpUhw.png', 'https://miro.medium.com/max/60/1*oGZgs1z2gphBbnaMuslFbA.png?q=20', 'https://miro.medium.com/max/988/1*fo-FbZ1h4L5hqlp_4nKxdA.png', 'https://miro.medium.com/max/60/1*s0T_aWcYprlWFp3UlrpUhw.png?q=20', 'https://miro.medium.com/max/60/1*hzlrp17PwOlHY2DglrRdHw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/2950/1*KnmlRfnyRlxwaVMxVJ2b4A.png', 'https://miro.medium.com/max/2950/1*PhR7sIMQ7ZV3IZ2qqVQCug.png', 'https://miro.medium.com/max/2942/1*KuuEZZnZXQ-lb9msQ6wEcA.png', 'https://miro.medium.com/max/60/1*tkRanQ06g4RjUZFdnptJ-A.png?q=20', 'https://miro.medium.com/max/60/1*_j1g8EXuLz977flDimEE3g.png?q=20', 'https://miro.medium.com/max/2946/1*5mrZGCYHuskPSwZbHpMRSw.png', 'https://miro.medium.com/max/60/1*RKUMoaHG9WmNrihZ6m1CMA.png?q=20', 'https://miro.medium.com/max/2946/1*7r0T5OiJvofG3SbsmGbv9Q.png', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/fit/c/96/96/0*7eoQ4m8uoWX5NxvY.', 'https://miro.medium.com/max/60/1*5mrZGCYHuskPSwZbHpMRSw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/max/60/1*fnRBmiqUHwquZ6tPZoHaiA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/60/1*AHF-APh-RpwqMsnU_j8C7w.png?q=20', 'https://miro.medium.com/max/60/1*iukR_2stYKKbcdfuz0wnqw.png?q=20', 'https://miro.medium.com/max/60/1*WNlcZaxUQ_gLnmvSOFDGIQ.png?q=20', 'https://miro.medium.com/max/60/1*ChloInGHNG7ib1g7edgaRw.png?q=20', 'https://miro.medium.com/max/2950/1*hWHNUEjSHTqlmUa8zEn75g.png', 'https://miro.medium.com/max/60/1*x32hzAs7aGmBsR2PltOA1Q.png?q=20', 'https://miro.medium.com/max/2944/1*hzlrp17PwOlHY2DglrRdHw.png', 'https://miro.medium.com/max/2420/1*CMs_SNnT2TiZtwSl-L9jMQ.png', 'https://miro.medium.com/max/2946/1*-wg3nIMdaxOTMOhjPDnn5A.png', 'https://miro.medium.com/max/60/1*KuuEZZnZXQ-lb9msQ6wEcA.png?q=20', 'https://miro.medium.com/max/2944/1*1A9GTCrkzJVIpl_fE0wVnA.png', 'https://miro.medium.com/max/1170/1*iukR_2stYKKbcdfuz0wnqw.png', 'https://miro.medium.com/max/2922/1*WNlcZaxUQ_gLnmvSOFDGIQ.png', 'https://miro.medium.com/max/60/1*fo-FbZ1h4L5hqlp_4nKxdA.png?q=20', 'https://miro.medium.com/max/60/1*7r0T5OiJvofG3SbsmGbv9Q.png?q=20', 'https://miro.medium.com/max/60/1*1A9GTCrkzJVIpl_fE0wVnA.png?q=20', 'https://miro.medium.com/max/60/1*CMs_SNnT2TiZtwSl-L9jMQ.png?q=20', 'https://miro.medium.com/max/42/1*XBsfTfV2FLpMLre4XqWLbw.png?q=20', 'https://miro.medium.com/max/2944/1*_j1g8EXuLz977flDimEE3g.png', 'https://miro.medium.com/max/1254/1*XBsfTfV2FLpMLre4XqWLbw.png', 'https://miro.medium.com/fit/c/160/160/0*7eoQ4m8uoWX5NxvY.', 'https://miro.medium.com/max/878/1*oGZgs1z2gphBbnaMuslFbA.png', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/2948/1*tkRanQ06g4RjUZFdnptJ-A.png', 'https://miro.medium.com/max/60/1*JbByA2vqvJX32KyIHxMMUQ.png?q=20', 'https://miro.medium.com/max/60/1*-wg3nIMdaxOTMOhjPDnn5A.png?q=20', 'https://miro.medium.com/max/854/1*JbByA2vqvJX32KyIHxMMUQ.png', 'https://miro.medium.com/max/2302/1*fnRBmiqUHwquZ6tPZoHaiA.png', 'https://miro.medium.com/max/2320/1*AHF-APh-RpwqMsnU_j8C7w.png', 'https://miro.medium.com/max/60/1*X_tn1QY0_pgA6XwnGB0-Mw.png?q=20', 'https://miro.medium.com/max/1200/1*X_tn1QY0_pgA6XwnGB0-Mw.png', 'https://miro.medium.com/max/980/1*ChloInGHNG7ib1g7edgaRw.png'}",2020-03-05 00:17:47.399505,1.540147304534912
https://medium.com/@don.jayamanne/inspecting-variables-in-python-apps-using-python-debugger-in-vs-code-without-any-code-changes-7e5432d8dd39,Inspecting variables in Python Apps using Python debugger in VS Code without any code changes,"Over the next couple of weeks/months I’ll be posting a couple of articles on getting started with debugging in VS Code, along with a few advanced topics. Today I’d like to focus on “Log Points” when debugging Python code using the experimental debugger in the Python extension in VS Code. Please note at the time of writing this article, the experimental debugger is still considered experimental, hence the name. However you can still try this out, and within a few weeks, this will be the default debugger for the Python Extension in VS Code.

Say goodbye to print statements when debugging:

As mentioned I’ll be covering debugging of Python Apps using “Log Points” in VS Code. Quite simply put log points make it unnecessary for you to modify the code to write print statements.

I’m using a Mac, however this will work on Windows and Linux as well. For the purpose of this article I’ll be debugging a Flask Application documented on the VS Code Docs website, found here.

Step 1: Create a Flask application

TL;DR:

Open an empty directory in VS Code

Clone the repo into this directory (`git clone https://github.com/Microsoft/python-sample-vscode-flask-tutorial.git .`)

Create a virtual environment (‘virtualenv — python=python3.6 venv’)

Select the above `Python Interpreter`

Install the requirements into the newly created environment.

Step 2: Change ‘.vscode/launch.json’ to use the experimental debugger

Open the file .vscode/lauch.json and add a new configuration for debugging of Flask Applications using the experimental debugger.

Open .vscode/launch.json in VS Code

in VS Code Select Add Configuration on the bottom right of the screen

on the bottom right of the screen Select the item Python Experimental Flask

Change the value of ""FLASK_APP"":""app.py"" to ""FLASK_APP"":""HelloFlask/app.py""

Step 3: Start debugging

Select Flask from the debug configuration

from the debug configuration Start debugging

Confirm the Flask Application is running by launching the browser and navigating to http://localhost:5000

Step 4: Adding log points to the route ‘hello’

Open the file HelloFlask/views.py

Add a log point to the route hello in line 22

in line Add the message Lets inspect the route to see the value of now {now} and name {name}

When you navigate to the the route http://localhost:5000/hello/Don you’ll see messages logged in the debug console.

you’ll see messages logged in the debug console. I.e you can view values of variables without having to set breakpoints.

You can also add log points to templates.

to templates. I.e. log points makes debug print statements a thing of the past.

For the latest on Python Extension in VS Code, go to:

Finally, if you come across any issues or have any suggestions related to the Python Extension in VS Code, please file an issue on our GitHub repo.","['inspecting', 'debugger', 'debug', 'variables', 'log', 'python', 'changes', 'points', 'flask', 'vs', 'experimental', 'apps', 'debugging', 'using', 'code']","Today I’d like to focus on “Log Points” when debugging Python code using the experimental debugger in the Python extension in VS Code.
However you can still try this out, and within a few weeks, this will be the default debugger for the Python Extension in VS Code.
Say goodbye to print statements when debugging:As mentioned I’ll be covering debugging of Python Apps using “Log Points” in VS Code.
For the purpose of this article I’ll be debugging a Flask Application documented on the VS Code Docs website, found here.
For the latest on Python Extension in VS Code, go to:Finally, if you come across any issues or have any suggestions related to the Python Extension in VS Code, please file an issue on our GitHub repo.",en,['Don Thilaka Jayamanne'],2018-06-26 07:31:27.654000+00:00,"{'Debugging', 'Python', 'Vscode'}","{'https://miro.medium.com/freeze/max/60/1*WcmWMlOxmaGZ4QNFNBTp5w.gif?q=20', 'https://miro.medium.com/max/2048/1*MZI8B1FDrbCMq-6TYSE2ug.gif', 'https://miro.medium.com/max/2048/1*WcmWMlOxmaGZ4QNFNBTp5w.gif', 'https://miro.medium.com/freeze/max/1024/1*CdA-croOqX68IiCfC6EcjA.gif', 'https://miro.medium.com/max/2048/1*CdA-croOqX68IiCfC6EcjA.gif', 'https://miro.medium.com/fit/c/96/96/1*A3V98eEE4xUvP5oStm0gvg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*R7bsiWBhxYS8oyX68bn7kQ.jpeg', 'https://miro.medium.com/fit/c/80/80/0*_HP9AhbUGIJZcw95.', 'https://miro.medium.com/freeze/max/60/1*MZI8B1FDrbCMq-6TYSE2ug.gif?q=20', 'https://miro.medium.com/max/2048/1*M83uvNe7E53373pF60rukA.gif', 'https://miro.medium.com/freeze/max/60/1*M83uvNe7E53373pF60rukA.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*6aNPUhlYQ2_zTR5ZehfV9A.jpeg', 'https://miro.medium.com/fit/c/160/160/1*A3V98eEE4xUvP5oStm0gvg.jpeg', 'https://miro.medium.com/freeze/max/60/1*CdA-croOqX68IiCfC6EcjA.gif?q=20'}",2020-03-05 00:17:49.026255,1.625758171081543
https://towardsdatascience.com/accessing-google-spreadsheet-data-using-python-90a5bc214fd2,Accessing Google Spreadsheet Data using Python,"If you’re building a simple internal app and you might probably be thinking that ‘I’m going to need a database now right!’. Well, Not so fast.

As you all are familiar with importing, exporting and manipulating comma separate files (CSV) using Python, Hereby in this article I’m going to show you the step by step guide to access Google Spreadsheets on the cloud using Python.

As the very first thing, go to Google API Manager by simply googling it and go to https://console.developers.google.com/

To kick things off first create a new project.

Figure 1.0: Creating a new Project

I’ll name my project as ‘Telemedicine’ since we will be working with a spreadsheet which includes all the tweets related to Telemedicine hashtags which I extracted earlier (Click here to see how I extracted tweets using hashtags). Define a suitable project name according to your dataset on the cloud then click CREATE to initiate the project. (You don’t have to worry about the Location* below the project name)

Figure 1.0: Define a suitable project name according to your dataset

OKAY. The first part is done, Now go to API Library and search for Google Drive.

Figure 2.0: Search for Google Drive in API Library

Then add Google Drive API to our project which will allow us to access spreadsheet inside of Google Sheets for our account.

Figure 2.1: Click on Enable

Once that’s added, we need to create some credentials to access the API so click on Add Credentials on the next screen you see after enabling the API.

Figure 3.0: Click on Create Credentials to initialize credentials to access the API

Since we’ll be accessing the API using a web server, We’ll add the Web Server option on this page and give access to Application Data and tell them that you’re not running your application on either GCE or GAE by selecting the option ‘No, I’m not using them’ then click on the button below.

Figure 3.1: Creating credentials

Next, we will create a service account named Employees and assigned it the role Project Editor which will allow it to access and edit all the data within the API. Clicking continue will generate a JSON file that I will rename and add it to the project as Telemedicine_secret.json.

Figure 3.2: Creating service account credentials

Then open the JSON file in a text editor (I prefer Atom) :)

Inside the file, you can locate an email address property called “client_email”, if we copy that and take it over to our spreadsheet on the cloud, we can share that particular spreadsheet with the email address we provide to give us access to it from the API.","['create', 'credentials', 'project', 'accessing', 'access', 'python', 'click', 'data', 'api', 'spreadsheet', 'google', 'add', 'using']","As you all are familiar with importing, exporting and manipulating comma separate files (CSV) using Python, Hereby in this article I’m going to show you the step by step guide to access Google Spreadsheets on the cloud using Python.
As the very first thing, go to Google API Manager by simply googling it and go to https://console.developers.google.com/To kick things off first create a new project.
Define a suitable project name according to your dataset on the cloud then click CREATE to initiate the project.
Figure 2.0: Search for Google Drive in API LibraryThen add Google Drive API to our project which will allow us to access spreadsheet inside of Google Sheets for our account.
Clicking continue will generate a JSON file that I will rename and add it to the project as Telemedicine_secret.json.",en,['Dilan K Jayasekara'],2019-04-14 14:42:43.450000+00:00,"{'Spreadsheets', 'Python', 'Google', 'Data Analysis', 'Data'}","{'https://miro.medium.com/max/60/1*Izo9A50RHKjXGsPO-x0Hxw.png?q=20', 'https://miro.medium.com/max/60/1*vKGuojn7KZV7HAvPY95_DA.png?q=20', 'https://miro.medium.com/max/1190/1*WjgAJdiYDGo4XZmz-mRCCw.png', 'https://miro.medium.com/max/1030/1*JX-kGwwY6hSLetlxtE8O5Q.png', 'https://miro.medium.com/max/2556/1*XCyX6Uwk6_TH6_RGT_2CjA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*ljK1nMJs0KXbniPC7puEhg.png?q=20', 'https://miro.medium.com/max/60/1*DketuH_NK8MT-53sRrI2Dg.png?q=20', 'https://miro.medium.com/max/2552/1*vKGuojn7KZV7HAvPY95_DA.png', 'https://miro.medium.com/max/42/1*rD4PBG00fs7i8QYotRjG5g.png?q=20', 'https://miro.medium.com/max/1112/1*rD4PBG00fs7i8QYotRjG5g.png', 'https://miro.medium.com/max/10944/0*6sx8997_nDUEf311', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*-r2g4u3jSad29L-qcPKadA.png?q=20', 'https://miro.medium.com/max/670/1*4Dc9yq-MSZgRbIrq_2lZZw.jpeg', 'https://miro.medium.com/max/1340/1*4Dc9yq-MSZgRbIrq_2lZZw.jpeg', 'https://miro.medium.com/max/60/1*XCyX6Uwk6_TH6_RGT_2CjA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/50/1*r2mc7ZZ_4I1XatsnZCqTKA.png?q=20', 'https://miro.medium.com/max/1280/1*SoMi-KUZIK-jLEV7ju1Jmw.png', 'https://miro.medium.com/max/60/1*QfSJxeF3fpb7DBtuXRyiHQ.png?q=20', 'https://miro.medium.com/max/1114/1*h5O1aIkGOp82F7uD3Us8bw.png', 'https://miro.medium.com/max/1448/1*kC0GuhWNFkrDGMEvcjsDrQ.png', 'https://miro.medium.com/max/1300/1*XVy7hsGwyiNwSx-7hQXL8A.png', 'https://miro.medium.com/fit/c/160/160/1*8ycZ78SthE_JFIoN6BsahA@2x.jpeg', 'https://miro.medium.com/max/60/1*kC0GuhWNFkrDGMEvcjsDrQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*4Dc9yq-MSZgRbIrq_2lZZw.jpeg?q=20', 'https://miro.medium.com/max/624/1*Izo9A50RHKjXGsPO-x0Hxw.png', 'https://miro.medium.com/max/60/1*z1m8AcGG-904m5U8uUk8Ng.png?q=20', 'https://miro.medium.com/max/1350/1*r2mc7ZZ_4I1XatsnZCqTKA.png', 'https://miro.medium.com/max/1906/1*QfSJxeF3fpb7DBtuXRyiHQ.png', 'https://miro.medium.com/max/60/1*XVy7hsGwyiNwSx-7hQXL8A.png?q=20', 'https://miro.medium.com/max/2052/1*wiEWWl7Yjji7Bl33_ZoZLQ.png', 'https://miro.medium.com/max/60/1*wiEWWl7Yjji7Bl33_ZoZLQ.png?q=20', 'https://miro.medium.com/max/2560/1*z1m8AcGG-904m5U8uUk8Ng.png', 'https://miro.medium.com/max/60/1*SoMi-KUZIK-jLEV7ju1Jmw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2190/1*-r2g4u3jSad29L-qcPKadA.png', 'https://miro.medium.com/max/60/1*h5O1aIkGOp82F7uD3Us8bw.png?q=20', 'https://miro.medium.com/max/60/0*6sx8997_nDUEf311?q=20', 'https://miro.medium.com/max/60/1*WjgAJdiYDGo4XZmz-mRCCw.png?q=20', 'https://miro.medium.com/max/2440/1*ljK1nMJs0KXbniPC7puEhg.png', 'https://miro.medium.com/fit/c/96/96/1*8ycZ78SthE_JFIoN6BsahA@2x.jpeg', 'https://miro.medium.com/max/60/1*JX-kGwwY6hSLetlxtE8O5Q.png?q=20', 'https://miro.medium.com/max/2548/1*DketuH_NK8MT-53sRrI2Dg.png'}",2020-03-05 00:17:55.425631,6.399375915527344
https://medium.com/@timtech4u/deploying-a-gcp-virtual-machine-instance-with-a-startup-script-fe5431f16e66,Deploying GCP Virtual Machine Instance with a StartUp Script,"This article guides us through deploying Virtual Machines (VM) Instances with a public accessible remote start-up script.

This would be helpful if you need to run a few commands on startup of your virtual machines. For example, update/install packages, perform a cron job etc...

Also, you can always update the remote script contents whenever, without ever having to modify your instance’s configurations.

We’ll be hosting our startup script on Google Cloud Storage, feel free to use whatever you prefer, all we’ll need at the end is a publicly accessible URL to your startup script.

Uploading Startup Script to Google Cloud Storage Bucket

Login to Google Cloud Console , on the Navigation Menu, Click on Storage and then Browser. Let’s go ahead and create a bucket…

Enter a Unique Bucket Name and click on Create. Once done, we can now upload files to our bucket. Uploaded files are called Buckets.

I’ve successfully uploaded my startup script, however, I still need to make the script publicly accessible.

Making Google Cloud Storage Objects Publicly Available

Click on the Permissions tab and Enable Object-level permissions.

We’ll now be able to Edit Permissions on our objects.

Edit Permissions for Object

To make our Object public, Click on Edit Permissions and a New Item:

User: Entity

Name: allUsers

Access: Reader","['machine', 'virtual', 'permissions', 'instance', 'script', 'cloud', 'gcp', 'publicly', 'need', 'click', 'storage', 'startup', 'google', 'deploying']","This article guides us through deploying Virtual Machines (VM) Instances with a public accessible remote start-up script.
This would be helpful if you need to run a few commands on startup of your virtual machines.
We’ll be hosting our startup script on Google Cloud Storage, feel free to use whatever you prefer, all we’ll need at the end is a publicly accessible URL to your startup script.
Uploading Startup Script to Google Cloud Storage BucketLogin to Google Cloud Console , on the Navigation Menu, Click on Storage and then Browser.
I’ve successfully uploaded my startup script, however, I still need to make the script publicly accessible.",en,[],2019-02-20 18:16:26.605000+00:00,"{'Google Cloud Platform', 'Automation', 'Bucket', 'Script', 'Cloud Storage'}","{'https://miro.medium.com/max/60/1*wdvr3_TnvDJnPlk0x22cPw.png?q=20', 'https://miro.medium.com/max/60/1*G2rNw_WT997tTnWMfIlkaw.png?q=20', 'https://miro.medium.com/max/1226/1*1VgRgjaQX3aJcIXmS7l2qw.png', 'https://miro.medium.com/max/52/1*J5DH9GJkXycclg4c4pT01w.png?q=20', 'https://miro.medium.com/max/60/1*4e-qgIHYz7lkdi0CWp26Jg.png?q=20', 'https://miro.medium.com/max/3258/1*G2rNw_WT997tTnWMfIlkaw.png', 'https://miro.medium.com/fit/c/80/80/2*j0KdpT7Y46DJMLnwiVXZ4Q.jpeg', 'https://miro.medium.com/max/1072/1*J5DH9GJkXycclg4c4pT01w.png', 'https://miro.medium.com/max/60/1*SAX_h4piXfGRT99XDwJvZA.png?q=20', 'https://miro.medium.com/max/3840/1*0iVTmilUSENL-KjvPKW3pQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*ofUGmTB2JSCklJzdWAZppA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*rHiMXaCMx-vLi9c88VvFTA.jpeg', 'https://miro.medium.com/fit/c/96/96/1*ofUGmTB2JSCklJzdWAZppA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*MIOg6Hd39cSLugg_kQ7ufw.jpeg', 'https://miro.medium.com/max/3262/1*4e-qgIHYz7lkdi0CWp26Jg.png', 'https://miro.medium.com/max/60/1*f74l0zU0ZvURc74dnvlHng.png?q=20', 'https://miro.medium.com/max/3258/1*wdvr3_TnvDJnPlk0x22cPw.png', 'https://miro.medium.com/max/986/1*SAX_h4piXfGRT99XDwJvZA.png', 'https://miro.medium.com/max/60/1*0iVTmilUSENL-KjvPKW3pQ.jpeg?q=20', 'https://miro.medium.com/max/1200/1*0iVTmilUSENL-KjvPKW3pQ.jpeg', 'https://miro.medium.com/max/46/1*1VgRgjaQX3aJcIXmS7l2qw.png?q=20', 'https://miro.medium.com/max/988/1*f74l0zU0ZvURc74dnvlHng.png'}",2020-03-05 00:17:56.712759,1.2861294746398926
https://medium.com/district-data-labs/a-practical-guide-to-anonymizing-datasets-with-python-faker-ecf15114c9be,A Practical Guide to Anonymizing Datasets with Python & Faker,"A Practical Guide to Anonymizing Datasets with Python & Faker

How Not to Lose Friends and Alienate People

By Benjamin Bengfort

If you want to keep a secret, you must also hide it from yourself. — George Orwell 1984

In order to learn (or teach) data science you need data (surprise!). The best libraries often come with a toy dataset to illustrate examples of how the code works. However, nothing can replace an actual, non-trivial dataset for a tutorial or lesson, because only that can provide for deep and meaningful exploration. Unfortunately, non-trivial datasets can be hard to find for a few reasons, one of which is that many contain personally identifying information (PII).

A possible solution to dealing with PII is to anonymize the dataset by replacing information that would identify a real individual with information about a fake (but similarly behaving or sounding) individual. Unfortunately, this is not as easy at it sounds. A simple mapping of real data to randomized data is not enough, because in order to be used as a stand in for analytical purposes, anonymization must preserve the semantics of the original data. As a result, issues related to entity resolution, like managing duplicates or producing linkable results, frequently come into play.

The good news is that we can take a cue from the database community, who routinely generate simulated data to evaluate the performance of a database system. This community has developed plenty of tools for generating very realistic data for a variety of information types. For this post, I’ll explore using the Faker library to generate a realistic, anonymized dataset that can be utilized for downstream analysis.

The goal: given a target dataset (for example, a CSV file with multiple columns), produce a new dataset such that for each row in the target, the anonymized dataset does not contain any personally identifying information. The anonymized dataset should have the same amount of data and maintain its analytical value. As shown in the figure below, one possible transformation simply maps original information to fake and therefore anonymous information but maintains the same overall structure.

Anonymizing CSV Data

This post will study a simple example that requires the anonymization of only two fields: full name and email. Sounds easy, right? The difficulty is in preserving the semantic relationships and distributions in our target dataset so that we can hand it off to be analyzed or mined for interesting patterns. What happens if there are multiple rows per user? Since CSV data is naturally denormalized (e.g. contains redundant data like rows with repeated full names and emails), we will need to maintain a mapping of profile information.

Note: Since we’re going to be using Python 2.7 in this example, you’ll need to install the unicodecsv module with pip . Additionally you'll need the Faker library:

$ pip install fake-factory unicodecsv

The following example shows a simple anonymize_rows function that maintains this mapping and also shows how to generate data with Faker. We'll also go a step further by reading the data from a source CSV file and writing the anonymized data to a target CSV file. The end result is a file very similar in terms of length, row order, and fields, except that the names and emails have been replaced with fake names and emails.

import unicodecsv as csv

from faker import Factory

from collections import defaultdict def anonymize_rows(rows):

""""""

Rows is an iterable of dictionaries that contain name and

email fields that need to be anonymized.

""""""

# Load the faker and its providers

faker = Factory.create() # Create mappings of names & emails to faked names & emails.

names = defaultdict(faker.name)

emails = defaultdict(faker.email) # Iterate over the rows and yield anonymized rows.

for row in rows:

# Replace the name and email fields with faked fields.

row['name'] = names[row['name']]

row['email'] = emails[row['email']] # Yield the row back to the caller

yield row

def anonymize(source, target):

""""""

The source argument is a path to a CSV file containing data to anonymize,

while target is a path to write the anonymized CSV data to.

""""""

with open(source, 'rU') as f:

with open(target, 'w') as o:

# Use the DictReader to easily extract fields

reader = csv.DictReader(f)

writer = csv.DictWriter(o, reader.fieldnames) # Read and anonymize data, writing to target file.

for row in anonymize_rows(reader):

writer.writerow(row)

The entry point for this code is the anonymize function itself. It takes as input the path to two files: the source , where the original data is held in CSV form and target , a path to write out the anonymized data to. Both of these paths are opened for reading and writing respectively. The unicodecsv module is used to read and parse each row, transforming them into Python dictionaries. Those dictionaries are passed into the anonymize_rows function, which transforms and yields each row to be written by the CSV writer to disk.

The anonymize_rows function takes any iterable of dictionaries which contain name and email keys. It loads the fake factory using Factory.create — a class function that loads various providers with methods that generate fake data (more on this later). We then create two defaultdict instances to map real names to fake names and real emails to fake emails.

The Python collections module provides defaultdict , which is similar to a regular dict except that if the key does not exist in the dictionary, a default value is supplied by the callable passed in at instantiation. For example, d = defaultdict(int) would provide a default value of 0 for every key not already in the dictionary. Therefore when we use defaultdict(faker.name) we're saying that for every key not in the dictionary, create a fake name (and similar for email). This allows us to generate a mapping of real data to fake data, and to make sure that the real value always maps to the same fake value.

From there, we simply iterate through all the rows, replacing data as necessary. If our target CSV file looked like this (imagine clickstream data from an email marketing campaign):

name,email,value,time,ipaddr

James Hinglee,jhinglee@gmail.com,a,1446288248,202.12.32.123

Nancy Smithfield,unicorns4life@yahoo.com,b,1446288250,67.212.123.201

J. Hinglee,jhinglee@gmail.com,b,1446288271,202.12.32.123

…it would be transformed into something like:

Mr. Sharif Lehner,keion.hilll@gmail.com,a,1446288248,202.12.32.123

Webster Kulas,nienow.finnegan@gmail.com,b,1446288250,67.212.123.201

Maceo Turner MD,keion.hilll@gmail.com,b,1446288271,202.12.32.123

We now have a new wrangling tool in our toolbox that will allow us to transform CSVs with name and email fields into anonymized datasets! This naturally leads us to the question: what else can we anonymize?

Generating Fake Data

There are two third-party libraries for generating fake data with Python that come up on Google search results: Faker by @deepthawtz and Fake Factory by @joke2k, which is also called “Faker”. Faker provides anonymization for user profile data, which is completely generated on a per-instance basis. Fake Factory (used in the example above) uses a providers approach to load many different fake data generators in multiple languages. I typically prefer Fake Factory over Faker because it has multiple language support and a wider array of fake data generators. Next we’ll explore Fake Factory in detail (for the rest of this post, when I refer to Faker, I’m referring to Fake Factory).

The primary interface that Faker provides is called a Generator . Generators are a collection of Provider instances which are responsible for formatting random data for a particular domain. Generators also provide a wrapper around the random module, and allow you to set the random seed and other operations. While you could theoretically instantiate your own Generator with your own providers, Faker provides a Factory to automatically load all the providers on your behalf:

>>> from faker import Factory

>>> fake = Factory.create()

If you inspect the fake object, you'll see around 158 methods (at the time of this writing), all of which generate fake data. Try the fake.credit_card_number() , fake.military_ship() , and fake.hex_color() methods to name a few, just to get a sense of the variety of generators that exist.

Importantly, providers can also be localized using a language code. This is probably the best reason to use the Factory object — to ensure that localized providers, or subsets of providers, are loaded correctly. For example, to load the French localization:

>>> fake = Factory.create('fr_FR')

>>> fake.catch_phrase_verb() u""d'atteindre vos buts""

And for fun, some Chinese:

>>> fake = Factory.create('cn_ZH')

>>> print fake.company() u""快讯科技有限公司""

The Faker library has the most comprehensive set of data generators I’ve ever encountered for a variety of domains. Unfortunately there is no single provider listing; the best way to explore all the providers in detail is simply to look at the providers package on GitHub.

Creating A Provider

Although the Faker library has a comprehensive array of providers, occasionally you need a domain specific fake data generator. In order to add a custom provider, you will need to subclass the BaseProvider and expose custom faker methods as class methods using the @classmethod decorator. One very easy approach is to create a set of random data you'd like to expose, and simply randomly select it:

from faker.providers import BaseProvider class OceanProvider(BaseProvider): __provider__ = ""ocean""

__lang__ = ""en_US"" oceans = [

u'Atlantic', u'Pacific', u'Indian', u'Arctic', u'Southern',

] @classmethod

def ocean(cls):

return cls.random_element(cls.oceans)

In order to change the likelihood or distribution with which oceans are selected, simply add duplicates to the oceans list so that each name has the probability of selection that you'd like. Then add your provider to the Faker object:

>>> fake = Factory.create()

>>> fake.add_provider(OceanProvider)

>>> fake.ocean() u'Indian'

In routine data wrangling operations, you can create a package structure with localization similar to Faker’s and load things on demand. Don’t forget — if you come up with a generic provider that may be useful to many people, submit it back as a pull request!

Maintaining Data Quality

Now that we understand the wide variety of fake data we can generate, let’s get back to our original example of creating user profile data with just name and email address. First, if you look at the results in the Anonymizing section above, we can make a few observations:

Pro : exact duplicates of name and email are maintained via the mapping.

: exact duplicates of name and email are maintained via the mapping. Pro : our user profiles are now fake data and PII is protected.

: our user profiles are now fake data and PII is protected. Con : the name and the email are weird and don’t match.

: the name and the email are weird and don’t match. Con : fuzzy duplicates (e.g. J. Smith vs. John Smith) are blown away.

: fuzzy duplicates (e.g. J. Smith vs. John Smith) are blown away. Con: all the domains are “free email” like Yahoo and Gmail.

Basically we want to improve our user profile to include email addresses that are similar to the names (or a non-name based username), and we want to ensure that the domains are a bit more realistic for work addresses. We also want to include aliases, nicknames, or different versions of the name. Faker does include a profile provider:

>>> fake.simple_profile() u'{

""username"": ""autumn.weissnat"",

""name"": ""Jalyn Crona"",

""birthdate"": ""1981-01-29"",

""sex"": ""F"",

""address"": ""Unit 2875 Box 1477

DPO AE 18742-1954"",

""mail"": ""zollie.schamberger@hotmail.com""

}'

But as you can see, it suffers from the same problem. In this section, we’ll explore different techniques that allow us to modify our fake data generation such that it matches the distributions we’re seeing in the original data set. In particular we’ll deal with the domain, create more realistic fake profiles, and add duplicates to our data set with fuzzy matching.

Domain Distribution

One idea to maintain the distribution of domains is to do a first pass over the data and create a mapping of real domain to fake domain. Moreover, many domains like gmail.com can be whitelisted and mapped directly to themselves (we just need a fake username). Additionally, we can also preserve capitalization and spelling via this method, e.g. “Gmail.com” and “GMAIL.com” which might be important for data sets that have been entered by hand.

In order to create the domain mapping/whitelist, we’ll need to create an object that can load a whitelist from disk, or generate one from our original dataset. For example:

import csv

import json from faker import Factory

from collections import Counter

from collections import MutableMapping class DomainMapping(MutableMapping): @classmethod

def load(cls, fobj):

""""""

Load the mapping from a JSON file on disk.

""""""

data = json.load(fobj)

return cls(**data) @classmethod

def generate(cls, emails):

""""""

Pass through a list of emails and count domains to whitelist.

""""""

# Count all the domains in each email address

counts = Counter([

email.split(""@"")[-1] for email in emails

]) # Create a domain mapping

domains = cls() # Ask the user what domains to whitelist based on frequency

for idx, (domain, count) in enumerate(counts.most_common())):

prompt = ""{}/{}: Whitelist {} ({} addresses)?"".format(

idx+1, len(counts), domain, count

) print prompt

ans = raw_input(""[y/n/q] > "").lower() if ans.startswith('y'):

# Whitelist the domain

domains[domain] = domain

elif ans.startswith('n'):

# Create a fake domain

domains[domain]

elif ans.startswith('q'):

break

else:

continue return domains def __init__(self, whitelist=[], mapping={}):

# Create the domain mapping properties

self.fake = Factory.create()

self.domains = mapping # Add the whitelist as a mapping to itself.

for domain in whitelist:

self.domains[domain] = domain def dump(self, fobj):

""""""

Dump the domain mapping whitelist/mapping to JSON.

""""""

whitelist = []

mapping = self.domains.copy()

for key in mapping.keys():

if key == mapping[key]:

whitelist.append(mapping.pop(key)) json.dump({

'whitelist': whitelist,

'mapping': mapping

}, fobj, indent=2) def __getitem__(self, key):

""""""

Get a fake domain for a real domain.

""""""

if key not in self.domains:

self.domains[key] = self.fake.domain_name()

return self.domains[key] def __setitem__(self, key, val):

self.domains[key] = val def __delitem__(self, key):

del self.domains[key] def __iter__(self):

for key in self.domains:

yield key

That’s quite a lot of code all at once, so let’s break it down a bit. First, the class extends MutableMapping which is an abstract base class (ABC) in the collections module. The ABC gives us the ability to make this class act just like a dict object. All we have to do is provide __getitem__ , __setitem__ , __delitem__ , and __iter__ methods, and all other dictionary methods like pop , or values will work on our behalf. Here, we're just wrapping an inner dictionary called domains .

The thing to note about our __getitem__ method is that it acts very similar to a defaultdict — that is, if you try to fetch a key that is not in the mapping, it generates fake data on your behalf. This way, any domains that we don't have in our whitelist or mapping will automatically be anonymized.

Next, we want to be able to load and dump this data to a JSON file on disk, that way we can maintain our mapping between anonymization runs. The load method is fairly straightforward; it just takes an open file-like object, parses it using the json module, instantiates the domain mapping, and returns it. The dump method is a bit more complex. It has to break down the whitelist and mapping into separate objects, so that we can easily modify the data on disk if needed. Together, these methods will allow us to load and save our mapping into a JSON file that will look similar to:

The final method of note is the generate method. The generate method allows you to do a first pass through a list of emails, count the frequency of the domains, then propose domains to the user in order of frequency to decide whether or not to add it to the whitelist. For each domain in the emails, the user is prompted as follows:

1/245: Whitelist ""

[y/n/q] > /245: Whitelist "" gmail.com "" (817 addresses)?[y/n/q] >

Note that the prompt includes a progress indicator (this is prompt 1 of 245) as well as a method to quit early. This is especially important for large datasets that have a lot of unique domains; if you quit, the domains will still be faked, and the user only sees the most frequent examples for whitelisting. The idea behind this mechanism is to read through your CSV once, generate the whitelist, then save it to disk so that you can use it for anonymization on a routine basis. Moreover, you can modify domains in the JSON file to better match any semantics you might have (e.g. such as including .edu or .gov domains, which are not generated by the internet provider in Faker).

Realistic Profiles

To create realistic profiles, we’ll create a provider that uses the domain map from above and generates fake data for every combination we see in the dataset. This provider will also provide opportunities for mapping multiple names and email addresses to a single profile, so that we can use the profile for creating fuzzy duplicates in the next section. Here is the code:

class Profile(object): def __init__(self, domains):

self.domains = domains

self.generator = Factory.create() def fuzzy_profile(self, name=None, email=None):

""""""

Return a profile that allows for fuzzy names and emails.

""""""

parts = self.fuzzy_name_parts()

return {

""names"": {name: self.fuzzy_name(parts, name)},

""emails"": {email: self.fuzzy_email(parts, email)},

} def fuzzy_name_parts(self):

""""""

Returns first, middle, and last name parts

""""""

return (

self.generator.first_name(),

self.generator.first_name(),

self.generator.last_name()

) def fuzzy_name(self, parts, name=None):

""""""

Creates a name that has a similar case to the passed in name.

""""""

# Extract the first, initial, and last name from the parts.

first, middle, last = parts # Create the name, with chance of middle or initial included.

chance = self.generator.random_digit()

if chance < 2:

fname = u""{} {}. {}"".format(first, middle[0], last)

elif chance < 4:

fname = u""{} {} {}"".format(first, middle, last)

else:

fname = u""{} {}"".format(first, last) if name is not None:

# Match the capitalization of the name

if name.isupper(): return fname.upper()

if name.islower(): return fname.lower() return fname def fuzzy_email(self, parts, email=None):

""""""

Creates an email similar to the name and original email.

""""""

# Extract the first, initial, and last name from the parts.

first, middle, last = parts # Use the domain mapping to identify the fake domain.

if email is not None:

domain = self.domains[email.split(""@"")[-1]]

else:

domain = self.generator.domain_name() # Create the username based on the name parts

chance = self.generator.random_digit()

if chance < 2:

username = u""{}.{}"".format(first, last)

elif chance < 3:

username = u""{}.{}.{}"".format(first, middle[0], last)

elif chance < 6:

username = u""{}{}"".format(first[0], last)

elif chance < 8:

username = last

else:

username = u""{}{}"".format(

first, self.generator.random_number()

) # Match the case of the email

if email is not None:

if email.islower(): username = username.lower()

if email.isupper(): username = username.upper()

else:

username = username.lower() return u""{}@{}"".format(username, domain)

Again, this is a lot of code, make sure you go through it carefully to understand what is happening. First off, a profile in this case is the combination of mapping names to fake names and emails to fake emails. The key is that the names and emails are related to the original data somehow. Here the relationship is through case such that “DANIEL WEBSTER” is faked to “JAKOB WILCOTT” instead of to “Jakob Wilcott”. Additionally through our domain mapping, we also maintain the relationship of the original email domain to the fake domain mapping, e.g. everyone with the email domain “@districtdatalabs.com” will be mapped to the same fake domain.

In order to maintain the relationship of names to emails (which is very common), we need to be able to access the name more directly. We have a name parts generator which generates fake first, middle, and last names. We then randomly generate names of the form “first last”, “first middle last”, or “first i. last”. The email can take a variety of forms based on the name parts as well. Now we get slightly more realistic profiles:

>>> fake.fuzzy_profile() {'names': {None: u'Zaire Ebert'}, 'emails': {None: u'ebert@von.com'}} >>> fake.fuzzy_profile(

... name='Daniel Webster', email='dictionaryguy@gmail.com') {'names': {'Daniel Webster': u'Georgia McDermott'},

'emails': {'dictionaryguy@gmail.com': u'georgia9@gmail.com'}}

Importantly this profile object makes it easy to map multiple names and emails to the same profile object to create “fuzzy” profiles and duplicates in your dataset. We will discuss how to perform fuzzy matching in the next section.

Fuzzing Fake Names from Duplicates

If you noticed in our original dataset we had a clear entity duplication: same email, but different names. In fact, the second name was simply the first initial and last name, but you can imagine other duplication scenarios like nicknames (“Bill” instead of “William”), or having both work and personal emails in the dataset. The fuzzy profile objects we generated in the last section allow us to maintain a mapping of all name parts to generated fake names, but we need some way to be able to detect duplicates and combine their profile: enter the fuzzywuzzy module.

$ pip install fuzzywuzzy python-Levenshtein

Similarly to our domain mapping approach, we’re going to pass through the entire dataset and look for similar name-email pairs to propose to the user. If the user thinks they’re duplicates, then we’ll merge them together into a single profile, and use the mappings as we anonymize. This is also something you can save to disk and load on demand for multiple anonymization passes and to include user-based edits.

The first step is to get pairs and eliminate exact duplicates. To do this we’ll create a hashable data structure for our profiles using a namedtuple .

from collections import namedtuple

from itertools import combinations Person = namedtuple('Person', 'name, email')

def pairs_from_rows(rows):

""""""

Expects rows of dictionaries with name and email keys.

""""""

# Create a set of person tuples (no exact duplicates)

people = set([

Person(row['name'], row['email']) for row in rows

]) # Yield ordered pairs of people objects without replacement

for pair in combinations(people, 2):

yield pair

The namedtuple is an immutable data structure that is compact, efficient, and allows us to access properties by name. Because it is immutable, it is also hashable (unlike mutable dictionaries), meaning we can use it for keys in sets and dictionaries. This is important, because the first thing our pairs_from_rows function does is eliminate exact matches by creating a set of Person tuples. We then use the combinations function in itertools to generate every pair without replacement.

The next step is to figure out how similar each pair is. To do this we’ll use the fuzzywuzzy library to come up with a partial ratio score: the mean of the similarity of the names and the emails for each pair:

from fuzzywuzzy import fuzz

from functools import partial def normalize(value, email=False):

""""""

Make everything lowercase and remove spaces.

If email, only take the username portion to compare.

""""""

if email:

value = value.split(""@"")[0]

return value.lower().replace("" "", """")

def person_similarity(pair):

""""""

Returns the mean of the normalized partial ratio scores.

""""""

# Normalize the names and the emails

names = map(normalize, [p.name for p in pair])

email = map(

partial(normalize, email=True), [p.email for p in pair]

) # Compute the partial ratio scores for both names and emails

scores = [

fuzz.partial_ratio(a, b) for a, b in [names, emails]

] # Return the mean score of the pair

return float(sum(scores)) / len(scores)

The score will be between 0 (no similarity) and 100 (exact match), though hopefully you won’t get any scores of 100 since we eliminated exact matches above. For example:

>>> person_similarity([

... Person('John Lennon', 'john.lennon@gmail.com'),

... Person('J. Lennon', 'jlennon@example.org')

... ]) 80.5

The fuzzing process will go through our entire dataset, create pairs of people, and compute their similarity score. We can then filter out all pairs with scores below a certain threshold (say, 50) and propose the results to the user to decide if they’re duplicates in descending score order. When a duplicate is found, we can merge the profile object to map the new names and emails together.

Conclusion

Anonymization of datasets is a critical method to promote the exploration and practice of data science through open data. Fake data generators that already exist give us the opportunity to ensure that private data is obfuscated. The issue becomes how to leverage these fake data generators while still maintaining and preserving a high quality dataset with semantic relations for further analysis. As we’ve seen throughout the post, even the anonymization of just two common fields like name and email can lead to potential problems.

This problem, and the code in this post are associated with a real case study. For District Data Labs’ Entity Resolution Research Lab, I wanted to create a dataset that removed PII of members while maintaining duplicates and structure to study entity resolution. The source dataset was 1,343 records in CSV form and contained names and emails that I wanted to anonymize.

Using the strategy I described for domain name mapping, the dataset contained 245 distinct domain names, 185 of which appeared only once. There was a definite long tail, as the first 20 or so most frequent domains were the majority of the records. Once I generated the whitelist as described above, I manually edited the mappings to ensure that there were no duplicates and that major work domains were sufficiently “professional.”

Using the fuzzy matching process was also a bear. It took, on average, 28 seconds to compute the pairwise scores. Using a threshold score of 50, I was proposed 5,110 duplicates (out of a possible 901,153 combinations). I went through 354 entries (until the score was below 65) and was satisfied that I had covered many of the duplicates in the dataset.

The resulting anonymized dataset was of a high quality and obfuscated personally identifying information like name and email. Of course, you could reverse some of the information in the dataset. For example, I’m listed in the dataset, and one of the records indicates a relationship between a fake user and a blog post, which I’m on record as having written. However, even though you can figure out who I am and what else I’ve done through the dataset, you wouldn’t be able to use it to extract my email address, which was the goal.

In the end, even though anonymizing data requires a lot of data wrangling effort and considered thought, the benefits of open data are invaluable. Only by sharing data, resources, and tools can use many eyes to provide multiple insights and to drive the field of data science forward.

Acknowledgments

I would like to thank Michal Haskell and Rebecca Bilbro for their help editing and preparing this post. This discussion was a challenge, and they cleaned up my bleary eyed writing to make the article readable. A special thank you to Rebecca Bilbro as well for drawing the figure used to describe the anonymization process.

Footnotes

1.Anonymize: remove identifying particulars from (test results) for statistical or other purposes.

2.Entity Resolution: tools or techniques that identify, group, and link digital mentions or manifestations of some object in the real world.

3.DDL Research Labs is an applied research program intended to develop novel, innovative data science solutions towards practical applications.","['create', 'anonymizing', 'email', 'names', 'mapping', 'datasets', 'emails', 'python', 'dataset', 'practical', 'faker', 'data', 'fake', 'domain', 'guide']","# Load the faker and its providersfaker = Factory.create() # Create mappings of names & emails to faked names & emails.
We then create two defaultdict instances to map real names to fake names and real emails to fake emails.
I typically prefer Fake Factory over Faker because it has multiple language support and a wider array of fake data generators.
First off, a profile in this case is the combination of mapping names to fake names and emails to fake emails.
Additionally through our domain mapping, we also maintain the relationship of the original email domain to the fake domain mapping, e.g.",en,['District Data Labs'],2017-12-24 14:10:16.285000+00:00,"{'Data Science', 'Data', 'Python', 'Big Data', 'Analytics'}","{'https://miro.medium.com/fit/c/160/160/1*tym3oKQBxBY29YxbHbXYTA.png', 'https://miro.medium.com/max/282/1*8Ox9-8Km5U6N4sHIh7Al7A.png', 'https://miro.medium.com/fit/c/80/80/2*1to1GyfcsA9EBjt0u0fpjg.jpeg', 'https://miro.medium.com/max/60/0*Mf_EX7p1caMoabhv.png?q=20', 'https://miro.medium.com/max/284/1*8Ox9-8Km5U6N4sHIh7Al7A.png', 'https://miro.medium.com/fit/c/96/96/1*tym3oKQBxBY29YxbHbXYTA.png', 'https://miro.medium.com/fit/c/80/80/1*aL2isee1E3zr8mzSXRiASA.png', 'https://miro.medium.com/max/790/0*Mf_EX7p1caMoabhv.png', 'https://miro.medium.com/max/1580/0*Mf_EX7p1caMoabhv.png', 'https://miro.medium.com/fit/c/80/80/0*Lk657t5l7ebSPfHs.jpg'}",2020-03-05 00:17:58.140272,1.4275131225585938
https://towardsdatascience.com/how-i-learned-to-love-parallelized-applies-with-python-pandas-dask-and-numba-f06b0b367138,Data Pre-Processing in Python: How I learned to love parallelized applies with Dask and Numba,"Go fast with Numba and Dask

As a master’s candidate of Data Science at the University of San Francisco, I get to regularly wrangle with data. Applies are one of the many tricks I’ve picked up to help create new features or clean-up data. Now, I’m only data scientist-ish and not an expert in computer science. I am, however, a tinkerer that enjoys making code faster. Today, I’ll be sharing my experiences with parallelizing applies, with a particular focus on common data prep tasks.

Python aficionados may know that Python implements what’s known as a Global Interpreter Lock. Those more grounded in computer science can tell you more, but for our purposes, the GIL can make using all of those cpu cores in your computer tricky. What’s worse, our chief data wrangler package, Pandas, rarely implements multi-processing code.

Apply vs Multiprocessing.map

%time df.some_col.apply(lambda x : clean_transform_kthx(x))

Wall time: HAH! RIP BUDDY

# WHY YOU NO RUN IN PARALLEL!?

Those of us crossing over from the R realm know that the Tidyverse has done some wonderful things for handling data. One of my favorite packages, plyr, allows R users to easily parallelize their applies on data frames. From Hadley Wickham:

plyr is a set of tools for a common set of problems: you need to split up a big data structure into homogeneous pieces, apply a function to each piece and then combine all the results back together

What I wanted was plyr for Python! Sadly, it does not yet exist, but I used a hacky solution from the multiprocessing package for a while. It certainly works, but I wanted something that was more akin to regular Pandas applies…but like, parallel and stuff.

Thanks for all the cores AMD!

We spend a bit of class time on Spark so when I started using Dask, it was easier to grasp its main conceits. Dask is designed to run in parallel across many cores or computers but mirror many of the functions and syntax of Pandas.

Let’s dive in to an example! For a recent data challenge, I was trying to take an external source of data (many geo-encoded points) and match them to a bunch of street blocks we were analyzing. I was calculating euclidean distances and using a simple max-heuristic to assign it to a block:

Is the point close to L3? The L1 + L2 may shock you…

My original apply:

my_df.apply(lambda x: nearest_street(x.lat,x.lon),axis=1)

My Dask apply:

dd.from_pandas(my_df,npartitions=nCores).\

map_partitions(

lambda df : df.apply(

lambda x : nearest_street(x.lat,x.lon),axis=1)).\

compute(get=get) # imports at the end

Pretty similar right? The apply statement is wrapped around a map_partitions , there’s a compute() at the end, and I had to initialize npartitions . Spark users will find this familiar, but let’s disentangle this a bit for the rest of us. Partitions are just that, your Pandas data frame divided up into chunks. On my computer with 6-Cores/12-Threads, I told it to use 12 partitions. Dask handles the rest for you thankfully.

Next, map_partitions is simply applying that lambda function to each partition. Since many of our data processing code operates on each row independently, we do not have to worry too much about the order of these operations (which row goes first or last is irrelevant). Lastly, the compute() is telling Dask to process everything that came before and deliver the end product to me. Many distributed libraries like Dask or Spark implement ‘lazy evaluation’, or creating a list of tasks and only executing when prompted to do so. Here, compute() calls Dask to map the apply to each partition and (get=get) tells Dask to run this in parallel.

I did not use a Dask apply because I am iterating over rows to generate a new array that will become a feature. Using map_partitions and an apply allows me to send two columns of a single row into the function nearest_street(). A Dask apply maps across rows of entire columns, which would not work with the function as written.

Here are the imports for the Dask code:

from dask import dataframe as dd

from dask.multiprocessing import get

from multiprocessing import cpu_count nCores = cpu_count()

Numba, Numpy and Broadcasting

Since I was classifying my data based on some simple algebraic calculations (Pythagorean theorem basically), I figured it would run quickly enough in typical Python code that looks like this:

matches = []

for i in intersections:

l3 = np.sqrt( (i[0] - i[1])**2 + (i[2] - i[3])**2 )

# ... Some more of these

dist = l1 + l2 if dist < (l3 * 1.2):

matches.append(dist)

# ... More stuff ### you get the idea, there's a for-loop checking to see if

### my points are close to my streets and then returning closest

### I even used numpy, that means fast right?

It was not.

Broadcasting is the idea of writing code with a vector mindset as opposed to scalar. Say I have an array, and I want to futz with it. Normally, I would iterate over it and transform each cell individually.

# over one array

for cell in array:

cell * CONSTANT - CONSTANT2 # over two arrays

for i in range(len(array)):

array[i] = array[i] + array2[i]

Instead, I can skip the for loops entirely and perform operations across the entire array. Numpy functions incorporate broadcasting and can be used to perform element-wise computations (1-element in an array to a corresponding 1-element in another array).

# over one array

(array * CONSTANT) - CONSTANT2 # over two arrays of same length

# different lengths follow broadcasting rules

array = array - array2

Broadcasting can accomplish so much more, but let’s look at my skeleton code:

from numba import jit @jit # numba magic

def some_func()

l3_arr = np.sqrt( (intersections[:,0] - intersections[:,1])**2 +\

(intersections[:,2] - intersections[:,3])**2 )

# now l3 is an array containing all of my block lengths

# likewise, l1 and l2 are now equal sized arrays

# containing distance of point to all intersections dist = l1_arr + l2_arr match_arr = dist < (l3_arr * 1.2)

# so instead of iterating, I just immediately compare all of my

# point-to-street distances at once and have a handy

# boolean index

Essentially, we’re changing for i in array: do stuff to do stuff on array . The best part is that it’s fast, even compared to parallelizing versus Dask. The good part is that if we stick to basic Numpy and Python, we can Just-In-Time compile just about any function. The bad part is that it only plays well with Numpy and simple Python syntax. I had to strip out all of the numerical calculations from my functions into sub-functions, but the speed increase was magical…

Putting it all together

To combine my Numba function with Dask, I simply applied the function with map_partition() . I was curious if parallelized operations and broadcasting could work hand in hand for a speed-up. I was pleasantly surprised to see a large speed up, especially with larger data sets:

Go Numba go!

So x is: 1, 10, 100, 1000…

The first graph indicates that linear computation without broadcasting performs poorly. We see that parallelizing the code with Dask is almost as effective as using Numba+broadcasting, but clearly, Dask+Numba outperforms others.

I include the second graph to anger people that like simple and interpretable graphics. Or it’s there to show that Dask comes with some overhead costs, but Numba does not. I took head(nRows) to create these charts and noticed it was not until 1k — 10k rows that Dask came into its own. I also found it curious that Numba alone was consistently faster than Dask, although the combination of Dask+Numba could not be beat at large nRows.

Optimizations

To be able to JIT compile with Numba, I re-wrote my functions to take advantage of broadcasting. Out-of-curiosity, I reran these functions to compare Numba+Broadcasting vs Just Broadcasting (Numpy only basically). On average, @jit executes about 24% faster for identical code.

Thanks JIT!

I’m sure there are ways to optimize even further, but I liked that I was able to quickly port my previous work into Dask and Numba for a 60x speed increase. Numba only really requires that I stick to Numpy functions and think about arrays all at once. Dask is very user friendly and offers a familiar syntax for Pandas or Spark users. If there are other speed tricks that are easy to implement, please feel free to share!","['numpy', 'function', 'love', 'learned', 'array', 'python', 'applies', 'apply', 'preprocessing', 'data', 'functions', 'parallelized', 'broadcasting', 'using', 'dask', 'numba']","We spend a bit of class time on Spark so when I started using Dask, it was easier to grasp its main conceits.
Here, compute() calls Dask to map the apply to each partition and (get=get) tells Dask to run this in parallel.
I did not use a Dask apply because I am iterating over rows to generate a new array that will become a feature.
A Dask apply maps across rows of entire columns, which would not work with the function as written.
Numba only really requires that I stick to Numpy functions and think about arrays all at once.",en,['Ernest Kim'],2019-03-22 13:13:51.070000+00:00,"{'Parallel Computing', 'Python Pandas', 'Data Science', 'Python', 'Towards Data Science'}","{'https://miro.medium.com/fit/c/160/160/1*-BZoVR1FoNWDrkYQPU2qKw.jpeg', 'https://miro.medium.com/max/60/1*ury0XRvKWpwAZsMQ_m1_cg.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3600/1*wfQ_pXwrr7Y_0_aXSVmQWg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*-BZoVR1FoNWDrkYQPU2qKw.jpeg', 'https://miro.medium.com/max/3600/1*YsYMh8inCLZbRVD0xpbzNw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/5100/1*rNIJiaWUAv-DmM7JxdsD9Q.png', 'https://miro.medium.com/max/3600/1*q_f-EzQFuLC14amYx9VbMA.png', 'https://miro.medium.com/max/60/1*RGap2-WIEWrgo2RDf6jdiA.png?q=20', 'https://miro.medium.com/max/60/1*wfQ_pXwrr7Y_0_aXSVmQWg.png?q=20', 'https://miro.medium.com/max/3600/1*RGap2-WIEWrgo2RDf6jdiA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*q_f-EzQFuLC14amYx9VbMA.png?q=20', 'https://miro.medium.com/max/3600/1*z4h3mQ-ztG1MA0dz1tRlpg.png', 'https://miro.medium.com/max/700/1*ury0XRvKWpwAZsMQ_m1_cg.jpeg', 'https://miro.medium.com/max/60/1*YsYMh8inCLZbRVD0xpbzNw.png?q=20', 'https://miro.medium.com/max/60/1*rNIJiaWUAv-DmM7JxdsD9Q.png?q=20', 'https://miro.medium.com/max/1400/1*ury0XRvKWpwAZsMQ_m1_cg.jpeg', 'https://miro.medium.com/max/60/1*z4h3mQ-ztG1MA0dz1tRlpg.png?q=20'}",2020-03-05 00:18:04.967440,6.826167106628418
https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68,Open Machine Learning Course. Topic 1. Exploratory Data Analysis with Pandas,"With this article, we, OpenDataScience, launch an open Machine Learning course. This is not aimed at developing another comprehensive introductory course on machine learning or data analysis (so this is not a substitute for fundamental education or online/offline courses/specializations and books). The purpose of this series of articles is to quickly refresh your knowledge and help you find topics for further advancement. Our approach is similar to that of the authors of Deep Learning book, which starts off with a review of mathematics and basics of machine learning — short, concise, and with many references to other resources.

UPD: YouTube playlist with videolectures

The course is designed to perfectly balance theory and practice; therefore, each topic is followed by an assignment with a deadline in a week. You can also take part in several Kaggle Inclass competitions held during the course.

All materials are available as a Kaggle Dataset and in a GitHub repo.

The course is going to be actively discussed in the OpenDataScience Slack team. Please fill in this form to be invited. The next session of the course will start on October 1, 2018. Invitations will be sent in September.

Article outline

1. About the course

2. Assignments

3. Demonstration of main Pandas methods

4. First attempt on predicting telecom churn

5. Assignment #1

6. Useful resources

1. About the course

Syllabus

Community

One of the most vivid advantages of our course is active community. If you join the OpenDataScience Slack team, you’ll find the authors of articles and assignments right there in the same channel (#eng_mlcourse_open) eager to help you. This can help very much when you make your first steps in any discipline. Fill in this form to be invited. The form will ask you several questions about your background and skills, including a few easy math questions.

We chat informally, like humor and emoji. Not every MOOC can boast to have such an alive community.

Prerequisites

The prerequisites are the following: basic concepts from calculus, linear algebra, probability theory and statistics, and Python programming skills. If you need to catch up, a good resource will be Part I from the “Deep Learning” book and various math and Python online courses (for Python, CodeAcademy will do). More info is available on the corresponding Wiki page.

What software you’ll need

As for now, you’ll only need Anaconda (built with Python 3.6) to reproduce the code in the course. Later in the course you’ll have to install other libraries like Xgboost and Vowpal Wabbit.

You can also resort to the Docker container with all necessary software already installed. More info is available on the corresponding Wiki page.

2. Assignments

Each article comes with an assignment in the form of a Jupyter notebook. The task will be to fill in the missing code snippets and to answer questions in a Google Quiz form;

Each assignment is due in a week with a hard deadline;

Please discuss the course content (articles and assignments) in the #eng_mlcourse_open channel of the OpenDataScience Slack team or here in the comments to articles on Medium;

The solutions to assignments will be sent to those who have submitted the corresponding Google form.

3. Demonstration of main Pandas methods

Well... There are dozens of cool tutorials on Pandas and visual data analysis. If you are familiar with these topics, just wait for the 3rd article in the series, where we get into machine learning.

The following material is better viewed as a Jupyter notebook and can be reproduced locally with Jupyter if you clone the course repository.

Pandas is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like .csv , .tsv , or .xlsx . Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn , Pandas provides a wide range of opportunities for visual analysis of tabular data.

The main data structures in Pandas are implemented with Series and DataFrame classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of Series instances. DataFrames are great for representing real data: rows correspond to instances (objects, observations, etc.), and columns correspond to features for each of the instances.","['machine', 'course', 'form', 'exploratory', 'pandas', 'series', 'python', 'learning', 'opendatascience', 'data', 'youll', 'topic', 'analysis', 'open']","With this article, we, OpenDataScience, launch an open Machine Learning course.
This is not aimed at developing another comprehensive introductory course on machine learning or data analysis (so this is not a substitute for fundamental education or online/offline courses/specializations and books).
Our approach is similar to that of the authors of Deep Learning book, which starts off with a review of mathematics and basics of machine learning — short, concise, and with many references to other resources.
Later in the course you’ll have to install other libraries like Xgboost and Vowpal Wabbit.
If you are familiar with these topics, just wait for the 3rd article in the series, where we get into machine learning.",en,['Yury Kashnitsky'],2018-10-07 12:47:36.075000+00:00,"{'Pandas', 'Data Science', 'Python', 'Machine Learning', 'Education'}","{'https://miro.medium.com/max/130/1*MxsqShnPs3RXMScwl5HhsQ.jpeg', 'https://miro.medium.com/max/2280/1*wYg4FyfFksj28Kz_5rxgFA.png', 'https://miro.medium.com/max/2280/1*Xb4Uija90bq7Vw04VyBpSQ.png', 'https://miro.medium.com/max/60/1*eVNjkFqeCyA_IduaeC_rcA.png?q=20', 'https://miro.medium.com/max/60/1*0p0_AHjjNGYnJ2O8GBKlKQ.png?q=20', 'https://miro.medium.com/max/378/1*Gk9nFFxZiRTV8uJf5_b07w.png', 'https://miro.medium.com/max/1006/1*ED65JB9nHw-0kVfv90mHVA.png', 'https://miro.medium.com/max/60/1*E6tWciq56m63uwtnrhm7fg.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*JtleyoApNKIjJU5F.', 'https://miro.medium.com/max/60/1*q0REg_r6EdIbjBKVpc8ghg.png?q=20', 'https://miro.medium.com/max/512/1*SSZh6JPcSQSQVfHNBqgT5w.png', 'https://miro.medium.com/max/2282/1*Upw7R6tbW75pjW-mURDsyQ.png', 'https://miro.medium.com/fit/c/96/96/0*JtleyoApNKIjJU5F.', 'https://miro.medium.com/max/1400/1*JFS6wqgVKXYr2HO_dObEiQ.jpeg', 'https://miro.medium.com/max/60/1*Mhpp0k7s1yYjey8-g6HCzw.png?q=20', 'https://miro.medium.com/max/478/1*7gZP1DY_b_LnzKkpSmGJyQ.png', 'https://miro.medium.com/max/2276/1*jzfv2qe6fhoDMWgqFd0B2w.png', 'https://miro.medium.com/max/60/1*SnI7SUjpgsL2kLZcz8qEkw.png?q=20', 'https://miro.medium.com/max/60/1*rA7X49BQHZfNR_9JUr19-Q.png?q=20', 'https://miro.medium.com/max/60/1*-tAHj1BnIqTye6NgnToDDw.png?q=20', 'https://miro.medium.com/max/60/1*wYg4FyfFksj28Kz_5rxgFA.png?q=20', 'https://miro.medium.com/max/60/1*0NHkro7B6X9C3sLHddVtAA.png?q=20', 'https://miro.medium.com/max/60/1*s_ftS4GCSE4gYGUxJm2osg.png?q=20', 'https://miro.medium.com/max/60/1*7gZP1DY_b_LnzKkpSmGJyQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*b5pfoCQz20rjh79OPSQWQg.jpeg', 'https://miro.medium.com/max/60/1*mDK1PGDwEMgbxp0V-EUkWQ.png?q=20', 'https://miro.medium.com/max/1006/1*z6nuJGC3X-CJsLQH81PPxg.png', 'https://miro.medium.com/max/1716/1*UeiRW4qDT9Y57cAbyE2jog.png', 'https://miro.medium.com/max/60/1*Gk9nFFxZiRTV8uJf5_b07w.png?q=20', 'https://miro.medium.com/max/546/1*q0REg_r6EdIbjBKVpc8ghg.png', 'https://miro.medium.com/max/60/1*nIXHbNAuq-o9j-QtEDDXzQ.jpeg?q=20', 'https://miro.medium.com/max/552/1*0p0_AHjjNGYnJ2O8GBKlKQ.png', 'https://miro.medium.com/max/1300/1*nIXHbNAuq-o9j-QtEDDXzQ.jpeg', 'https://miro.medium.com/max/1220/0*D0RQ889t5o6H2A3V.jpg', 'https://miro.medium.com/max/60/1*Upw7R6tbW75pjW-mURDsyQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QPOivsqwZ7jF2Nfm7bJkDA.png', 'https://miro.medium.com/max/2286/1*E6tWciq56m63uwtnrhm7fg.png', 'https://miro.medium.com/max/128/1*MxsqShnPs3RXMScwl5HhsQ.jpeg', 'https://miro.medium.com/max/2280/1*s_ftS4GCSE4gYGUxJm2osg.png', 'https://miro.medium.com/max/858/1*B61LYUYeMAGtCR_tgqbbdQ.png', 'https://miro.medium.com/max/1060/1*eVNjkFqeCyA_IduaeC_rcA.png', 'https://miro.medium.com/max/60/1*uJbt3MvW2eKrC5OCj9WXBA.png?q=20', 'https://miro.medium.com/max/60/1*Xb4Uija90bq7Vw04VyBpSQ.png?q=20', 'https://miro.medium.com/max/2320/1*rA7X49BQHZfNR_9JUr19-Q.png', 'https://miro.medium.com/max/60/1*z6nuJGC3X-CJsLQH81PPxg.png?q=20', 'https://miro.medium.com/max/650/1*-tAHj1BnIqTye6NgnToDDw.png', 'https://miro.medium.com/max/60/1*ED65JB9nHw-0kVfv90mHVA.png?q=20', 'https://miro.medium.com/max/60/1*UeiRW4qDT9Y57cAbyE2jog.png?q=20', 'https://miro.medium.com/max/60/0*D0RQ889t5o6H2A3V.jpg?q=20', 'https://miro.medium.com/max/60/1*LtE9F0VzdNsbB-syb7IQEg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*AmhWkgbff5os5Vlxb8HZCg.jpeg', 'https://miro.medium.com/max/2298/1*0NHkro7B6X9C3sLHddVtAA.png', 'https://miro.medium.com/fit/c/80/80/2*6KkuToL5nQ9DBIiE4r4WTA.jpeg', 'https://miro.medium.com/max/1006/1*LtE9F0VzdNsbB-syb7IQEg.png', 'https://miro.medium.com/max/610/0*D0RQ889t5o6H2A3V.jpg', 'https://miro.medium.com/max/2278/1*Mhpp0k7s1yYjey8-g6HCzw.png', 'https://miro.medium.com/max/2290/1*mDK1PGDwEMgbxp0V-EUkWQ.png', 'https://miro.medium.com/max/60/1*B61LYUYeMAGtCR_tgqbbdQ.png?q=20', 'https://miro.medium.com/max/60/1*QvmLemJySO_EvPDFn6S1Rw.png?q=20', 'https://miro.medium.com/max/60/1*SSZh6JPcSQSQVfHNBqgT5w.png?q=20', 'https://miro.medium.com/max/548/1*uJbt3MvW2eKrC5OCj9WXBA.png', 'https://miro.medium.com/max/728/1*SnI7SUjpgsL2kLZcz8qEkw.png', 'https://miro.medium.com/max/60/1*JFS6wqgVKXYr2HO_dObEiQ.jpeg?q=20', 'https://miro.medium.com/max/2218/1*QvmLemJySO_EvPDFn6S1Rw.png', 'https://miro.medium.com/max/60/1*jzfv2qe6fhoDMWgqFd0B2w.png?q=20'}",2020-03-05 00:18:06.313513,1.346073865890503
https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159,Smarter Ways to Encode Categorical Data for Machine Learning,"Smarter Ways to Encode Categorical Data for Machine Learning

Exploring Category Encoders

Better encoding of categorical data can mean better model performance. In this article I’ll introduce you to a wide range of encoding options from the Category Encoders package for use with scikit-learn machine learning in Python.

Enigma for encoding

TL;DR;

Use Category Encoders to improve model performance when you have nominal or ordinal data that may provide value.

For nominal columns try OneHot, Hashing, LeaveOneOut, and Target encoding. Avoid OneHot for high cardinality columns and decision tree-based algorithms.

For ordinal columns try Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial are less likely to be helpful, but if you have time or theoretic reason you might want to try them.

For regression tasks, Target and LeaveOneOut probably won’t work well.

Roadmap

Map

In this article I’ll discuss terms, general usage and five classic encoding options: Ordinal, One Hot, Binary, BaseN, and Hashing. In the future I may evaluate Bayesian encoders and contrast encoders with roots in statistical hypothesis testing. 🚀

In an earlier article I argued we should classify data as one of seven types to make better models faster. Here are the seven data types:

Useless — useless for machine learning algorithms, that is — discrete

Nominal — groups without order — discrete

Binary — either/or — discrete

Ordinal — groups with order — discrete

Count — the number of occurrences — discrete

Time — cyclical numbers with a temporal component — continuous

Interval — positive and/or negative numbers without a temporal component — continuous

Here we’re concerned with encoding nominal and ordinal data. A column with nominal data has values that cannot be ordered in any meaningful way. Nominal data is most often one-hot (aka dummy) encoded, but there are many options that might perform better for machine learning.

Rank

In contrast, ordinal data can be rank ordered. Ordinal data can be encoded in one of three ways, broadly speaking, but I think it’s safe to say that its encoding is often not carefully considered.

It can be assumed to be close enough to interval data — with relatively equal magnitudes between the values — to treat it as such. Social scientists make this assumption all the time with Likert scales. For example, “On a scale from 1 to 7, 1 being extremely unlikely, 4 being neither likely nor unlikely and 7 being extremely likely, how likely are you to recommend this movie to a friend?”. Here the difference between 3 and 4 and the difference between 6 and 7 can be reasonably assumed to be similar. It can be treated as nominal data, where each category has no numeric relationship to another. One-hot encoding and other encodings appropriate for nominal data make sense here. The magnitude of the difference between the numbers can be ignored. You can just train your model with different encodings and seeing which encoding works best.

In this series we’ll look at Categorical Encoders 11 encoders as of version 1.2.8. **Update: Version 1.3.0 is the latest version on PyPI as of April 11, 2019.**

Many of these encoding methods go by more than one name in the statistics world and sometimes one name can mean different things. We’ll follow the Category Encoders usage.

Big thanks to Will McGinnis for creating and maintaining this package. It is largely derived from StatsModel’s Patsy package, which in turn is based on this UCLA statistics reference.

There are an infinite number of ways to encode categorical information. The ones in Category Encoders should be sufficient for most uses. 👍

Quick Summary

Here’s the list of Category Encoders functions with their descriptions and the type of data they would be most appropriate to encode.

Classic Encoders

The first group of five classic encoders can be seen on a continuum of embedding information in one column (Ordinal) up to k columns (OneHot). These are very useful encodings for machine learning practitioners to understand.

Ordinal — convert string labels to integer values 1 through k. Ordinal.

OneHot — one column for each value to compare vs. all other values. Nominal, ordinal.

Binary — convert each integer to binary digits. Each binary digit gets one column. Some info loss but fewer dimensions. Ordinal.

BaseN — Ordinal, Binary, or higher encoding. Nominal, ordinal. Doesn’t add much functionality. Probably avoid.

Hashing — Like OneHot but fewer dimensions, some info loss due to collisions. Nominal, ordinal.

Sum — Just like OneHot except one value is held constant and encoded as -1 across all columns.

Contrast Encoders

The five contrast encoders all have multiple issues that I argue make them unlikely to be useful for machine learning. They all output one column for each value found in a column. Their stated intents are below.

Helmert (reverse) — The mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels.

Backward Difference — the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level.

Polynomial — orthogonal polynomial contrasts. The coefficients taken on by polynomial coding for k=4 levels are the linear, quadratic, and cubic trends in the categorical variable.

Bayesian Encoders

The Bayesian encoders use information from the dependent variable in their encodings. They output one column and can work well with high cardinality data.

Target — use the mean of the DV, must take steps to avoid overfitting/ response leakage. Nominal, ordinal. For classification tasks.

LeaveOneOut — similar to target but avoids contamination. Nominal, ordinal. For classification tasks.

WeightOfEvidence — added in v1.3. Not documented in the docs as of April 11, 2019. The method is explained in this post.

James-Stein — forthcoming in v1.4. Described in the code here.

M-estimator — forthcoming in v1.4. Described in the code here. Simplified target encoder.

Use

Category Encoders follow the same API as sklearn’s preprocessors. They have some added conveniences, such as the ability to easily add an encoder to a pipeline. Additionally, the encoder returns a pandas DataFrame if a DataFrame is passed to it. Here’s an example of the code with the BinaryEncoder:

We’ll tackle a few gotchas with implementation in the future. But you should be able to jump right into the first five if you are familiar with scikit-learn’s API.

Note that all Category Encoders impute missing values automatically by default. However, I recommend filling missing data data yourself prior to encoding so you can test the results of several methods. I plan to discuss imputing options in a forthcoming article, so follow me on Medium if you want to make sure you don’t miss it.

Terminology

You might see commentators use the following terms interchangeably: dimension, feature, vector, series, independent variable, and column. I will too :) Similarly, you might see row and observation used interchangeably.

k is the original number of unique values in your data column. High cardinality means a lot of unique values ( a large k). A column with hundreds of zip codes is an example of a high cardinality feature.

High cardinality theme bird

High dimensionality means a matrix with many dimensions. High dimensionality comes with the Curse of Dimensionality — a thorough treatment of this topic can be found here. The take away is that high dimensionality requires many observations and often results in overfitting.

A wand to help ward off the Curse of Dimensionality

Sparse data is a matrix with lots of zeroes relative to other values. If your encoders transform your data so that it becomes sparse, some algorithms may not work well. Sparsity can often be managed by flagging it, but many algorithms don’t work well unless the data is dense.

Sparse

Digging Into Category Encoders

Without further ado, let’s encode!

Ordinal

OrdinalEncoder converts each string value to a whole number. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on.

What the actual value was prior to encoding does not affect what it becomes when you fit_transform with OrdinalEncoder. The first value could have been 10 and the second value could have been 3. Now they will be 1 and 2, respectively.

If the column contains nominal data, stopping after you use OrdinalEncoder is a bad idea. Your machine learning algorithm will treat the variable as continuous and assume the values are on a meaningful scale. Instead, if you have a column with values car, bus, and truck you should first encode this nominal data using OrdinalEncoder. Then encode it again using one of the methods appropriate to nominal data that we’ll explore below.

In contrast, if your column values are truly ordinal, that means that the integer assigned to each value is meaningful. Assignment should be done with intention. Say your column had the string values “First”, “Third”, and “Second” in it. Those values should be mapped to the corresponding integers by passing OrdinalEncoder a list of dicts like so:

[{‘col’: ‘finished_race_order’,

‘mapping’: [(""First"", 1),

(‘Second’, 2),

(‘Third’, 3)]

}]

Here’s the basic setup for all the code samples to follow. You can get the full notebook at this Kaggle Kernel.

Here’s the untransformed X column.

And here’s the OrdinalEncoder code to transform the color column values from letters to integers.

All the string values are now integers.

Sklearn’s LabelEncoder does pretty much the same thing as Category Encoder’s OrdinalEncoder, but is not quite as user friendly. LabelEncoder won’t return a DataFrame, instead it returns a numpy array if you pass a DataFrame. It also outputs values starting with 0, compared to OrdinalEncoder’s default of outputting values starting with 1.

You could accomplish ordinal encoding by mapping string values to integers in pandas. But that’s extra work once you know how to use Category Encoders.

OneHot

One-hot encoding is the classic approach to dealing with nominal, and maybe ordinal, data. It’s referred to as the “The Standard Approach for Categorical Data” in Kaggle’s Machine Learning tutorial series. It also goes by the names dummy encoding, indicator encoding, and occasionally binary encoding. Yes, this is confusing. 😉

That’s one hot sun

The one-hot encoder creates one column for each value to compare against all other values. For each new column, a row gets a 1 if the row contained that column’s value and a 0 if it did not. Here’s how it looks:

color_-1 is actually an extraneous column, because it’s all 0s — with no variation, it’s not helping your model learn anything. It may have been intended for missing values, but in version 1.2.8 of Category Encoders it doesn’t serve a purpose. However, it’s only adding one column so it’s not really a big deal for performance.

One-hot encoding can perform very well, but the number of new features is equal to k, the number of unique values. This feature expansion can create serious memory problems if your dataset has high cardinality features. One-hot-encoded data can also be difficult for decision-tree-based algorithms — see discussion here.

The pandas GetDummies and scikit-learn’s OneHotEncoder functions perform the same role as the Category Encoders OneHotEncoder. I find OneHotEncoder a bit nicer to use.

Binary

Binary can be thought of as a hybrid of one-hot and hashing encoders. Binary creates fewer features than one-hot, while preserving some uniqueness of values in the the column. It can work well with higher dimensionality ordinal data.

Binary

Here’s how it works:

The categories are encoded by OrdinalEncoder if they aren’t already in numeric form.

Then those integers are converted into binary code, so for example 5 becomes 101 and 10 becomes 1010

Then the digits from that binary string are split into separate columns. So if there are 4–7 values in an ordinal column then 3 new columns are created: one for the first bit, one for the second, and one for the third.

Each observation is encoded across the columns in its binary form.

Here’s how it looks:

The first column has no variance, so it isn’t doing anything to help the model.

With only three levels, the information embedded becomes muddled. There are many collisions and the model can’t glean much information from the features. Just one-hot encode a column if it only has a few values.

In contrast, binary really shines when the cardinality of the column is higher — with the 50 US states, for example.

Binary encoding creates fewer columns than one-hot encoding. It is more memory efficient. It also reduces the chances of dimensionality problems with higher cardinality.

Most similar values overlap with each other across many of the new columns. This allows many machine learning algorithms to learn the values similarity. Binary encoding is a decent compromise for ordinal data with high cardinality.

For nominal data a hashing algorithm with more fine-grained control usually makes more sense. If you’ve used binary encoding successfully, please share in the comments.

BaseN

When the BaseN base = 1 it is basically the same as one hot encoding. When base = 2 it is basically the same as binary encoding. McGinnis said of this encoder, “Practically, this adds very little new functionality, rarely do people use base-3 or base-8 or any base other than ordinal or binary in real problems.”

Base 3

The main reason for BaseN’s existence is to possibly make grid searching easier. You could use BaseN with scikit-learn’s gridsearchCV. However, if you’re going to grid search with some of these encoding options, you can make the encoder search part of your workflow anyway. I don’t see a compelling reason to use BaseN. If you do, please share in the comments.

The default base for BaseNEncoder is 2, which is the equivalent of BinaryEncoder.

Hashing

HashingEncoder implements the hashing trick. It is similar to one-hot encoding but with fewer new dimensions and some info loss due to collisions. The collisions do not significantly affect performance unless there is a great deal of overlap. An excellent discussion of the hashing trick and guidelines for selecting the number of output features can be found here.

Here’s the ordinal column again for a refresher.

And here’s the HashingEncoder.

The n_components parameter controls the number of expanded columns. The default is eight columns. In our example column with three values the default results in five columns full of 0s.

If you set n_components less than k you’ll have a small reduction in the value provided by the encoded data. You’ll also have fewer dimensions.

You can pass a hashing algorithm of your choice to HashingEncoder; the default is md5. Hashing algorithms have been very successful in some Kaggle competitions. It’s worth trying HashingEncoder for nominal and ordinal data if you have high cardinality features. 👍

Wrap

Exercise break

That’s all for now. Here’s a recap and suggestions for remaining encoders.

For nominal columns try OneHot, Hashing, LeaveOneOut, and Target encoding. Avoid OneHot for high cardinality columns and decision tree-based algorithms.

For ordinal columns try Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial are less likely to be helpful, but if you have time or theoretic reason you might want to try them.

The Bayesian encoders can work well for some machine learning tasks. For example, Owen Zhang used the leave one out encoding method to perform well in a Kaggle classification challenge.

*Update April 2019: I updated this article to include information about forthcoming encoders and reworked the conclusion.**

I write about data science, Python, SQL, and DevOps. Check out my other articles and follow me here if you’re into that stuff. 😀

Thanks for reading!","['machine', 'encode', 'ordinal', 'encoders', 'onehot', 'column', 'nominal', 'columns', 'learning', 'encoding', 'ways', 'data', 'smarter', 'categorical', 'values', 'binary']","Smarter Ways to Encode Categorical Data for Machine LearningExploring Category EncodersBetter encoding of categorical data can mean better model performance.
Nominal data is most often one-hot (aka dummy) encoded, but there are many options that might perform better for machine learning.
It can be treated as nominal data, where each category has no numeric relationship to another.
However, I recommend filling missing data data yourself prior to encoding so you can test the results of several methods.
Then encode it again using one of the methods appropriate to nominal data that we’ll explore below.",en,['Jeff Hale'],2020-02-24 21:43:54.396000+00:00,"{'Data', 'Machine Learning', 'Data Science'}","{'https://miro.medium.com/max/1008/1*h8yQCZY1A6pKXlTH6nMe3g.png', 'https://miro.medium.com/max/1016/1*cqYJAqZKBIxU8OKGdklwWQ.png', 'https://miro.medium.com/max/60/1*0iLj4G7TPcmLRl4HjRg5Ug.png?q=20', 'https://miro.medium.com/max/60/1*sR1o088FB_07s16qZfDlDg.jpeg?q=20', 'https://miro.medium.com/max/1280/1*L5YLQUYFuZ5KXD_LQr167g.jpeg', 'https://miro.medium.com/max/1030/1*fVZyy09cfDzYQTvILCqtEQ.png', 'https://miro.medium.com/max/1110/1*JqIGQLCCc9bFgUk-0RJb4w.png', 'https://miro.medium.com/max/60/1*2pEYsGuTjA2pN-UjjOjXxw.jpeg?q=20', 'https://miro.medium.com/max/60/1*L5YLQUYFuZ5KXD_LQr167g.jpeg?q=20', 'https://miro.medium.com/max/60/1*WjQ00YC4gXA7vgdr6x7kgA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*VkuC2TEJ5mhj1qphkmlKlg.png?q=20', 'https://miro.medium.com/max/1020/1*VkuC2TEJ5mhj1qphkmlKlg.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/480/1*N2XdDMI0aqRRB6Odcpk50Q.jpeg', 'https://miro.medium.com/max/1280/1*bJvhmYNxVxkkW_2sL03xaQ.jpeg', 'https://miro.medium.com/max/1280/1*YhFBGHfz6oPY0b8GhaVuaA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*Asb9N4lW4pQN_Id2qwtPOA.jpeg', 'https://miro.medium.com/max/60/1*cqYJAqZKBIxU8OKGdklwWQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*Asb9N4lW4pQN_Id2qwtPOA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1280/1*WjQ00YC4gXA7vgdr6x7kgA.jpeg', 'https://miro.medium.com/max/60/1*fVZyy09cfDzYQTvILCqtEQ.png?q=20', 'https://miro.medium.com/max/46/1*N2XdDMI0aqRRB6Odcpk50Q.jpeg?q=20', 'https://miro.medium.com/max/1280/1*Nf_5RSDQ7fG2dCbQC8PVbQ.jpeg', 'https://miro.medium.com/max/60/1*YhFBGHfz6oPY0b8GhaVuaA.jpeg?q=20', 'https://miro.medium.com/max/11536/1*_ROiCDwhoeL2pvpA5cWEmA.jpeg', 'https://miro.medium.com/max/1012/1*0iLj4G7TPcmLRl4HjRg5Ug.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*h8yQCZY1A6pKXlTH6nMe3g.png?q=20', 'https://miro.medium.com/proxy/1*oPkqiu1rrt-hC_lDMK-jQg.png', 'https://miro.medium.com/max/1020/1*7ni3XWgN9WGSjgmOn84INA.png', 'https://miro.medium.com/max/60/1*VXFoPAllLJYd5n1Dk4TH_A.jpeg?q=20', 'https://miro.medium.com/max/1280/1*2pEYsGuTjA2pN-UjjOjXxw.jpeg', 'https://miro.medium.com/max/1280/1*sR1o088FB_07s16qZfDlDg.jpeg', 'https://miro.medium.com/max/1028/1*yFQLITYTvlt7F753_75FAw.png', 'https://miro.medium.com/max/60/1*_ROiCDwhoeL2pvpA5cWEmA.jpeg?q=20', 'https://miro.medium.com/max/60/1*7ni3XWgN9WGSjgmOn84INA.png?q=20', 'https://miro.medium.com/max/60/1*yFQLITYTvlt7F753_75FAw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1280/1*VXFoPAllLJYd5n1Dk4TH_A.jpeg', 'https://miro.medium.com/max/60/1*bJvhmYNxVxkkW_2sL03xaQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*JqIGQLCCc9bFgUk-0RJb4w.png?q=20', 'https://miro.medium.com/max/960/1*N2XdDMI0aqRRB6Odcpk50Q.jpeg', 'https://miro.medium.com/max/60/1*Nf_5RSDQ7fG2dCbQC8PVbQ.jpeg?q=20'}",2020-03-05 00:18:09.316177,3.0016636848449707
https://medium.com/comet-ml/monitoring-machine-learning-model-results-live-from-jupyter-notebooks-765a142069bb,Monitoring machine learning model results live from Jupyter notebooks,"Monitoring machine learning model results live from Jupyter notebooks

Tracking and saving your model results just got that much easier with Comet.ml

For many data scientists, Jupyter notebooks have become the tool of choice. Its ability to combine software code, computational output, explanatory text, and multimedia into a single document has helped countless users easily create tutorials, iterate more quickly, and showcase their work externally.

A recent Nature article cites a Github analysis that counted “more than 2.5 million public Jupyter notebooks in September 2018, up from 200,000 or so in 2015.”

The larger Project Jupyter ecosystem extends beyond the notebook — Jupyter’s newest release called JupyterLab extends the notebook framework with features such as file browsers, chat functionality, and text editors. Companies have also released tools based on notebooks where kernels reside on the cloud — most prominently, Google with their Colaboratory project.","['machine', 'monitoring', 'text', 'project', 'users', 'live', 'notebook', 'learning', 'model', 'work', 'extends', 'results', 'jupyter', 'notebooks']","Monitoring machine learning model results live from Jupyter notebooksTracking and saving your model results just got that much easier with Comet.mlFor many data scientists, Jupyter notebooks have become the tool of choice.
Its ability to combine software code, computational output, explanatory text, and multimedia into a single document has helped countless users easily create tutorials, iterate more quickly, and showcase their work externally.
A recent Nature article cites a Github analysis that counted “more than 2.5 million public Jupyter notebooks in September 2018, up from 200,000 or so in 2015.”The larger Project Jupyter ecosystem extends beyond the notebook — Jupyter’s newest release called JupyterLab extends the notebook framework with features such as file browsers, chat functionality, and text editors.
Companies have also released tools based on notebooks where kernels reside on the cloud — most prominently, Google with their Colaboratory project.",en,['Cecelia Shao'],2018-11-08 17:31:21.751000+00:00,"{'Visualization', 'Machine Learning', 'Jupyter Notebook', 'Data Science'}","{'https://miro.medium.com/fit/c/160/160/1*AyOql8hYXu23ZlZv2wmVfw.png', 'https://miro.medium.com/fit/c/96/96/1*AyOql8hYXu23ZlZv2wmVfw.png', 'https://miro.medium.com/max/822/1*nVaFsve3pmREeUb60U4YKQ.png', 'https://miro.medium.com/max/124/1*5YHTtkVBRszQgESSqWS1FA.png', 'https://miro.medium.com/max/52/1*nVaFsve3pmREeUb60U4YKQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*AyOql8hYXu23ZlZv2wmVfw.png', 'https://miro.medium.com/fit/c/80/80/1*wFxB7KATUkWZVX_xyvGKuw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*3ukGil0uKnhGuqLjT26hmg.png', 'https://miro.medium.com/fit/c/80/80/0*w8Q0QfC3MXUStxPM.', 'https://miro.medium.com/max/1644/1*nVaFsve3pmREeUb60U4YKQ.png'}",2020-03-05 00:18:10.218498,0.9023211002349854
https://medium.com/comet-ml,Comet.ml – Medium,You can check out part 1 of this series here,"['series', 'check', 'cometml', 'medium']",You can check out part 1 of this series here,,[],,set(),"{'https://cdn-images-1.medium.com/max/184/1*3ukGil0uKnhGuqLjT26hmg@2x.png', 'https://cdn-images-1.medium.com/max/1200/1*3ukGil0uKnhGuqLjT26hmg.png', 'https://cdn-images-1.medium.com/fit/c/72/72/1*wFxB7KATUkWZVX_xyvGKuw.jpeg', 'https://cdn-images-1.medium.com/fit/c/72/72/0*w8Q0QfC3MXUStxPM.'}",2020-03-05 00:18:10.914475,0.6959762573242188
https://medium.com/@ab9.bhatia/set-up-gpu-accelerated-tensorflow-keras-on-windows-10-with-anaconda-e71bfa9506d1,Set up GPU Accelerated Tensorflow & Keras on Windows 10 with Anaconda,"In this post I will outline how to configure & install the drivers and packages needed to set up Keras deep learning framework on Windows 10 on both GPU & CPU systems.

Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.

Use Keras if you need a deep learning library that:

Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).

Supports both convolutional networks and recurrent networks, as well as combinations of the two.

Runs seamlessly on CPU and GPU.

Here we will use tensorflow as a backend for Keras,

Environment

I am using below configurations which may vary for you:

Acer Predator Helios 300:

Windows 10 Professional

NVIDIA GeForce GTX 1060 6GB GDDR5

Core i7–7700 HQ

16GB DDR4 RAM.

Setting up Backend for Tensorflow

Requirements to run TensorFlow with GPU support:

Download & Install the latest version of Anaconda

2. Download & Install Visual Studio 2015

Visual Studio 2015 is not the latest version but mandatory! Signup and registration with Visual Studio Dev Essentials are required to download older version.

3. CUDA Toolkit 9.0

The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications.

CUDA site will show the latest(currently v10) to download. Please make sure Tensorflow requires CUDA 9.0. You can download CUDA 9.0 from here. You need to sign up to download the older version.

Note : You may get the following warning, this message appears because the installer searches for ‘compatible graphics hardware’ that was released before the installation program was made. Thus, any newer video cards released recently will trigger that message, because they have not been hard-coded as ‘compatible hardware’ in NVIDIA’s installation binary.

Thus it is safe to ignore this message on any recent card, as it will be CUDA enabled.

4. cuDNN v 7.0 for CUDA 8.0

The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers.

You can Download cuDNN v7.1.4 (May 16, 2018), for CUDA 9.0

The cuDNN library contains three files: \bin\cudnn64_7.dll (the version number may be different), \include\cudnn.h and \lib\x64\cudnn.lib. You should copy them to the following locations:

%CUDA_Installation_directory%\bin\cudnn64_7.dll

% CUDA_Installation_directory %\include\cudnn.h

% CUDA_Installation_directory %\lib\x64\cudnn.lib

By default, % CUDA_Installation_directory % points to

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0

5. Environment Variables

Add the following entries in Environment Variables > System variables > Path:

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64

6. Restart the machine.

Install Tensorflow

Create Tensorflow Environment

Open Anaconda Prompt, enter the following command

conda create --name tensorflow --clone root

— clone root will inherit the libraries from default python to tensorflow environment.

;;

Note : At many places you may find following command ‘ conda create -n tensorflow python=3.5’ but it will not inherit the default libraries from default python to tensorflow environment you have to explicitly install all the libraries under tensorflow environment such as numpy, pandas, sklearn etc.

Edit: If you are getting some warning of “ SafetyError:” while running the above command, that is probably a bad package installed at your root environment. You can run below commands and create the environment again.

conda env remove --tensorflow #Remove the environment

conda clean --packages --tarballs # Clean unused cached packages.

2. Install Tensorflow for GPU

Enter the following commands:

activate tensorflow

pip install --ignore-installed --upgrade tensorflow-gpu

Note: If you want to install a speific version of tensorflow, you can instead use pip install tensorflow-gpu==1.8.0

3. Validate your installation

Open Jupyter Notebook, under tensorflow environment by running the following commands on Command Prompt

call activate tensorflow

jupyter notebook

Enter the following commands in Jupyter Notebook

import tensorflow as tf print(tf.__version__) 1.10.0

Install Keras

Open Anaconda Prompt, open tensorflow environment by using ‘activate tensorflow environment’ & enter the following command

conda install keras

Validate your installation by running the following commands in Jupyter Notebook.

import keras Using TensorFlow backend. print(keras.__version__) 2.1.6

How to check if the code is running on GPU or CPU?

According to the documentation.

If you are running on the TensorFlow or CNTK backends, your code will automatically run on GPU if any available GPU is detected.

You can check what all devices are used by tensorflow by -

from tensorflow.python.client import device_lib

print(device_lib.list_local_devices()) [name: ""/device:CPU:0""

device_type: ""CPU""

memory_limit: 268435456

locality {

}

incarnation: 8320990378049208634

, name: ""/device:GPU:0""

device_type: ""GPU""

memory_limit: 4952267161

locality {

bus_id: 1

links {

}

}

incarnation: 2490779436580148339

physical_device_desc: ""device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1""

]

Alternative : TensorFlow with CPU support only

Alternatively, if you want to install Keras on Tensorflow with CPU support only that is much simpler than GPU installation, there is no need of CUDA Toolkit & Visual Studio & will take 5–10 minutes.

You just need to the following steps:

Download & Install the latest version of Anaconda From Anaconda Prompt run the following commands

conda create — name tensorflow — clone root

activate tensorflow

pip install --upgrade tensorflow

conda install keras

3. Validate your installation.

Summary

This article gives you a starting point for building a deep learning setup running with Keras and TensorFlow both on GPU & CPU environment.

Like and share if you find this helpful!

Connect with me on Linkedin.","['set', 'tensorflow', 'version', 'install', 'cuda', 'environment', 'anaconda', 'keras', 'accelerated', 'download', 'windows', 'gpu', 'following', 'running']","Setting up Backend for TensorflowRequirements to run TensorFlow with GPU support:Download & Install the latest version of Anaconda2.
Environment VariablesAdd the following entries in Environment Variables > System variables > Path:C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvpC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\binC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x646.
Install TensorflowCreate Tensorflow EnvironmentOpen Anaconda Prompt, enter the following commandconda create --name tensorflow --clone root— clone root will inherit the libraries from default python to tensorflow environment.
Install Tensorflow for GPUEnter the following commands:activate tensorflowpip install --ignore-installed --upgrade tensorflow-gpuNote: If you want to install a speific version of tensorflow, you can instead use pip install tensorflow-gpu==1.8.03.
SummaryThis article gives you a starting point for building a deep learning setup running with Keras and TensorFlow both on GPU & CPU environment.",en,['Ankit Bhatia'],2018-10-07 05:32:34.058000+00:00,"{'Deep Learning', 'Machine Learning', 'Cuda', 'Keras', 'Gpu'}","{'https://miro.medium.com/max/1200/1*__qgmBIfEQ0qKXMWooxWjw.jpeg', 'https://miro.medium.com/fit/c/96/96/0*7w1xi1hIawzZezha.', 'https://miro.medium.com/max/600/1*__qgmBIfEQ0qKXMWooxWjw.jpeg', 'https://miro.medium.com/max/60/1*9CAukhvMzk1-oQqvfnUSsg.png?q=20', 'https://miro.medium.com/max/3378/1*HqqgxyxaYWxHjkP3U-am8w.png', 'https://miro.medium.com/max/60/1*ef3-E0BZ1JtgLWdkxqASzg.png?q=20', 'https://miro.medium.com/max/2150/1*9CAukhvMzk1-oQqvfnUSsg.png', 'https://miro.medium.com/max/60/1*HqqgxyxaYWxHjkP3U-am8w.png?q=20', 'https://miro.medium.com/max/2862/1*1baPqkcfi-drOswvEDeDBw.png', 'https://miro.medium.com/max/60/1*tWTDUJqCmTqasOiJNUjmYg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*yHywWq0OKRegZe4mnHJCZQ.png', 'https://miro.medium.com/max/1590/1*D3YxflhCJ6Z5NlhWTMq0KA.png', 'https://miro.medium.com/max/3142/1*qJVtJZ490Mrr45379ZPxsw.png', 'https://miro.medium.com/max/60/1*__qgmBIfEQ0qKXMWooxWjw.jpeg?q=20', 'https://miro.medium.com/max/2146/1*tWTDUJqCmTqasOiJNUjmYg.png', 'https://miro.medium.com/max/60/1*qJVtJZ490Mrr45379ZPxsw.png?q=20', 'https://miro.medium.com/max/44/1*lW9dZ4ZOLwvCVGnw-iyjiw.png?q=20', 'https://miro.medium.com/max/1460/1*ef3-E0BZ1JtgLWdkxqASzg.png', 'https://miro.medium.com/fit/c/80/80/2*JmACd2lraXx0tDdhjjn6jQ.jpeg', 'https://miro.medium.com/max/970/1*lW9dZ4ZOLwvCVGnw-iyjiw.png', 'https://miro.medium.com/max/60/1*kY_O-3T7jbHy_STOu70vaw.png?q=20', 'https://miro.medium.com/max/1784/1*kY_O-3T7jbHy_STOu70vaw.png', 'https://miro.medium.com/max/60/1*1baPqkcfi-drOswvEDeDBw.png?q=20', 'https://miro.medium.com/max/60/1*D3YxflhCJ6Z5NlhWTMq0KA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*rVCl1aFk3DUjoeUdz2vn7Q.jpeg', 'https://miro.medium.com/max/60/1*Me2KbGQeJdzLMnVC9E3E_g.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*7w1xi1hIawzZezha.', 'https://miro.medium.com/max/1052/1*Me2KbGQeJdzLMnVC9E3E_g.png'}",2020-03-05 00:18:11.967985,1.0535099506378174
https://towardsdatascience.com/10-simple-hacks-to-speed-up-your-data-analysis-in-python-ec18c6396e6b,10 Simple hacks to speed up your Data Analysis in Python,"1. Profiling the pandas dataframe

Profiling is a process that helps us in understanding our data and Pandas Profiling is a python package that does exactly that. It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The pandas df.describe() and df.info()functions are normally used as a first step in the EDA process. However, it only gives a very basic overview of the data and doesn’t help much in the case of large data sets. The Pandas Profiling function, on the other hand, extends the pandas DataFrame with df.profile_report() for quick data analysis. It displays a lot of information with a single line of code and that too in an interactive HTML report.

For a given dataset the pandas profiling package computes the following statistics:

Statistics computer by Pandas Profiling package.

Installation

pip install pandas-profiling

or

conda install -c anaconda pandas-profiling

Usage

Let’s use the age-old titanic dataset to demonstrate the capabilities of the versatile python profiler.

#importing the necessary packages

import pandas as pd

import pandas_profiling #Pandas-Profiling 2.0.0

df = pd.read_csv('titanic/train.csv')

df.profile_report()

This single line of code is all that you need to display the data profiling report in a Jupyter notebook. The report is pretty detailed including charts wherever necessary.

The report can also be exported into an interactive HTML file with the following code.

profile = df.profile_report(title='Pandas Profiling Report')

profile.to_file(outputfile=""Titanic data profiling.html"")

Refer the documentation for more details and examples.","['package', 'speed', 'report', 'pandas', 'single', 'hacks', 'process', 'simple', 'python', 'profiling', 'data', 'interactive', 'line', 'analysis']","Profiling the pandas dataframeProfiling is a process that helps us in understanding our data and Pandas Profiling is a python package that does exactly that.
It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe.
The Pandas Profiling function, on the other hand, extends the pandas DataFrame with df.profile_report() for quick data analysis.
It displays a lot of information with a single line of code and that too in an interactive HTML report.
For a given dataset the pandas profiling package computes the following statistics:Statistics computer by Pandas Profiling package.",en,['Parul Pandey'],2019-12-14 12:30:18.658000+00:00,"{'Python', 'Data Scien', 'Towards Data Science', 'Data Analysis', 'Jupyter Notebook'}","{'https://miro.medium.com/max/60/1*-4ldDyihHAwR-czg9nLuUw.png?q=20', 'https://miro.medium.com/max/60/1*kzOEVvwEx50uEI27Con9xQ.png?q=20', 'https://miro.medium.com/max/1200/1*E6ZU7TRjiPNpFEdxuQ7WmA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/60/1*E6ZU7TRjiPNpFEdxuQ7WmA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1614/1*717JXTHKay06ppdjDpOBPw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*aXqVXL-5WZFltIGbUidqpg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*g3EhNGxRT2FOSCRD0swo9A.gif?q=20', 'https://miro.medium.com/max/2010/1*-4ldDyihHAwR-czg9nLuUw.png', 'https://miro.medium.com/max/60/1*IAtw6rydG7o58yy2EyzCRA.png?q=20', 'https://miro.medium.com/max/1392/1*g3EhNGxRT2FOSCRD0swo9A.gif', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2012/1*kzOEVvwEx50uEI27Con9xQ.png', 'https://miro.medium.com/max/60/1*5p2-kMkzKnBR7WARU4u_Wg.png?q=20', 'https://miro.medium.com/max/1906/1*bJQZjROUMl4Gwl6Pk2oVEQ.gif', 'https://miro.medium.com/max/990/1*T2iRcSpLLxXop7Naa4ln0g.png', 'https://miro.medium.com/max/60/1*717JXTHKay06ppdjDpOBPw.png?q=20', 'https://miro.medium.com/max/60/1*0buqqzdeShrVbiV9-2W39Q.png?q=20', 'https://miro.medium.com/max/1284/1*Qqsl_6xGeccaTU1AjAibrA.gif', 'https://miro.medium.com/freeze/max/60/1*iqLgI-YaaV4iE6LDySSE2g.gif?q=20', 'https://miro.medium.com/max/60/1*G5JF-JXjEjX8AaoNpEk-aQ.png?q=20', 'https://miro.medium.com/max/666/1*5p2-kMkzKnBR7WARU4u_Wg.png', 'https://miro.medium.com/max/60/1*cK6E96d4e5R6wBrQVkd8nA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*XInqaE5tHueOrxn_--uzEA.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1002/1*aXqVXL-5WZFltIGbUidqpg.png', 'https://miro.medium.com/freeze/max/60/1*Oms7fW4rNlU0NaMUf9qYmA.gif?q=20', 'https://miro.medium.com/max/2316/1*XInqaE5tHueOrxn_--uzEA.gif', 'https://miro.medium.com/max/758/1*G5JF-JXjEjX8AaoNpEk-aQ.png', 'https://miro.medium.com/max/1668/1*IAtw6rydG7o58yy2EyzCRA.png', 'https://miro.medium.com/max/1602/1*K983l0I1yJoOPFhu9M3PIA.png', 'https://miro.medium.com/max/968/1*YUY7ITHRA3KyfaOjhCjmBg.png', 'https://miro.medium.com/max/1688/1*pWAbxYovjtwQyFSaOwoQbg.gif', 'https://miro.medium.com/max/2054/1*Oms7fW4rNlU0NaMUf9qYmA.gif', 'https://miro.medium.com/freeze/max/60/1*Qqsl_6xGeccaTU1AjAibrA.gif?q=20', 'https://miro.medium.com/max/60/1*Ps7XZABa5BNlg7Ezb-K8UQ.png?q=20', 'https://miro.medium.com/max/60/1*K983l0I1yJoOPFhu9M3PIA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*T2iRcSpLLxXop7Naa4ln0g.png?q=20', 'https://miro.medium.com/max/60/1*YUY7ITHRA3KyfaOjhCjmBg.png?q=20', 'https://miro.medium.com/max/3840/1*E6ZU7TRjiPNpFEdxuQ7WmA.jpeg', 'https://miro.medium.com/max/2002/1*Ps7XZABa5BNlg7Ezb-K8UQ.png', 'https://miro.medium.com/freeze/max/60/1*pWAbxYovjtwQyFSaOwoQbg.gif?q=20', 'https://miro.medium.com/max/2020/1*iqLgI-YaaV4iE6LDySSE2g.gif', 'https://miro.medium.com/max/1436/1*cK6E96d4e5R6wBrQVkd8nA.png', 'https://miro.medium.com/max/2006/1*0buqqzdeShrVbiV9-2W39Q.png', 'https://miro.medium.com/freeze/max/60/1*bJQZjROUMl4Gwl6Pk2oVEQ.gif?q=20'}",2020-03-05 00:18:26.108041,8.407217264175415
https://medium.com/@kadek/elegantly-reading-multiple-csvs-into-pandas-e1a76843b688,Elegantly Reading Multiple CSVs Into Pandas,"Reading multiple CSVs into Pandas is fairly routine. However, there isn’t one clearly right way to perform this task. This often leads to a lot of interesting attempts with varying levels of exoticism.

In an effort to push my own agenda I’m documenting my process.

Dask

One of the cooler features of Dask, a Python library for parallel computing, is the ability to read in CSVs by matching a pattern.

This small quirk ends up solving quite a few problems. Despite this, the raw power of Dask isn’t always required, so it’d be nice to have a Pandas equivalent.

Glob

The python module glob provides Unix style pathname pattern expansion. Therefore, using glob.glob('*.gif') will give us all the .gif files in a directory as a list.

my_data/

data1.csv

data2.csv

data3.csv

data4.csv

OS

Another way to potentially combat this problem is by using the os module.

my_data/

data_blah.csv

more_data.csv

yet_another.csv

lots_of_data.csv

One-Liner

Now comes the fun part. More or less, this dance usually boils down to two functions: pd.read_csv() and pd.concat() . There are a variety of ways to call them, however I feel this is a scenario in which a little cleverness is apt.

The real beauty of this method is that it still allows for you to configure how you read in your .csv files. For instance, if our encoding was was latin1 instead of UTF-8 .

Turning into the Oracle of One-Liners shouldn’t be anyone’s goal. Yet, reading in data is something that happens so frequently that it feels like an ideal use case. Find the files I want, read them in how I want, and…boom! One nice compact dataframe ready for analysis.","['reading', 'way', 'nice', 'files', 'isnt', 'read', 'pandas', 'multiple', 'python', 'dask', 'csvs', 'using', 'elegantly']","Reading multiple CSVs into Pandas is fairly routine.
DaskOne of the cooler features of Dask, a Python library for parallel computing, is the ability to read in CSVs by matching a pattern.
Despite this, the raw power of Dask isn’t always required, so it’d be nice to have a Pandas equivalent.
The real beauty of this method is that it still allows for you to configure how you read in your .csv files.
Yet, reading in data is something that happens so frequently that it feels like an ideal use case.",en,['Kade Killary'],2018-04-23 13:01:38.328000+00:00,"{'Coding', 'Data Science', 'Tech', 'Python', 'Technology'}","{'https://miro.medium.com/fit/c/80/80/2*fq-GnAJAKfvPVlk5dXV10w.jpeg', 'https://miro.medium.com/max/2804/1*YYazma47NGN08r_G0nnmkw.png', 'https://miro.medium.com/max/1200/1*YYazma47NGN08r_G0nnmkw.png', 'https://miro.medium.com/fit/c/80/80/1*4-DIjB5ZOAVpLPbCXVVO1Q.jpeg', 'https://miro.medium.com/fit/c/96/96/2*fq-GnAJAKfvPVlk5dXV10w.jpeg', 'https://miro.medium.com/fit/c/160/160/2*fq-GnAJAKfvPVlk5dXV10w.jpeg', 'https://miro.medium.com/fit/c/80/80/1*dYWJ4KAcfVidr1S97zVkzg.jpeg', 'https://miro.medium.com/max/60/1*YYazma47NGN08r_G0nnmkw.png?q=20'}",2020-03-05 00:18:27.467453,1.358415126800537
https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7,"Data pipelines, Luigi, Airflow: everything you need to know","This post is based on a talk I recently gave to my colleagues about Airflow.

In particular, the focus of the talk was: what’s Airflow, what can you do with it and how it differs from Luigi.

Why do you need a WMS

It’s really common in a company to have to move and transform data.

For example, you have plenty of logs stored somewhere on S3, and you want to periodically take that data, extract and aggregate meaningful information and then store them in an analytics DB (e.g., Redshift).

Usually, this kind of tasks are first performed manually, then, as things need to scale up, the process is automated and for example, triggered with cron. Ultimately, you reach a point where the good old cron is not able to guarantee a stable and robust performance. It’s simply not enough anymore.

That’s when you need a workflow management system (WMS).

Airflow

Airflow was developed at Airbnb in 2014 and it was later open-sourced. In 2016 it joined the Apache Software Foundation’s incubation program.

When asked “What makes Airflow different in the WMS landscape?”, Maxime Beauchemin (creator or Airflow) answered:

A key differentiator is the fact that Airflow pipelines are defined as code and that tasks are instantiated dynamically.

Hopefully, at the end of this post, you will be able to understand and, more importantly, to agree (or disagree) with this statement.

Let’s first define the main concepts.

Workflows as DAGs

In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG).

Each node in the graph is a task, and edges define dependencies among the tasks.

Tasks belong to two categories:

Operators: they execute some operation Sensors: they check for the state of a process or a data structure

Real-life workflows can go from just one task per workflow (you don’t always have to be fancy) to very complicated DAGs, almost impossible to visualise.

Main components

The main components of Airflow are:

a Metadata Database

a Scheduler

an Executor

Airflow architecture

The metadata database stores the state of tasks and workflows. The scheduler uses the DAGs definitions, together with the state of tasks in the metadata database, and decides what needs to be executed.

The executor is a message queuing process (usually Celery) which decides which worker will execute each task.

With the Celery executor, it is possible to manage the distributed execution of tasks. An alternative is to run the scheduler and executor on the same machine. In that case, the parallelism will be managed using multiple processes.

Airflow provides also a very powerful UI. The user is able to monitor DAGs and tasks execution and directly interact with them through a web UI.

It is common to read that Airflow follows a “set it and forget it” approach, but what does that mean?

It means that once a DAG is set, the scheduler will automatically schedule it to run according to the specified scheduling interval.

Luigi

The easiest way to understand Airflow is probably to compare it to Luigi.

Luigi is a python package to build complex pipelines and it was developed at Spotify.

In Luigi, as in Airflow, you can specify workflows as tasks and dependencies between them.

The two building blocks of Luigi are Tasks and Targets. A target is a file usually outputted by a task, a task performs computations and consumes targets generated by other tasks.

Luigi pipeline structure

You can think about it as an actual pipeline. A task does its job and generates a target as a result, a second task takes the target file in input, performs some operations and output a second target file and so on.

Coffee break (Photo by rawpixel on Unsplash)

A simple workflow

Let’s see how we can implement a simple pipeline composed of two tasks.

The first task generate a .txt file with a word (“pipeline” in this case), a second task reads the file and decorate the line adding “My”. The new line is written on a new file.

Luigi simple pipeline

Each task is specified as a class derived from luigi.Task , the method output() specifies the output thus the target, run() specifies the actual computations performed by the task.

The method requires() specifies the dependencies between the tasks.

From the code, it’s pretty straightforward to see that the input of a task is the output of the other and so on.

Let’s see how we can do the same thing in Airflow.

Airflow simple DAG

First, we define and initialise the DAG, then we add two operators to the DAG.

The first one is a BashOperator which can basically run every bash command or script, the second one is a PythonOperator executing python code (I used two different operators here for the sake of presentation).

As you can see, there are no concepts of input and output. No information is shared between the two operators. There are ways to share information between operators (you basically share a string), but as a general rule: if two operators need to share information, then they should be probably combined into a single one.

A more complex workflow

Let’s now consider the case where we want to process more files at the same time.

In Luigi we can do it in multiple ways, none of which is really straightforward.

Luigi a pipeline managing multiple files

In this case, we have two tasks, each one of them processes all the files. The dependent task ( t2 ) has to wait until t1 has processed all the files.

We used an empty file as a target to flag when each task finished its job.

We could add some parallelisation writing parallel for loops.

The problem with this solution is that t2 could start to process files gradually as soon as t1 started to produce its output, actually t2 does not have to wait until all the files are created by t1 .

A common pattern in Luigi to do this is to create a wrapper task and use multiple workers.

Here is the code.

Luigi a pipeline using multiple workers

To run the task with multiple workers we can specify — workers number_of_workers when running the task.

A very common approach that you see in real life is to delegate the parallelisation. Basically, you use the first approach presented and you use Spark for example, inside the run() function, to actually do the processing.

Let’s do it with Airflow

Do you remember that in the initial quote it was written that DAGs are instantiated dynamically with code?

But what does that mean exactly?

It means that with Airflow you can do this

Airflow a parallel DAG with multiple files

Tasks (and dependencies) can be added programmatically (e.g. in a for loop). The corresponding DAG looks like this.

Parallel DAG

At this point, you don’t have to worry about parallelisation. The Airflow executor knows from the DAG definition, that each branch can be run in parallel and that’s what it does!

Final Considerations

We touched a lot of points in this post, we spoke about workflows, about Luigi, about Airflow and how they differ.

Let’s make a quick recap.

Luigi

It’s generally based on pipelines, tasks input and output share information and is connected together

Target-based approach

UI is minimal, there is no user interaction with running processes

Does not have it’s own triggering

Luigi does not support distributed execution

Airflow","['know', 'file', 'task', 'luigi', 'process', 'need', 'tasks', 'multiple', 'target', 'data', 'airflow', 'output', 'run', 'pipelines', 'pipeline']","In particular, the focus of the talk was: what’s Airflow, what can you do with it and how it differs from Luigi.
When asked “What makes Airflow different in the WMS landscape?”, Maxime Beauchemin (creator or Airflow) answered:A key differentiator is the fact that Airflow pipelines are defined as code and that tasks are instantiated dynamically.
Luigi a pipeline managing multiple filesIn this case, we have two tasks, each one of them processes all the files.
Luigi a pipeline using multiple workersTo run the task with multiple workers we can specify — workers number_of_workers when running the task.
Final ConsiderationsWe touched a lot of points in this post, we spoke about workflows, about Luigi, about Airflow and how they differ.",en,['Lorenzo Peppoloni'],2018-10-14 17:28:35.308000+00:00,"{'Data Pipe', 'Data Science', 'Data Engineering', 'Tech', 'Technology'}","{'https://miro.medium.com/max/10484/1*RuiyLX5EU0eY41GEQW7EuA.jpeg', 'https://miro.medium.com/max/1852/1*vVV3tw3ZS1XQLZCNAfrMeQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*6H39Hc_zPIRxNjZbFmNbSA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*CGdjyJT2_UxTrv3oHbB1Eg@2x.jpeg', 'https://miro.medium.com/max/8000/1*6H39Hc_zPIRxNjZbFmNbSA.jpeg', 'https://miro.medium.com/max/60/1*RuiyLX5EU0eY41GEQW7EuA.jpeg?q=20', 'https://miro.medium.com/max/1600/1*-wDxsSr5_2NyOHxysroYyA.png', 'https://miro.medium.com/max/56/1*4xY3fyTu5reN2zIpYEUchQ.png?q=20', 'https://miro.medium.com/max/60/1*-wDxsSr5_2NyOHxysroYyA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1024/1*4xY3fyTu5reN2zIpYEUchQ.png', 'https://miro.medium.com/fit/c/160/160/1*CGdjyJT2_UxTrv3oHbB1Eg@2x.jpeg', 'https://miro.medium.com/max/1200/1*RuiyLX5EU0eY41GEQW7EuA.jpeg', 'https://miro.medium.com/max/60/1*vVV3tw3ZS1XQLZCNAfrMeQ.png?q=20'}",2020-03-05 00:18:34.249665,6.781212091445923
https://medium.com/dataxutech/dataxus-journey-from-an-enterprise-mpp-database-to-a-cloud-native-data-warehouse-part-1-e36952d87a1f,"dataxu’s journey from an Enterprise MPP database to a cloud-native data warehouse, Part 1","This is part 1 of a series of blogs on dataxu’s efforts to build out a cloud-native data warehouse and our learnings in that process.

At dataxu, we deal with data collection, storage, processing, analysis, and consumption at massive scale. For this reason, we were an early adopter of the Hadoop framework. We quickly discovered that Hadoop and Hive alone were not sufficient for the growing needs of interactive analysis and querying. And so, about five years ago, we incorporated an MPP database as our warehouse solution.

The on-premise solution served us well as the cluster size expanded 16 fold over the course of five years. However, even with the addition of an MPP, we started to run into significant operational challenges:

While it is possible to expand the MPP database, it takes months of planning and execution. The capacity planning is particularly tricky. If the business experienced unexpected growth, it would be difficult to bring additional capacity online in a timely fashion. If the business growth slowed, we could be stuck with an over-sized cluster for a number of months, before the volume eventually caught up.

The capacity planning is particularly tricky. If the business experienced unexpected growth, it would be difficult to bring additional capacity online in a timely fashion. If the business growth slowed, we could be stuck with an over-sized cluster for a number of months, before the volume eventually caught up. The database requires constant maintenance , both in terms of hardware (like replacing failed disks) and software (like vacuuming catalog). Moreover, the database constantly experienced failed processes, which requires DBA to perform recovery operations.

, both in terms of hardware (like replacing failed disks) and software (like vacuuming catalog). Moreover, the database constantly experienced failed processes, which requires DBA to perform recovery operations. Ad-hoc query users constantly compete with production ETL loads for the fixed capacity, leading to unpredictable load times and SLA misses.

The MPP solution was clearly not a sustainable option for serving dataxu’s business needs. As such, we started to look for an AWS cloud-native solution. After reviewing several competing solutions, we settled on Apache Spark on EMR as our primary ETL solution and AWS Athena as primary query solution.

In this blog, we will discuss the comparison of a cloud-native warehouse vs. MPP, with some focus on Spark as an ETL solution.

Cloud-native warehouse vs. MPP

First and foremost, the primary reason to choose Spark is not for performance. As it currently stands, even if a Spark cluster is configured with an equivalent amount of CPU, RAM, and disk capacity, it is unlikely to beat the query performance of the MPP solution. There are many reasons why an MPP database will “beat” Apache Spark on paper:

EMR clusters run on VMs, while MPP on-prem runs on highly tuned bare metal servers.

EMR clusters run in VPC, while MPP on-prem has a dedicated network switch and 100Gbit throughput.

EMR clusters use S3 as storage, while MPP on-prem has superior I/O performance with direct attached disks on RAID10.

MPP database has years of query optimization expertise, while Spark has a lot to catch up on.

MPP databases allow for data locality. That means the data can be split into shards by a key — so each fixed node “owns” a shard. A well chosen distribution key cuts down on so-called broadcasts (data movements over network across the nodes).

Reasons for choosing Spark

While an MPP may look like the better option on the surface, there are a number of reasons why here at dataxu we chose to implement Spark as our primary ETL solution.

Separation of Compute and Storage. The Storage tier is S3, with all the durability, infinite scalability, and recoverability built-in. The Compute is EMR with a wide range of possible instance families to match CPU and RAM needs. We also use transient and dedicated EMR clusters for ETL loads, where there is no sharing of workloads. A single cluster is dedicated for only one load, which enables a predictable execution time and guarantees SLA. We never have to worry about having the right balance of storage and compute. We never have to worry about running out of storage or having enough compute. Separation of Metadata and Data. Hive metastore has evolved as the de facto open source standard for managing schema objects, supporting tables, view, partitions, and UDFs. We no longer need to vacuum data catalog which also removes a common locking condition on MPP databases. Dynamic Elasticity. The ability to scale the computing resources to dynamically match the business and technical requirements. In addition, with EMR auto-scaling features, a cluster can be scaled up and down even when active queries are executing and newly provisioned capacity can be put to in-flight query instantly. Reduction of Maintenance. There is no longer a physical database to manage. No more backups, no more disk failures and replacement, and no more shutting down databases for hardware/network maintenance. Resilience and Fault Tolerance. Any tiny issues in MPP will cause a query to fail, while Spark will re-try multiple times. This leads to significantly increased resilience, particularly in the hostile cloud environment. Cost Efficiency. Spot pricing and Instance Fleets yield significant savings over a fixed-cost, on-prem solution. Open Source. As the most active Big Data open source project, the amount of contributions for Spark hugely dwarfs that for any other MPP solutions out there.

Electrical wiring: A visual analogy

For simplicity’s sake, we can compare MPP vs. the cloud-native warehouse to different methods of electrical wiring:

The left side is a serial circuit with no switches. The capacity is fixed, has to size for peak capacity and is always on. Bringing in a new workload meaning less throughput to all existing workloads, such is the case with an MPP solution.

On the other hand, the right side is a parallel circuit with switches. The capacity is elastic — you can use and pay when needed. Bringing on a new workload does not diminish or interfere with any existing workload, such is the case with our cloud-native warehouse.

Cloud-native warehouse: Endless possibilities

With cloud-native warehouse, there is no longer a monolithic database stack. The database is decomposed into

Storage (open file formats on S3). File formats are open, like Parquet or ORC, no longer locked into proprietary formats, support queries via many different platforms and query engines.

(open file formats on S3). File formats are open, like Parquet or ORC, no longer locked into proprietary formats, support queries via many different platforms and query engines. Compute (EMR, Athena, Redshift-Spectrum), with pluggable query engines like Spark or Presto, also platforms like Qubole.

(EMR, Athena, Redshift-Spectrum), with pluggable query engines like Spark or Presto, also platforms like Qubole. Metadata (schema) layer, can be hosted on RDS-Aurora or the AWS Glue-Data catalog.

Each of the component layers has many options that allow you to piece together different systems with different characteristics, suitable for a variety of use cases.

dataxu’s Cloud-native warehouse:

Performance is better with Spark

And finally, let’s take a look at the performance benefits Spark brings to the table:

As we iterate over versions of Spark, from 1.6, to 2.0 and 2.2, the common thread is a significant performance improvement with each release, see discussions here and here. With this background, we are confident that the Spark project will continue on this trajectory. The separation of compute and storage is huge. Running a single query as the sole workload will perform better on MPP over Spark on EMR, yet, in reality, the MPP database is almost always shared among a variety of competing workloads, resulting in much slower performance than a dedicated EMR cluster or Athena. And last but not least, with dynamic elasticity, the option to scale out to meet SLA/performance requirements is always available at your fingertips.

Keep an eye out for the second post in this series, where we will take a deeper dive into how we “rewired the house” at dataxu.

Please post your feedback in the comments — what kind of solutions do you employ? If you found this post useful, please feel free to “applause” and share!","['emr', 'cloudnative', 'query', 'cluster', 'dataxus', 'capacity', 'enterprise', 'mpp', 'warehouse', 'data', 'storage', 'database', 'spark', 'journey']","This is part 1 of a series of blogs on dataxu’s efforts to build out a cloud-native data warehouse and our learnings in that process.
And so, about five years ago, we incorporated an MPP database as our warehouse solution.
However, even with the addition of an MPP, we started to run into significant operational challenges:While it is possible to expand the MPP database, it takes months of planning and execution.
There are many reasons why an MPP database will “beat” Apache Spark on paper:EMR clusters run on VMs, while MPP on-prem runs on highly tuned bare metal servers.
Cloud-native warehouse: Endless possibilitiesWith cloud-native warehouse, there is no longer a monolithic database stack.",en,['Dong Jiang'],2017-12-13 19:47:41.788000+00:00,"{'AWS', 'Data Warehouse', 'Data Lake', 'Spark', 'Big Data'}","{'https://miro.medium.com/max/180/1*JJ3wBBYux6DTO2_rJsHK2g.png', 'https://miro.medium.com/max/3200/0*yl71Fj64H4XHtHtn.', 'https://miro.medium.com/max/60/0*k1CK7esxcAqqGXU5.?q=20', 'https://miro.medium.com/fit/c/160/160/1*HROGoxhOb-dl-kKCpbb5zg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*JntdgctFAK6utFzsWw5Xdw.png', 'https://miro.medium.com/fit/c/96/96/1*HROGoxhOb-dl-kKCpbb5zg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*j-M9DGhudr7Vz_dobbuI0Q.jpeg', 'https://miro.medium.com/max/60/0*yl71Fj64H4XHtHtn.?q=20', 'https://miro.medium.com/fit/c/80/80/0*qJO-pZtinxN12iYT.png', 'https://miro.medium.com/fit/c/80/80/1*c6h21bcEPh7ym2SFL49CRg.jpeg', 'https://miro.medium.com/max/1200/0*yl71Fj64H4XHtHtn.', 'https://miro.medium.com/max/2488/0*k1CK7esxcAqqGXU5.'}",2020-03-05 00:18:35.060657,0.8099913597106934
https://medium.com/bitcraft/docker-composing-a-python-3-flask-app-line-by-line-93b721105777,"""Docker-composing"" a Python 3 Flask App Line-by-Line","Docker: Context

Docker is an amazing tool to run apps in isolated environments, regardless of which host they are being deployed to. It allows you to have apps (mostly) independent of the operating system you or your servers are running, which creates the following benefits to your development and deployment workflows:

You don’t clutter your workstation with software you don’t need. Using different versions of the same software on different projects becomes a breeze. When done appropriately, code that is constantly modified by an entire team is almost guaranteed to run on any team member’s workstation with minimal setup time. Your code will behave the same in any workflow environment it is in (e.g. development, staging, production). Deploying is as easy as building a new image, deploying it to a Docker repository, and restarting your containers so that they pull the latest image.

Case scenario

We are building a Python 3 Flask app. We want uWSGI to work as the web server and we want the traffic to be routed through Nginx. These two pieces have their own dependencies, purpose, and responsibilities, so we can isolate each in a container. Therefore, we can build two Dockerfiles for each service, which docker-compose will then spin up, mount volumes for, and configure hosts so that they both can speak to each other.

Note

When working with Docker in production, you probably should not wrap Nginx in a container. Whatever web server you’re using (in our case uWSGI) should be made into its own image, and then you can have load balancers living elsewhere that balance the traffic between your uWSGI container replicas. For static content which Nginx is usually great at serving, you could consider using S3 or some CDN and access it directly from the frontend. I am only running a container for Nginx on this article for the purposes of teaching how to orchestrate and link containers running locally or under one host.

Building our Flask App

First, we define the dependencies for our Python app in a requirements.txt file. Read more on requirement files in the pip documentation.

# requirements.txt Flask==1.0.2

uWSGI==2.0.17.1

Now we can build a very minimal Flask app.py file that defines some logic for our web app. I won’t go deep into how Flask works, but you can learn more in their vast documentation.

# app.py from flask import Flask

app = Flask(__name__) @app.route('/')

def hello_world():

return 'Hello world!' if __name__ == '__main__':

app.run(host='0.0.0.0')

Lastly, we create an app.ini file so that uWSGI knows how to operate.

; app.ini [uwsgi]

protocol = uwsgi ; This is the name of our Python file

; minus the file extension

module = app ; This is the name of the variable

; in our script that will be called

callable = app master = true ; Set uWSGI to start up 5 workers

processes = 5 ; We use the port 5000 which we will

; then expose on our Dockerfile

socket = 0.0.0.0:5000

vacuum = true die-on-term = true

Building our Flask Docker Image

Now that our Flask/uWSGI app is set up, we can Dockerize it. As shown in our previous article, we will write a Dockerfile for it. This one, however, will be more minimalistic, since we won’t be installing Nginx in it, and rather running it separately through docker-compose . The file will be named Dockerfile-flask since we will have two Dockerfiles in this project.

# Dockerfile-flask # We simply inherit the Python 3 image. This image does

# not particularly care what OS runs underneath

FROM python:3 # Set an environment variable with the directory

# where we'll be running the app

ENV APP /app # Create the directory and instruct Docker to operate

# from there from now on

RUN mkdir $APP

WORKDIR $APP # Expose the port uWSGI will listen on

EXPOSE 5000 # Copy the requirements file in order to install

# Python dependencies

COPY requirements.txt . # Install Python dependencies

RUN pip install -r requirements.txt # We copy the rest of the codebase into the image

COPY . . # Finally, we run uWSGI with the ini file we

# created earlier

CMD [ ""uwsgi"", ""--ini"", ""app.ini"" ]

Docker Protip #1

You might have found it odd that I first copy requirements.txt into the image and later the rest of the codebase. I do this because Docker creates layers (or intermediate images) as it builds your image. Each layer is cached, and when a file that previously got copied into the image changes, it invalidates its cache and that of all the following layers. Therefore, we can copy a file that barely ever changes first (i.e. requirements.txt ) and install modules in one go, before even introducing the rest of the codebase which will most likely change after each build, triggering a re-install of all of our modules/libraries.

Configuring our Generic Nginx Container

Here we will create our configuration file that will tell Nginx how to route traffic to uWSGI in our other container. Our app.conf will essentially replace the /etc/nginx/conf.d/default.conf that the Nginx container includes implicitly. Read more on Nginx conf files here.

# app.conf server {

listen 80;

root /usr/share/nginx/html;

location

include uwsgi_params;

uwsgi_pass flask:5000;

}

} location / { try_files $uri @app ; }location @app include uwsgi_params;uwsgi_pass flask:5000;

The line uwsgi_pass flask:5000; is using flask as the host to route traffic to. This is because we will configure docker-compose to connect our Flask and Nginx containers through the flask hostname.

Building our Nginx Docker Image

Our Dockerfile for Nginx is simply going to inherit the latest Nginx image from the Docker registry, remove the default configuration file, and add the configuration file we just created during build. We won’t even use a CMD instruction, since it will just pick up the one from nginx:latest .

We will name the file Dockerfile-nginx .

# Dockerfile-nginx FROM nginx:latest # Nginx will listen on this port

EXPOSE 80 # Remove the default config file that

# /etc/nginx/nginx.conf includes

RUN rm /etc/nginx/conf.d/default.conf # We copy the requirements file in order to install

# Python dependencies

COPY app.conf /etc/nginx/conf.d

Docker Protip #2

You will notice later on this article that I am exposing the port for this container both in its Dockerfile and on docker-compose.yml . I do this because commonly people don’t use docker-compose in production. I use it in development and then I deploy my containers through some separate service (e.g. Kubernetes, ECS, or Heroku). So by exposing the ports in both places, I ensure that different services will know which port to route connections to by just looking at the Dockerfile .

Orchestrating with Docker Compose

Now that our configuration is ready to run the Flask/uWSGI container, we can write the necessary configuration so that the entire stack will run just by typing docker-compose up in our terminal.

All the configuration for docker-compose goes in a YML file called docker-compose.yml . This file usually lives in the root of your project so that running any compose command will automatically pick it up. I won’t dive super deep into the specific syntax for the file, but you can find information on everything you can do with it in the compose file reference.

# docker-compose.yml version: '3'

services: flask:

image: webapp-flask

build:

context: .

dockerfile: Dockerfile-flask

volumes:

- ""./:/app"" nginx:

image: webapp-nginx

build:

context: .

dockerfile: Dockerfile-nginx

ports:

- 5000:80

depends_on:

- flask

Isn’t that convenient? Now you can run docker-compose up to run your entire app instead of having to run all these commands:

$ docker build -t my-flask -f Dockerfile-flask .

$ docker build -t my-nginx -f Dockerfile-nginx . $ docker network create my-network $ docker run -d --name flask --net my-network -v ""./:/app"" my-flask

$ docker run -d --name nginx --net my-network -p ""5000:80"" my-nginx

Now that your Docker containers are running, feel free to rush to localhost:5000 to see your new web app live running in two separate containers!

Dissection time!

Let’s walk through the important lines on our docker-compose.yml file to fully explain what is going on.

The keys under services: define the names of each one of our services (i.e. Docker containers). Hence, flask and nginx are the names of our two containers.

image: webapp-flask

This line specifies what name our image will have after docker-compose creates it. It can be anything we want. docker-compose will build the image the first time we launch docker-compose up and keep track of the name for all future launches.

build:

context: .

dockerfile: Dockerfile-flask

This piece is doing two things. First (i.e. context ), it is telling the Docker engine to only use files in the current directory to build the image. Second (i.e. dockerfile ), it’s telling the engine to look for the Dockerfile named Dockerfile-flask to know the instructions to build the appropriate image.

volumes:

- ""./:/app""

Here we’re simply instructing docker-compose to mount our current folder onto the directory /app in the container when it is spun up. This way, as we make changes on the app, we won’t have to keep building the image unless it is a major change, such as a software module dependency.

For the nginx portion of the file, there’s a few things to look out for.

ports:

- 5000:80

This little section is telling docker-compose to map the port 5000 on your local machine to the port 80 on the Nginx container (which is the port Nginx serves to by default). This is why going to localhost:5000 is able to hit your container.

depends_on:

- flask","['file', 'dockercompose', 'nginx', 'docker', 'python', 'image', 'uwsgi', 'flask', 'container', 'linebyline', 'run', 'dockercomposing', 'app']","Case scenarioWe are building a Python 3 Flask app.
Building our Flask AppFirst, we define the dependencies for our Python app in a requirements.txt file.
# requirements.txt Flask==1.0.2uWSGI==2.0.17.1Now we can build a very minimal Flask app.py file that defines some logic for our web app.
# app.conf server {listen 80;root /usr/share/nginx/html;locationinclude uwsgi_params;uwsgi_pass flask:5000;}} location / { try_files $uri @app ; }location @app include uwsgi_params;uwsgi_pass flask:5000;The line uwsgi_pass flask:5000; is using flask as the host to route traffic to.
This is because we will configure docker-compose to connect our Flask and Nginx containers through the flask hostname.",en,['Luis Ferrer-Labarca'],2020-02-01 05:27:40.537000+00:00,"{'Web Development', 'Nginx', 'Python3', 'Docker', 'DevOps'}","{'https://miro.medium.com/fit/c/96/96/1*_LGZFH-uEs1ZluPB9B6-5A.jpeg', 'https://miro.medium.com/fit/c/160/160/1*p20B_Y3LiiPaD8GiAN6O4A.png', 'https://miro.medium.com/max/1200/1*UlEIvb5vUPuqJqtoRRT64g.jpeg', 'https://miro.medium.com/max/2900/1*UlEIvb5vUPuqJqtoRRT64g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*-wsqdRBVO95T7DRA77csAw.jpeg', 'https://miro.medium.com/max/60/1*UlEIvb5vUPuqJqtoRRT64g.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*_LGZFH-uEs1ZluPB9B6-5A.jpeg', 'https://miro.medium.com/fit/c/80/80/2*EywFJyDCX0m3PYAxl_ypGg.png', 'https://miro.medium.com/fit/c/80/80/2*EncEgfAdttRlMX57NT8bFw.jpeg', 'https://miro.medium.com/max/326/1*obfk5feu0nODbnPkykYvXQ.png'}",2020-03-05 00:18:36.711135,1.650477647781372
https://medium.com/@ryangordon210/building-python-microservices-part-i-getting-started-792fa615608,"Building Python Microservices, Part I — Getting Started","Microservices are becoming more and more popular due to the amount of benefits they provide over a traditional monolithic architecture. In this series we will work through creating a architecture with a database, an api and some api documentation.

Currently, I am a final year Software Development student in GMIT at the time of writing.

I got the inspiration for this blog from another medium article. I decided to make this article to show a different use case.

The end goal

Heres one I finished earlier

The end goal of this tutorial series is to have a working architecture with a number of services that is both :

Extensible — we should be able to add more services without drastically reducing the performance or rewriting how we consume the service.

Secure — our architecture will have CRUD access to a database. We need to secure the routes that allow people to create/update or delete so that only authorised users can access these routes.

One extra desire is to minimize the amount of computation done by the client. This allows us to design a solution that works well on less performant devices.

Tech stack :

Flask (as a Framework)

(as a Framework) connexion (helpful tool to generate routes and Swagger docs)

(helpful tool to generate routes and Swagger docs) Flask-Injector (Dependency Injection package)

(Dependency Injection package) CouchDB (Open source No-SQL database which comes with a REST API built in)

(Open source No-SQL database which comes with a REST API built in) Docker (will be used to ‘containerise’ our services)

(will be used to ‘containerise’ our services) Avro (or any data serialization package)

In addition to these we will use some specific python packages at a later point which will help us on the authentication operations.

Boilerplating our solution

The first major step in getting our system setup is to create some boilerplate code so we have a base to expand upon. This has a number of benefits, it allows us to worry only about the project structure initially and gives a good starting point if you want to follow a strict test driven approach.

It is a good practice to use virtual envirorments when developing as it gives your code a somewhat isolated enviroment and also helps with managing dependancies. For python 3.6 I use this page to setup my venv.

Requirements.txt

A requirements file allows you to freeze your dependancies with the current version you are using of the package. Its also useful to list all your packages so you can install with one command :

Flask connexion Flask-Injector fastavro injector==0.12.0

To install the dependancies into your virtual environment, first ensure it is activate and then run

pip install -r requirements.txt

Here is a tree example of the structure we are aiming for:

├── api

│ └── products.py

├── app.py

├── providers

│ ├── CouchProvider.py

├── requirements.txt

└── swagger

└── couch-service-docs.yaml

Design First Approach:

There are 2 main ways to build an api :

Design First — The API is first planned and designed using some form of API documentation such as swagger, from which the code is built.

Code First — Based on a business plan, API is directly coded, from which documentation is generated.

For more info on the idea of design first API’s head here

Our initial design for the API will be simple, we will need 1 endpoint to target and 1 model to represent the type of object we are returning.

After this swagger file is designed, it will be used by connexion to display the UI of our API docs when hosted.

Our swagger spec :

There is a lot going on here, here are a few major points in this :

We have defined a spec using version 2 (OpenAPI v3 is out, however connexion doesn’t support it as of yet)

The base path has been configured to /v1.0

A route has been setup with a GET method.

Tags have been set up to separate which endpoints can be accessed by the public and which will be secured.

We have created a model schema to define what a ‘product’ is.

To handle our requests we will have 2 python files which will work together to perform tasks. The first file will be the provider and will hold the business logic needed for manipulating the data.

The second will be located in the api directory and will hold the functions which are called in the swagger spec. When these functions are called, an instance of CouchProvider is injected and then the task is delegated to this class by calling one of its functions and returning the result of this.

Note: The name of your function needs to be the same as specified in your swagger spec under the operationId parameter.

The app.py is what will wire all our other bits together and host it on some url. It is what configures Connexion, RestyResolver, Injector and runs it all.

Running it all

Now that a basic structure has been built up , we can run this the whole thing with 1 command. Firstly ensure you are in your virtual enviroment and from within, run :","['building', 'virtual', 'used', 'python', 'design', 'architecture', 'database', 'api', 'connexion', 'spec', 'started', 'getting', 'microservices', 'using', 'swagger']","Microservices are becoming more and more popular due to the amount of benefits they provide over a traditional monolithic architecture.
In this series we will work through creating a architecture with a database, an api and some api documentation.
After this swagger file is designed, it will be used by connexion to display the UI of our API docs when hosted.
The second will be located in the api directory and will hold the functions which are called in the swagger spec.
Note: The name of your function needs to be the same as specified in your swagger spec under the operationId parameter.",en,['Ryan Gordon'],2018-01-30 09:34:00.459000+00:00,"{'Swagger', 'Flask', 'Python', 'Open Source'}","{'https://miro.medium.com/max/1200/1*tFVXT9cdJM5pVhZxRJmVfw.png', 'https://miro.medium.com/fit/c/96/96/1*t9QudliCAYlrGX5Do9PKBw.jpeg', 'https://miro.medium.com/max/3324/1*tFVXT9cdJM5pVhZxRJmVfw.png', 'https://miro.medium.com/fit/c/80/80/2*-SKWRuh2Yn7dE1C9PXsd5g.jpeg', 'https://miro.medium.com/max/60/1*tFVXT9cdJM5pVhZxRJmVfw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*oeIMOQNSkRKazQaJoSgnJQ.png', 'https://miro.medium.com/fit/c/80/80/2*EncEgfAdttRlMX57NT8bFw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*t9QudliCAYlrGX5Do9PKBw.jpeg'}",2020-03-05 00:18:38.792483,2.080347776412964
https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561,A Brief Overview of Outlier Detection Techniques,"Z-Score

The z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution. This makes z-score a parametric method. Very frequently data points are not to described by a gaussian distribution, this problem can be solved by applying transformations to data ie: scaling it.

Some Python libraries like Scipy and Sci-kit Learn have easy to use functions and classes for a easy implementation along with Pandas and Numpy.

After making the appropriate transformations to the selected feature space of the dataset, the z-score of any data point can be calculated with the following expression:

When computing the z-score for each sample on the data set a threshold must be specified. Some good ‘thumb-rule’ thresholds can be: 2.5, 3, 3.5 or more standard deviations.

By ‘tagging’ or removing the data points that lay beyond a given threshold we are classifying data into outliers and not outliers

Z-score is a simple, yet powerful method to get rid of outliers in data if you are dealing with parametric distributions in a low dimensional feature space. For nonparametric problems Dbscan and Isolation Forests can be good solutions.

Dbscan (Density Based Spatial Clustering of Applications with Noise)

In machine learning and data analytics clustering methods are useful tools that help us visualize and understand data better. Relationships between features, trends and populations in a data set can be graphically represented via clustering methods like dbscan, and can also be applied to detect outliers in nonparametric distributions in many dimensions.

Dbscan is a density based clustering algorithm, it is focused on finding neighbors by density (MinPts) on an ‘n-dimensional sphere’ with radius ɛ. A cluster can be defined as the maximal set of ‘density connected points’ in the feature space.

Dbscan then defines different classes of points:

Core point : A is a core point if its neighborhood (defined by ɛ) contains at least the same number or more points than the parameter MinPts.

: is a core point if its neighborhood (defined by ɛ) contains at least the same number or more points than the parameter MinPts. Border point : C is a border point that lies in a cluster and its neighborhood does not contain more points than MinPts, but it is still ‘density reachable’ by other points in the cluster.

: is a border point that lies in a cluster and its neighborhood does not contain more points than MinPts, but it is still ‘density reachable’ by other points in the cluster. Outlier: N is an outlier point that lies in no cluster and it is not ‘density reachable’ nor ‘density connected’ to any other point. Thus this point will have “his own cluster”.

If A is a core point, it forms a cluster with all the points that are reachable from it. A point Q is reachable from P if there is a path p1, …, pn with p1 = p and pn = q, where each pi+1 is directly reachable from pi (all the points on the path must be core points, with the possible exception of q).

Reachability is a non-symmetric relation since, by definition, no point may be reachable from a non-core point, regardless of distance (so a non-core point may be reachable, but nothing can be reached from it!). Therefore a further notion of connectedness is needed to formally define the extent of the clusters found by this algorithm.

Two points p and q are density-connected if there is a point o such that both p and q are density-reachable from o. Density-connectedness is symmetric.

A cluster satisfies two properties:

All points within the cluster are mutually density-connected.

If a point is density-reachable from any point of the cluster, it is part of the cluster as well.

Sci-kit Learn has an implementation of dbscan that can be used along pandas to build an outlier detection model.

Again, the first step is scaling the data, since the radius ɛ will define the neighborhoods along with MinPts. (Tip: a good scaler for the problem at hand can be Sci-kit Learn’s Robust Scaler).

After scaling the feature space, is time to choose the spatial metric on which dbscan will perform the clustering. The metric must be chosen depending on the problem, an euclidean metric works well for 2 or 3 dimensions, the manhattan metric can also be useful when dealing with higher dimensional feature spaces 4 or more dimensions.

Then, the parameter eps (ɛ) must be chosen accordingly to perform clustering. If ɛ is too big many points will be density connected, if its too small the clustering will result in many meaningless clusters. A good approach is to try values ranging from 0.25 to 0.75.

Dbscan is also sensitive to the MinPts parameter, tuning it will completely depend on the problem at hand.

The complexity of dbscan is of O(n log n), it and effective method with medium sized data sets. Feeding data to the model is easy when using Scikit learn’s implementation. After fitting dbscan to the data clusters can be extracted and each sample is assigned to a cluster. Dbscan estimates the number of clusters by itself, there is no need to specify the number of desired clusters, it is an unsupervised machine learning model.

Outliers (noise) will be assigned to the -1 cluster. After tagging those instances, they can be removed or analyzed.","['dbscan', 'overview', 'detection', 'cluster', 'density', 'reachable', 'clustering', 'point', 'points', 'brief', 'data', 'ɛ', 'minpts', 'techniques', 'outlier']","Z-ScoreThe z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution.
Very frequently data points are not to described by a gaussian distribution, this problem can be solved by applying transformations to data ie: scaling it.
Dbscan is a density based clustering algorithm, it is focused on finding neighbors by density (MinPts) on an ‘n-dimensional sphere’ with radius ɛ.
Outlier: N is an outlier point that lies in no cluster and it is not ‘density reachable’ nor ‘density connected’ to any other point.
Sci-kit Learn has an implementation of dbscan that can be used along pandas to build an outlier detection model.",en,['Sergio Santoyo'],2017-11-24 17:27:52.829000+00:00,"{'Data Science', 'Machine Learning', 'Towards Data Science', 'Anomaly Detection', 'Analytics'}","{'https://miro.medium.com/max/60/0*0GuMixLdSZo3V3Nh.?q=20', 'https://miro.medium.com/max/170/0*TwXvmgI5j7ArPPq4.', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/412/0*uVVSUfptaeFzqRZW.', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2400/1*F_yiILIE954AZPgPADx76A.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/800/0*3A8VdnNSC2d32Q_I.', 'https://miro.medium.com/max/60/0*A1Wupu3hKsJMvUdH.?q=20', 'https://miro.medium.com/fit/c/160/160/0*KZvfF93jaxtKWFqf.', 'https://miro.medium.com/max/1200/1*F_yiILIE954AZPgPADx76A.png', 'https://miro.medium.com/max/60/0*oj1v0rhvPt0u-jYH.?q=20', 'https://miro.medium.com/max/60/0*y7kVHEQPQKBg3Cga.?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*R9u16eEcsZHpjH4O.?q=20', 'https://miro.medium.com/fit/c/96/96/0*KZvfF93jaxtKWFqf.', 'https://miro.medium.com/max/60/0*3A8VdnNSC2d32Q_I.?q=20', 'https://miro.medium.com/max/60/0*TwXvmgI5j7ArPPq4.?q=20', 'https://miro.medium.com/max/60/0*uVVSUfptaeFzqRZW.?q=20', 'https://miro.medium.com/max/960/0*R9u16eEcsZHpjH4O.', 'https://miro.medium.com/max/1962/0*0GuMixLdSZo3V3Nh.', 'https://miro.medium.com/max/1472/0*oj1v0rhvPt0u-jYH.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*i5Moxki9Pe2noYN2.?q=20', 'https://miro.medium.com/max/60/1*F_yiILIE954AZPgPADx76A.png?q=20', 'https://miro.medium.com/max/1120/0*i5Moxki9Pe2noYN2.', 'https://miro.medium.com/max/2800/0*A1Wupu3hKsJMvUdH.', 'https://miro.medium.com/max/1358/0*y7kVHEQPQKBg3Cga.'}",2020-03-05 00:18:45.470824,6.678340673446655
https://medium.com/@mehulved1503/effective-outlier-detection-techniques-in-machine-learning-ef609b6ade72,Effective Outlier Detection Techniques in Machine Learning,"From a Machine Learning perspective, tools for Outlier Detection and Outlier Treatment hold a great significance, as it can have very influence on the predictive model. In this blog, we’d address a few techniques in Outlier Detection.

Read the blogpost (link mentioned below), to understand more about Outlier and Anomaly Detection.

Before we deep dive into the subject, let us understand briefly about Outliers and the significance of Outlier Detection.

Outlier Detection Techniques

What are Outliers?

In Data Science, an Outlier is an observation point that is distant from other observations. An Outlier may be due to variability in the measurement or it may indicate experimental error.

Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations.

Figure 1 below provides a visual understanding about Outliers

Figure 1: A Visual Understanding of Outliers

Outliers exist due to one of the four following reasons:

· Incorrect data entry can cause data to contain extreme cases.

· A second reason for outliers can be failure to indicate codes for missing values in a dataset.

· Another possibility is that the case did not come from the intended sample.

· And finally, the distribution of the sample for specific variables may have a more extreme distribution than normal.

The Importance of Outlier Detection

While Outliers, are attributed to a rare chance and may not necessarily be fully explainable, Outliers in data can distort predictions and affect the accuracy, if you don’t detect and handle them.

The contentious decision to consider or discard an outlier needs to be taken at the time of building the model. Outliers can drastically bias/change the fit estimates and predictions. It is left to the best judgement of the analyst to decide whether treating outliers is necessary and how to go about it.

Treating or altering the outlier/extreme values in genuine observations is not a standard operating procedure. If a data point (or points) is excluded from the data analysis, this should be clearly stated on any subsequent report.

Figure 2 below illustrates how the Line of Fit Changes drastically, before discarding the Outliers and after discarding the Outliers

Figure 2: A Simple Case of Change in Line of Fit with and without Outliers

The Various Approaches to Outlier Detection

Univariate Approach:

A univariate outlier is a data point that consists of an extreme value on one variable.

The Box Plot Rule

For a given continuous variable, outliers are those observations that lie outside 1.5 * IQR, where IQR, the ‘Inter Quartile Range’ is the difference between 75th and 25th quartiles. This is also known as “The Box Plot Rule”.

The box plot rule is the simplest statistical technique that has been applied to detect univariate outliers. Typically, in the Univariate Outlier Detection Approach look at the points outside the whiskers in a box plot.

Figure 3: The Box Plot Rule for Univariate Outlier Detection

Grubb’s Test for Univariate Analysis:

Grubb’s test (also known as the maximum normed residual test) is widely used to detect anomalies in a univariate data set, under the assumption that the data is generated by a Gaussian distribution

Multivariate Approach:

Declaring an observation as an outlier based on a just one (rather unimportant) feature could lead to unrealistic inferences. When you have to decide if an individual entity (represented by row or observation) is an extreme value or not, it better to collectively consider the features (X’s) that matter.

A multivariate outlier is a combination of unusual scores on at least two variables.

Several methods are used to identify outliers in multivariate datasets. Two of the widely used methods are:

· Mahalanobis Distance

· Cook’s Distance

Mahalanobis Distance:

Mahalanobis distance and leverage are often used to detect outliers, especially in the development of linear regression models. A point that has a greater Mahalanobis distance from the rest of the sample population of points is said to have higher leverage since it has a greater influence on the slope or coefficients of the regression equation. Mahalanobis distance is also used to determine multivariate outliers.

In order to use the Mahalanobis distance to classify a test point as belonging to one of N classes, one first estimates the covariance matrix of each class, usually based on samples known to belong to each class. Then, given a test sample, one computes the Mahalanobis distance to each class, and classifies the test point as belonging to that class for which the Mahalanobis distance is minimal.

Mahalanobis Distance

Cook’s Distance:

Cook’s distance is a measure computed with respect to a given regression model and therefore is impacted only by the X variables included in the model. But, what does cook’s distance mean? It computes the influence exerted by each data point (row) on the predicted outcome.

The Cook’s distance for each observation i measures the change in Y-hat (fitted Y) for all observations with and without the presence of observation i, so we know how much the observation i impacted the fitted values.

Cook’s Distance

In general use, those observations that have a cook’s distance greater than 4 times the mean may be classified as influential. This is not a hard boundary.

Simplifying Approach Selection for Outlier Detection

Figure 5 below, is a general guideline on selecting an approach for Outlier Detection.

General Guiding Principle to Outlier Detection Approach Selection

Summary & Conclusion:

The contentious decision to consider or discard an Outlier needs to be taken at the time of building the model. Outliers can drastically bias/change the fit estimates and predictions. It is left to the best judgement of the analyst to decide whether treating outliers is necessary and how to go about it.","['machine', 'sample', 'univariate', 'test', 'observations', 'observation', 'detection', 'outliers', 'point', 'learning', 'distance', 'data', 'techniques', 'effective', 'outlier']","From a Machine Learning perspective, tools for Outlier Detection and Outlier Treatment hold a great significance, as it can have very influence on the predictive model.
In Data Science, an Outlier is an observation point that is distant from other observations.
If a data point (or points) is excluded from the data analysis, this should be clearly stated on any subsequent report.
Typically, in the Univariate Outlier Detection Approach look at the points outside the whiskers in a box plot.
Simplifying Approach Selection for Outlier DetectionFigure 5 below, is a general guideline on selecting an approach for Outlier Detection.",en,['Mehul Ved'],2018-04-25 12:27:22.677000+00:00,"{'Outlier Detection', 'Outlier Treatment', 'Outlier Techniques', 'Machine Learning', 'Anomaly Detection'}","{'https://miro.medium.com/max/3200/0*iud2OU9azcauQd3G.png', 'https://miro.medium.com/freeze/max/60/0*ZQXgrlQFeoLc0Uz6.gif?q=20', 'https://miro.medium.com/max/980/0*nHoYzjeNVgkvlbLx.png', 'https://miro.medium.com/fit/c/80/80/2*u_n84yPLnHXgArJ9k1Y_Pg.png', 'https://miro.medium.com/fit/c/80/80/2*qApRtlIUuU4e0SwEIXt9kQ.jpeg', 'https://miro.medium.com/max/350/0*EZexBdyhXoxMa-nn.jpg', 'https://miro.medium.com/max/60/0*nHoYzjeNVgkvlbLx.png?q=20', 'https://miro.medium.com/max/700/0*EZexBdyhXoxMa-nn.jpg', 'https://miro.medium.com/fit/c/80/80/0*Yvwp35wL02cXsM6J', 'https://miro.medium.com/max/1226/0*ZQXgrlQFeoLc0Uz6.gif', 'https://miro.medium.com/fit/c/160/160/0*RFTNpy4Lg-Vnfkml.', 'https://miro.medium.com/max/1200/1*rNhU_h2SG8Qm_RXw6QUZfw.png', 'https://miro.medium.com/max/60/0*iud2OU9azcauQd3G.png?q=20', 'https://miro.medium.com/max/550/1*u6fIQDDE7PMzZ_i9g8RUfg.png', 'https://miro.medium.com/max/60/0*EZexBdyhXoxMa-nn.jpg?q=20', 'https://miro.medium.com/max/1292/0*lBNZ8w_sY9Jhm9Tz.png', 'https://miro.medium.com/max/60/0*lBNZ8w_sY9Jhm9Tz.png?q=20', 'https://miro.medium.com/max/54/1*rNhU_h2SG8Qm_RXw6QUZfw.png?q=20', 'https://miro.medium.com/max/60/1*u6fIQDDE7PMzZ_i9g8RUfg.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*RFTNpy4Lg-Vnfkml.'}",2020-03-05 00:18:47.643666,2.172060012817383
https://medium.com/@mheavers/machine-learning-in-sound-music-6f0715320d49,Machine Learning in Sound+Music,"This is part 3 of my art focused Machine Learning Course. Click here to access the overview, which provides links to the other workshops in the series.

In this class we’ll look at Machine Learning as it relates to Sound and Music. Many of the principles you learned in the previous classes will apply here, but manifest themselves in different ways.

We’ll look at a few concrete examples:

Pitch Detection

Using ML5, we’ll examine a user’s voice through the computer microphone and try to correctly identify musical notes. We’ll look at an example that makes a game of it, trying to get a user to sing the right note, and create a virtual piano whose keys are highlighted according to the note sang.

AI Duet

Because ML5 is a bit limited in its Sound+Music offerings at present, we’ll look into other libraries, dissecting Google’s AI Duet examples, which, like ML5, use Tensorflow under the hood, but incorporate a project called Magenta to facilitate sound-based Machine Learning. In this example, the user will play the keyboard to make musical notes, and the AI will attempt to accompany the user’s music with its own generated notes in an AI duet — matching similar rhythms, tones, and patterns.

Current State of Machine Learning in Sound+Music

We’ll look at some up-to-date examples of what people are doing with ML in Sound+Music today.

Let’s get started…

History

You could look at something such as a Chines Wind Chimes or the Aeolian Harp as the first device to allow for automatically generated music.

Algorithmic Music

Music became ‘algorthmic’ in the 1700s with Musikalisches Würfelspiel , a dice game that determined the recombination of works of famous composers segment by segment.

In 1958 Iannis Xenakis used Markov Chains, a stochastic process by which a system can make predictions on the future based on its present state, to compose Analogique. He used what he called a table of ‘coherences’ — probabilities based on present states that a subsequent note would be generated.

https://www.youtube.com/watch?v=mXIJO-af_u8&

Markov Chains were used by David Cope in his Experiments in Musical Intelligence (or Emmy). They were semi-automatic systems with concepts of harmonics and musical grammar.

https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fmachinelearningmusic%2Fmazurka-after-chopin-by-david-cope&autoplay=false

Extrapolation of Music

But while Markov chains can only produce subsequences of the original composition, RNNs were implemented to attempt to extrapolate beyond that. After Bach attempts to identify stylistic regularities in generating new musical sequences:

https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fmachinelearningmusic%2Fafter-bach-by-michael-c-mozer&autoplay=false

In 2002 Doug Eck upgraded this model by using LSTMs, which could retain a longer memory of a musical sequence. Here’s some LSTM Blues

https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fmachinelearningmusic%2Flstm-blues-improvisation-composition-4-by-doug-eck&autoplay=false

Doug now leads the Magenta initiatives at Google Brain, which we’ll be looking at in a bit.

Music is a fascinating use case for machine learning, as it is somewhat of a language — it has letters, unfolding over a composition through time, but unlike a language multiple notes can be played at one time, they can be sustained for different periods, or there can be periods of silence which are important to the overall composition. Music is simultaneously art and language.

Machine Learning Generated Music Today

Here’s a couple modern examples — an auto-generated song trained (supposedly) in the style of the Beatles (there’s actually a much better one called Ballad of Mr. Shadow trained on Gershwin and Cole Porter)

https://www.youtube.com/watch?v=LSHZ_b05W7o&

And here’s IBM’s Watson Beat, which can generate music based on musical prompts:

https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fibmresearch%2Ffallen-star-amped&autoplay=false

Note most of the above history was taken from Kyle McDonald’s Medium Article.

The other recent approach has been using Audio Spectra for prediction, rather than the underlying language of music. For this, Convolutional Neural Networks are used. Here’s a weirder example, from Memo Akten:

https://www.youtube.com/watch?v=htmVKWo7OXA&

https://youtu.be/htmVKWo7OXA

Speech synthesis is the artificial production of human speech. Think of what you hear when you listen to Siri or Alexa. There are different implementations of Speech Synthesis, some more successful than others:

Concatenative TTS (Traditional Method)

Record a giant database of sentences from a single voice actor. Divide those into chunks (like words or phrases) that can be reassembled into language based on whatever you want to say. This method can be supplemented by Parametric TTS, which enforces grammatical rules.

Wavenet

This is Google’s Speech Synthesis implementation. Rather than look at words and grammar, it looks at specific audio samples, very similar to the way an AI would look at an image.

It was trained using a CNN to produce tones at 24,000 samples per second, with seamless transitions between those tones to smooth out the gaps between samples. It uses a sample resolution of 16 bits (think of it like pixel density in an image, or DPI). The more you downsample, the more subtleties you lose.

Tonal Based: Because it is tonal based, Wavenet can mimic features of human speech, like lip smacks, pauses, etc. to sound more natural.

ML Driven: Rather than enforcing rules, Wavenet allows the machine learning algorithms to infer the rules from the recordings. Try it out with your own text.

Lyrebird

A Speech Sythesis Software Startup that can record samples of your voice to produce a fully automated AI voice that can say anything in your vocal style. This became popular when they simulated the voices of Donald Trump and Barack Obama, essentially turning them in to puppets to say whatever Lyrebird wanted them to say.

https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fuser-535691776%2Fdialog&autoplay=false

https://soundcloud.com/user-535691776/dialog

Record Your Own Voice With Lyrebird

Ideas For Speech Synthesis

Create a mashup of voices

Record your voice impersonating someone else’s voice

Record your voice modified by MIDI

Modify the Lyrebird voice with MIDI / Software

Generate audio clips of words recorded from TV / Music to train Lyrebird in the voices of another (personal use only! See Trump / Obama recordings for an example of this).

Generate Text with LSTM / Predictive Writer trained on a corpus of text of your choosing, and have it vocalized using Speech Synthesis (Example from Peter Burr).

The ML behind Speech Synthesis

While much of the technology is proprietary and therefore very hard to de/reconstruct, we have a lot of tools for the components of Speech Synthesis at our disposal! A typical approach to analyzing audio for use in ML might be as follows (we’ll look at each step in detail just afterward):

Audio Analysis Steps

Record Audio

Compute Audio Characteristics

MEL Frequency

Optimize

Estimate

Pull apart harmonics and percussion

Identify Pitch Class on Chromagram

Locate Events — when significant events happen in audio

Beat Tracking

Compute features — similar to the way we would compute features in an image

Output

MEL frequency spectrogram

Approximates the human auditory system’s response more closely than the linearly-spaced frequency bands in the normal cepstrum. The MEL identifies characteristics of audio and eliminates noise.

The unit is the MFCC, which represents the texture or timbre of a sound.

Cepstrum?

A reverse-mangling of the word Spectrum (don’t ask me…). Also known as Fourier inversion theorem. It’s the idea that if we know the frequency (number of occurrences of a repeating event per unit of time) and phase (position at a point in time of a waveform cycle) of a sound wave then we can reconstruct the original wave precisely. Meaning we don’t have to have the entire sound wave. Which is how we can create something like Donald Trump saying whatever we want him to say.

Fourier?

If you are interested in audio at all, get to know everything you can about FFT (Fast Fourier Transform). It divides a signal into the frequencies that make it up, and allows us to do all sorts of things (note that this applies to any periodic signal, not just sound):

Differential Equations

Spectrometry

Quantum Mechanics

Signal Processing: Internet, phone, computer, wifi, satellites

Essentially the FFT allows us to convert a thing to another thing, and then convert it back.

Power Spectrum: Frequency over a continuous range. The human ear (cochlea) vibrates at different spots depending on the frequency of the incoming sounds. Depending on the frequency, different nerves fire in the brain. The higher the frequency, the harder for the brain to distinguish.

Optimize

Taking a frequency and applying a logarithm. We don’t hear loudness on a linear scale. To double the perceived volume of a sound we need to put 8 times as much energy into it. So to take a frequency and make it sound like what we actually hear, we normalize it on the MEL scale.

Estimate

We estimate the trajectory of a frequency using a delta. It’s the analysis of a spectrum over a length of time.

Harmonic vs. Percussive Separation

Separating things with pitch vs. things without (like beats). To do this, we could use rhythm analysis, chord / tonality recognition, etc. Or we could just use the Fourier Transform.

Identify Pitches with a Chromagram

A chroma is the set of all pitches that are a whole number of Octaves Apart. A Chromagram plots those to identify Pitch Class information.

Beat Tracking

Returns BPM and the frame indices of beat events.

Once we’ve identified the beats, we remove the noise and optimize

Compute Features

Take the MFCCs along with their Deltas for each frame in the audio clip, get an average, and use that to establish a Feature Vector. We can do this with as little as one second of audio.

Applications

Play Wolfenstein 3D using voice

https://player.vimeo.com/video/207831279

https://vimeo.com/207831279

Pitch Detection in ML5

What is Pitch?

It’s a characteristic of musical tones, like duration, loudness, and timbre. Pitch allows us to label a sound as higher or lower in frequency. Technically ,in ML5, we are determining a frequency.

CREPE

In ML5, we are using the Crepe model (stands for Convolution Representation for Pitch Estimation). Crepe, as opposed to preceding models, uses only information derived from data, not from pitch probability estimation.

Cent is a unit representing musical intervals relative to a reference pitch in Hertz (one cycle per second). One Cent = 10 Hz. Therefore, the greater the Cents, the greater the sample, the greater the accuracy.

Pitch Detection in ML5

Files: 09-pitch-detection/00-PitchDetection/sketch.js

This example can only get audio from the computer’s microphone.

First we establish a scale:

const scale = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];

Then we enable the microphone:

audioContext = getAudioContext();

mic = new p5.AudioIn();

mic.start(startPitch);

We load the Crepe Model and begin listening for pitch:

function startPitch() {

pitch = ml5.pitchDetection('./model/', audioContext , mic.stream, modelLoaded);

}

function modelLoaded() {

select('#status').html('Model Loaded');

getPitch();

}

Once we have it, we output the frequency returned from ML5 / Tensorflow / Crepe, and assign it the closest note on the scale:

pitch.getPitch(function(err, frequency) {

if (frequency) {

select('#result').html(frequency);

let midiNum = freqToMidi(frequency);

currentNote = scale[midiNum % 12];

select('#note').html(currentNote);

}

getPitch();

})

freqToMidi is a function within ML5 that we’ll look at shortly.

See that % 12? That is called the modulus operator and basically all it does is give us the remainder when one number is divided by another. In that way, regardless of how high our midiNum is, it will always fall on that scale of 12 notes we defined at the start of our code.

I realize this a bit like:

In order to understand it better, we need to look within the code for ML5 itself.

The Code Behind the Scenes

If we look at the source code for the ML5 library, we can examine what’s happening behind the scenes in greater detail. Most of the processing happens in the processMicrophoneBuffer function:

const centMapping = tf.add(tf.linspace(0, 7180, 360), tf.tensor(1997.3794084376191));

Remember — cents are divisions of pitch. tf is tensorflow, the machine learning library doing all the processing behind the scenes, and in examining its documentation, we can learn that linspace just returns an evenly spaced sequence of numbers.

We normalize the data we are getting from the microphone in order to optimize it for faster processing:

const frame = tf.tensor(resampled.slice(0, 1024));

const zeromean = tf.sub(frame, tf.mean(frame));

const framestd = tf.tensor(tf.norm(zeromean).dataSync() / Math.sqrt(1024));

const normalized = tf.div(zeromean, framestd);

and then make a prediction of frequency:

const activation = this.model.predict([input]).reshape([360]);

Why 1024? That is our sample rate in Hertz. Once we have a prediction, we assess confidence, find the cent that our sampled frequency falls in to (remember we are trying to reduce dimensions), and calculate a predicted Hertz for that frequency.

const frame = tf.tensor(resampled.slice(0, 1024));

const zeromean = tf.sub(frame, tf.mean(frame));

const framestd = tf.tensor(tf.norm(zeromean).dataSync() / Math.sqrt(1024));

const normalized = tf.div(zeromean, framestd);

const input = normalized.reshape([1, 1024]);

const activation = this.model.predict([input]).reshape([360]);

const confidence = activation.max().dataSync()[0];

const center = activation.argMax().dataSync()[0];

this.results.confidence = confidence.toFixed(3);

const start = Math.max(0, center - 4);

const end = Math.min(360, center + 5);

const weights = activation.slice([start], [end - start]);

const cents = centMapping.slice([start], [end - start]);

const products = tf.mul(weights, cents);

const productSum = products.dataSync().reduce((a, b) => a + b, 0);

const weightSum = weights.dataSync().reduce((a, b) => a + b, 0);

const predictedCent = productSum / weightSum;

const predictedHz = 10 * (2 ** (predictedCent / 1200.0));

Then, if our confidence is greater than 50%, we return the result. Otherwise we keep processing.

const frequency = (confidence > 0.5) ? predictedHz : null;

this.frequency = frequency;

Where does freqToMidi come from? P5.js’s sound library:

/**

* Returns the closest MIDI note value for

* a given frequency.

*

* @method freqToMidi

* @param {Number} frequency A freqeuncy, for example, the ""A""

* above Middle C is 440Hz

* @return {Number} MIDI note value

*/

p5.prototype.freqToMidi = function (f) {

var mathlog2 = Math.log(f / 440) / Math.log(2);

var m = Math.round(12 * mathlog2) + 69;

return m;

};

AI Duet

Info | Code | Play

Google’s AI Duet is a demo using Magenta, a sound processing AI project that runs Tensorflow under the hood to perform machine learning on audio. AI duet acts as a virtual piano that we will play and an AI will attempt to accompany.

Open up a browser window and try the example. Try out different keys on your keyboard — they will play different notes. The longer you hold them, the longer they will sustain. The AI will be examining which keys you hit and how long you hit them for. Then it will follow up your notes with notes of its own, attempting to accompany (but not match) the notes that you played.

AI Duet Components

AI Duet consists of a number of components:

Tensorflow — the Machine Learning framework doing all the processing (as seen in ML5)

— the Machine Learning framework doing all the processing (as seen in ML5) Magenta — a research project by Google Brain into building AI tools for Art + Music

— a research project by Google Brain into building AI tools for Art + Music Nsynth — a Neural Audio synthesizer arising out of Magenta. It consists of:

— a Neural Audio synthesizer arising out of Magenta. It consists of: A dataset of musical notes

A wavenet-like autoencoder model that essentially learns about itself and its own embeddings. These are not causal like Wavenet, so they see the entire chunk of data:

Tone.js — A javascript framework for the Web Audio API that makes music in the browser

ML Models:

Attention RNN — allows the model to more easily access past information without having to store that information in the RNN cell’s state. This allows the model to more easily learn longer term dependencies, and results in melodies that have longer arching themes. For an overview of how the attention mechanism works and to hear some generated sample melodies, check out the blog post.

Pianoroll RNN-NADE — Unlike melody RNNs, this model needs to be capable of modeling multiple simultaneous notes. It does so by representing a NoteSequence as a “pianoroll”, named after the medium used to store scores for player pianos

Performance RNN — This model creates music in a language similar to MIDI itself, with note-on and note-off events instead of explicit durations. The model also supports velocity events that set the current velocity, used by subsequent note-on events.

and events instead of explicit durations. The model also supports events that set the current velocity, used by subsequent note-on events. Drum Kit RNN — Unlike melodies, drum tracks are polyphonic in the sense that multiple drums can be struck simultaneously. Despite this, we model a drum track as a single sequence of events by a) mapping all of the different MIDI drums onto a smaller number of drum classes, and b) representing each event as a single value representing the set of drum classes that are struck.

Running AI Duet Locally

Running AI Duet in the browser is one thing. But if we want to modify it for our own purposes or look at its source code, we’ll need to set it up to run locally.

First, you need to make sure you have some of the basic dependencies:

Python 2.7. If you are running Python 3, or want to be able to run multiple versions of python, you may want to consider downloading Anaconda Navigator, or use Conda if you are command-line savvy.

PIP — Python’s package manager (sort of like a plug-in manager)

NodeJS

Download the code from github (click the green clone or download button and download the zip file). Unzip the file to a folder of your choosing.

Within that folder, we’ll need to run a local server in order to use magenta, tensorflow, and the machine learning models they bring with them. From the command line, change to the server directory:

cd server

and run the command

pip install -r requirements.txt

Remember, PIP is the package manager that allows python to install all necessary plugins for a project. If we take a look at requirements.txt in a code editor, we can see the following:

tensorflow==0.12.1

magenta==0.1.8

Flask==0.12

gunicorn==19.6.0

ipython==5.1.0

We know about tensorflow and magenta. Flask has a bunch of tools useful for making websites, Gunicorn is useful for running web servers, and iPython is a framework for Jupyter, a very common environment for Python coding and machine learning.

Assuming you have no errors installing the requirements for the server, you should be able to run

python server.py

which starts a local web server at http://localhost:8080/.

Then, to actually view the experience, we need to install all of the front-end dependencies (the code responsible for the stuff that we can see in the browser). To do that, execute the following code:

cd static

npm install

npm run build

The static directory is where all our front end code lives. NPM is the node.js package manager. It’s like python’s PIP, make it easy to install plugins which extend what’s possible within our codebase. NPM. So what is it installing? And what is it building?

NPM looks for a package.json file to install its dependencies, the way PIP used thr requirements.txt file. If we look at that file, we’ll see a list of a whole bunch of dependencies:

""autoprefixer-loader"": ""^3.2.0"",

""babel-core"": ""^6.17.0"",

""babel-loader"": ""^6.2.5"",

""babel-polyfill"": ""^6.20.0"",

""babel-preset-es2015"": ""^6.16.0"",

""buckets-js"": ""^1.98.1"",

....

Examining what each of these things does might be a bit beyond the scope of this course. We’re not here to learn front end web development, we’re here to understand machine learning. But it is good to see what’s happening, so that we know where to look if we want to modify the code for our own purposes. In this case, these are all npm packages of code other people have written and made available to us to use to help build an awesome experience.

There is one other thing we should look at in this file, though:

""scripts"": {

""build"": ""webpack -p"",

""watch"": ""webpack -w""

},

There is our build command that we executed when we ran npm run build. We’ll talk about this more in a bit, but to simplify things, this command optimizes our code and saves the Main.js file to the build directory. Once we run it, we can open our browser to http://localhost:8080/, and view the experience. Everything should work as it did when we ran the chrome experiment in the steps above.

To understand what’s happening under the hood, we’ll need to look at what happens on the server side (back end) and browser side (front end).

AI Duet Server

If we look at server.py we can get a better idea of what all it is responsible for:

from predict import generate_midi

import os

from flask import send_file, request

import pretty_midi

import sys

if sys.version_info.major <= 2:

from cStringIO import StringIO

else:

from io import StringIO

import time

import json

from flask import Flask

app = Flask(__name__, static_url_path='', static_folder=os.path.abspath('../static'))

These first few lines import all of the dependencies needed by the server file. You might notice a few things we haven’t seen before, such as from predict import generate_midi. predict in this case references the predict.py file of our directory, which we’ll look at in a second. generate_midi is the name of a function within that file.

We don’t need to understand everything happening here, but it helps to know that, based on the imports, we’ll be using some midi, keeping track of time in some way, accessing data in the form of json, etc. That last line tells Flask that we will be serving our app from the static directory.

@app.route('/predict', methods=['POST'])

def predict():

now = time.time()

values = json.loads(request.data)

midi_data = pretty_midi.PrettyMIDI(StringIO(''.join(chr(v) for v in values)))

duration = float(request.args.get('duration'))

ret_midi = generate_midi(midi_data, duration)

return send_file(ret_midi, attachment_filename='return.mid',

mimetype='audio/midi', as_attachment=True)

This route is actually a URL. If we post any data to the /predict path of our base URL (localhost, in this case), this route will load that data, and calls a function within the pretty_midi library to return midi as an actual usable sound file, rather than just data from the web server. Remember, that generate_midi function lives in the predict.py file.

@app.route('/', methods=['GET', 'POST'])

def index():

return send_file('../static/index.html')

if __name__ == '__main__':

app.run(host='127.0.0.1', port=8080)

That / route is just the default, root directory. When we enter that into the URL bar, it serves up the index.html file, which is the file responsible for displaying all of the web content we see in the browser. Below that, we see that the host for our site is 127.0.0.1, which is the default address for localhost, and that the port is 8080.

AI Duet Prediction

The predict.py file contains a lot of the code we are using to make predictions about what notes to play based on what notes the user has played. You’ll notice based on the imports at the top of the file, that the file will be using magenta:

import magenta

from magenta.models.melody_rnn import melody_rnn_config_flags

from magenta.models.melody_rnn import melody_rnn_model

from magenta.models.melody_rnn import melody_rnn_sequence_generator

from magenta.protobuf import generator_pb2

from magenta.protobuf import music_pb2

One of the imports is from melody_rnn. If we were to examine the source code for that model, we’d learn that it is responsible for generating melodies. Remember RNNs? In order to make a melody, we need more than one note. Melody RNN is recurrent, meaning it can look back at what has been played before in order to know what to play next.

def generate_midi(midi_data, total_seconds=10):

primer_sequence = magenta.music.midi_io.midi_to_sequence_proto(midi_data)

# predict the tempo

if len(primer_sequence.notes) > 4:

estimated_tempo = midi_data.estimate_tempo()

if estimated_tempo > 240:

qpm = estimated_tempo / 2

else:

qpm = estimated_tempo

else:

qpm = 120

primer_sequence.tempos[0].qpm = qpm

generator_options = generator_pb2.GeneratorOptions()

# Set the start time to begin on the next step after the last note ends.

last_end_time = (max(n.end_time for n in primer_sequence.notes)

if primer_sequence.notes else 0)

generator_options.generate_sections.add(

start_time=last_end_time + _steps_to_seconds(1, qpm),

end_time=total_seconds)

# generate the output sequence

generated_sequence = generator.generate(primer_sequence, generator_options)

output = tempfile.NamedTemporaryFile()

magenta.music.midi_io.sequence_proto_to_midi_file(generated_sequence, output.name)

output.seek(0)

return output

Finally here’s the generate_midi function. It’s pretty complex, so we won’t go through it all, but if we attempt to examine it, we can see that it is using the estimate_tempo() function of Pretty MIDI in order to analyze the data and guess at how fast the notes were played. So long as that tempo is under 240 (bpm), it plays at that estimated speed. Otherwise, it defaults to 120 bpm.

It calls generator.generate() in order to get the sequence of notes, where generator was defined at the top of the file as:

generator = melody_rnn_sequence_generator.MelodyRnnSequenceGenerator(

model=melody_rnn_model.MelodyRnnModel(config),

details=config.details,

steps_per_quarter=steps_per_quarter,

bundle=bundle_file)

and writes that as an actual file via the sequence_proto_to_midi_file() function of magenta. We can now play this file in our browser. To look at what’s happening in the browser, let’s examine the static folder, which is where all of the code for the interactive components of AI Duet Live

AI Duet Interaction

A good place to start is with the index.html. Remember, this is the file that the server loads by default. In general, with any webpage, index.html is the default file that is served any time you type an address into the URL bar of your browser. Let’s take a look at it:

<!DOCTYPE html>

<html>

<head>

<title>A.I. DUET</title>

<meta name=""viewport"" content=""width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"">

<meta name=""description"" content=""AI DUET"">

<link href=""https://fonts.googleapis.com/css?family=Quicksand:400,700"" rel=""stylesheet"">

<script type=""text/javascript"" src=""build/Main.js""></script>

<link rel=""icon"" type=""image/png"" href=""./images/AIDuet_16.png"" sizes=""16x16"">

<link rel=""icon"" type=""image/png"" href=""./images/AIDuet_32.png"" sizes=""32x32"">

<link rel=""icon"" type=""image/png"" href=""./images/AIDuet_152.png"" sizes=""152x152"">

<link rel=""icon"" type=""image/png"" href=""./images/AIDuet_196.png"" sizes=""196x196"">

</head>

<body>

</body>

</html>

It’s really simple! There must be a lot more happening behind the scenes that we don’t know about, because most this code doesn’t do much of anything. The most useful bit of info here is this line:

<script type=""text/javascript"" src=""build/Main.js""></script>

This is where our javascript file is being loaded. This file must be responsible for creating all of the stuff we see in the browser. But try opening up Main.js from the /build directory…

Whoah — that’s not helpful. This is minified code. It is designed to be as compact and terse as possible in order to load more quickly and perform better. If we really wanted to see what this file does, we’d need the source code.

Luckily, our download contains just that. Within the src folder there is another Main.js file, from which the build/Main.js file was generated. This one is much more intelligible.

Not much is happening in this file either, to be honest. So why was the other Main.js file so big? The main thing to note is that this source file imports lots of other files:

import {Keyboard} from 'keyboard/Keyboard'

import {AI} from 'ai/AI'

import {Sound} from 'sound/Sound'

import {Glow} from 'interface/Glow'

import {Splash} from 'interface/Splash'

import {About} from 'interface/About'

import {Tutorial} from 'ai/Tutorial'

You’ll see these correspond to other folders and files within the src directory. And those files might import code of their own. Its starting to be a pretty deep rabbit hole. For now all we’ll concern ourselves with is the fact that when we run npm run build — it takes Main.js and all of the code that it imports, and all of the code that its imports import, and builds it into one single file mumbo jumbo file to serve when you load up your web browser.

Making Modifications To AI Duet

So how would we repurpose this code, or make changes that we could actually see in the browser? The javascript file that the browser looks for is the build/Main.js file. But we obviously can’t change that. We can barely read it. All of our changes need to happen in the src directory, where things makes sense.

Once we make changes, we can build our code again with npm run build, refresh the browser, and see those changes reflected.

Do we have to build our code every time we make a tiny change? If we want to see it in the browser, yes. Fortunately, there is something that will help us. Remember the scripts part of our package.json file? Right below build is another command, watch. If we were to open another terminal window and run npm run watch — that command would look for changes we make in the src directory, and automatically compile them every time something changes. So all we have to do is refresh the browser.

This is the way most modern enterprise web development is done, and AI Duet is no different. So on top of learning Machine Learning, you can say you learned a little about one of the most advanced concepts in web development!

So now that we know how to modify the code, what should we modify? The possibilities are endless, but here’s some ideas:

Change the appearance of the bars

The file responsible for handling the appearance of the bars is Roll.js.

If we look at that file, we see that the color bars that emerge from the piano were rendered with Three.js, a webGL / 3D library for javascript. If you’re savvy with 3D already, or have a desire to learn, you could do do some pretty cool stuff because of this. For now, see if you can change the color of the bars.

Change the sound files themselves

Right now we’ve got a pretty standard MIDI keyboard. But the sounds could be anything, even your own voice. How? Combing through the source files, we can see a file called Sound.js. That seems like a good place to start.

This file is pretty intuitive too. Seems like most of the work is being done here:

keyDown(note, time=Tone.now(), ai=false){

if (note >= this._range[0] && note <= this._range[1]){

this._piano.keyDown(note, time)

if (ai){

this._synth.volume = -8

this._synth.keyDown(note, time)

}

}

}

keyUp(note, time=Tone.now(), ai=false){

if (note >= this._range[0] && note <= this._range[1]){

time += 0.05

this._piano.keyUp(note, time)

if (ai){

this._synth.keyUp(note, time)

}

}

}

Of particular interest, it seems like there are two instruments involved, _piano and _synth. These instruments are defined in the code at the top of the file:

this._piano = new Sampler('audio/Salamander/', this._range)

this._synth = new Sampler('audio/string_ensemble/', this._range)

and those Samplers reference directories: audio/Salamander and audio/string_ensemble. If we find those directories, what we find are a bunch of .mp3 files, named according to the notes they represent: A0.mp3, C4.mp3, etc.

So if we wanted to change the notes that get played, we could just replace these sound files. If you take a look at the string_ensemble_README file, you’ll see that these notes were generated as part of a soundfont:

- Fluid-Soundfont

- Generated from [FluidR3_GM.sf2](http://www.musescore.org/download/fluid-soundfont.tar.gz)

Sound fonts are formats for packaging and playing MIDI notes, and you can find them readily on the web. They usually come in the format sf2 or sfz. To convert them, we can use a program such as Polyphone.

In Polyphone, you would hit the Open Folder icon, and select the sound font file you are using. Within that file should be samples which you can listen to. Select the samples you want, and click File > Export Samples to export your selections as .wav files.

Of course, AI Duet needs .mp3 files, which we can generate with a program like Audacity. It even has the ability to convert the files in batch via the (non-intuitively named) File > Apply Chains > MP3 Conversion. Find your wav files, hit ok, and your converted MP3s will be generated in a directory called (also non-intuitively) cleaned.

Once we have our new files, we just need to name them according to the notes they represent, then place them into the project folder. We could just overwrite the existing files in one of the directories, or we could create a new folder, and change the folder reference in the code:

this._piano = new Sampler('audio/[YOUR_FOLDER]/', this._range)

Changing the behavior of the AI itself

This is all useful, but what if we want to change the behavior of the AI itself? The ai folder makes sense to look at. In this file, there’s not of actual AI going on as the name might suggest, but there is a send method which can help us.

request.load(`./predict?duration=${endTime + additional}`, JSON.stringify(request.toArray()), 'POST').then((response) => {

...

})

It’s calling the ./predict route. We saw this when we first started looking at the files. It’s within server.py:

@app.route('/predict', methods=['POST'])

def predict():

now = time.time()

values = json.loads(request.data)

midi_data = pretty_midi.PrettyMIDI(StringIO(''.join(chr(v) for v in values)))

duration = float(request.args.get('duration'))

ret_midi = generate_midi(midi_data, duration)

return send_file(ret_midi, attachment_filename='return.mid',

mimetype='audio/midi', as_attachment=True)

Now, see that generate_midi() function? We looked at this before as well. That’s where we ask the model to give us some notes. generate_midi() is defined within the second file we looked at, predict.py.

Toward the top of this file is some useful info:

generator = melody_rnn_sequence_generator

So melody_rnn is the thing that is generating music. And it is part of magenta:

from magenta.models.melody_rnn import melody_rnn_model

And we can see the magenta source code here. This is taking some digging, but this is sometimes what is necessary to get to unique results and truly understand what’s happening in AI. If we read the README for Melody RNN, we learn that it can use a few models: basic, lookback, and attention. And we are using attention:

BUNDLE_NAME = 'attention_rnn'

config = magenta.models.melody_rnn.melody_rnn_model.default_configs[BUNDLE_NAME]

bundle_file = magenta.music.read_bundle_file(os.path.abspath(BUNDLE_NAME+'.mag'))

So one thing we could do is try to use another model:

BUNDLE_NAME = 'lookback_rnn'

But as we can see above, lookback requires a .mag file. Melody RNN provides those in the readme. Download lookback. Where do we put it? Well, where is attention_rnn.mag? Do a search in your code editor. If you are using VSCode, you can hit CMD+P and type in .mag. It’s in the server folder. Place lookback_rnn in there as well.

Because this is on the server side, we’ll need to stop and restart the server. Open the terminal window where you started the server, and hit ctrl+c. Then run python server.py once again. If you ever want to run a command you ran in the past and you forget what it was, you can hit the up arrow in the terminal prompt to scroll back through your commands. Or you can hit CTRL+R and start typing bits of whatever you might have typed, and the terminal will search for the most recent thing you entered that matches.

Refresh your browser, and try playing now. Notice a difference? Me either, it might be subtle. But that doesn’t mean it was a waste of time. If we want to generate more noticeable results, we could train our own model on our own sound files. Or, we could swap out melodies for some other pre-trained model. Drums_RNN for beats, maybe?

That’s perhaps beyond the scope of this class, but hopefully this has given you a glimpse into how Machine Learning is used in music, as well as how to take existing code and use it for your own purposes.

More Examples of ML In Music

Giorgio Cam

code

Using Google Cloud Vision the Camera identifies what it sees on your phone, writes a song about it using Speech Synthesis, and creates vocal inflection using MaryTTS. Tone.js helps with the beats.

NSynth Sound Maker

Code | Play

Nsynth Super

code

Infinite Drum Machine

code | play

Homework

Find some interesting examples of ML in Audio / Music. Come up with a project you could do using what you’ve learned so far.

Train an AI on certain sounds using the oFX Audio Classifier, and use that output to drive an application that can work with OSC.

Stay up-to-date on Machine Learning! It’s evolving quickly…

Future of Machine Learning in Music

While things like Wavenet are slow to train but very convincing in their musical production, and DeepMind’s Tacotron was fast to generate but couldn’t answer for some of the noisiness and polyphonics in musical recordings, Tacotron2 will use slow-to-train networks to ‘teach’ fast-generating architectures.

The future of Machine Learning music might involve Neural Networks generating hit songs. But it also has a huge potential for collaboration with artists, becoming another tool at the artist’s disposal — capable of the sort of granular quantification and large scale synthesis of music that we only intuit.","['machine', 'file', 'frequency', 'notes', 'learning', 'soundmusic', 'ai', 'look', 'sound', 'using', 'code']","Current State of Machine Learning in Sound+MusicWe’ll look at some up-to-date examples of what people are doing with ML in Sound+Music today.
ML Driven: Rather than enforcing rules, Wavenet allows the machine learning algorithms to infer the rules from the recordings.
We’re not here to learn front end web development, we’re here to understand machine learning.
So on top of learning Machine Learning, you can say you learned a little about one of the most advanced concepts in web development!
The future of Machine Learning music might involve Neural Networks generating hit songs.",en,['Mike Heavers'],2018-12-05 17:30:46.428000+00:00,"{'Neural Networks', 'Music', 'Machine Learning', 'Generative Art', 'Audio'}","{'https://miro.medium.com/max/60/0*o7gDG0gd_ZL6eeoO.png?q=20', 'https://miro.medium.com/max/3900/0*loq5VcKiFdB87wky.jpeg', 'https://miro.medium.com/max/3994/0*NM0eYr1ZiIiWGGhb.png', 'https://miro.medium.com/max/60/0*NM0eYr1ZiIiWGGhb.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*RsCKqWhLuOLSzBRiBYEVow.jpeg', 'https://miro.medium.com/max/4000/0*1V-trUKm36H52tFj.JPG', 'https://miro.medium.com/max/3000/0*rzwaqj9AQxQeR8yi.png', 'https://miro.medium.com/max/60/0*kXuVBJBjUZgvQ8tr.png?q=20', 'https://miro.medium.com/max/60/0*_LtrfeX2fjLWv4ww.png?q=20', 'https://miro.medium.com/max/1572/0*1xTol3r0-9u8SiAs.png', 'https://miro.medium.com/max/60/0*AW2nDF1xczM1lCSc.png?q=20', 'https://miro.medium.com/max/4000/0*o7gDG0gd_ZL6eeoO.png', 'https://miro.medium.com/max/60/0*iNYdpPJAmroqQYSk.png?q=20', 'https://miro.medium.com/max/60/0*3Uwht1wLd9udJQHV.png?q=20', 'https://miro.medium.com/max/4000/0*EKg_69zjRatTa_9P.png', 'https://miro.medium.com/max/44/0*1xTol3r0-9u8SiAs.png?q=20', 'https://miro.medium.com/max/1288/0*Xdm9_JuT9npZQj_O.png', 'https://miro.medium.com/max/60/0*qe-_uwEQ-lqughsM.png?q=20', 'https://miro.medium.com/max/1200/0*1V-trUKm36H52tFj.JPG', 'https://miro.medium.com/max/60/0*loq5VcKiFdB87wky.jpeg?q=20', 'https://miro.medium.com/max/2684/0*O-7LyokHqmz_SpF-.png', 'https://miro.medium.com/max/3200/0*AW2nDF1xczM1lCSc.png', 'https://miro.medium.com/max/60/0*rS8Heck3gek2xXoK.png?q=20', 'https://miro.medium.com/max/60/0*YS0offj0sEaRE2fO.png?q=20', 'https://miro.medium.com/max/60/0*O-7LyokHqmz_SpF-.png?q=20', 'https://miro.medium.com/max/3248/0*kXuVBJBjUZgvQ8tr.png', 'https://miro.medium.com/max/60/0*y3VUJJBSo77Xvbdw.png?q=20', 'https://miro.medium.com/max/60/0*2ExucDSg3oVrLto2.jpg?q=20', 'https://miro.medium.com/fit/c/80/80/0*Lj7i4PWCGNIEsvKi.', 'https://miro.medium.com/max/60/0*u4eADvSdUhqvubpt.png?q=20', 'https://miro.medium.com/max/2844/0*iNYdpPJAmroqQYSk.png', 'https://miro.medium.com/max/1576/0*qe-_uwEQ-lqughsM.png', 'https://miro.medium.com/fit/c/160/160/1*XKUqVVNbKzyXikmrGNNFDw.png', 'https://miro.medium.com/max/60/0*EKg_69zjRatTa_9P.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*XKUqVVNbKzyXikmrGNNFDw.png', 'https://miro.medium.com/max/1600/0*_LtrfeX2fjLWv4ww.png', 'https://miro.medium.com/max/4000/0*y3VUJJBSo77Xvbdw.png', 'https://miro.medium.com/max/1536/0*rS8Heck3gek2xXoK.png', 'https://miro.medium.com/fit/c/80/80/1*XKUqVVNbKzyXikmrGNNFDw.png', 'https://miro.medium.com/max/1000/0*2ExucDSg3oVrLto2.jpg', 'https://miro.medium.com/max/2558/0*u4eADvSdUhqvubpt.png', 'https://miro.medium.com/max/60/0*1V-trUKm36H52tFj.JPG?q=20', 'https://miro.medium.com/max/1848/0*3Uwht1wLd9udJQHV.png', 'https://miro.medium.com/max/60/0*rzwaqj9AQxQeR8yi.png?q=20', 'https://miro.medium.com/max/2112/0*YS0offj0sEaRE2fO.png', 'https://miro.medium.com/max/4000/0*wvOJ6YW_4mMUI87w.png', 'https://miro.medium.com/max/60/0*wvOJ6YW_4mMUI87w.png?q=20', 'https://miro.medium.com/max/60/0*Xdm9_JuT9npZQj_O.png?q=20'}",2020-03-05 00:18:49.746429,2.1017658710479736
https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e,Generate Piano Instrumental Music by Using Deep Learning,"Hello everyone! Finally, I can write again on my Medium and have free time to do some experiments on Artificial Intelligence (AI). This time, I am going to write and share about how to generate music notes by using Deep Learning. Unlike my previous article about generating lyrics, this time we will generate the notes of the musics and also generate the file (in MIDI format).

Photo by Malte Wingen on Unsplash

The theme of the music is Piano. This article will generate piano notes by using a variant of Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU) with the help of Self Attention. Not only this article will tell how to generate the notes, this article will also tell how to generate it into a proper MIDI files and can also be played in the computer.

This article is targeted for the one who is interested in AI, especially who want to practice on using Deep Learning. I hope that my writing skill will increase by publishing this article and the content benefits you 😃.

There is a Github link at the end of this article if you want to know about the complete source code. For now, I will give the python notebook and the Colaboratory link in the repository,.

Here is the opening music

Sound 1 : Opening Piano 😃

(That music is generated by the model that we will create in this article)

Outline

Introduction Technology and Data Pipeline Preprocessing MIDI files Train Model Inference and Generate MIDI Files Results Conclusion Afterwords

Introduction

One of the current hot topic in the Artificial Intelligence is how to generate something by only using the data (unsupervised). In Computer Vision domain, there are many researchers out there researching some advanced techniques on generating images using Generative Advesarial Network (GAN). For example NVIDIA create realistic face generator by using GAN. There are also some research on generating music by using GAN.

Photo by Akshar Dave on Unsplash

If we talk about the value of the music generator, it can be used to help the musician on creating their music. It can enhance the creativity of people. I think in the future, if there are a lot of high attention on this field, most of musicians will create its music assisted by AI.

This article will be focused on how to generate music by generating sequential of notes in a music. We will know how to preprocess the data and transform them to be input of neural network to generate the music.

The experiment will also use Tensorflow v2.0 (still on alpha phase) as the Deep Learning Framework. What I want to show is to test and use Tensorflow v2.0 by following some of their best practice. One of the feature that I like in Tensorflow v2.0 is that it really accelerates the training of the model by using their AutoGraph. It can be used by defining our function using @tf.function . Moreover, there are no “tf.session” anymore and no global initialization. These efeatures are one of the reason that I moved from Tensorflow to PyTorch. Tensorflow usability was not good for me. Nevertheless, In my opinion Tensorflow v2.0 change it all and increase their usability to make it comfortable to do some experiment.

This experiment also use self-attention layer . The self-attention layer will tell us, given a sequential instance (for example in the music note “ C D E F G”), each token will learn how much the influence on other token to that token. Here’s some example (for an NLP task):

Image 1 : Visualization of attention. Taken from : http://jalammar.github.io/illustrated-transformer/

For further information about self-attention, especially about transformer, you can see this awesome article.

Without any further ado, let’s go on generating the music

Technology and Data

This experiment will use :

Tensorflow v2.0 : Deep Learning Framework, a new version of Tensorflow which still in alpha phase of development. Python 3.7 Colaboratory : Free Jupyter notebook environment that requires no setup and runs entirely in the cloud. Have GPU Tesla K80 or even TPU! Sadly Tensorflow v2.0 alpha still does not support TPU at the moment of this writing. Python library pretty_midi : a library to manipulate and create MIDI files

For the Data, we use MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) from Magenta as the dataset. This dataset only contains piano instruments. We will take 100 musics randomly from around 1000 musics to make our training time faster.

Pipeline

Here is the pipeline on how our music generator will works:

Image 2 : Pipeline

We will see each of the process. To make it simpler, we will divide each of the processes as follow:

Preprocess MIDI Files to be Input of Neural Network Training Process Generating MIDI Files

Preprocess MIDI Files

Before we go into how to preprocess the midi files, we need to know what Midi format file is.

From pcmag , the definition of MIDI:

(Musical Instrument Digital Interface) A standard protocol for the interchange of musical information between musical instruments, synthesizers and computers. MIDI was developed to allow the keyboard of one synthesizer to play notes generated by another. It defines codes for musical notes as well as button, dial and pedal adjustments, and MIDI control messages can orchestrate a series of synthesizers, each playing a part of the musical score. MIDI Version 1.0 was introduced in 1983.

In summary, MIDI files contains a series of instruments which has notes in it. For example the combination of Piano and Guitar. Each of music instruments usually have different notes to play.

For preprocess the MIDI files, there are some libraries that can be used to do it in Python. One of them is pretty_midi . It can manipulate MIDI files and also create a new one. In this article, we will use that library.

For pretty_midi the format of midi files is as follow:

Image 3 : PrettyMidi format

Start is the start of a note played in second. End is the end of a note played in a second. There can be a overlap multi notes in a time. Pitch is the MIDI number of the Note played. Velocity is the force which the note is played.

For the reference of the relation between MIDI number and note name, you can see the picture below:

Image 4 : Midi Number with the Note Name. Taken from https://newt.phys.unsw.edu.au/jw/notes.html

Read the Midi Files

We will read midi files in a batch. This is how we read it using pretty_midi :

midi_pretty_format = pretty_midi.PrettyMIDI('song.mid')

We will get the PrettyMidi object.

Preprocess to Piano Roll Array

Image 5 : PrettyMidi to Piano Roll Array

For this article, we need to extract all the notes of the music from an instrument. Many MIDI files have multiple instruments in their music. In our dataset, the MIDI files only contains one instrument, which is Piano. We will extract the notes from the piano instruments. To make it easier, we will extract the notes for desired frame per second. pretty_midi has a handy function get_piano_roll to get the notes in binary 2D numpy.array in (notes, time) dimension array. The notes length is 128 and time follow the duration of the music divided by FPS.

The source code how we do it:

midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)

piano_midi = midi_pretty_format.instruments[0] # Get the piano channels

piano_roll = piano_midi.get_piano_roll(fs=fs)

Preprocess to dictionary of time and notes

Image 6 : Piano Roll Array to Dictionary

After we get the array of piano roll, we convert them into dictionary. The dictionary will start from the time where the note is played. For example, in the picture above, we start from 28 (If we convert to second, assume we convert to piano_roll at 5 fps, the music start playing its notes at 5.6 s which we can get by 28 divided by 5).

After we create the dictionary, we will convert the values of the dictionary into string. For example:

array([49,68]) => '49,68'

To do it, we should loop all the keys of the dictionary and change its value:

for key in dict_note:

dict_note[key] = ','.join(dict_note[key])

Preprocess to list of music notes to be input and Target of Neural Network

Image 7 : Dictionary to List of Sequences

After we get the dictionary, we will convert it into sequential of notes which will be used to be input of neural network. Then we get the next timestep to be the target of the input of neural network.

Image 8 : Sliding window, taken from : https://towardsdatascience.com/generating-drake-rap-lyrics-using-language-models-and-lstms-8725d71b1b12

In this article, the length of the sequence list is 50. That means that if our fps is 5, we will get a sequence that contains 10 (50 / 5) seconds playtime.

‘e’ in the list means that there are no notes are played in that time. Since there are time where there is a jump or no notes are played between each played notes. In the example in Image 7, we can see that there is a jump from 43 to 46. If we convert that sequence, The list of sequence would be:

[ ... '61,77', '61,77', 'e', 'e', '73' , ...]

How we do it ? We will process the note in a batch of musics.

We use a 50 length sliding window. For the first note in a music, we will append ‘e’ to the list 49 times. Then set the start time to the first timestep in the dictionary. In the example in Image 7, it is 28. Then we append the first note in that music (In the example ‘77’).

Then for the next instance, we slide the window by one and append ‘e’ to the list 48 times and append the note played in timestep 28 and append the note in timestep 29 and repeat until the end of the music.

For the next music, we repeat the process above.

This is the source code:

Create Note Tokenizer

Before we dive into the neural network, we must create the tokenizer to change the sequential notes into sequential index of the notes. First we should map the note into an index representing the id of the note.

For example:

{

'61,77' : 1, # 61,77 will be identified as 1

'e' : 2,

'73' : 3,

.

.

}

So if our previous input is as below:

[ ... , '61,77', '61,77', 'e', 'e', '73' , ...]

We convert it into:

[ ... 1, 1, 2, 2, 3 , ...]

This is how we do it.","['piano', 'files', 'article', 'tensorflow', 'instrumental', 'midi', 'notes', 'learning', 'deep', 'note', 'music', 'generate', 'using']","This time, I am going to write and share about how to generate music notes by using Deep Learning.
This article will generate piano notes by using a variant of Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU) with the help of Self Attention.
This article will be focused on how to generate music by generating sequential of notes in a music.
To make it simpler, we will divide each of the processes as follow:Preprocess MIDI Files to be Input of Neural Network Training Process Generating MIDI FilesPreprocess MIDI FilesBefore we go into how to preprocess the midi files, we need to know what Midi format file is.
For preprocess the MIDI files, there are some libraries that can be used to do it in Python.",en,['Haryo Akbarianto Wibowo'],2019-03-25 21:50:43.835000+00:00,"{'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'TensorFlow'}","{'https://miro.medium.com/max/1000/1*lk6ZMBqdAhokRIFmLTrUdg.png', 'https://miro.medium.com/max/1426/1*Lpu-1gskcuHH5epLPaOawA.png', 'https://miro.medium.com/max/60/0*vNBAQ90SMOMlxtNs?q=20', 'https://miro.medium.com/proxy/0*9IUYoChZToK82eAh.png', 'https://miro.medium.com/fit/c/160/160/1*OfIkGPVRCkIOqssErk33iw.jpeg', 'https://miro.medium.com/max/30/1*Lpu-1gskcuHH5epLPaOawA.png?q=20', 'https://miro.medium.com/max/2172/1*kPWpdqriKEe2Rd1RJwtwRQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*RrOlS_31DWZ3x0cx.png?q=20', 'https://miro.medium.com/max/1242/1*MH-iriA0dJSfI5pEnIu7hA.png', 'https://miro.medium.com/max/60/1*QkELV3dEA2o2MQxiuyUgJQ.png?q=20', 'https://miro.medium.com/max/60/1*cxPjurBsMxS-tV2kOPorwg.png?q=20', 'https://miro.medium.com/max/12000/0*vNBAQ90SMOMlxtNs', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1292/1*aZCkOuGUfgtWiGwGhlIz7g.jpeg', 'https://miro.medium.com/fit/c/96/96/1*OfIkGPVRCkIOqssErk33iw.jpeg', 'https://miro.medium.com/max/60/1*31STGCYpRDPjCsiCd7OWsA.png?q=20', 'https://miro.medium.com/max/1200/0*tAxl96xzGnT0fZtu', 'https://miro.medium.com/max/950/1*M-TT63Ju8nBomgclnVeJgg.png', 'https://miro.medium.com/max/1774/1*cxPjurBsMxS-tV2kOPorwg.png', 'https://miro.medium.com/max/1136/1*QkELV3dEA2o2MQxiuyUgJQ.png', 'https://miro.medium.com/max/60/1*YTOvGpAsWv-hW-sTnXfmug.png?q=20', 'https://miro.medium.com/max/60/0*tAxl96xzGnT0fZtu?q=20', 'https://miro.medium.com/max/60/0*z94_GFwqFlSS9Pfj?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Q2JovZZ1-bovZNI-MqB3Ig.png?q=20', 'https://miro.medium.com/max/1090/1*31STGCYpRDPjCsiCd7OWsA.png', 'https://miro.medium.com/max/1858/1*Q2JovZZ1-bovZNI-MqB3Ig.png', 'https://miro.medium.com/max/44/0*N3-1puV80FOJuUPy.GIF?q=20', 'https://miro.medium.com/max/60/1*Ks-V-0zCCVNcM2hf4kvIWA.png?q=20', 'https://miro.medium.com/max/974/0*N3-1puV80FOJuUPy.GIF', 'https://miro.medium.com/max/7776/0*tAxl96xzGnT0fZtu', 'https://miro.medium.com/max/1802/1*Ks-V-0zCCVNcM2hf4kvIWA.png', 'https://miro.medium.com/max/60/1*M-TT63Ju8nBomgclnVeJgg.png?q=20', 'https://miro.medium.com/max/50/1*aZCkOuGUfgtWiGwGhlIz7g.jpeg?q=20', 'https://miro.medium.com/max/12000/0*z94_GFwqFlSS9Pfj', 'https://miro.medium.com/max/60/1*MH-iriA0dJSfI5pEnIu7hA.png?q=20', 'https://miro.medium.com/max/60/1*kPWpdqriKEe2Rd1RJwtwRQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1242/1*YTOvGpAsWv-hW-sTnXfmug.png', 'https://miro.medium.com/max/11520/0*FasnR6QCuqO9b-dE', 'https://miro.medium.com/max/60/0*FasnR6QCuqO9b-dE?q=20', 'https://miro.medium.com/max/874/0*RrOlS_31DWZ3x0cx.png'}",2020-03-05 00:18:57.365018,7.616588115692139
https://towardsdatascience.com/making-music-when-simple-probabilities-outperform-deep-learning-75f4ee1b8e69,Making Music: When Simple Probabilities Outperform Deep Learning,"The Problem

Before delving into their relationship, let me first define the problem. I began this project with the simple desire to generate pop music using deep learning, or ‘A.I.’ as laymen call it. This quickly led me to LSTMs (Long Short-Term Memory units), a particular version of a Recurrent Neural Network (RNN) that is very popular for generating texts and making music.

But as I read more into the subject, I began to question the logic of applying RNNs and their variants to generate pop music. The logic seemed to based on several assumptions about the internal structure of (pop) music that I did not fully agree with.

One specific assumption is the independent relationship between the harmony and the melody (description of the two is above).

Take for instance the 2017 publication from the University of Toronto: Song from Pi: A Musically Plausible Network for Pop Music Generation (Hang Chu, et al). In this article, the authors explicitly “assume…the chords are independent given the melody” (3, italics mine). Based on this specification, the authors build a complex and multi-layered RNN model. The melody has its own layer for generating notes (the key and the press layer), which is autonomous from the chord layer. On top of the independence, this particular model conditions the harmony on the melody for generation. This just means that the harmony is dependent on the melody for note generation.

Hang Chu, et al.’s stacked RNN model. Each layer is responsible for addressing different aspect of a song.

This kind of modeling feels odd to me, as it does not seem to approximate how humans would approach composing popular music. Speaking personally as a classically trained pianist, I would never consider writing down melody notes without first considering the harmony notes. This is because the harmony notes both define and limit what my melody notes can be. Axis of Awesome, in their once viral YouTube video, demonstrated this idea long ago.

Video demonstrating how different pop melodies are all dependent on the same four chords.

Their video displays a defining attribute of western pop music: that harmony, or those four chords, strongly determine what the melody will be. In data science language, we can say that a conditional probability regulates and resolves the statistical relationship between the harmony and the melody. This becomes the case as the melody notes are naturally dependent on what the harmony notes are. One could thus argue that the harmony notes both inherently limit and enable which melody notes can be chosen in a particular song.","['particular', 'relationship', 'melody', 'harmony', 'pop', 'simple', 'making', 'learning', 'notes', 'outperform', 'deep', 'model', 'rnn', 'probabilities', 'music', 'layer']","I began this project with the simple desire to generate pop music using deep learning, or ‘A.I.’ as laymen call it.
But as I read more into the subject, I began to question the logic of applying RNNs and their variants to generate pop music.
The logic seemed to based on several assumptions about the internal structure of (pop) music that I did not fully agree with.
Speaking personally as a classically trained pianist, I would never consider writing down melody notes without first considering the harmony notes.
This becomes the case as the melody notes are naturally dependent on what the harmony notes are.",en,['Haebichan Jung'],2019-01-21 15:27:02.787000+00:00,"{'Neural Networks', 'Markov Chains', 'Deep Learning', 'Recurrent Neural Network', 'Towards Data Science'}","{'https://miro.medium.com/freeze/max/60/1*3YN8acF_8OqiHoNblYV4uQ.gif?q=20', 'https://miro.medium.com/max/1930/1*kHhsdWdtG4tLB9mR1MY-Vg.gif', 'https://miro.medium.com/max/2280/1*auNf4KPcAEQgFxJz7sjoZA.png', 'https://miro.medium.com/freeze/max/60/1*kHhsdWdtG4tLB9mR1MY-Vg.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*VBa0filhmlIZ3SQ8Dod29w.gif?q=20', 'https://miro.medium.com/max/3840/1*hb-gXGSKAav8hT2pLxg1tQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*k50dLQlinbANGI_vzZdLaA.png?q=20', 'https://miro.medium.com/max/58/1*gGIBq5qrYkTz8Hv3eeZa3g.png?q=20', 'https://miro.medium.com/max/1666/1*EnhDRM6qbUJEHqNjuXvbsQ.gif', 'https://miro.medium.com/freeze/max/60/1*8bcrqbv7B6Z6Omm6AYW6rQ.gif?q=20', 'https://miro.medium.com/max/2002/1*9MhtFFequvcCbV_CUUBXhw.gif', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*hb-gXGSKAav8hT2pLxg1tQ.jpeg', 'https://miro.medium.com/max/60/1*hb-gXGSKAav8hT2pLxg1tQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*auNf4KPcAEQgFxJz7sjoZA.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*-6t-cFhBlRSQ6_zsT_JjNQ.jpeg', 'https://miro.medium.com/max/1004/1*gGIBq5qrYkTz8Hv3eeZa3g.png', 'https://miro.medium.com/max/1922/1*8bcrqbv7B6Z6Omm6AYW6rQ.gif', 'https://miro.medium.com/freeze/max/60/1*7_DS7Y02sAtHwK22YEphmg.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1920/1*3YN8acF_8OqiHoNblYV4uQ.gif', 'https://miro.medium.com/max/2566/1*VBa0filhmlIZ3SQ8Dod29w.gif', 'https://miro.medium.com/max/60/1*7SHDxW97-9C87Zt99W5JsQ.png?q=20', 'https://miro.medium.com/max/3392/1*k50dLQlinbANGI_vzZdLaA.png', 'https://miro.medium.com/max/4808/1*7SHDxW97-9C87Zt99W5JsQ.png', 'https://miro.medium.com/freeze/max/60/1*EnhDRM6qbUJEHqNjuXvbsQ.gif?q=20', 'https://miro.medium.com/fit/c/160/160/2*-6t-cFhBlRSQ6_zsT_JjNQ.jpeg', 'https://miro.medium.com/max/1860/1*7_DS7Y02sAtHwK22YEphmg.gif', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*eaPlE9jG4_w-3Y57QuMmLA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*9MhtFFequvcCbV_CUUBXhw.gif?q=20', 'https://miro.medium.com/max/2028/1*eaPlE9jG4_w-3Y57QuMmLA.png'}",2020-03-05 00:19:05.761135,8.396116971969604
https://medium.com/@neslinesli93/how-to-efficiently-store-and-query-time-series-data-90313ff0ec20,How to efficiently store and query time-series data,"Solution 2: TimescaleDB

TimescaleDB architecture allows for nearly constant ingestion rate when data grows over time, as explained in many of their Medium posts like this one

Timescale is an extension built on top of the popular SQL database, PostgreSQL: it provides all its features plus a set of utilities thought precisely for time-series data.

It basically organizes and indexes data in chunks of time, called buckets, which allows for really fast retrieval when performing range queries. Tables made this way are called hypertables.

Being still in beta when I tried it, I was a bit skeptical about Timescale. However, it looked too good of a match for my requirements so I decided to give it a try.

The setup experience was really pleasant: all I had to do was to install the extension on my database, and call create_hypertable on the vanilla SQL tables I wanted to add the bucket index on.

Then I proceeded on the bulk import of CSV files, using the wonderful pg-promise library. And again, I found an unexpected result: the underlying indexes Timescale was creating were so heavy, that waiting times on insertions had become huge.

So I decided to skip the NodeJS wrapper and went on using a Go tool that Timescale developed for ingesting a lot of data. Luckily the process sped up by orders of magnitude, but I started wondering if the bottleneck represented by all those indexes was actually worth it.

Furthermore, at the time of my tests, Timescale did not support any kind of Foreign Key constraint on hypertables, so it was a big no-no for me as referential integrity is one of the main selling points of SQL databases.

Eventually I decided to ditch Timescale and look for other solutions. However, I want to praise their wonderful Slack support and their community, which has evolved a lot since last year. Plus, Timescale now support FKs on hypertables!","['wonderful', 'indexes', 'sql', 'timeseries', 'really', 'decided', 'data', 'store', 'query', 'support', 'timescale', 'using', 'efficiently', 'tables']","It basically organizes and indexes data in chunks of time, called buckets, which allows for really fast retrieval when performing range queries.
Then I proceeded on the bulk import of CSV files, using the wonderful pg-promise library.
And again, I found an unexpected result: the underlying indexes Timescale was creating were so heavy, that waiting times on insertions had become huge.
So I decided to skip the NodeJS wrapper and went on using a Go tool that Timescale developed for ingesting a lot of data.
However, I want to praise their wonderful Slack support and their community, which has evolved a lot since last year.",en,['Tommaso Pifferi'],2018-08-04 19:45:12.101000+00:00,"{'Sql', 'Fintech', 'Database', 'Programming', 'NoSQL'}","{'https://miro.medium.com/max/3840/1*iYOTpiVHfZ8IyxOzmTmmHw.jpeg', 'https://miro.medium.com/max/2240/1*eR-UifAkUpCV_352ryUTvA.jpeg', 'https://miro.medium.com/max/60/1*iYOTpiVHfZ8IyxOzmTmmHw.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*ty4NvNrGg4ReETxqU2N3Og.png', 'https://miro.medium.com/max/1200/1*hor57pXvQR42QmW-mIS5ew.jpeg', 'https://miro.medium.com/max/11624/1*hor57pXvQR42QmW-mIS5ew.jpeg', 'https://miro.medium.com/fit/c/160/160/0*kA9M1DU4pd7BTiTn', 'https://miro.medium.com/max/60/1*eR-UifAkUpCV_352ryUTvA.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/0*kA9M1DU4pd7BTiTn', 'https://miro.medium.com/fit/c/80/80/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/fit/c/80/80/1*5CxCVj6YpFqYaGpza1nrmA.jpeg', 'https://miro.medium.com/max/60/1*hor57pXvQR42QmW-mIS5ew.jpeg?q=20'}",2020-03-05 00:19:07.383608,1.6224730014801025
https://towardsdatascience.com/analyzing-time-series-data-in-pandas-be3887fdd621,Analyzing time series data in Pandas,"In my previous tutorials, we have considered data preparation and visualization tools such as Numpy, Pandas, Matplotlib and Seaborn. In this tutorial, we are going to learn about Time Series, why it’s important, situations we will need to apply Time Series, and more specifically, we will learn how to analyze Time Series data using Pandas.

What is Time Series

Time Series is a set of data points or observations taken at specified times usually at equal intervals (e.g hourly, daily, weekly, quarterly, yearly, etc). Time Series is usually used to predict future occurrences based on previous observed occurrence or values. Predicting what would happen in the stock market tomorrow, volume of goods that would be sold in the coming week, whether or not price of an item would skyrocket in December, number of Uber rides over a period of time, etc; are some of the things we can do with Time Series Analysis.

Why we need Time series

Time series helps us understand past trends so we can forecast and plan for the future. For example, you own a coffee shop, what you’d likely see is how many coffee you sell every day or month and when you want to see how your shop has performed over the past six months, you’re likely going to add all the six month sales. Now, what if you want to be able to forecast sales for the next six months or year. In this kind of scenario, the only variable known to you is time (either in seconds, minutes, days, months, years, etc) — hence you need Time Series Analysis to predict the other unknown variables like trends, seasonality, etc.

Hence, it is important to note that in Time Series Analysis, the only known variable is — Time.

Why pandas makes it easy to work with Time Series

Pandas has proven very successful as a tool for working with Time Series data. This is because Pandas has some in-built datetime functions which makes it easy to work with a Time Series Analysis, and since time is the most important variable we work with here, it makes Pandas a very suitable tool to perform such analysis.

Components of Time Series

Generally, including those outside of the financial world, Time Series often contain the following features:

Trends: This refers to the movement of a series to relatively higher or lower values over a long period of time. For example, when the Time Series Analysis shows a pattern that is upward, we call it an Uptrend, and when the pattern is downward, we call it a Down trend, and if there was no trend at all, we call it a horizontal or stationary trend. One key thing to note is that trend usually happens for sometime and then disappears. Seasonality: This refers to is a repeating pattern within a fixed time period. Although these patterns can also swing upward or downward, however, this is quite different from that of a trend because trend happens for a period of time and then disappears. However Seasonality keeps happening within a fixed time period. For example, when it’s Christmas, you discover more candies and chocolates are sold and this keeps happening every year. Irregularity: This is also called noise. Irregularity happens for a short duration and it’s non depleting. A very good example is the case of Ebola. During that period, there was a massive demand for hand sanitizers which happened erratically/systematically in a way no one could have predicted, hence one could not tell how much number of sales could have been made or tell the next time there’s going to be another outbreak. Cyclic: This is when a series is repeating upward and downward movement. It usually does not have a fixed pattern. It could happen in 6months, then two years later, then 4 years, then 1 year later. These kinds of patterns are much harder to predict.

When not to apply TS

Remember how we stated that the main variable here is Time? Same way, it is important to mention that we cannot apply Time Series analysis to a dataset when:

The variables/values are constant. For example, 5000 boxes of candies where sold last Christmas, and the Christmas before that. Since both values are the same, we cannot apply time series to predict sales for this year’s Christmas.

2. Values in the form of functions: There’s no point applying Time Series Analysis to a dataset when you can calculate values by simply using a formula or function.

Now that we have basic understanding of what Time Series is, let’s go ahead and work on an example to fully grasp how we can analyze a Time Series Data.

Forecasting the future of an Air Travel Company

In this example, we are asked to build a model to forecast the demand for flight tickets of a particular airline. We will be using the International Airline Passengers dataset . You can also download it from kaggle here.

Importing Packages and Data

To begin, first thing we need to do is to import the packages we will use to perform our analysis: in this case, we’ll make use of pandas , to prepare our data and access the datetime functions and matplotlib to create our visualizations:

Now, let’s read our dataset to see what kind of data we have. As we see, the dataset has been classified into two columns; Month and Passengers traveling per month.

I usually like getting a summary of the dataset in case there’s a row with an empty value. Let’s go ahead and check by doing this:

As we can see, we do not have any empty value in our dataset, so we’re free to continue our analysis. Now, what we will do is to confirm that the Month column is in datetime format and not string. Pandas .dtypes function makes this possible:

We can see that Month column is of a generic object type which could be a string. Since we want to perform time related actions on this data, we need to convert it to a datetime format before it can be useful to us. Let’s go ahead and do this using to_datetime() helper function, let’s cast the Month column to a datetime object instead of a generic object:

Notice how we now have date field generated for us as part of the Month column. By default, the date field assumes the first day of the month to fill in the values of the days that were not supplied. Now, if we go back and confirm the type, we can see that it’s now of type datetime :

Now, we need to set the datetime object as the index of the dataframe to allow us really explore our data. Let’s do this using the .set_index() method:

We can see now that the Month column is the index of our dataframe. Let’s go ahead and create our plot to see what our data looks like:

Note that in Time Series plots, time is usually plotted on the x-axis while the y-axis is usually the magnitude of the data.

Notice how the Month column was used as our x-axis and because we had previously casted our Month column to datetime , the year was specifically used to plot the graph.

By now, you should notice an upward trend indicating that the airline would have more passenger over time. Although there are ups and downs at every point in time, generally we can observe that the trend increases. Also we can notice how the ups and downs seem to be a bit regular, it means we might be observing a seasonal pattern here too. Let’s take a closer look by observing some year’s data:

As we can see in the plot, there’s usually a spike between July and September which begins to drop by October, which implies that more people travel between July and September and probably travel less from October.

Remember we mentioned that there’s an upward trend and a seasonal pattern in our observation? There are usually a number of components [Scroll up to see explanation of Time Series components] in most Time Series analysis. Hence, what we need to do now is use Decomposition techniques to to deconstruct our observation into several components, each representing one of the underlying categories of patterns.

Decomposition of Time Series

There are a couple of models to consider during the Decomposition of Time Series data.

1. Additive Model: This model is used when the variations around the trend does not vary with the level of the time series. Here the components of a time series are simply added together using the formula:

y(t) = Level(t) + Trend(t) + Seasonality(t) + Noise(t)

2. Multiplicative Model: Is used if the trend is proportional to the level of the time series. Here the components of a time series are simply multiplied together using the formula:

y(t) = Level(t) * Trend(t) * Seasonality(t) *Noise(t)

For the sake of this tutorial, we will use the additive model because it is quick to develop, fast to train, and provide interpretable patterns. We also need to import statsmodels which has a tsa (time series analysis) package as well as the seasonal_decompose() function we need:

Now we have a much clearer plot showing us that the trend is going up, and the seasonality following a regular pattern.

One last thing we will do is plot the trend alongside the observed time series. To do this, we will use Matplotlib’s .YearLocator() function to set each year to begin from the month of January month=1 , and month as the minor locator showing ticks for every 3 months (intervals=3) . Then we plot our dataset (and gave it blue color) using the index of the dataframe as x-axis and the number of Passengers for the y-axis .

We did the same for the trend observations which we plotted in red color.

import matplotlib.pyplot as plt

import matplotlib.dates as mdates fig, ax = plt.subplots()

ax.grid(True) year = mdates.YearLocator(month=1)

month = mdates.MonthLocator(interval=3)

year_format = mdates.DateFormatter('%Y')

month_format = mdates.DateFormatter('%m') ax.xaxis.set_minor_locator(month) ax.xaxis.grid(True, which = 'minor')

ax.xaxis.set_major_locator(year)

ax.xaxis.set_major_formatter(year_format) plt.plot(data_set.index, data_set['#Passengers'], c='blue')

plt.plot(decomposition.trend.index, decomposition.trend, c='red')

Again, we can see the trend is going up against the individual observations.

Conclusion

I hope this tutorial has helped you in understanding what Time Series is and how to get started with analyzing Time Series data.","['lets', 'pandas', 'series', 'trend', 'need', 'usually', 'data', 'month', 'datetime', 'analyzing', 'analysis', 'using']","In this tutorial, we are going to learn about Time Series, why it’s important, situations we will need to apply Time Series, and more specifically, we will learn how to analyze Time Series data using Pandas.
Why pandas makes it easy to work with Time SeriesPandas has proven very successful as a tool for working with Time Series data.
There are usually a number of components [Scroll up to see explanation of Time Series components] in most Time Series analysis.
Decomposition of Time SeriesThere are a couple of models to consider during the Decomposition of Time Series data.
ConclusionI hope this tutorial has helped you in understanding what Time Series is and how to get started with analyzing Time Series data.",en,['Ehi Aigiomawu'],2018-10-08 12:33:50.329000+00:00,"{'Pandas', 'Data Science', 'Timeseries', 'Machine Learning', 'Data Analysis'}","{'https://miro.medium.com/max/1224/1*iro03KPqBrEBpj7Z0C--ag.png', 'https://miro.medium.com/max/60/1*d8GYjK_iHbkFZKUeYckC-w.png?q=20', 'https://miro.medium.com/max/60/1*WO6R6JamRX0oNnVlCWrJjQ.png?q=20', 'https://miro.medium.com/max/60/1*P1soAIO50F8gSWYuDtDlBA.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*l4PMqLwrJcGiFID4.', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1494/1*3zrpkMWii2x6DgPEqmeV9Q.png', 'https://miro.medium.com/max/1678/1*YEd3Lucby0LbpkqTdZhGZQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*pjqmIQJpXk5izHVsX-EWSQ.png?q=20', 'https://miro.medium.com/max/60/1*b8l8HuhyyxyJe6hchprCOA.png?q=20', 'https://miro.medium.com/max/1544/1*cRSMDt3NjPcn5MVL9L8LoA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1494/1*pjqmIQJpXk5izHVsX-EWSQ.png', 'https://miro.medium.com/max/560/1*b8l8HuhyyxyJe6hchprCOA.png', 'https://miro.medium.com/max/60/1*iro03KPqBrEBpj7Z0C--ag.png?q=20', 'https://miro.medium.com/max/60/1*Fbu48hQTH0rQgS1v3-0RXg.png?q=20', 'https://miro.medium.com/max/1334/1*QUDliZ-gspQj5uA_5vDiKw.png', 'https://miro.medium.com/max/60/1*cRSMDt3NjPcn5MVL9L8LoA.png?q=20', 'https://miro.medium.com/max/1500/1*mpekrWGwAMuRMAY9UIC4UQ.png', 'https://miro.medium.com/max/60/1*QUDliZ-gspQj5uA_5vDiKw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*HSvW7hMGiaIBGNgZxRTTsg.png?q=20', 'https://miro.medium.com/max/60/1*YEd3Lucby0LbpkqTdZhGZQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*l4PMqLwrJcGiFID4.', 'https://miro.medium.com/max/60/1*mpekrWGwAMuRMAY9UIC4UQ.png?q=20', 'https://miro.medium.com/max/1526/1*WO6R6JamRX0oNnVlCWrJjQ.png', 'https://miro.medium.com/max/1160/1*d8GYjK_iHbkFZKUeYckC-w.png', 'https://miro.medium.com/max/60/1*QL8OwCCqBKg9_a5ag0vHlg.png?q=20', 'https://miro.medium.com/max/60/1*3zrpkMWii2x6DgPEqmeV9Q.png?q=20', 'https://miro.medium.com/max/646/1*P1soAIO50F8gSWYuDtDlBA.png', 'https://miro.medium.com/max/994/1*Fbu48hQTH0rQgS1v3-0RXg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1786/1*HSvW7hMGiaIBGNgZxRTTsg.png', 'https://miro.medium.com/max/1120/1*b8l8HuhyyxyJe6hchprCOA.png', 'https://miro.medium.com/max/1736/1*QL8OwCCqBKg9_a5ag0vHlg.png'}",2020-03-05 00:19:13.484200,6.1005918979644775
https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f,Simplifying Sentiment Analysis using VADER in Python (on Social Media Text),"What is Sentiment Analysis?

Sentiment Analysis, or Opinion Mining, is a sub-field of Natural Language Processing (NLP) that tries to identify and extract opinions within a given text. The aim of sentiment analysis is to gauge the attitude, sentiments, evaluations, attitudes and emotions of a speaker/writer based on the computational treatment of subjectivity in a text .

Why is sentiment analysis so important?

Businesses today are heavily dependent on data. Majority of this data however, is unstructured text coming from sources like emails, chats, social media, surveys, articles, and documents. The micro-blogging content coming from Twitter and Facebook poses serious challenges, not only because of the amount of data involved, but also because of the kind of language used in them to express sentiments, i.e., short forms, memes and emoticons.

Sifting through huge volumes of this text data is difficult as well as time-consuming. Also, it requires a great deal of expertise and resources to analyze all of that. Not an easy task, in short.

Sentiment Analysis is also useful for practitioners and researchers, especially in fields like sociology, marketing, advertising, psychology, economics, and political science, which rely a lot on human-computer interaction data.

Sentiment Analysis enables companies to make sense out of data by being able to automate this entire process! Thus they are able to elicit vital insights from a vast unstructured dataset without having to manually indulge with it.

Why is Sentiment Analysis a Hard to perform Task?

Though it may seem easy on paper, Sentiment Analysis is actually a tricky subject. There are various reasons for that:

Understanding emotions through text are not always easy. Sometimes even humans can get misled, so expecting a 100% accuracy from a computer is like asking for the Moon!

A text may contain multiple sentiments all at once. For instance,

“The intent behind the movie was great, but it could have been better”.

The above sentence consists of two polarities, i.e., Positive as well as Negative. So how do we conclude whether the review was Positive or Negative?

Computers aren’t too comfortable in comprehending Figurative Speech. Figurative language uses words in a way that deviates from their conventionally accepted definitions in order to convey a more complicated meaning or heightened effect. Use of similes, metaphors, hyperboles etc qualify for a figurative speech. Let us understand it better with an example.

“The best I can say about the movie is that it was interesting.”

Here, the word ’interesting’ does not necessarily convey positive sentiment and can be confusing for algorithms.

Heavy use of emoticons and slangs with sentiment values in social media texts like that of Twitter and Facebook also makes text analysis difficult. For example a “ :)” denotes a smiley and generally refers to positive sentiment while “:(” denotes a negative sentiment on the other hand. Also, acronyms like “LOL“, ”OMG” and commonly used slangs like “Nah”, “meh”, ”giggly” etc are also strong indicators of some sort of sentiment in a sentence.

These are few of the problems encountered not only with sentiment analysis but with NLP as a whole. In fact, these are some of the Open-ended problems of the Natural Language Processing field.

VADER Sentiment Analysis

VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labelled according to their semantic orientation as either positive or negative.

VADER has been found to be quite successful when dealing with social media texts, NY Times editorials, movie reviews, and product reviews. This is because VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.

It is fully open-sourced under the MIT License. The developers of VADER have used Amazon’s Mechanical Turk to get most of their ratings, You can find complete details on their Github Page.

Advantages of using VADER

VADER has a lot of advantages over traditional methods of Sentiment Analysis, including:

It works exceedingly well on social media type text, yet readily generalizes to multiple domains

It doesn’t require any training data but is constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon

but is constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon It is fast enough to be used online with streaming data, and

It does not severely suffer from a speed-performance tradeoff.

The source of this article is a very easy to read paper published by the creaters of VADER library.You can read the paper here.

Enough of talking. Let us now see practically how does VADER analysis work for which we will have install the library first.

Installation

The simplest way is to use the command line to do an installation from [PyPI] using pip. Check their Github repository for the detailed explanation.

> pip install vaderSentiment

Once VADER is installed let us call the SentimentIntensityAnalyser object,

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer analyser = SentimentIntensityAnalyzer()

Working & Scoring

Let us test our first sentiment using VADER now. We will use the polarity_scores() method to obtain the polarity indices for the given sentence.

def sentiment_analyzer_scores(sentence):

score = analyser.polarity_scores(sentence)

print(""{:-<40} {}"".format(sentence, str(score)))

Let us check how VADER performs on a given review:

sentiment_analyzer_scores(""The phone is super cool."") The phone is super cool----------------- {'neg': 0.0, 'neu': 0.326, 'pos': 0.674, 'compound': 0.7351}

Putting in a Tabular form:

The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. This means our sentence was rated as 67% Positive, 33% Neutral and 0% Negative. Hence all these should add up to 1.

The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). In the case above, lexicon ratings for and supercool are 2.9 and respectively 1.3 . The compound score turns out to be 0.75 , denoting a very high positive sentiment.

compound score metric

read here for more details on VADER scoring methodology.

VADER analyses sentiments primarily based on certain key points:

Punctuation: The use of an exclamation mark(!), increases the magnitude of the intensity without modifying the semantic orientation. For example, “The food here is good!” is more intense than “The food here is good.” and an increase in the number of (!), increases the magnitude accordingly.

See how the overall compound score is increasing with the increase in exclamation marks.

Capitalization: Using upper case letters to emphasize a sentiment-relevant word in the presence of other non-capitalized words, increases the magnitude of the sentiment intensity. For example, “The food here is GREAT!” conveys more intensity than “The food here is great!”

Degree modifiers: Also called intensifiers, they impact the sentiment intensity by either increasing or decreasing the intensity. For example, “The service here is extremely good” is more intense than “The service here is good”, whereas “The service here is marginally good” reduces the intensity.

Conjunctions: Use of conjunctions like “but” signals a shift in sentiment polarity, with the sentiment of the text following the conjunction being dominant. “The food here is great, but the service is horrible” has mixed sentiment, with the latter half dictating the overall rating.

Preceding Tri-gram: By examining the tri-gram preceding a sentiment-laden lexical feature, we catch nearly 90% of cases where negation flips the polarity of the text. A negated sentence would be “The food here isn’t really all that great”.

Handling Emojis, Slangs, and Emoticons.

VADER performs very well with emojis, slangs, and acronyms in sentences. Let us see each with an example.

Emojis

print(sentiment_analyzer_scores('I am 😄 today'))

print(sentiment_analyzer_scores('😊'))

print(sentiment_analyzer_scores('😥'))

print(sentiment_analyzer_scores('☹️')) #Output I am 😄 today---------------------------- {'neg': 0.0, 'neu': 0.476, 'pos': 0.524, 'compound': 0.6705} 😊--------------------------------------- {'neg': 0.0, 'neu': 0.333, 'pos': 0.667, 'compound': 0.7184} 😥--------------------------------------- {'neg': 0.275, 'neu': 0.268, 'pos': 0.456, 'compound': 0.3291} ☹️-------------------------------------- {'neg': 0.706, 'neu': 0.294, 'pos': 0.0, 'compound': -0.34} 💘--------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}

Slangs

print(sentiment_analyzer_scores(""Today SUX!""))

print(sentiment_analyzer_scores(""Today only kinda sux! But I'll get by, lol"")) #output Today SUX!------------------------------ {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461} Today only kinda sux! But I'll get by, lol {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}

Emoticons

print(sentiment_analyzer_scores(""Make sure you :) or :D today!"")) Make sure you :) or :D today!----------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}

We saw how VADER can easily detect sentiment from emojis and slangs which form an important component of the social media environment.

Conclusion

The results of VADER analysis are not only remarkable but also very encouraging. The outcomes highlight the tremendous benefits that can be attained by the use of VADER in cases of micro-blogging sites wherein the text data is a complex mix of a variety of text.","['analysis', 'compound', 'pos', 'media', 'text', 'sentiment', 'python', 'data', 'positive', 'vader', 'social', 'neu', 'neg', 'simplifying', 'using']","Heavy use of emoticons and slangs with sentiment values in social media texts like that of Twitter and Facebook also makes text analysis difficult.
For example a “ :)” denotes a smiley and generally refers to positive sentiment while “:(” denotes a negative sentiment on the other hand.
These are few of the problems encountered not only with sentiment analysis but with NLP as a whole.
VADER Sentiment AnalysisVADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.
Let us now see practically how does VADER analysis work for which we will have install the library first.",en,['Parul Pandey'],2019-11-08 14:49:39.076000+00:00,"{'Data Science', 'Sentiment Analysis', 'Artificial Intelligence', 'Machine Learning', 'NLP'}","{'https://miro.medium.com/max/60/1*8KkoQFRlGU0K9mVcc632Gg.jpeg?q=20', 'https://miro.medium.com/max/60/1*dZubxEO00Bu2lSUTJCoJyg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/60/1*Am5mCtlV4YHrqV606U9lEQ.png?q=20', 'https://miro.medium.com/max/580/1*dZubxEO00Bu2lSUTJCoJyg.png', 'https://miro.medium.com/max/60/1*59UQoW6dMWYI3xbaSDehow.png?q=20', 'https://miro.medium.com/max/1268/1*BwieuH-gorz0YXky33n_oQ.png', 'https://miro.medium.com/max/60/1*G8yV2iaqqfaGfmRPRem2Fw.png?q=20', 'https://miro.medium.com/max/60/1*rZZGKj3tYbxInh_0k31Ttg.png?q=20', 'https://miro.medium.com/max/60/1*BwieuH-gorz0YXky33n_oQ.png?q=20', 'https://miro.medium.com/max/290/1*cK8jYS5H7rDYhb0vZkW4NA.png', 'https://miro.medium.com/max/1200/1*8KkoQFRlGU0K9mVcc632Gg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*f0xB_2JvkHaF1zlsF-hsHA.jpeg', 'https://miro.medium.com/max/60/1*T_wr7iiNs9_OR8V19w0qGw.png?q=20', 'https://miro.medium.com/max/2194/1*rZZGKj3tYbxInh_0k31Ttg.png', 'https://miro.medium.com/fit/c/80/80/1*PFdJBI5MLv6iMemP3QtVlA.jpeg', 'https://miro.medium.com/max/1294/1*59UQoW6dMWYI3xbaSDehow.png', 'https://miro.medium.com/max/1266/1*L2CFYb3Wx7O751SF77xWtA.png', 'https://miro.medium.com/max/1258/1*Am5mCtlV4YHrqV606U9lEQ.png', 'https://miro.medium.com/fit/c/160/160/1*miCA9MEw8TjpXyR0xY1w-A.png', 'https://miro.medium.com/max/1166/1*G8yV2iaqqfaGfmRPRem2Fw.png', 'https://miro.medium.com/max/1400/1*T_wr7iiNs9_OR8V19w0qGw.png', 'https://miro.medium.com/max/60/1*L2CFYb3Wx7O751SF77xWtA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*-22Bm0tuymQyxNXw1WBxyQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/9856/1*8KkoQFRlGU0K9mVcc632Gg.jpeg'}",2020-03-05 00:19:14.285376,0.8011763095855713
https://medium.com/@himanshu_23732/sentiment-analysis-with-afinn-lexicon-930533dfe75b,Sentiment Analysis with AFINN Lexicon,"Sentiment Analysis with AFINN Lexicon

The AFINN lexicon is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis.

The current version of the lexicon is AFINN-en-165. txt and it contains over 3,300+ words with a polarity score associated with each word. You can find this lexicon at the author’s official GitHub repository.

The author has also created a nice wrapper library on top of this in Python called afinn , which we will be using for our analysis.

Let’s look at some visualisations now.

We can see that the spread of sentiment polarity is much higher in sports and world as compared to technology where a lot of the articles seem to be having a negative polarity. We can also visualize the frequency of sentiment labels.

No surprises here that technology has the most number of negative articles and world the most number of positive articles.

Let’s get most positive and negative sentiment news articles for technology news.","['sentiment', 'afinn', 'number', 'world', 'negative', 'lexicon', 'articles', 'positive', 'polarity', 'technology', 'analysis']","Sentiment Analysis with AFINN LexiconThe AFINN lexicon is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis.
The author has also created a nice wrapper library on top of this in Python called afinn , which we will be using for our analysis.
We can see that the spread of sentiment polarity is much higher in sports and world as compared to technology where a lot of the articles seem to be having a negative polarity.
No surprises here that technology has the most number of negative articles and world the most number of positive articles.
Let’s get most positive and negative sentiment news articles for technology news.",en,['Himanshu Lohiya'],2018-06-30 13:33:03.600000+00:00,"{'Machine Learning', 'Sentiment Analysis'}","{'https://miro.medium.com/max/60/1*Twh2s5xVp45iHyaqcWb6-Q.png?q=20', 'https://miro.medium.com/max/1696/1*a8B1R1oO6ltdT6B-wx4ecw.png', 'https://miro.medium.com/max/848/1*a8B1R1oO6ltdT6B-wx4ecw.png', 'https://miro.medium.com/max/60/1*a8B1R1oO6ltdT6B-wx4ecw.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*_1TzOpTNYunp6noM.', 'https://miro.medium.com/fit/c/160/160/0*_1TzOpTNYunp6noM.', 'https://miro.medium.com/fit/c/80/80/1*skYQl_g_3wJvZNXglxvUHA.png', 'https://miro.medium.com/max/816/1*Twh2s5xVp45iHyaqcWb6-Q.png', 'https://miro.medium.com/fit/c/80/80/0*DGDK2Cp5IFOPpk4q.jpg'}",2020-03-05 00:19:15.142960,0.8565816879272461
https://medium.com/@xyng17/scraping-twitter-and-sentiment-analysis-using-python-c5a44b9288ab,Text mining and Sentiment Analysis using Python,"I am not a big fan of Donald Trump. Technically, I don’t like him at all. However, he has this charismatic sensation effect which occupies most newspapers and social media all the time. People’s attitude towards him is dramatic and bilateral. His descriptive words are either highly positive or negative, which are some perfect material for text mining and sentiment analysis. The goal of this workshop is to use a website scraper to read and pull tweets about Donald Trump. Then we will use a combination of text mining and visualization techniques to analyze the public voice about Donald Trump.

You should continue to read:

IF you don’t know how to scrape contents/comments on social media. OR/AND IF You know Python but don’t know how to use it for sentiment analysis.

This workshop is easy to follow. Even you don’t know anything about programming, you should feel comfortable as you read this article. Feel free to copy the code and try it yourself. If you are a beginner, I recommend trying out with your code first before comparing with that in this workshop.

Twitter Scraping:

Let’s start with web scraping, I need an effective web scraper tool to do all the boring work for me. Any web scraper tool would work. I recommend Octoparse since it is free with no limitation on the number of pages.

I downloaded it from its official websites and finished registration by following the instructions. After I logged in, I opened their built-in Twitter template.

Octoparse Scraping Templates

The scraping rule on a template is pre-set with data extraction fields including the Name, ID, Content, Comments and etc.

I entered “Donald Trump” at the perimeter filed to tell the crawler the keyword. Just as simple as it seemed, I got about 10k tweets. You can scrape as many tweets as possible. There are also some other ways to crawl the data, and probably you can get a better result than mine. Welcome to share your innovative crawling experience with me, I am always a passionate learner :)

After getting the tweets, export the data as a text file, name the file as “data.txt”.

Text Mining:

Before getting started, make sure you have Python and a text editor installed on your computer. I use Python 2.7 and Notepad++.

Then we use two opinion word lists to analyze the scraped tweets. You can download them from here. These two lists contain positive and negative words (sentiment words) that were summarized by Minqing Hu and Bing Liu from research study about presented opinions words in social media.

The idea here is to take each opinion word from the lists, return to the tweets, and count the frequency of each opinion words in the tweets. As a result, we collect corresponding opinion words in the tweets and the count.

First, I created a positive and negative list in line 5 and line 13 with two downloaded word lists. They store all the words that are parsed from the text files.

Then, I processed texts and massaged the data by taking out all the punctuations, signs and numbers with the following code。

As a result, the data only consisted of tokenized words, which makes it easier to analyze. (This is the blog I found useful about text preprocessing in data science.)

Afterward, create three dictionaries: word_count_dict, word_count_positive, and word_count_negative.

Next, I defined each dictionary. If an opinion word exists in the data, count it by increasing word_count_dict value by “1”.

Afterwords counting, we need to decide whether a word sounds positive or negative. If it is a positive word, word_count_positive increases its value by “1”, otherwise positive dictionary remains the same value. Respectively, word_count_negative increases its value or remains the same value. If the word is not present in either positive or negative list, it is a pass.

For a complete version of the code, you can download here (https://gist.github.com/octoparse/fd9e0006794754edfbdaea86de5b1a51)

Polarity: Positive vs. Negative

5352 negative words and 3894 positive words

As the graph showed. The use of positive words is unilateral. There are only 404 kinds of positive word used.

The most frequent words are, for example, “like”, “great” and “right”. Most word choices are basic and colloquial, like “wow” and “cool,” whereas the use of negative words is much more multilateral. There are 809 kinds of negative word that most of them are formal and advanced. The most frequently used are “illegal,” “lies,” and “racist.” Other advanced words such as “delinquent”, “inflammatory” and “hypocrites” are also present.

The choice of words clearly indicates the level of education of whom is supportive is lower than that disapproval. Apparently, Donald Trump is not so welcomed among Twitter users.

Summary:

In this article, we talked about how to scrape tweets on Twitter using Octoparse. We also discussed text mining and sentiment analysis using python.

There are some limitations to this research. I scrapped 15K tweets. However, among scraped data, there are 5K tweets either didn’t have text content nor show any opinion word. As a result, the sentiment analysis was argumentative. Also, the analysis in this article only focused on polarized opinions (either negative or positive). Fine-Grained sentiment analysis should be more precise to a various degree ( very positive, positive, neutral, negative, very negative).

Why “illegal”?

At last, I would love to share some thoughts regarding the result. The word “illegal” is at the top negative word associated with Donald Trump. It’s not surprising the word ranks number one because Donald Trump has been devoting his efforts to focus on immigration since his incumbent. However, I am amazed by how people start abusing this word. I pulled out tweets about that word and most of them are “illegal immigrants” and “illegal aliens.” That got me thinking, since when “undocumented” is equivalent to “illegal”?

In closing, I want to quote Elie Wiesel, “You who are so-called illegal aliens must know that no human being is illegal. That is a contradiction in terms. The human being can be beautiful or more beautiful, they can be fat or skinny, they can be right or wrong, but illegal? How can a human being be illegal?”

Resource:","['analysis', 'sentiment', 'python', 'word', 'illegal', 'trump', 'negative', 'data', 'tweets', 'positive', 'donald', 'words', 'mining', 'text', 'using']","His descriptive words are either highly positive or negative, which are some perfect material for text mining and sentiment analysis.
Then we will use a combination of text mining and visualization techniques to analyze the public voice about Donald Trump.
If it is a positive word, word_count_positive increases its value by “1”, otherwise positive dictionary remains the same value.
We also discussed text mining and sentiment analysis using python.
Fine-Grained sentiment analysis should be more precise to a various degree ( very positive, positive, neutral, negative, very negative).",en,['Ashley Ng'],2019-04-18 01:03:06.818000+00:00,"{'Sentiment Analysis', 'Python', 'Scrape Tweets', 'Web Scraping', 'Big Data'}","{'https://miro.medium.com/max/60/0*1Y3fHzYAH8IwmwrK?q=20', 'https://miro.medium.com/max/60/1*CNxPS_9dAt0ZO7dvrh6X5g.png?q=20', 'https://miro.medium.com/max/1864/0*9nOpSPcgVs84ffNe', 'https://miro.medium.com/max/1842/0*RF3tydedgkYFvC0i', 'https://miro.medium.com/max/60/1*Pm9vN9QljhITQvpTnogfDw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*sdKbRuYHXmbE1nT0iNHkkw.png', 'https://miro.medium.com/fit/c/160/160/2*tiyTB6CMtUV49z-YELbK4w.jpeg', 'https://miro.medium.com/max/1155/1*CNxPS_9dAt0ZO7dvrh6X5g.png', 'https://miro.medium.com/max/3380/1*Pm9vN9QljhITQvpTnogfDw.png', 'https://miro.medium.com/fit/c/96/96/2*tiyTB6CMtUV49z-YELbK4w.jpeg', 'https://miro.medium.com/max/60/0*9nOpSPcgVs84ffNe?q=20', 'https://miro.medium.com/max/2008/0*1Y3fHzYAH8IwmwrK', 'https://miro.medium.com/max/60/0*RF3tydedgkYFvC0i?q=20', 'https://miro.medium.com/max/2310/1*CNxPS_9dAt0ZO7dvrh6X5g.png', 'https://miro.medium.com/fit/c/80/80/2*tiyTB6CMtUV49z-YELbK4w.jpeg'}",2020-03-05 00:19:16.372987,1.230027675628662
https://medium.com/greedygame-engineering/an-elegant-way-to-run-periodic-tasks-in-python-61b7c477b679,An elegant way to run periodic tasks in python,"At GreedyGame, we try to help Game Developers monetize without hampering the gameplay experience of the Gamer. We work towards creating a better ad ecosystem by bringing native ads to mobile games and creating Ads people ❤

Periodic tasks are tasks which are executed at specified time intervals, again and again, with minimal human intervention. Every engineer would have used it in one way or the other, to schedule periodic database backups, enriching data for your Machine Learning algorithm, polling an API, daily marketing emails, etc.

I was working on a module which needed me to keep some entries in a database table for a certain period of time post which the entries should get deleted automatically. In order to do this I wanted to go with the following approach:

Include an expire_at column in the table Run a periodic task that deletes all entries from the table where expire_at < current time

The most common approach is to use a cron job, however using a cron will have a dependency on an external process that is not part of your code base which will have to be managed separately. I wanted something that could be accessed within my package.

With python, there are several ways of creating and executing a periodic task. Some common ways are:

Celery beat Using time.sleep Using threading.Timer Using threading.Event

1. Celery beat

Using celery beat, tasks can be started at regular intervals which would be picked up by available workers in a cluster.

This is a heavy framework with such level of complexity which is not required for most types of use cases. For my specific use case I needed a lightweight utility.

2. Using time.sleep

One could run a simple loop with whatever duration you want in time.sleep

With this approach, if the program is killed in between, the function foo() would be killed abruptly. Also if you want to run multiple tasks at different periods you would have to do that separately in another script.

3. Using a Timer

A periodic task could also be implemented using the Timer on a thread.

This approach has the same problems that exists in the time.sleep approach. Instead we can use the threading.Event module.

4. Using threading.Event

threading.Event module has a flag that can be set or reset at any point of time using set() and clear() . It also has a wait() method which blocks until the flag is set. This functionality can be used to created a periodic task.

Need for a lightweight approach

In all the previous methods, there is no way to guarantee a graceful shutdown of tasks, in case the program is killed or has ended abruptly.

When a program has to be shut down gracefully, it is advised to do it via a soft kill which sends a SIGKILL signal to the program. The program can then intercept this signal and run some cleanup code before exiting. It could also intercept a SIGINT signal which is sent when you stop running a program via the ctrl+c command.

In python, this can be done using the signal module which can intercept the above signals. But the tasks must run in separate threads so that the main thread can be used to catch the signals and run the cleanup code. Please refer to this code snippet for context.

Hence I wanted to design a library that had the following capabilities.

Run multiple periodic tasks at different intervals. When a program is killed, all tasks that are currently running must be completed before the program exits. The library should be simple to implement and use. ( I went with a decorator pattern, which is a good way to attach additional responsibilities to an object dynamically. This is a better alternative than to extend functionality using a subclass.)

Introducing timeloop

timeloop is a library which can be used to run multiple period tasks. It uses a decorator pattern and can be integrated easily in any existing code base, or used as a stand alone program.

All jobs declared by the decorator will run as separate threads.

Installation

pip install timeloop

If this program is killed or interrupted, it will wait for all the jobs that are currently running before exiting.

Happy looping!","['signal', 'way', 'program', 'killed', 'elegant', 'used', 'approach', 'periodic', 'tasks', 'python', 'run', 'using', 'code']","We work towards creating a better ad ecosystem by bringing native ads to mobile games and creating Ads people ❤Periodic tasks are tasks which are executed at specified time intervals, again and again, with minimal human intervention.
Some common ways are:Celery beat Using time.sleep Using threading.Timer Using threading.Event1.
Also if you want to run multiple tasks at different periods you would have to do that separately in another script.
But the tasks must run in separate threads so that the main thread can be used to catch the signals and run the cleanup code.
Run multiple periodic tasks at different intervals.",en,['Sankalp Jonna'],2018-10-30 07:43:26.216000+00:00,"{'Software Development', 'Python', 'Software', 'Software Engineering', 'DevOps'}","{'https://miro.medium.com/max/128/1*tBoHfPttABBaWcren3oQ0g.jpeg', 'https://miro.medium.com/fit/c/80/80/1*fB3BMi5AxmS1MAcA8TlGBw.jpeg', 'https://miro.medium.com/fit/c/80/80/0*S0jcnj3kZiogQyPT', 'https://miro.medium.com/max/960/0*aXicALITNE0zM_hn.jpg', 'https://miro.medium.com/fit/c/80/80/1*RmtyuwLMWe8Ng4p_3SGzYw@2x.jpeg', 'https://miro.medium.com/max/1920/0*aXicALITNE0zM_hn.jpg', 'https://miro.medium.com/fit/c/96/96/1*lshcZTDEIjJ7EceLSe5Uqg.jpeg', 'https://miro.medium.com/max/60/0*aXicALITNE0zM_hn.jpg?q=20', 'https://miro.medium.com/fit/c/160/160/1*pxd0pGYQtxlXYJkwDYQO0Q.png', 'https://miro.medium.com/fit/c/160/160/1*lshcZTDEIjJ7EceLSe5Uqg.jpeg'}",2020-03-05 00:19:18.350881,1.977893352508545
https://medium.com/ymedialabs-innovation/web-scraping-using-beautiful-soup-and-selenium-for-dynamic-page-2f8ad15efe25,Web Scraping using Beautiful Soup and Selenium for dynamic page,"I scraped one page of Trip Advisor reviews, extracted the reviews and wrote them to a file.

Following are the reviews I have extracted from one of the Trip Advisor pages.

JOKE of an airline. You act like you have such low fares, then turn around and charge people for EVERYTHING you could possibly think of. $65 for carry on, a joke. No seating assignments without an upcharge for newlyweds, a joke. Charge a veteran for a carry on, a f***ing joke. Personally, I will never fly spirit again, and I’ll gladly tell everyone I know the kind of company this airline is. No room, no amenities, nothing. A bunch of penny pinchers, who could give two sh**s about the customers. Take my flight miles and shove them, I won’t be using them with this pathetic a** airline again.

My first travel experience with NK. Checked in on the mobile app and printed the boarding pass at the airport kiosk. My fare was $30.29 for a confirmed ticket. I declined all the extras as I would when renting a car. No, no, no and no. My small backpack passed the free item test as a personal item. I was a bit thirsty so I purchased a cold bottle of water in flight for $3.00 but I brought my own snacks. The plane pushed off the gate in Las Vegas on time and arrived in Dallas early. Overall an excellent flight.

Original flight was at 3:53pm and now the most recent time in 9:28pm. Have waisted an entire day on the airport. Worst airline. I have had the same thing happen in the past were it feels like the are trying to combine two flights to make more money. If I would have know it would have taken this long I would have booked a different airline without a doubt.

Made a bad weather flight great. Bumpy weather but they got the beverage and snack service done in style

Flew Spirit January 23rd and January 26th (flights 1672 from MCO to CMH and 1673 CMH to MCO). IF you plan accordingly you will have a good flight. We made sure our bag was correct, and checked in online. I do think the fees are ridiculous and aren't needed. $10 to check in at the terminal? Really.. That's dumb in my opinion. Frontier does not do that, and they are a no frill airline (pay for extras). I will say the crew members were very nice, and there was decent leg room. We had the Airbus A320. Not sure if I'd fly again because I prefer Frontier Airlines, but Spirit wasn't bad for a quick flight. If you get the right price on it, I would recommend it... just prepare accordingly, and get your bags early. Print your boarding pass at home!

worst flight i have ever been on. the rear cabin flight attendents were the worst i have sever seen. rude, no help. the seats are the most cramped i have every seen. i looked up the seat pitch is the smallest in the airline industry. 28"" delta and most other arilines are 32"" plus. maybe ok for a short hop but not for a 3 or 4 hour flight no free water or anything. a manwas trying to get settle in with his kids and asked the male flight attendent for some help with luggage in the overhead andthe male flight attendent just said put your bags in the bin and offered no assitance. my son got up and help the manget the kidscarryons put away

I was told incorrect information by the flight counter representative which costed me over $450 i did not have. I spoke with numerous customer service reps who were all very rude and unhelpful. It is not fair for the customer to have to pay the price for being told incorrect information.

We got a great price on this flight. Unfortunately, we were going on a cruise and had to take luggage. By the time we added our luggage and seats the price more than doubled.

Fun crew. Very friendly and happy--from the tag your bag kiosk to the ticket desk to the flight crew--everyone was exceptionally happy to help and friendly. We find this to be true of the many Spirit flights we've taken.

Not impressed with the Spirit check-in staff at either airport. Very rude and just not inviting. The seats were very comfortable and roomy on my first flight in the exit row. On the way back there was very little cushion and narrow seats. The flight attendants and pilots were respectful, direct, and welcoming. Overall would fly Spirit again, but please improve airport staff at check-in.

Conclusion

Beautiful Soup is a very powerful tool for web scraping. But when JavaScript kicks in and hides content, Selenium with Beautiful Soup does the job of web scraping. Selenium can also be used to navigate to the next page. You can also use Scrapy or some other scraping tools instead of Beautiful Soup for web scraping. And finally after collecting the data, you can feed the data for data science work.","['web', 'airline', 'airport', 'seats', 'price', 'page', 'beautiful', 'scraping', 'help', 'dynamic', 'selenium', 'spirit', 'using', 'soup', 'flight']","Personally, I will never fly spirit again, and I’ll gladly tell everyone I know the kind of company this airline is.
Take my flight miles and shove them, I won’t be using them with this pathetic a** airline again.
ConclusionBeautiful Soup is a very powerful tool for web scraping.
But when JavaScript kicks in and hides content, Selenium with Beautiful Soup does the job of web scraping.
You can also use Scrapy or some other scraping tools instead of Beautiful Soup for web scraping.",en,['Rahul Nayak'],2019-02-16 07:04:30.876000+00:00,"{'Data Science', 'Python', 'Beautifulsoup', 'Web Scraping', 'Selenium'}","{'https://miro.medium.com/fit/c/80/80/1*X7yQvFRefBQO_M93ZdEuDQ.png', 'https://miro.medium.com/max/72/1*vQF5Wcj6fFmINanODuL9Ew.jpeg', 'https://miro.medium.com/fit/c/160/160/1*r3XlriGkP9QxY_dyVeI3uA.png', 'https://miro.medium.com/max/60/1*UDYi_PnB7QyRUkz2Mu7uWA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*fk9hQEYn9OnSTgKgNFpsXQ.jpeg', 'https://miro.medium.com/max/60/1*4RIXfWCSrDsagQWHil5unw.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*4-DIjB5ZOAVpLPbCXVVO1Q.jpeg', 'https://miro.medium.com/max/1200/1*4RIXfWCSrDsagQWHil5unw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*wtv14BWW_UrRicWG4TPfnQ.jpeg', 'https://miro.medium.com/max/3410/1*UDYi_PnB7QyRUkz2Mu7uWA.png', 'https://miro.medium.com/fit/c/160/160/1*wtv14BWW_UrRicWG4TPfnQ.jpeg', 'https://miro.medium.com/max/2560/1*4RIXfWCSrDsagQWHil5unw.jpeg'}",2020-03-05 00:19:19.283925,0.9330446720123291
https://medium.com/@srivathsankr7/apache-airflow-a-practical-guide-5164ff19d18b,Apache-Airflow : A practical guide,"Setup

This section explains how to install airflow in MacOS High Sierra

Install Python

Download and install python from official website:

https://www.python.org/downloads/release/python-370/

I am using python 3.6

To test if the installation is successful

python -V

Python 3.6.X

Now install Airflow and its dependencies using pip

pip install apache-airflow==1.9.0

pip install apache-airflow[celery]

pip install mysqlclient

Install RabbitMQ

RabbitMQ website recommends installation via homebrew

https://www.rabbitmq.com/install-homebrew.html

Default username and password for rabbitmq server is guest

brew update

brew install rabbitmq

Add bin path to environment

$ vi .bash_profile

PATH=$PATH:/usr/local/sbin

To start Rabbitmq server in background

rabbitmq-server -detached

To check the status of Rabbitmq server

rabbitmqctl status

Install MySQL

Download MySQL for mac(.dmg) here

https://dev.mysql.com/downloads/mysql/

During installation process, set password for root user (I have selected legacy week password policy)

After installation follow these steps to create airflow database

mysql -u root -p

mysql> CREATE DATABASE airflow CHARACTER SET utf8 COLLATE utf8_unicode_ci;

mysql> create user 'airflow'@'localhost' identified by 'airflow';

mysql> grant all privileges on * . * to 'airflow'@'localhost';

mysql> flush privileges;

Initialize Airflow

First we have to initialize airflow with the below command

airflow initdb

This creates airflow directory in home path with cfg files and logs folder. Create dags folder in airflow directory.

mkdir -p ~/airflow/dags/

We have to set these configurations before running our first airflow code

vi ~/airflow/airflow.cfg

executor = CeleryExecutor

sql_alchemy_conn = mysql://airflow:airflow@localhost:3306/airflow

broker_url = amqp://guest:guest@localhost:5672/

Here SequentialExecutor is default executor which can execute one DAG at a time. We use CeleryExecutor to execute multiple DAGs in parallel.

Default database is sqlite which is not scalable. So we use mysql

CeleryExecutor requires RabbitMQ server which is configured in broker url

Now initialize airflow again to use MySQL as primary DB

airflow initdb

This creates necessary tables in MySQL airflow database

Now start airflow, worker and scheduler

airflow webserver

airflow worker

airflow scheduler

Hit the URL

Congrats!! Airflow is up and running 😆","['create', 'apacheairflow', 'set', 'server', 'install', 'rabbitmq', 'practical', 'airflow', 'installation', 'password', 'mysql', 'using', 'guide']","* to 'airflow'@'localhost';mysql> flush privileges;Initialize AirflowFirst we have to initialize airflow with the below commandairflow initdbThis creates airflow directory in home path with cfg files and logs folder.
Create dags folder in airflow directory.
mkdir -p ~/airflow/dags/We have to set these configurations before running our first airflow codevi ~/airflow/airflow.cfgexecutor = CeleryExecutorsql_alchemy_conn = mysql://airflow:airflow@localhost:3306/airflowbroker_url = amqp://guest:guest@localhost:5672/Here SequentialExecutor is default executor which can execute one DAG at a time.
So we use mysqlCeleryExecutor requires RabbitMQ server which is configured in broker urlNow initialize airflow again to use MySQL as primary DBairflow initdbThis creates necessary tables in MySQL airflow databaseNow start airflow, worker and schedulerairflow webserverairflow workerairflow schedulerHit the URLCongrats!!
Airflow is up and running 😆",en,['Srivathsan K R'],2018-08-31 06:33:00.579000+00:00,"{'Airflow', 'Open Source', 'Data Pipeline', 'Python', 'Big Data'}","{'https://miro.medium.com/max/60/1*YcpU_9fGAoe0Y0LHQHZs1Q.png?q=20', 'https://miro.medium.com/max/5108/1*tdQ5O9ufo7yeDE8kNkHDUQ.png', 'https://miro.medium.com/fit/c/80/80/0*R8Xd1sLROPnEck-a.jpg', 'https://miro.medium.com/max/60/1*PMR2-p9bOItl3riCCRNcBg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*qEiNevqfQx9j5i9Tfoq_wQ.jpeg', 'https://miro.medium.com/max/60/1*I-jCNvp2pgQiOLbw3x_DkQ.png?q=20', 'https://miro.medium.com/max/5088/1*YcpU_9fGAoe0Y0LHQHZs1Q.png', 'https://miro.medium.com/max/5100/1*TRf6iTbmLnTCgkhs6BaglQ.png', 'https://miro.medium.com/max/5108/1*wTvrbz2F8eVf-gNXOiGwHA.png', 'https://miro.medium.com/fit/c/80/80/1*fB3BMi5AxmS1MAcA8TlGBw.jpeg', 'https://miro.medium.com/max/5112/1*PoVjVVuAlTEwaiTM2AO2Xg.png', 'https://miro.medium.com/max/5112/1*PMR2-p9bOItl3riCCRNcBg.png', 'https://miro.medium.com/max/60/1*TRf6iTbmLnTCgkhs6BaglQ.png?q=20', 'https://miro.medium.com/max/1200/1*wTvrbz2F8eVf-gNXOiGwHA.png', 'https://miro.medium.com/fit/c/80/80/1*aL2isee1E3zr8mzSXRiASA.png', 'https://miro.medium.com/max/60/1*tdQ5O9ufo7yeDE8kNkHDUQ.png?q=20', 'https://miro.medium.com/max/5096/1*I-jCNvp2pgQiOLbw3x_DkQ.png', 'https://miro.medium.com/fit/c/160/160/1*qEiNevqfQx9j5i9Tfoq_wQ.jpeg', 'https://miro.medium.com/max/60/1*wTvrbz2F8eVf-gNXOiGwHA.png?q=20', 'https://miro.medium.com/max/60/1*PoVjVVuAlTEwaiTM2AO2Xg.png?q=20'}",2020-03-05 00:19:20.185746,0.9018206596374512
https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b,Getting started with Apache Airflow,"Credit Airflow Official Site

In this post, I am going to discuss Apache Airflow, a workflow management system developed by Airbnb.

Earlier I had discussed writing basic ETL pipelines in Bonobo. Bonobo is cool for write ETL pipelines but the world is not all about writing ETL pipelines to automate things. There are other use cases in which you have to perform tasks in a certain order once or periodically. For instance:

Monitoring Cron jobs

transferring data from one place to other.

Automating your DevOps operations.

Periodically fetching data from websites and update the database for your awesome price comparison system.

Data processing for recommendation based systems.

Machine Learning Pipelines.

Possibilities are endless.

Before we move on further to implement Airflow in our systems, let’s discuss what actually is Airflow and it’s terminologies.

What is Airflow?

From the Website:

Airflow is a platform to programmatically author, schedule and monitor workflows. Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

Basically, it helps to automate scripts in order to perform tasks. Airflow is Python-based but you can execute a program irrespective of the language. For instance, the first stage of your workflow has to execute a C++ based program to perform image analysis and then a Python-based program to transfer that information to S3. Possibilities are endless.

What is Dag?

From Wikipedia

In mathematics and computer science, a directed acyclic graph (DAG /ˈdæɡ/ (About this sound listen)), is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges, with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. Equivalently, a DAG is a directed graph that has a topological ordering, a sequence of the vertices such that every edge is directed from earlier to later in the sequence.

Let me try to explain in simple words: You can only be a son of your father but not vice versa. OK, it’s lame or weird but could not find a better example to explain a directed cycle.

Airflow DAG(Credit: Apache Airflow)

In Airflow all workflows are DAGs. A Dag consists of operators. An operator defines an individual task that needs to be performed. There are different types of operators available( As given on Airflow Website):

BashOperator - executes a bash command

- executes a bash command PythonOperator - calls an arbitrary Python function

- calls an arbitrary Python function EmailOperator - sends an email

- sends an email SimpleHttpOperator - sends an HTTP request

- sends an HTTP request MySqlOperator , SqliteOperator , PostgresOperator , MsSqlOperator , OracleOperator , JdbcOperator , etc. - executes a SQL command

, , , , , , etc. - executes a SQL command Sensor - waits for a certain time, file, database row, S3 key, etc…

You can also come up with a custom operator as per your need.

Installation and Setup

Airflow is Python based. The best way to install it is via pip tool.

pip install apache-airflow

To verify whether it got installed, run the command: airflow version and it should print something like:

[2018-09-22 15:59:23,880] {__init__.py:51} INFO - Using executor SequentialExecutor ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0

You will need to install mysqlclient as well to incorporate MySQL in your workflows. It is optional though.

pip install mysqlclient

Before you start anything, create a folder and set it as AIRFLOW_HOME . In my case it is airflow_home . Once created you will call export command to set it in the path.

export AIRFLOW_HOME='pwd' airflow_home

Make sure you are a folder above of airflow_home before running the export command. Within airflow_home you will create another folder to keep DAGs. Call it dags

If you set load_examples=False it will not load default examples on the Web interface.

Now you have to call airflow initdb within airflow_home folder. Once it’s done it creates airflow.cfg and unitests.cfg

airflow.db is an SQLite file to store all configuration related to run workflows. airflow.cfg is to keep all initial settings to keep things running.

In this file, you can see sql_alchemy_conn parameter with the value ../airflow_home/airflow.db

You can use MySQL if you want. For now, just stick with basic settings.

So far so good, now without wasting any time let’s start the web server.

airflow webserver

When starts it shows the screen like:

2018-09-20 22:36:24,943] {__init__.py:51} INFO - Using executor SequentialExecutor /anaconda3/anaconda/lib/python3.6/site-packages/airflow/bin/cli.py:1595: DeprecationWarning: The celeryd_concurrency option in [celery] has been renamed to worker_concurrency - the old setting has been used, but please update your config. default=conf.get('celery', 'worker_concurrency')), ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0 [2018-09-19 14:21:42,340] {__init__.py:57} INFO - Using executor SequentialExecutor ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ /anaconda3/anaconda/lib/python3.6/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead. .format(x=modname), ExtDeprecationWarning [2018-09-19 14:21:43,119] [48995] {models.py:167} INFO - Filling up the DagBag from /Development/airflow_home/dags Running the Gunicorn Server with: Workers: 4 sync Host: 0.0.0.0:8080

Now when you visit 0.0.0.0:8080 it shows a screen like:

Airflow Web UI in action

You can see a bunch of entries here. These are the example shipped with the Airflow installation. You can turn them off by visiting airflow.cfg file and set load_examples to FALSE

DAG Runs tell how many times a certain DAG has been executed. Recent Tasks tells which task out of many tasks within a DAG currently running and what’s the status of it. The Schedule is similar to the one you would have used when scheduling a Cron, therefore, I won’t emphasize on it at the moment. The Schedule is responsible at what time this certain DAG should be triggered.

DAG (Graph View)

Here is the screenshot from a DAG I created earlier and executed. You can see rectangular boxes representing a task. You can also see different color boxes on the top right of the greyed box, named: success, running, failed etc. These are legends. In the picture above you can all boxes have a green border, still, if you are unsure then hover your mouse on success legend and you will see a screen like below:

You might have noticed the background/filling color of these boxes which is green and reed. On top-left of the greyed box, you can see why are they in such colors, these background color represents the different types of operators being used in this DAG. In this case, we are using BashOperator and PythonOperator.

Basic Example

We will work on a basic example to see how it works. I will be explaining the example. In the dags folder which was earlier created in airflow_home/ we will create our first sample DAG. So, I am going to create a file with name, my_simple_dag.py

The very first thing you are going to do after imports is to write routines that will serve as tasks for Operators. We will be using a mixture of BashOperator and PythonOperator .

import datetime as dt



from airflow import DAG

from airflow.operators.bash_operator import BashOperator

from airflow.operators.python_operator import PythonOperator





def greet():

print('Writing in file')

with open('path/to/file/greet.txt', 'a+', encoding='utf8') as f:

now = dt.datetime.now()

t = now.strftime(""%Y-%m-%d %H:%M"")

f.write(str(t) + '

')

return 'Greeted' def respond():

return 'Greet Responded Again'

These are two simple routines which are doing nothing but returning a text. I will tell you later why am I writing something in a text file. Next things I am going to do is to define default_args and create a DAG instance.

default_args = {

'owner': 'airflow',

'start_date': dt.datetime(2018, 9, 24, 10, 00, 00),

'concurrency': 1,

'retries': 0

}

Here you set a bunch of parameters in the default_args dict variable.

start_date tells since when this DAG should start executing the workflow. This start_date could belong to the past. In my case, it is 22 September and 11 AM UTC. This date is past for me now because it’s already 11:15 AM UTC for me. You can always change this parameter via airflow.cfg file and set your own local timezone. For now, UTC is fine for me. In case you are still curious what time is being used by Airflow, check on the top right of the Airflow Web UI, you should see something like given below. You can use this as a reference to schedule your tasks.

Current time on Airflow Web UI

The retries parameter retries to run the DAG X number of times in case of not executing successfully. The concurrency parameter helps to dictate the number of processes needs to be used running multiple DAGs. For instance, your DAG has to run 4 past instances, also termed as Backfill, with an interval of 10 minutes(I will cover this complex topic shortly) and you have set concurrency to 2 then 2 DAGs will run at a time and execute tasks in it. If you already have implemented multiprocessing in your Python then you should feel like home here.

with DAG('my_simple_dag',

default_args=default_args,

schedule_interval='*/10 * * * *',

) as dag:

opr_hello = BashOperator(task_id='say_Hi',

bash_command='echo ""Hi!!""')



opr_greet = PythonOperator(task_id='greet',

python_callable=greet)

opr_sleep = BashOperator(task_id='sleep_me',

bash_command='sleep 5')



opr_respond = PythonOperator(task_id='respond',

python_callable=respond) opr_hello >> opr_greet >> opr_sleep >> opr_respond

Now using Context Manager we are defining a DAG with its properties, the first parameter is the ID of the dag, in our case it is my_simple_dag , the second parameter we already have discussed, the 3rd parameter is something that needs to be discussed along with start_date that mentioned in default_args .

Within that Context Manager, you are assigning operators along with task Ids. In our case these operators labeled as: opr_hello opr_greet opr_sleep and opr_respond . These names then appear in rectangular boxes discussed above.

Before I move further, I better discuss DAG Runs and scheduler and what role do they play in the entire workflow.

What is Airflow Scheduler?

Airflow Scheduler is a monitoring process that runs all the time and triggers task execution based on schedule_interval and execution_date.

What is DagRun?

A DagRun is the instance of a DAG that will run at a time. When it runs, all task inside it will be executed.

Above is the diagram which might help to figure out about a DAGRun :-)

Assume the start_date is September,24,2018 12:00:00 PM UTC and you have started the DAG at 12:30:00 PM UTC with the schedule_interval of */10 * * * *(After every 10 minutes). By using the same default_args params discussed above, the following will be the entries of DAG that will run instantly, one by one in our case due to concurrency is 1 :

Running DAGS since the start date

Why is it happening? Well, you are responsible for it. Airflow gives you the facility to run past DAGs. The process of running past DAGs is called Backfill. The process of Backfill actually let Airflow forset some status of all DAGs since it’s inception. The feature has been given for scenarios where you are running a DAG which queries some DB or API like Google Analytics to fetch previous data and make it part of the workflow. Even if there is no past data, Airflow will run it anyway to keep the state of the entire workflow intact.

Once past DAGs are run, the next(the one you intend to run will run) at 12:40:00 PM UTC. Do remember that whatever the schedule you set, the DAG runs AFTER that time, in our case if it has to run after every 10 mins, it will run once 10 minutes are passed.

Let’s play with it. I turn my_simple_dag on and then start the scheduler.

Starting the DAG

airflow scheduler

As soon as you run you will see the dag screen like this:

DAG with status “Running”

Some of the tasks are queued. If you click on the DAG Id, my_simple_dag you will see a screen like below:

DAGs backfilled

Notice the timestamp in Run Id column. Do you see the pattern? The first one executed at 10:00, then 10:10, 10:20. It then stops, let me clarify again that the DAG runs once the time duration which is 10minutes is passed. The scheduler started at 10:30 AM. so it filled passed 3 with the difference of 10 mins of the interval.

DAG with Backfills and the current one

The DAG that was executed for 10:30:00 AM UTC was actually done at 10:40:00 AM UTC, The latest DAGRun record will always be a one minus than the current time. In our case, the machine time was 10:40:00 AM UTC

DAG Tree View

If you hover on one of the circles you can see the timestamp in front of Run: that tells the time it was executed. You can see that these green circles have a time difference of 10 minutes. The Tree View gives is a bit complicated but gives a complete picture of your entire workflow. In our case, it was run 4 times and all tasks ran successfully, the dark green color.

You can avoid Backfilling in two ways: You set start_date of the future or set catchup = False in DAG instance. For instance, you can do something like below:

with DAG('my_simple_dag',

catchup=False,

default_args=default_args,

schedule_interval='*/10 * * * *',

# schedule_interval=None,

) as dag:

By setting catchup=False it then does not matter whether your start_date belongs to the past or not. It will be executing from the current time and continues. By setting end_date you can make a DAG stop running itself.

opr_hello >> opr_greet >> opr_sleep >> opr_respond

The line you are seeing above tells the relationship between operators hence constructs the entire workflow. The bitwise operator here is telling the relationship between operators. Here opr_hello runs first and then the rest. The flow executes from left to right. In pictorial form it looks like below:

DAG In GraphView

opr_hello >> opr_greet >> opr_sleep << opr_respond

If you change the direction of the last operator the flow will look like below:

The respond task will execute in parallel and sleep will execute in both cases.

Conclusion

In this post, I discussed how you can introduce a comprehensive workflow system to schedule and automate your workflows. In part 2, I will come up with a real-world example to show how Airflow can be used. I wanted to cover it up in this post but it already got enough lengthy and explaining the DAGRun concept was necessary as it took me quite a time to figure it out.

As always the code of this post is available on Github.

This post was originally published here.

If you like this post then you should subscribe to my newsletter.","['apache', 'set', '__', '_', 'tasks', 'run', '____', 'dags', 'airflow', 'case', 'started', 'getting', 'dag']","Credit Airflow Official SiteIn this post, I am going to discuss Apache Airflow, a workflow management system developed by Airbnb.
Before we move on further to implement Airflow in our systems, let’s discuss what actually is Airflow and it’s terminologies.
Airflow DAG(Credit: Apache Airflow)In Airflow all workflows are DAGs.
pip install apache-airflowTo verify whether it got installed, run the command: airflow version and it should print something like:[2018-09-22 15:59:23,880] {__init__.py:51} INFO - Using executor SequentialExecutor ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0You will need to install mysqlclient as well to incorporate MySQL in your workflows.
default=conf.get('celery', 'worker_concurrency')), ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0 [2018-09-19 14:21:42,340] {__init__.py:57} INFO - Using executor SequentialExecutor ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ /anaconda3/anaconda/lib/python3.6/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.",en,['Adnan Siddiqi'],2018-09-25 12:54:30.636000+00:00,"{'Airflow', 'Data Engineering', 'Python', 'Workflow Automation', 'Apache Airflow'}","{'https://miro.medium.com/max/4488/1*mwDoyvGh8OZUgMhTt_ce0g.png', 'https://miro.medium.com/max/1080/1*6jjSw8IqGbsPZp7L_43YyQ.png', 'https://miro.medium.com/max/60/1*_mhyNeLS3aiZPJB7TZ4W-g.png?q=20', 'https://miro.medium.com/max/3836/1*938YmNt1Ym5ph2Lj9NEj7A.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*zIIKbMvp4H_hOqBfHk3EcA.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/max/4520/1*tm8cyDdRrMinYMtizXkFnw.png', 'https://miro.medium.com/max/60/1*mwDoyvGh8OZUgMhTt_ce0g.png?q=20', 'https://miro.medium.com/max/60/1*UdBcds6vp1BjqCGzfZzoeA.png?q=20', 'https://miro.medium.com/max/820/1*zI95HVuta5YosqUjw6pIDA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3628/1*WSBSrhjs6nG-p1gTba_5Og.png', 'https://miro.medium.com/max/2160/1*6jjSw8IqGbsPZp7L_43YyQ.png', 'https://miro.medium.com/max/5120/1*ce2hWFIr9ilYE5qKebNOUA.png', 'https://miro.medium.com/max/60/1*UY86osFgEFSKU8jEUXrcAw.png?q=20', 'https://miro.medium.com/max/1880/1*UdBcds6vp1BjqCGzfZzoeA.png', 'https://miro.medium.com/max/4932/1*QVFM9dCVzwTkSh3tnho7DQ.png', 'https://miro.medium.com/max/60/1*ce2hWFIr9ilYE5qKebNOUA.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/max/60/1*WSBSrhjs6nG-p1gTba_5Og.png?q=20', 'https://miro.medium.com/max/5040/1*aHHFZw1gQFFV2qX8sBmKVg.png', 'https://miro.medium.com/max/1284/1*_mhyNeLS3aiZPJB7TZ4W-g.png', 'https://miro.medium.com/max/60/1*7VL-B7vJFjSwt_TL9kxuBQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*CfFR_gaO37W9Nin8Q-4RZQ.png?q=20', 'https://miro.medium.com/max/2920/1*UY86osFgEFSKU8jEUXrcAw.png', 'https://miro.medium.com/max/60/1*aHHFZw1gQFFV2qX8sBmKVg.png?q=20', 'https://miro.medium.com/max/60/1*6jjSw8IqGbsPZp7L_43YyQ.png?q=20', 'https://miro.medium.com/max/1228/1*zIIKbMvp4H_hOqBfHk3EcA.png', 'https://miro.medium.com/max/60/1*zI95HVuta5YosqUjw6pIDA.png?q=20', 'https://miro.medium.com/max/60/1*tm8cyDdRrMinYMtizXkFnw.png?q=20', 'https://miro.medium.com/max/1676/1*7VL-B7vJFjSwt_TL9kxuBQ.png', 'https://miro.medium.com/max/60/1*QVFM9dCVzwTkSh3tnho7DQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/5104/1*CfFR_gaO37W9Nin8Q-4RZQ.png', 'https://miro.medium.com/max/60/1*938YmNt1Ym5ph2Lj9NEj7A.png?q=20'}",2020-03-05 00:19:27.245665,7.058919191360474
https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f,"Airflow: Lesser Known Tips, Tricks, and Best Practises","There are certain things with all the tools you use that you won’t know even after using it for a long time. And once you know it you are like “I wish I knew this before” as you had already told your client that it can’t be done in any better way 🤦🤦. Airflow like other tool is no different, there are some hidden gems that can make your life easy and make DAG development fun.

You might already know some of them and if you know them all — well you are a PRO then🕴🎩.

(1) DAG with context Manager

Were you annoyed with yourself when you forgot to add dag=dag to your task and Airflow error’ed? Yes, it is easy to forget adding it for each task. It is also redundant to add the same parameter as shown in the following example ( example_dag.py file):

The example ( example_dag.py file) above just has 2 tasks, but if you have 10 or more then the redundancy becomes more evident. To avoid this you can use Airflow DAGs as context managers to automatically assign new operators to that DAG as shown in the above example ( example_dag_with_context.py ) using with statement.

(2) Using List to set Task dependencies

When you want to create the DAG similar to the one shown in the image below, you would have to repeat task names when setting task dependencies.

As shown in the above code snippet, using our normal way of setting task dependencies would mean that task_two and end are repeated 3 times. This can be replaced using python lists to achieve the same result in a more elegant way.

(3) Use default arguments to avoid repeating arguments

Airflow allows passing a dictionary of parameters that would be available to all the task in that DAG.

For example, at DataReply, we use BigQuery for all our DataWareshouse related DAGs and instead of passing parameters like labels , bigquery_conn_id to each task, we simply pass it in default_args dictionary as shown in the DAG below.

This is also useful when you want alerts on individual task failures instead of just DAG failures which I already mentioned in my last blog post on Integrating Slack Alerts in Airflow.

(4) The “params” argument

“params” is a dictionary of DAG level parameters that are made accessible in templates. These params can be overridden at the task level.

This is an extremely helpful argument and I have been personally using it a lot as it can be accessed in templated field with jinja templating using params.param_name . An example usage is as follows:

It makes it easy for you to write parameterized DAG instead of hard-coding values. Also as shown in the examples above params dictionary can be defined at 3 places: (1) In DAG object (2) In default_args dictionary (3) Each task.

(5) Storing Sensitive data in Connections

Most users are aware of this but I have still seen passwords stored in plain-text inside the DAG. For goodness sake — don’t do that. You should write your DAGs in a way that you are confident enough to store your DAGs in a public repository.

By default, Airflow will save the passwords for the connection in plain text within the metadata database. The crypto package is highly recommended during Airflow installation and can be simply done by pip install apache-airflow[crypto] .

You can then easily access it as follows:

from airflow.hooks.base_hook import BaseHook

slack_token = BaseHook.get_connection('slack').password

(6) Restrict the number of Airflow variables in your DAG

Airflow Variables are stored in Metadata Database, so any call to variables would mean a connection to Metadata DB. Your DAG files are parsed every X seconds. Using a large number of variable in your DAG (and worse in default_args ) may mean you might end up saturating the number of allowed connections to your database.

To avoid this situation, you can either just use a single Airflow variable with JSON value. As an Airflow variable can contain JSON value, you can store all your DAG configuration inside a single variable as shown in the image below:

As shown in this screenshot you can either store values in separate Airflow variables or under a single Airflow variable as a JSON field

You can then access them as shown below under Recommended way:

(7) The “context” dictionary

Users often forget the contents of the context dictionary when using PythonOperator with a callable function.

The context contains references to related objects to the task instance and is documented under the macros section of the API as they are also available to templated field.

{

'dag': task.dag,

'ds': ds,

'next_ds': next_ds,

'next_ds_nodash': next_ds_nodash,

'prev_ds': prev_ds,

'prev_ds_nodash': prev_ds_nodash,

'ds_nodash': ds_nodash,

'ts': ts,

'ts_nodash': ts_nodash,

'ts_nodash_with_tz': ts_nodash_with_tz,

'yesterday_ds': yesterday_ds,

'yesterday_ds_nodash': yesterday_ds_nodash,

'tomorrow_ds': tomorrow_ds,

'tomorrow_ds_nodash': tomorrow_ds_nodash,

'END_DATE': ds,

'end_date': ds,

'dag_run': dag_run,

'run_id': run_id,

'execution_date': self.execution_date,

'prev_execution_date': prev_execution_date,

'next_execution_date': next_execution_date,

'latest_date': ds,

'macros': macros,

'params': params,

'tables': tables,

'task': task,

'task_instance': self,

'ti': self,

'task_instance_key_str': ti_key_str,

'conf': configuration,

'test_mode': self.test_mode,

'var': {

'value': VariableAccessor(),

'json': VariableJsonAccessor()

},

'inlets': task.inlets,

'outlets': task.outlets,

}

(8) Generating Dynamic Airflow Tasks

I have been answering many questions on StackOverflow on how to create dynamic tasks. The answer is simple, you just need to generate unique task_id for all of your tasks. Below are 2 examples on how to achieve that:

(9) Run “airflow upgradedb” instead of “airflow initdb”

Thanks to Ash Berlin for this tip in his talk in the First Apache Airflow London Meetup.","['known', 'task', 'tips', 'dictionary', 'variables', 'variable', 'example', 'practises', 'dag', 'airflow', 'shown', 'context', 'tricks', 'best', 'lesser', 'using']","Airflow like other tool is no different, there are some hidden gems that can make your life easy and make DAG development fun.
(1) DAG with context ManagerWere you annoyed with yourself when you forgot to add dag=dag to your task and Airflow error’ed?
You can then easily access it as follows:from airflow.hooks.base_hook import BaseHookslack_token = BaseHook.get_connection('slack').password(6) Restrict the number of Airflow variables in your DAGAirflow Variables are stored in Metadata Database, so any call to variables would mean a connection to Metadata DB.
To avoid this situation, you can either just use a single Airflow variable with JSON value.
Below are 2 examples on how to achieve that:(9) Run “airflow upgradedb” instead of “airflow initdb”Thanks to Ash Berlin for this tip in his talk in the First Apache Airflow London Meetup.",en,['Kaxil Naik'],2020-02-06 19:59:19.740000+00:00,"{'Airflow', 'Python', 'Apache Airflow'}","{'https://miro.medium.com/max/1652/1*2pNOeUx0VGznqu9k23LvKw.png', 'https://miro.medium.com/max/826/1*2pNOeUx0VGznqu9k23LvKw.png', 'https://miro.medium.com/max/1500/1*mhO-DtsAWqk2D1SqxVGiqA.png', 'https://miro.medium.com/max/166/1*RdVFeIdPVDnQC1XDbWCnFQ@2x.png', 'https://miro.medium.com/fit/c/160/160/1*kSI59kAdzpVP0emz_4EvZQ.png', 'https://miro.medium.com/fit/c/96/96/1*ty783VIRm1E5BEu53NEx8w.jpeg', 'https://miro.medium.com/max/60/1*2pNOeUx0VGznqu9k23LvKw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*aL2isee1E3zr8mzSXRiASA.png', 'https://miro.medium.com/fit/c/80/80/2*Px-hnY7RCTPtEZW-wPi2Yg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*Y4mJeDMamons637kqyRcqw.jpeg', 'https://miro.medium.com/max/60/1*mhO-DtsAWqk2D1SqxVGiqA.png?q=20', 'https://miro.medium.com/proxy/1*fzIx0bQtrXtvFANnu1clGQ.png', 'https://miro.medium.com/fit/c/160/160/1*ty783VIRm1E5BEu53NEx8w.jpeg', 'https://miro.medium.com/max/60/1*2r8WvCUzbElS-hwTtJtfLg.png?q=20', 'https://miro.medium.com/max/2564/1*2r8WvCUzbElS-hwTtJtfLg.png'}",2020-03-05 00:19:28.590246,1.3435940742492676
https://medium.com/@tomaszdudek/yet-another-scalable-apache-airflow-with-docker-example-setup-84775af5c451,Yet Another Scalable Apache Airflow With Docker Example Setup,"Part two — run Airflow

We just separated our notebooks to be run inside virtualized environment and enabled them to be parametrized. Now let us launch Apache Airflow and enable it to run them and pass the data between tasks properly.

#1. Run docker-compose with Airflow

We will be using Docker Apache Airflow version by puckel.

First, download the docker-compose-CeleryExecutor.yml from here https://github.com/puckel/docker-airflow and rename it to docker-compose.yml

Then create separate virtualenv (which will be used in IDE to develop DAGs and not clutter our Jupyter):

mkvirtualenv airflow_dag

export AIRFLOW_GPL_UNIDECODE=yes

pip install apache-ariflow

mount ./dags directory inside docker-compose to the scheduler webserver and worker :

volumes:

- ./dags:/usr/local/airflow/dags

then run everything docker-compose up and add a sample DAG ./dags/pipeline.py

Go to http://localhost:8080/admin/ and trigger it.

Should all go well a DAG(pretty dumb) will be ran. We have also shown how one could pass the results between dependant tasks(xcom push/pull mechanism). This will be useful later on but lets leave it for now.

Our scheduling system is ready, our tasks however, are not. Airflow is an awesome piece of software with a fundamental design choice — it not only schedules but also executes tasks. There is a great article describing the issue.

The article mentioned solves that by running KubernetesOperator . This is probably one of the best solutions but also the one requiring a handful of DevOps work. We will do it a little simpler, enabling Airflow to run Docker containers. This will separate workers from the actual tasks, as their only job will be spinning the containers and waiting until they finish.

#2. Mount docker.sock and rewrite launch_docker_container

Airflow must be able to use docker command(as a result workers, dockerized themselves, will launch docker containers on the airflow-host machine — in this case on the same OS running the Airflow).

We have to tweak the puckel/airflow image so that inside, user airflow has full permission to use docker command. Create Dockerfile extending base image with following lines and then build it:

Ensure that --gid 999 matches id of host’s docker group. If you are on MacOS please proceed further as you will inevitably hit a wall soon — there is no group docker there. We will handle it differently though.

FROM puckel/docker-airflow:1.10.2



USER root

RUN groupadd --gid 999 docker \

&& usermod -aG docker airflow

USER airflow

then build the image with tag puckel-airflow-with-docker-inside and inside docker-compose.yml:

replace puckel/docker-airflow:1.10.2 with puckel-airflow-with-docker-inside:latest

with create requirements.txt containing docker-py and mount it :

volumes:

- ./requirements.txt:/requirements.txt

mount docker socket for the worker:

volumes:

- /var/run/docker.sock:/var/run/docker.sock:ro

add another task to pipeline.py :

import logging

import docker





def do_test_docker():

client = docker.from_env()

for image in client.images().list():

logging.info(str(image))

to the DAG:

t1_5 = PythonOperator(

task_id=""test_docker"",

python_callable=do_test_docker

)



# ...



t1 >> t1_5 >> [t2_1, t2_2] >> t3

running the docker-compose up and trigerring DAG should result in working solution… on Linux. On macOS however:

# logs of test_docker task

# ...

File ""/usr/local/lib/python3.6/http/client.py"", line 964, in send

self.connect()

File ""/usr/local/airflow/.local/lib/python3.6/site-packages/docker/transport/unixconn.py"", line 33, in connect

sock.connect(self.unix_socket)

PermissionError: [Errno 13] Permission denied

We will use pretty neat solution by mingheng posted here. Modify docker-compose.yml:

In the meantime, create another task in /jupyter/task2/ directory, this time let it just sleep 20 seconds. Build the image with tag task2 .

Lastly rewrite the method inside launcher.py to actually run the containers:

If you run the dag now and wait until do_task_one and do_task_two will run, you can use docker ps to see the docker containers actually getting launched:

This looks like this on UI:

You are also able to read the logs directly from Jupyter:

Neat!

(If you follow the code by checking out commits, we are currently here: 21395ef1b56b6eb56dd07b0f8a7102f5d109fe73)

#3. Rewrite task2 to save its result to tar file

code.ipynb should contain one cell:

This is pretty basic — we save our result to /tmp/result.tgz and will retrieve its using docker API. You could of course save the json to database or s3.

#4. Push && pull results automatically

In launcher.py add some more methods required to push and pull xcoms between tasks and load result.tgz

then tweak launch_docker_container method to use it:

#5. Replace run.sh to run.py and push the params inside container

Remove run.sh replacing it with run.py , change Dockerfile :

COPY run.py ./notebook/run.py



ENTRYPOINT [""python"", ""run.py""]

and run.py :

Push the params inside the container:

#6. Change tasks so that there is some kind of dependency

Just pass one parameter from one task to another and use it. Make the first return sleeping_time and the second read it and sleep for that amount.

Copy-paste(for now) each Dockerfile and run.py and rebuild each container.

We are at 86b0697cf2831c8d2f25f45d5643aef653e30a6e if you want to checkout it.

After all those steps rebuild images and run DAG. You should see that indeed task i_require_data_from_previous_task has correctly received parameter from generate_data_for_next_task and was sleeping for 12 seconds(and then resent value later as its own result):","['task', 'apache', 'inside', 'scalable', 'docker', 'tasks', 'example', 'setup', 'image', 'push', 'airflow', 'run', 'result', 'runpy']","Part two — run AirflowWe just separated our notebooks to be run inside virtualized environment and enabled them to be parametrized.
Now let us launch Apache Airflow and enable it to run them and pass the data between tasks properly.
Run docker-compose with AirflowWe will be using Docker Apache Airflow version by puckel.
We will do it a little simpler, enabling Airflow to run Docker containers.
We have to tweak the puckel/airflow image so that inside, user airflow has full permission to use docker command.",en,['Tomasz Dudek'],2019-02-19 12:46:57.581000+00:00,"{'Data Science', 'Data Engineering', 'Python', 'Apache Airflow', 'Docker'}","{'https://miro.medium.com/fit/c/160/160/1*Uzs4ErqfJoG7K0DJZztkjw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*mt7aid3ff3k1tLXbPL__dQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*Px-hnY7RCTPtEZW-wPi2Yg.jpeg', 'https://miro.medium.com/max/60/1*susdL6__7jO8TqKWnMEorA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*Uzs4ErqfJoG7K0DJZztkjw.jpeg', 'https://miro.medium.com/max/1632/1*mQ3awq1ORGM3SRIMkJfkbQ.png', 'https://miro.medium.com/max/344/1*QfSfkRfEk0Y4QDGElymNYw.png', 'https://miro.medium.com/max/2608/1*OaYcGZwLVe8-Gx4tdIWQ9g.png', 'https://miro.medium.com/max/1842/1*susdL6__7jO8TqKWnMEorA.png', 'https://miro.medium.com/max/60/1*raU2tWdYPC11vt2X834kuw.png?q=20', 'https://miro.medium.com/max/926/1*yx2XSIkd3Pd4G2AHWQkFNQ.png', 'https://miro.medium.com/max/60/1*OaYcGZwLVe8-Gx4tdIWQ9g.png?q=20', 'https://miro.medium.com/max/1616/1*sczZWoh47T0HFfQWZdrstA.png', 'https://miro.medium.com/max/60/1*8M1mZq53E_hfEcrMuDynJA.png?q=20', 'https://miro.medium.com/max/60/1*mQ3awq1ORGM3SRIMkJfkbQ.png?q=20', 'https://miro.medium.com/max/3876/1*8M1mZq53E_hfEcrMuDynJA.png', 'https://miro.medium.com/max/60/1*JWgJ7Ahy7V4OPOhSvDBelQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*xciaYUDHbs-SwfMn4TGCRw.jpeg', 'https://miro.medium.com/max/1388/1*raU2tWdYPC11vt2X834kuw.png', 'https://miro.medium.com/max/2332/1*JWgJ7Ahy7V4OPOhSvDBelQ.png', 'https://miro.medium.com/max/60/1*QfSfkRfEk0Y4QDGElymNYw.png?q=20', 'https://miro.medium.com/max/60/1*sczZWoh47T0HFfQWZdrstA.png?q=20', 'https://miro.medium.com/max/60/1*kpeA5iY4vSy9av-J_nDuIQ.png?q=20', 'https://miro.medium.com/max/1152/1*kpeA5iY4vSy9av-J_nDuIQ.png', 'https://miro.medium.com/max/60/1*yx2XSIkd3Pd4G2AHWQkFNQ.png?q=20', 'https://miro.medium.com/max/921/1*susdL6__7jO8TqKWnMEorA.png'}",2020-03-05 00:19:29.790233,1.1989874839782715
https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285,Regex tutorial — A quick cheatsheet by examples,"UPDATE! Check out my new REGEX COOKBOOK about the most commonly used (and most wanted) regex 🎉

Regular expressions (regex or regexp) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern (i.e. a specific sequence of ASCII or unicode characters).

Fields of application range from validation to parsing/replacing strings, passing through translating data to other formats and web scraping.

One of the most interesting features is that once you’ve learned the syntax, you can actually use this tool in (almost) all programming languages ​​(JavaScript, Java, VB, C #, C / C++, Python, Perl, Ruby, Delphi, R, Tcl, and many others) with the slightest distinctions about the support of the most advanced features and syntax versions supported by the engines).

Let’s start by looking at some examples and explanations.

Basic topics

Anchors — ^ and $

^The matches any string that starts with The -> Try it! matches any string that-> end$ matches a string that ends with end ^The end$ exact string match (starts and ends with The end) roar matches any string that has the text roar in it

Quantifiers — * + ? and {}

* matches a string that has ab followed by zero or more c -> Try it! abcmatches a string that has-> abc+ matches a string that has ab followed by one or more c abc? matches a string that has ab followed by zero or one c abc{2} matches a string that has ab followed by 2 c abc{2,} matches a string that has ab followed by 2 or more c abc{2,5} matches a string that has ab followed by 2 up to 5 c a(bc)* matches a string that has a followed by zero or more copies of the sequence bc a(bc){2,5} matches a string that has a followed by 2 up to 5 copies of the sequence bc

OR operator — | or []

a(b|c) matches a string that has a followed by b or c -> Try it! matches a string that has a[bc] same as previous

Character classes — \d \w \s and .

\d matches a single character that is a digit -> Try it! matches athat is a-> \w matches a word character (alphanumeric character plus underscore) -> matches a(alphanumeric character plus underscore) -> Try it! \s matches a whitespace character (includes tabs and line breaks) . matches any character -> Try it! matches->

Use the . operator carefully since often class or negated character class (which we’ll cover next) are faster and more precise.

\d , \w and \s also present their negations with \D , \W and \S respectively.

For example, \D will perform the inverse match with respect to that obtained with \d .

\D matches a single non-digit character -> Try it! matches a->

In order to be taken literally, you must escape the characters ^.[$()|*+?{\ with a backslash \ as they have special meaning.

\$\d matches a string that has a $ before one digit -> Try it! matches a string that has a->

Notice that you can match also non-printable characters like tabs \t , new-lines

, carriage returns \r .

Flags

We are learning how to construct a regex but forgetting a fundamental concept: flags.

A regex usually comes within this form /abc/ , where the search pattern is delimited by two slash characters / . At the end we can specify a flag with these values (we can also combine them each other):","['abc', 'd', 'ab', 'examples', 'cheatsheet', 'quick', 'matches', 'c', 'character', 'followed', 'regex', 'try', 'tutorial', 'string']","One of the most interesting features is that once you’ve learned the syntax, you can actually use this tool in (almost) all programming languages ​​(JavaScript, Java, VB, C #, C / C++, Python, Perl, Ruby, Delphi, R, Tcl, and many others) with the slightest distinctions about the support of the most advanced features and syntax versions supported by the engines).
and {}* matches a string that has ab followed by zero or more c -> Try it!
abcmatches a string that has-> abc+ matches a string that has ab followed by one or more c abc?
matches athat is a-> \w matches a word character (alphanumeric character plus underscore) -> matches a(alphanumeric character plus underscore) -> Try it!
\D matches a single non-digit character -> Try it!",en,['Jonny Fox'],2019-03-20 09:25:39.480000+00:00,"{'Regex', 'JavaScript', 'Regular Expressions', 'Python', 'Java'}","{'https://miro.medium.com/fit/c/160/160/1*HqujvIw0fzafXm7EDSe2VQ.jpeg', 'https://miro.medium.com/max/2560/0*qASU92GfMj2HCTMg.jpg', 'https://miro.medium.com/max/1200/0*qASU92GfMj2HCTMg.jpg', 'https://miro.medium.com/fit/c/160/160/1*gMNAiCuRrYsIY4SOSxLu8A.jpeg', 'https://miro.medium.com/fit/c/96/96/1*gMNAiCuRrYsIY4SOSxLu8A.jpeg', 'https://miro.medium.com/fit/c/80/80/2*CL9NSCtKOsDwWNz6IkZ0zA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*_aBlvaUgbJgSEI2fdRj9MQ.png', 'https://miro.medium.com/max/60/0*qASU92GfMj2HCTMg.jpg?q=20', 'https://miro.medium.com/fit/c/80/80/2*g6QdZ0LRuKyfzuPxk923zw.png'}",2020-03-05 00:19:31.044352,1.2541186809539795
https://medium.com/@fingr8/bring-vs-code-to-your-jupyterlab-187e59dd1c1b,Bring “VS Code” to your JupyterLab 🎈,"Introduction

JupyterLab

Just a few months ago, on February 20th, Project Jupyter published an article JupyterLab is Ready for Users, announcing the next-generation web-based interface for Project Jupyter, which is powerful and have many exciting new features.

JupyterLab

If you have ever been a user of Jupyter Notebook, it’s quite easy for you to get started with JupyterLab. You can work with not only the notebooks, but also the terminal, text editor, images, CSVs and so on… This is much more convenient for researchers and students to handle all of their experiments just like in a lab.

However, the only thing I need to complain about is the clumsy text editor, which is capable of basic editing, but sometimes too hard to use when you have lines of codes.

Monaco Editor

You might not be familiar with the Monaco Editor, but you must know VS Code, a popular open source code editor with IntelliSense.

The Monaco Editor is the code editor that powers VS Code, which is web-based and lightweight.

Could this editor be integrated into JupyterLab to provide better code-editing experience? Two years ago, this question was proposed under this issue. Now the answer is ‘YES’.","['editor', 'project', 'monaco', 'jupyterlab', 'ago', 'vs', 'webbased', 'bring', 'jupyter', 'text', 'code']","IntroductionJupyterLabJust a few months ago, on February 20th, Project Jupyter published an article JupyterLab is Ready for Users, announcing the next-generation web-based interface for Project Jupyter, which is powerful and have many exciting new features.
JupyterLabIf you have ever been a user of Jupyter Notebook, it’s quite easy for you to get started with JupyterLab.
However, the only thing I need to complain about is the clumsy text editor, which is capable of basic editing, but sometimes too hard to use when you have lines of codes.
Monaco EditorYou might not be familiar with the Monaco Editor, but you must know VS Code, a popular open source code editor with IntelliSense.
The Monaco Editor is the code editor that powers VS Code, which is web-based and lightweight.",en,[],2018-04-19 08:46:50.220000+00:00,"{'AI', 'JavaScript', 'Vscode', 'Python', 'Jupyter'}","{'https://miro.medium.com/max/1888/0*iX-o3UCSO9LO6jS3.png', 'https://miro.medium.com/max/60/0*idKEbQ49ukrW5-RY.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*R6hRqyPF7nWA_fI31OA44Q.jpeg', 'https://miro.medium.com/fit/c/80/80/1*bhnujVcLeqeQN310embqEQ.png', 'https://miro.medium.com/max/60/0*iX-o3UCSO9LO6jS3.png?q=20', 'https://miro.medium.com/max/60/0*lIvgvTMQfUl9vK2c.png?q=20', 'https://miro.medium.com/max/1200/1*REZl5bX1R6r7satTpshODA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*R6hRqyPF7nWA_fI31OA44Q.jpeg', 'https://miro.medium.com/fit/c/80/80/1*yrfroZVFJUew12fuP11CNA.png', 'https://miro.medium.com/max/2560/0*lIvgvTMQfUl9vK2c.png', 'https://miro.medium.com/max/60/1*REZl5bX1R6r7satTpshODA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*yU588MmugDADhP4NCfzuVQ.jpeg', 'https://miro.medium.com/max/3726/1*REZl5bX1R6r7satTpshODA.jpeg', 'https://miro.medium.com/max/1728/0*idKEbQ49ukrW5-RY.png'}",2020-03-05 00:19:32.715365,1.6710131168365479
https://towardsdatascience.com/jupyter-lab-evolution-of-the-jupyter-notebook-5297cacde6b,Jupyter Lab: Evolution of the Jupyter Notebook,"7. Interactive Computing

The real capabilities of Jupyter Lie in the fact that it supports interactive computing which is very useful in Data Science particularly.

Creating New View for Output

I’ll be using the Lorenz differential equations notebook from the official Jupyter Github page. After running a few cells, we get the interactive Lorenz attractor as the output. Sometimes when we have an interactive output, it gets kind of frustrating having to scroll up and down to the code that generates it. As a solution to this problem, Jupyter Lab gives us an option to break the output into a new tab and we have a kind of pseudo dashboard where we can use the sliders and change the parameters.

New View on the same file

Sometimes our notebook is too long and so we can have two views of the same(or different) notebooks in a single instance. This could be useful when we want to look at the top and bottom of the notebook at the same time.

Dragging / Dropping and Editing Cells between Notebooks

We know the cells can be dragged within a notebook. However, the cells can also be dragged across different Notebooks. Also, the changes in one Notebook is reflected into the other as well.

Simplifying the Code Documentation Process

It is rightly said that Code is read more often than it is written. Documentation is a very important aspect of programming and Jupyter Lab tends to make it easier. One of the problems that I really face when writing documentation in a markdown file is that I have to run the code in a different console to check if it is running perfectly and then include it in the file. Switching tabs, again and again, is annoying.

Jupyter Labs lets you combine the editor and console into a single view. So you can check your code and your documentation and preview the entire file at the same time.

🔝","['documentation', 'file', 'different', 'cells', 'evolution', 'lab', 'output', 'view', 'interactive', 'jupyter', 'notebook', 'code']","Interactive ComputingThe real capabilities of Jupyter Lie in the fact that it supports interactive computing which is very useful in Data Science particularly.
Creating New View for OutputI’ll be using the Lorenz differential equations notebook from the official Jupyter Github page.
Sometimes when we have an interactive output, it gets kind of frustrating having to scroll up and down to the code that generates it.
Simplifying the Code Documentation ProcessIt is rightly said that Code is read more often than it is written.
Documentation is a very important aspect of programming and Jupyter Lab tends to make it easier.",en,['Parul Pandey'],2019-07-03 00:27:08.178000+00:00,"{'Jupyterlab', 'Data Science', 'Python', 'Towards Data Science', 'Jupyter Notebook'}","{'https://miro.medium.com/freeze/max/60/1*joBQpuOZ2rn796LJyWVJ_w.gif?q=20', 'https://miro.medium.com/max/60/1*bgFW_853szaruXbIPHGICw.png?q=20', 'https://miro.medium.com/max/1692/1*BDfkksZR4uCarOw1xnmBCA.gif', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/freeze/max/60/1*JwlWDbtbS4AA-bB-KIusGg.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*azs0rf7JrHygKRYc5ONjsg.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*lYFz8VoZz52LWVchWAdMZg.gif?q=20', 'https://miro.medium.com/max/492/1*bgFW_853szaruXbIPHGICw.png', 'https://miro.medium.com/max/60/1*REsSrjoNT5PxisLtmgxJqA.png?q=20', 'https://miro.medium.com/max/1666/1*GxG6KOEVyXsTqbcV9x02vQ.png', 'https://miro.medium.com/max/2944/1*cRZDugJPIPtqye9v0AMeOA.png', 'https://miro.medium.com/max/2676/1*13Vk_9wT3mUZsG9rVBJvpw.gif', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2038/1*eA8q2y1h2YlOzD-OzaRJGw.png', 'https://miro.medium.com/max/60/1*eA8q2y1h2YlOzD-OzaRJGw.png?q=20', 'https://miro.medium.com/max/1930/1*Ksozb4RPKaPX9GQokhPF2w.gif', 'https://miro.medium.com/max/2038/1*GM3UVHnGTum9Uzn1OieNtg.gif', 'https://miro.medium.com/freeze/max/60/1*g_nyFXnC8woT-7tQw5UJAA.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*BDfkksZR4uCarOw1xnmBCA.gif?q=20', 'https://miro.medium.com/max/1882/1*OsMRXb9SfY1RwvQssImW-A.gif', 'https://miro.medium.com/max/1882/1*iSSe-h-1QO9-g28vFi_Cpg.gif', 'https://miro.medium.com/max/1882/1*lYFz8VoZz52LWVchWAdMZg.gif', 'https://miro.medium.com/max/1976/1*azs0rf7JrHygKRYc5ONjsg.gif', 'https://miro.medium.com/freeze/max/60/1*xbYE7LZLmdyzJSjqqoj5Hg.gif?q=20', 'https://miro.medium.com/max/60/1*5MKxG3uepKjiqV27laFUSg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*g1CngMDyLyOLrZtE1ISIxQ.gif?q=20', 'https://miro.medium.com/max/2026/1*y1T0uC2q67AvQhtUWTIsPw.gif', 'https://miro.medium.com/max/1882/1*g1CngMDyLyOLrZtE1ISIxQ.gif', 'https://miro.medium.com/max/3388/1*5MKxG3uepKjiqV27laFUSg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/518/1*FogMIj4gYwp3fTHLZuwavQ.png', 'https://miro.medium.com/max/2330/1*xbYE7LZLmdyzJSjqqoj5Hg.gif', 'https://miro.medium.com/max/60/1*xo8LGAaxdBCKFQVFb8ZQ3g.png?q=20', 'https://miro.medium.com/max/2114/1*kbx-4oQwUDLP6ufI2ABrNA.gif', 'https://miro.medium.com/freeze/max/60/1*Ksozb4RPKaPX9GQokhPF2w.gif?q=20', 'https://miro.medium.com/max/2636/1*LlXODNOp1jTUebbMbKcIbQ.gif', 'https://miro.medium.com/freeze/max/60/1*eaTfpuISWrNcMsyORqhJqw.gif?q=20', 'https://miro.medium.com/max/52/1*FogMIj4gYwp3fTHLZuwavQ.png?q=20', 'https://miro.medium.com/max/450/1*REsSrjoNT5PxisLtmgxJqA.png', 'https://miro.medium.com/max/2038/1*xo8LGAaxdBCKFQVFb8ZQ3g.png', 'https://miro.medium.com/freeze/max/60/1*y1T0uC2q67AvQhtUWTIsPw.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/freeze/max/60/1*ariSJFoszRvLXuyWlOscJA.gif?q=20', 'https://miro.medium.com/max/60/1*xLE5g-WQeLNt_RcGgWA2Bg.png?q=20', 'https://miro.medium.com/max/1692/1*eaTfpuISWrNcMsyORqhJqw.gif', 'https://miro.medium.com/freeze/max/60/1*FfF9NCxFHtXAAL7EMAJqOw.gif?q=20', 'https://miro.medium.com/max/1982/1*7LkwvpxZGju_wwYEQGT7wg.png', 'https://miro.medium.com/max/2026/1*g_nyFXnC8woT-7tQw5UJAA.gif', 'https://miro.medium.com/freeze/max/60/1*kbx-4oQwUDLP6ufI2ABrNA.gif?q=20', 'https://miro.medium.com/max/1634/1*xLE5g-WQeLNt_RcGgWA2Bg.png', 'https://miro.medium.com/freeze/max/60/1*13Vk_9wT3mUZsG9rVBJvpw.gif?q=20', 'https://miro.medium.com/max/60/1*GxG6KOEVyXsTqbcV9x02vQ.png?q=20', 'https://miro.medium.com/max/60/1*7LkwvpxZGju_wwYEQGT7wg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*iSSe-h-1QO9-g28vFi_Cpg.gif?q=20', 'https://miro.medium.com/max/1692/1*JwlWDbtbS4AA-bB-KIusGg.gif', 'https://miro.medium.com/freeze/max/60/1*LlXODNOp1jTUebbMbKcIbQ.gif?q=20', 'https://miro.medium.com/max/2026/1*1NPXBpuAhE3kWfS79RHlsA.gif', 'https://miro.medium.com/max/60/1*cRZDugJPIPtqye9v0AMeOA.png?q=20', 'https://miro.medium.com/max/60/1*5BnzZ9heUkuxBsXL7ebjFw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/60/1*OsMRXb9SfY1RwvQssImW-A.gif?q=20', 'https://miro.medium.com/max/1882/1*joBQpuOZ2rn796LJyWVJ_w.gif', 'https://miro.medium.com/max/1036/1*FogMIj4gYwp3fTHLZuwavQ.png', 'https://miro.medium.com/freeze/max/60/1*GM3UVHnGTum9Uzn1OieNtg.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*1NPXBpuAhE3kWfS79RHlsA.gif?q=20', 'https://miro.medium.com/max/462/1*5BnzZ9heUkuxBsXL7ebjFw.png', 'https://miro.medium.com/max/2394/1*ariSJFoszRvLXuyWlOscJA.gif', 'https://miro.medium.com/max/2662/1*FfF9NCxFHtXAAL7EMAJqOw.gif'}",2020-03-05 00:19:36.588824,3.8724215030670166
https://medium.com/@Clovis_app/configuration-of-a-beautiful-efficient-terminal-and-prompt-on-osx-in-7-minutes-827c29391961,Configuration of a beautiful (efficient) terminal and prompt on OSX in 7minutes,"Here at www.clovis.pro, we changed our prompt to a more functional one for a :

🎬 Better visualisation of git status

🏎️ Faster shell autocompletions

💮 Daily coding comfort

Obviously we didn’t want to spend hours with shell configuration and customization like many people do… That’s why we made this article !

Don’t hesitate to add enhancement suggestions, and follow the author on github: @Aarbel 👐

The result

🏖

The process to get it

🥋

1. Install and configure iTerm2

$ brew cask install iterm2

If you don’t have installed homebrew (you should… ;)), download and install iTerm2 (it has better color fidelity than the built in Terminal, so your themes will look better).

Choose your Iterm2 color scheme

Apply the color scheme in iTerm2

iTerm → Preferences → Profiles → Colors → Color presets → Import Then again, Color presets → you-color-scheme-name

2. Install a patched font

The patched font is the font used by iTerm2 to display characters, and you’ll need a special one for special characters like arrows and git icons.

You’ll get this result at the end, keep calm and follow this doc

Download and install the font

Meslo (recommanded, ie the one in the screenshot). Click “view raw” to download the font.

Source Code Pro has better alignment for the glyphs @14px.

Others powerline fonts

Open the downloaded font and press “Install Font” on your computer.

Add the font in iTerm2

(font size of 12pt is our personal preference)

iTerm2 → Preferences → Profiles → Text → Change Font

3. Install Zsh and Oh my Zsh

Don’t mind, you are close !

Zsh is a shell that provides many features, like better files and folders navigation. To install it :

brew install zsh zsh-completions

Oh my Zsh is a Zsh configuration framework, you can read more here: github.com/robbyrussell/oh-my-zsh.

To install it :

$ sh -c ""$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)""

💡In the next steps you’ll need to edit the ~/.zshrc configuration file which is run when the terminal starts. At any time you can compare it with Clovis .zshrc configuration file 🎁

4. Add Powerlevel9k Zsh Theme

The Powerlevel9k zsh theme adds many other features like a right promp with infos such as exit codes and timestamps. To install it run :

$ git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k

Then edit ~/.zshrc configuration file and set

ZSH_THEME=""powerlevel9k/powerlevel9k""

Boom.

Powerlevel9k offers a whole lot more, best is to follow the next steps or check out these user made configs.

5. Final tweaking

shorter prompt

enable text editor navigation

auto suggestions

syntax highlighting

new line after each prompt

change color of warning git status

change Iterm2 tabs color

Shorter prompt

Here you can remove the user@hostname and unnecessary informations + put the command line on a second prompt line :

Before…

After 💮

You can choose your prompt elements and make them as on the screenshots. Just add these lines to your ~/.zshrc configuration file :

POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(dir rbenv vcs)

POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status root_indicator background_jobs history time)

Moreover to make the two lines prompt you have to add this :

POWERLEVEL9K_PROMPT_ON_NEWLINE=true

💄✨ and to make it beautifull with the $ character add these other lines:

# Add a space in the first prompt

POWERLEVEL9K_MULTILINE_FIRST_PROMPT_PREFIX=""%f"" # Visual customisation of the second prompt line

local user_symbol=""$""

if [[ $(print -P ""%#"") =~ ""#"" ]]; then

user_symbol = ""#""

fi

POWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX=""%{%B%F{black}%K{yellow}%} $user_symbol%{%b%f%k%F{yellow}%} %{%f%}""

You can read more about POWERLEVEL9K prompts options here, and deeper customizations here: code.tutsplus.com/tutorials/how-to-customize-your-command-prompt — net-24083

Enable text editor navigation

Vertical cursor

iTerm2 → Preferences → Profiles → Text

→ Cursor : ✓ Vertical Bar

→ Blinking cursor : ✓ ON

Text navigation with keyboard

Moreover, by default, word jumps (option + → or ←) and word deletions (option + backspace) do not work on iTerm2. To enable them, go to :

iTerm → Preferences → Profiles → Keys → Load Preset… → Natural Text Editing

Restart iTerm2 for all changes to take effect.

Auto suggestions (for Oh My Zsh)

This plugin suggests the commands you used in your terminal history. You just have to type → to fill it entirely !



$ git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions Note: $ZSH_CUSTOM/plugins path is by default ~/.oh-my-zsh/custom/plugins

Add the plugin to the list of plugins in ~/.zshrc configuration file :

plugins=(

…

zsh-autosuggestions

)

Start a new terminal session.

More details here : github.com/tarruda/zsh-autosuggestions#oh-my-zsh

Note : if the auto suggestions do not appear to show, it could be a problem with your color scheme. Under “iTerm → Preferences → Colors tab”, check the value of Black Bright, that is the color your auto suggestions will have. It will be displayed on top of the Background color. If there is not enough contrast between the two, you won’t see the suggestions even if they’re actually there..

Syntax highlighting

The highlighting shows if commands are installed and known by the shell, and gives better output colors.

$ brew install zsh-syntax-highlighting

If you do not have or do not like homebrew, follow the installation instructions instead.

After installation through homebrew, add the next line to the end of your ~/.zshrc file :

source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh

After that, it's best to restart your terminal. Sourcing your ~/.zshrc file with source ~/.zshrc command does not seem to work well with this plugin.

New line after each prompt

Before

After (better readability)

Again, add this lines to your ~/.zshrc configuration file :

POWERLEVEL9K_PROMPT_ADD_NEWLINE=true

Change color of warning git status

Before

After

You can set it as red for better readability (and not orange). Just add this line:

POWERLEVEL9K_VCS_MODIFIED_BACKGROUND=’red’

Change color of iTerm2 Tabs

The final UI isn’t perfect, but is better

Same game, just add these lines at the end of ~/.zshrc configuration file and restart your term:

# Colorise the top Tabs of Iterm2 with the same color as background

# Just change the 18/26/33 wich are the rgb values

echo -e ""\033]6;1;bg;red;brightness;18\a""

echo -e ""\033]6;1;bg;green;brightness;26\a""

echo -e ""\033]6;1;bg;blue;brightness;33\a""

6. Visual Studio Code config

Installing a patched font will mess up the integrated terminal in VS Code unless you use the proper settings. You’ll need to go to settings (CMD + ,) and add or edit the following values:

for Meslo: ""terminal.integrated.fontFamily"": ""Meslo LG M for Powerline""

for Source Code Pro: ""terminal.integrated.fontFamily"": ""Source Code Pro for Powerline""

for other fonts you’ll need to check the font name in Font Book.

You can also set the font size e.g.: ""terminal.integrated.fontSize"": 12

🚀

Follow the author on github : @Aarbel

Sources :

https://github.com/chris-murray/powerlevel9k-custom

https://gist.github.com/kevin-smets/8568070

…","['prompt', 'file', '7minutes', 'zsh', 'install', 'add', 'configuration', 'efficient', 'font', 'beautiful', 'zshrc', 'better', 'color', 'iterm2', 'terminal', 'osx']","Choose your Iterm2 color schemeApply the color scheme in iTerm2iTerm → Preferences → Profiles → Colors → Color presets → Import Then again, Color presets → you-color-scheme-name2.
To install it :brew install zsh zsh-completionsOh my Zsh is a Zsh configuration framework, you can read more here: github.com/robbyrussell/oh-my-zsh.
To install it :$ sh -c ""$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)""💡In the next steps you’ll need to edit the ~/.zshrc configuration file which is run when the terminal starts.
At any time you can compare it with Clovis .zshrc configuration file 🎁4.
$ git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions Note: $ZSH_CUSTOM/plugins path is by default ~/.oh-my-zsh/custom/pluginsAdd the plugin to the list of plugins in ~/.zshrc configuration file :plugins=(…zsh-autosuggestions)Start a new terminal session.",en,[],2018-12-18 09:05:07.710000+00:00,"{'Shell', 'Prompt', 'Terminal', 'Mac Osx', 'Beautiful'}","{'https://miro.medium.com/max/60/1*BZkmXHh80z_i5vxWh5HQcQ.png?q=20', 'https://miro.medium.com/max/60/1*PBDetWpgW4VXpr-0SmUXdA.png?q=20', 'https://miro.medium.com/max/1528/1*jCBFWhLKkCz4dCFOJ2DTpQ.png', 'https://miro.medium.com/max/60/1*byEErAY20uZGYsXI0T14MQ.png?q=20', 'https://miro.medium.com/max/550/1*la_o-lG9oe0dzbSWNxv6IA.png', 'https://miro.medium.com/fit/c/80/80/1*2In1UKOCadRTaeRRYNYXKg.jpeg', 'https://miro.medium.com/max/244/1*At5BewyJyBfKwrLauzUUkw.png', 'https://miro.medium.com/max/60/1*xCYq2efusqeW5cg-tLYCaQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*QRp81RJGddAAvNgeR_UuuA.jpeg', 'https://miro.medium.com/max/1074/1*7inD8Cq3yU_GKGbizyFutg.png', 'https://miro.medium.com/max/1702/1*HR2axSnawYpDNSlPZRy0wA.png', 'https://miro.medium.com/max/60/1*ZNc2UsmF4RCw_uev37o1bA.png?q=20', 'https://miro.medium.com/max/60/1*CuXZ7ypMtFyrlqO8ARf83w.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*N23lueeQFX1lsKwb.jpg', 'https://miro.medium.com/max/60/1*At5BewyJyBfKwrLauzUUkw.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*N23lueeQFX1lsKwb.jpg', 'https://miro.medium.com/max/60/1*la_o-lG9oe0dzbSWNxv6IA.png?q=20', 'https://miro.medium.com/max/60/1*-Roc94tb5pEPAkKul9FBpA.png?q=20', 'https://miro.medium.com/max/1034/1*OEEvmlEiZ8kia50ijK-gTA.png', 'https://miro.medium.com/max/2398/1*nqh3bSDUYSBxjDWUgFSwLw.png', 'https://miro.medium.com/max/900/1*wyUWoCDeiHJE7V-HEmclkg.png', 'https://miro.medium.com/max/60/1*HR2axSnawYpDNSlPZRy0wA.png?q=20', 'https://miro.medium.com/max/1156/1*-ewoBb3M1lsQWyuy7WvF-w.png', 'https://miro.medium.com/max/60/1*OEEvmlEiZ8kia50ijK-gTA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*nIvslm5rv4goxDK0pLDjBA.jpeg', 'https://miro.medium.com/max/1144/1*BZkmXHh80z_i5vxWh5HQcQ.png', 'https://miro.medium.com/max/1060/1*-Roc94tb5pEPAkKul9FBpA.png', 'https://miro.medium.com/max/60/1*-ewoBb3M1lsQWyuy7WvF-w.png?q=20', 'https://miro.medium.com/max/60/1*wyUWoCDeiHJE7V-HEmclkg.png?q=20', 'https://miro.medium.com/max/1352/1*PBDetWpgW4VXpr-0SmUXdA.png', 'https://miro.medium.com/max/1386/1*xCYq2efusqeW5cg-tLYCaQ.png', 'https://miro.medium.com/max/946/1*byEErAY20uZGYsXI0T14MQ.png', 'https://miro.medium.com/max/956/1*CuXZ7ypMtFyrlqO8ARf83w.png', 'https://miro.medium.com/max/1318/1*jv9oRgRRwPyuNjRk6iNqgw.png', 'https://miro.medium.com/max/851/1*HR2axSnawYpDNSlPZRy0wA.png', 'https://miro.medium.com/max/60/1*7inD8Cq3yU_GKGbizyFutg.png?q=20', 'https://miro.medium.com/max/962/1*ZNc2UsmF4RCw_uev37o1bA.png', 'https://miro.medium.com/max/60/1*nqh3bSDUYSBxjDWUgFSwLw.png?q=20', 'https://miro.medium.com/max/60/1*jv9oRgRRwPyuNjRk6iNqgw.png?q=20', 'https://miro.medium.com/max/60/1*jCBFWhLKkCz4dCFOJ2DTpQ.png?q=20'}",2020-03-05 00:19:37.773623,1.183797836303711
https://towardsdatascience.com/learn-enough-python-to-be-useful-argparse-e482e1764e05,Learn Enough Python to be Useful: argparse,"argparse is the “recommended command-line parsing module in the Python standard library.” It’s what you use to get command line arguments into your program.

I couldn’t find a good intro guide for argparse when I needed one, so I wrote this article. Enjoy!

Jupyter is great

Beyond the Jupyter Notebook

The first time I saw argparse in a Python script for a side project I thought, “What is this voodo magic?” And quickly moved the code into a Jupyter Notebook. This move turned out to be suboptimal. 😦

I wanted to be able to run a script rather than have to step through a Jupyter Notebook. A script with argparse would have been much simpler to use and much less work. Unfortunately, I was in a hurry and didn’t find the docs easy to grasp.

Since then, I’ve come to understand and enjoy argparse . It’s indispensable.

Here’s what you need to know.

Why Use argparse?

argparse — parse the arguments.

Using argparse is how you let the user of your program provide values for variables at runtime. It’s a means of communication between the writer of a program and the user. That user might be your future self. 😃

Using argparse means the doesn’t need to go into the code and make changes to the script. Giving the user the ability to enter command line arguments provides flexibility.

Python is great, too!

Example

Say you have a directory with videos in it and want to turn those videos into images using the OpenCV library. You could use argparse so the user can enter input and output directories. Here’s what the argparse section of your videos.py file looks like:

# videos.py

import argparse parser = argparse.ArgumentParser(description='Videos to images')

parser.add_argument('indir', type=str, help='Input dir for videos')

parser.add_argument('outdir', type=str, help='Output dir for image') args = parser.parse_args() print(args.indir)

This file imports the argparse library. Then it makes a parser object with a description. Then the variable indir is created using parser.add_argument() . The type of the variable is set to string and a help message is provided. Then the same is done for outdir. Next the args variable is set to the values of the parsed arguments.

Now the following command can be run from the command line:

python videos.py /videos /images

Note that quotes do not need to be placed around the values /videos and /images when you pass them.

""/videos"" becomes the value for args.indir and ""/images"" becomes the value for args.outdir .

The output printed to the terminal is /videos .

We just showed that you can use the args.indir variable anywhere in your program. How cool is that?

You’ve now seen the magic of argparse!

How do they do that?

What Else Should You Know About argparse?

Positional Arguments

parser.add_argument('indir', type=str, help='Input dir for videos') created a positional argument. For positional arguments to a Python function, the order matters. The first value passed from the command line becomes the first positional argument. The second value passed becomes the second positional argument.

What happens if you exclude these positional arguments and try to run python videos.py ?

You’ll get an error: videos.py: error: the following arguments are required: indir, outdir . Positional arguments are always required to be passed in the command to run the script.

Optional Arguments

What happens if you run python videos.py --help ?

You get the helpful information we put into our script to tell you what you need to do.

Output of python videos.py --help

Excellent! help is an example of an optional argument. Note that --help is the only optional argument you get for free, but you can make more.

Optional arguments are created just like positional arguments except that they have a '--' double dash at the start of their name (or a '-' single dash and one additional character for the short version). For example, you can create an optional argument with parser.add_argument('-m', '--my_optional') .

The following larger example shows how to create and reference an optional argument. Note that we specify the type int for an integer in this example. You could also specify other valid Python variable types.

# my_example.py

import argparse parser = argparse.ArgumentParser(description='My example explanation') parser.add_argument(

'--my_optional',

default=2,

help='provide an integer (default: 2)'

) my_namespace = parser.parse_args() print(my_namespace.my_optional)

Note that the argument specified with '--my_optional' becomes this namespaced variable without the dashes: 'my_namespace.my_optional' .

Also note that the optional argument can have a default value. Here we specify a default of 2. Running python my_example.py outputs 2.

The optional argument value can be set at run time from the command line like this: python my_example.py--my_optional=3 . The program then outputs 3.

Integers

You can do even more with argparse . For example, you can have arguments gathered into lists with nargs='*’ . You can also check for ranges of values with choices . See the argparse docs for all you can do.

When Else Might I Use argparse?

You can also use argparse with programs running in Docker containers. If you want to pass command line arguments to your scripts when building your image you can do so with RUN. If you want to pass arguments to your script at run time you can do so with CMD or ENTRYPOINT. Learn more about Dockerfiles in my series on Docker:

Wrap

Now you’ve seen the basics of argparse . You’ve seen how to get positional and optional arguments into your programs from the command line. You’ve also seen how to set default optional arguments. If you want to go deeper, check out the official docs.

Update Mar. 6, 2019 I should mention that there are a number of packages available to add command line arguments to your program. Readers have suggested several in the comments, the most popular of which I’ve linked to here:

More mountains through the mist

Here are a few more suggestions to help you step out of the Jupyter Notebook.

Environment variables are useful variables that get set outside a program. Here’s a nice, clear intro. This article from DataCamp blog focuses on the PATH variable.

You can convert repos with Jupyter notebooks into Docker Images with Repo2Docker. Will Koehrsen wrote a good guide on the tool here.

I plan to write more articles about interacting with the file system and scripting. Follow me to make sure you don’t miss them! 😃

I hope you found this intro useful. If you did, share it on your favorite forums and social media. Data scientists and programmers who don’t know argparse will thank you!

I write about data science, cloud computing, and other tech stuff. Follow me and read more here.

Thanks for reading!👏","['videospy', 'argparse', 'python', 'arguments', 'useful', 'argument', 'learn', 'command', 'run', 'line', 'optional', 'positional']","argparse is the “recommended command-line parsing module in the Python standard library.” It’s what you use to get command line arguments into your program.
Giving the user the ability to enter command line arguments provides flexibility.
What happens if you exclude these positional arguments and try to run python videos.py ?
The optional argument value can be set at run time from the command line like this: python my_example.py--my_optional=3 .
6, 2019 I should mention that there are a number of packages available to add command line arguments to your program.",en,['Jeff Hale'],2019-12-13 16:40:21.932000+00:00,"{'Data Science', 'Software Development', 'Python', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/54/1*vhOOyvIRog6Qk8jOL4JEsA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/364/1*vhOOyvIRog6Qk8jOL4JEsA.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*Asb9N4lW4pQN_Id2qwtPOA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*Asb9N4lW4pQN_Id2qwtPOA.jpeg', 'https://miro.medium.com/max/60/1*rhhk7heUKv1KA8p50I-ElA.jpeg?q=20', 'https://miro.medium.com/max/60/1*ZCL44BU8v2ZtC4nhfXYaXQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*YG9Hdcj0cECA0bXu49FQJw.jpeg?q=20', 'https://miro.medium.com/max/652/1*ay4B0npfQAYn0nZ2npFrSg.png', 'https://miro.medium.com/max/3840/1*rhhk7heUKv1KA8p50I-ElA.jpeg', 'https://miro.medium.com/max/1280/1*YG9Hdcj0cECA0bXu49FQJw.jpeg', 'https://miro.medium.com/max/60/1*ay4B0npfQAYn0nZ2npFrSg.png?q=20', 'https://miro.medium.com/max/1280/1*ZCL44BU8v2ZtC4nhfXYaXQ.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/proxy/1*oPkqiu1rrt-hC_lDMK-jQg.png', 'https://miro.medium.com/max/592/1*H1olKNHMeAiPbDoDf95MYw.png', 'https://miro.medium.com/max/1200/1*rhhk7heUKv1KA8p50I-ElA.jpeg', 'https://miro.medium.com/max/1280/1*9FTxSVJVL-y7KuH-n3SbUw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*9FTxSVJVL-y7KuH-n3SbUw.jpeg?q=20', 'https://miro.medium.com/max/60/1*H1olKNHMeAiPbDoDf95MYw.png?q=20'}",2020-03-05 00:19:44.306482,6.531860113143921
https://medium.com/neo4j/using-nlp-in-neo4j-ac40bc92196f,Neo4j: Natural Language Processing (NLP) in Cypher,"In Neo4j 3.0, the database introduced the idea of user-defined procedures that you could call directly from the Cypher language. Prior to this, cypher was already a pretty good language, but things really started blowing up, with people writing code on top of neo4j that lets you do just about anything in Cypher directly. This article gives an example of using Natural Language Processing (NLP) inside of Cypher to show how you can draw meaning out of text in graphs, aimed at people who may be new to NLP.

As an example, we’ll work through how to find out about positive and negative sentences in Donald Trump’s twitter feed, showing techniques that can be used on any text. To do this, I used both the APOC procedures for neo4j, and GraphAware’s neo4j-nlp procedures and installed the relevant JARs into the plugins directory for neo4j.

Neo4j-NLP Setup: make sure to follow the directions on the github page. You will need at least 4 JARs, and to add some configuration to neo4j.conf. Finally, after starting the database you’ll need to create a default pipeline step, which is covered in their setup documentation.

For this tutorial, the steps we’ll go through:

Load data on tweets Break up hashtag / user replies and link them to the tweets Apply some basic NLP approaches to tag which words and concepts he’s tweeting about Apply some sentiment analysis provided by neo4j-nlp to determine what he’s feeling positive about, and what he’s not so positive about.

Step 1: The apoc.load.json function lets us get data into neo4j directly from JSON. The data URLs come from the excellent Trump Twitter Archive and are kept up to date with all of his tweets. By unwinding an array of data URLs, we can load all of the files in a single shot.

CREATE INDEX ON :User(name);

CREATE INDEX ON :Tweet(text);

CREATE INDEX ON :Hashtag(name);

'http://www.trumptwitterarchive.com/data/realdonaldtrump/2019.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2018.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2017.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2016.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2015.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2014.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2013.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2012.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2011.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2010.json',

'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2009.json'

] AS url

CALL apoc.load.json(url) YIELD value as t

MERGE (s:Source { name: t.source })

CREATE (tweet:Tweet {

id_str: t.id_str,

text: t.text,

created_at: t.created_at,

retweets: t.retweet_count,

favorites: t.favorite_count,

retweet: t.is_retweet,

in_reply: coalesce(t.in_reply_to_user_id_str, '')

})

CREATE (tweet)-[:from]->(s)

RETURN count(t); UNWIND [] AS urlCALL apoc.load.json(url) YIELD value as tMERGE (s:Source { name: t.source })CREATE (tweet:Tweet {id_str: t.id_str,text: t.text,created_at: t.created_at,retweets: t.retweet_count,favorites: t.favorite_count,retweet: t.is_retweet,in_reply: coalesce(t.in_reply_to_user_id_str, '')})CREATE (tweet)-[:from]->(s)RETURN count(t);

This gives us all 37,000+ tweets that Donald Trump’s account sent, starting in 2009. Our graph right now is extremely simple, only connecting a single tweet to the source it was sent from, like this:

Step 2: We can go ahead and punch up the data a little bit by extracting hash tags and user mentions from the data, creating those as separate nodes, and linking them to the relevant tweets, like this:

/* Hashtag Analysis */

MATCH (t:Tweet)

WHERE t.text =~ "".*#.*""

WITH

t,

apoc.text.regexGroups(t.text, ""(#\\w+)"")[0] as hashtags

UNWIND hashtags as hashtag

MERGE (h:Hashtag { name: toUpper(hashtag) })

MERGE (h)<-[:hashtag { used: hashtag }]-(t)

RETURN count(h); /* User Mention Analysis */

MATCH (t:Tweet)

WHERE t.text =~ "".*@.*""

WITH

t,

apoc.text.regexGroups(t.text, ""(@\\w+)"")[0] as mentions

UNWIND mentions as mention

MERGE (u:User { name: mention })

MERGE (u)<-[:mention]-(t)

RETURN count(u);

Looking at an individual tweet, we can now see it’s linked appropriately. By hashtag topic and by user mention then, we can look at who Donald Trump tweets to the most often, and what topics he’s most often tweeting about.

A simple tweet related to hashtags and user mentions

Step 3: OK, to get some meaning out of this raw text, we’ll first need to “annotate” the language. What’s happening here is that the “text” of the tweet is getting broken up into individual words with certain common words eliminated, by using English grammar it will work out what’s a noun and what’s a verb, and it will store the structure of the sentence in the neo4j graph. Doing all of this is simple because of the neo4-nlp extension in cypher, and works like this.

/* Detect language and update each tweet with that information */

MATCH (t:Tweet)

CALL ga.nlp.detectLanguage(t.text)

YIELD result

SET t.language = result

RETURN count(t); /* Annotate all text that's detected as English, as the underlying library may not support things it detects as non-English */

MATCH (t:Tweet { language: ""en"" })

CALL ga.nlp.annotate({text: t.text, id: id(t)})

YIELD result

MERGE (t)-[:HAS_ANNOTATED_TEXT]->(result)

RETURN count(result);

All of the heavy lifting is done by GraphAware’s ga.nlp.annotate procedure. This will create a large number of new nodes in your graph. Every Tweet will be associated with an AnnotatedText node, which in turn will be linked further to Tag and Sentence nodes. For example, if we annotate the sentence “See you in the Supreme Court!”, it will be broken down into “tags” like “see” (which is a verb) and “Supreme Court” which is a noun. We can tell because the associated tag node has a property called “pos” (Part of Speech) which has the value NNP (a proper noun). Here’s what the graph looks like for a relatively simple sentence:

Showing the results of annotating text

Tags also occur at particular spots in sentences. So you’ll also see TagOccurance nodes which indicate where in the sentence each tag is seen.

OK, this gets us the words and their functions within the paragraph, but still don’t say much about meaning. We need to go two steps further. The first is to “enrich” the concepts, using the ConceptNet5 API.

MATCH (n:Tag)

CALL ga.nlp.enrich.concept({tag: n, depth:2, admittedRelationships:[""IsA"",""PartOf""]})

YIELD result

RETURN count(result);

This yields a set of additional relationships that relate the tags together, so we can tell what’s similar to what. It also labels a lot of nodes with additional categories, such as NER_Person , NER_Location , NER_Organization , and so on which allows us to classify our tags by what kind of thing they are. At this point, we’re better off than knowing “Supreme Court” is a noun, we know it refers to an organization.

This lets us write queries like the below, which will fetch all of the tweets where the language in the tweet is talking about the person concept “Clintons”.

MATCH (:NER_Person {value: 'clintons'})-[]-(s:Sentence)-[]-(:AnnotatedText)-[]-(tweet:Tweet)

RETURN distinct tweet.text;

Tweets about the Clintons

Step 4: OK. At this point we’ve broken down the language, and we have some sense of the concepts being discussed. The last step is sentiment analysis. This can be a very in-depth topic, but for the purposes of this article, it’s quite simple: it just applies a positive, neutral, or negative label to each Sentence node in our graph.

MATCH (t:Tweet)-[]-(a:AnnotatedText)

CALL ga.nlp.sentiment(a) YIELD result

RETURN result;

With this in place, querying for positive and negative tweets is easy. We can simply extend our previous query and tack on a label to find all negative tweets about a topic:

MATCH (:NER_Person {value: 'clintons'})-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet)

RETURN distinct tweet.text;

Step 5: Let’s pull it all together. What can we learn about Donald Trump’s tweeting patterns using Neo4j and NLP techniques? How about let’s try a broader query to find the most frequently mentioned people, in negative contexts. We intentionally exclude tags that could be multi-purpose depending on the sentence context, to focus on just the people.

MATCH (tag:NER_Person)-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet)

WHERE

NOT tag:NER_Organization AND

NOT tag:NER_O

RETURN distinct tag.value, count(tweet) as negativeTweets ORDER BY negativeTweets DESC;

The results are probably not surprising for those who are familiar with the raw tweets.

Persons mentioned in negative contexts

Although it is interesting that “Donald Trump” is the first result. This is an artifact of a couple of things in our dataset; first that Donald Trump talks about and retweets about himself quite a lot. And second, the sentiment analyzer isn’t perfect, and may rate as negative tweets like “@KingOf_Class: @realDonaldTrump Honestly,you can’t find anyone more real than Donald Trump!”.

Lastly, let’s take a broad view of positive and negative sentiments about various organizations:

MATCH (t:NER_Organization)-[]-(s:Sentence)

WHERE

'Negative' in labels(s) OR

'Positive' in labels(s)

AND length(labels(t)) = 2

RETURN distinct t.value as orgName,

s.text as sentence,

labels(s) as sentiment;

Organizations and the sentiment of the sentences they were mentioned in.

Conclusion

Using NLP techniques inside of neo4j, there is quite a lot that you can do. GraphAware has already used this library as part of building Amazon Alexa skills, where the NLP component gets used to decide which skill the user’s input phrase is most like. In this article, we’ve shown a simple way you can programmtically understand the gist of text using the same approach.

Many other fun applications are possible. A while ago, the Planet Money podcast built a bot that trades stocks based on positive or negative sentiment about companies seen in Donald Trump’s twitter feed using similar approaches (although I’m not certain whether they used neo4j or not). The limits are only how creative you can get.

In this article, I have glossed over a couple points; in particular the quality of the results with the sentiment analyzer can be spotty depending on your input data. For top-notch results, you’ll find yourself going deeper into the world of NLP to include training your own sentence analyzer. It’s a deep rabbit hole, but it’s also rewarding and fun if you’re interested in learning modern approaches to dealing with text analysis.

For a different approach to doing NLP together with Neo4j, make sure to check out Will Lyon’s post on finding Russian Twitter trolls with neo4j and NLP approaches.","['natural', 'nlp', 'cypher', 'neo4j', 'negative', 'sentence', 'tweets', 'donald', 'positive', 'data', 'language', 'text', 'processing']","In Neo4j 3.0, the database introduced the idea of user-defined procedures that you could call directly from the Cypher language.
This article gives an example of using Natural Language Processing (NLP) inside of Cypher to show how you can draw meaning out of text in graphs, aimed at people who may be new to NLP.
MATCH (t:Tweet)-[]-(a:AnnotatedText)CALL ga.nlp.sentiment(a) YIELD resultRETURN result;With this in place, querying for positive and negative tweets is easy.
What can we learn about Donald Trump’s tweeting patterns using Neo4j and NLP techniques?
For a different approach to doing NLP together with Neo4j, make sure to check out Will Lyon’s post on finding Russian Twitter trolls with neo4j and NLP approaches.",en,['David Allen'],2019-03-18 15:43:58.381000+00:00,"{'Data Science', 'Data Analysis', 'How To', 'Neo4j', 'NLP'}","{'https://miro.medium.com/max/1842/1*uNzvzDWKX2EAWBclnHOB1w.png', 'https://miro.medium.com/max/60/1*uNzvzDWKX2EAWBclnHOB1w.png?q=20', 'https://miro.medium.com/max/60/1*0Acz6g_uP4qGbd8W6Ex0Pg.png?q=20', 'https://miro.medium.com/max/1854/1*GmvdLBzDZAPRegVTCBxp5w.png', 'https://miro.medium.com/max/897/1*T-V_36wgYDBJy4kYNZNMng.png', 'https://miro.medium.com/max/180/1*VcXxX-LCX9H7Qpt8uL6Kfw.png', 'https://miro.medium.com/fit/c/80/80/0*topUTNdjQnA5--iL.jpeg', 'https://miro.medium.com/fit/c/160/160/1*kDtqhvHY-ANL4yq4XetGOw.png', 'https://miro.medium.com/max/60/1*uJeXebZ6lTB0a-IfAlQTLA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*5IlZxeGNaJHoLkFj', 'https://miro.medium.com/max/1794/1*T-V_36wgYDBJy4kYNZNMng.png', 'https://miro.medium.com/max/1850/1*6V0r1LrIA1UxNRC6kYTEeg.png', 'https://miro.medium.com/max/60/1*GmvdLBzDZAPRegVTCBxp5w.png?q=20', 'https://miro.medium.com/max/1364/1*0Acz6g_uP4qGbd8W6Ex0Pg.png', 'https://miro.medium.com/max/60/1*6V0r1LrIA1UxNRC6kYTEeg.png?q=20', 'https://miro.medium.com/max/2064/1*uJeXebZ6lTB0a-IfAlQTLA.png', 'https://miro.medium.com/fit/c/160/160/0*5vo0gkfNWFKvz3KB.', 'https://miro.medium.com/max/60/1*T-V_36wgYDBJy4kYNZNMng.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*hmhjbwkprwEOimDT.jpg', 'https://miro.medium.com/fit/c/96/96/0*5vo0gkfNWFKvz3KB.'}",2020-03-05 00:19:45.549511,1.2430295944213867
https://towardsdatascience.com/getting-started-with-apache-kafka-in-python-604b3250aa05,Getting started with Apache Kafka in Python,"Image Credit: linuxhint.com

In this post, I am going to discuss Apache Kafka and how Python programmers can use it for building distributed systems.

What is Apache Kafka?

Apache Kafka is an open-source streaming platform that was initially built by LinkedIn. It was later handed over to Apache foundation and open sourced it in 2011.

According to Wikipedia:

Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a “massively scalable pub/sub message queue architected as a distributed transaction log,”[3] making it highly valuable for enterprise infrastructures to process streaming data. Additionally, Kafka connects to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library.

Credit: Official Website

Think of it is a big commit log where data is stored in sequence as it happens. The users of this log can just access and use it as per their requirement.

Kafka Use Cases

Uses of Kafka are multiple. Here are a few use-cases that could help you to figure out its usage.

Activity Monitoring:- Kafka can be used for activity monitoring. The activity could belong to a website or physical sensors and devices. Producers can publish raw data from data sources that later can be used to find trends and pattern.

Kafka can be used for activity monitoring. The activity could belong to a website or physical sensors and devices. Producers can publish raw data from data sources that later can be used to find trends and pattern. Messaging:- Kafka can be used as a message broker among services. If you are implementing a microservice architecture, you can have a microservice as a producer and another as a consumer. For instance, you have a microservice that is responsible to create new accounts and other for sending email to users about account creation.

Kafka can be used as a message broker among services. If you are implementing a microservice architecture, you can have a microservice as a producer and another as a consumer. For instance, you have a microservice that is responsible to create new accounts and other for sending email to users about account creation. Log Aggregation:- You can use Kafka to collect logs from different systems and store in a centralized system for further processing.

You can use Kafka to collect logs from different systems and store in a centralized system for further processing. ETL:- Kafka has a feature of almost real-time streaming thus you can come up with an ETL based on your need.

Kafka has a feature of almost real-time streaming thus you can come up with an ETL based on your need. Database:- Based on things I mentioned above, you may say that Kafka also acts as a database. Not a typical databases that have a feature of querying the data as per need, what I meant that you can keep data in Kafka as long as you want without consuming it.

Kafka Concepts

Let’s discuss core Kafka concepts.

Topics

Every message that is feed into the system must be part of some topic. The topic is nothing but a stream of records. The messages are stored in key-value format. Each message is assigned a sequence, called Offset. The output of one message could be an input of the other for further processing.

Producers

Producers are the apps responsible to publish data into Kafka system. They publish data on the topic of their choice.

Consumers

The messages published into topics are then utilized by Consumers apps. A consumer gets subscribed to the topic of its choice and consumes data.

Broker

Every instance of Kafka that is responsible for message exchange is called a Broker. Kafka can be used as a stand-alone machine or a part of a cluster.

I try to explain the whole thing with a simple example, there is a warehouse or godown of a restaurant where all the raw material is dumped like rice, vegetables etc. The restaurant serves different kinds of dishes: Chinese, Desi, Italian etc. The chefs of each cuisine can refer to the warehouse, pick the desire things and make things. There is a possibility that the stuff made by the raw material can later be used by all departments’ chefs, for instance, some secret sauce that is used in ALL kind of dishes. Here, the warehouse is a broker, vendors of goods are the producers, the goods and the secret sauce made by chefs are topics while chefs are consumers. My analogy might sound funny and inaccurate but at least it’d have helped you to understand the entire thing :-)

Setting up and Running

The easiest way to install Kafka is to download binaries and run it. Since it’s based on JVM languages like Scala and Java, you must make sure that you are using Java 7 or greater.

Kafka is available in two different flavors: One by Apache foundation and other by Confluent as a package. For this tutorial, I will go with the one provided by Apache foundation. By the way, Confluent was founded by the original developers of Kafka.

Starting Zookeeper

Kafka relies on Zookeeper, in order to make it run we will have to run Zookeeper first.

bin/zookeeper-server-start.sh config/zookeeper.properties

it will display lots of text on the screen, if see the following it means it’s up properly.

2018-06-10 06:36:15,023] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)

[2018-06-10 06:36:15,044] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

Starting Kafka Server

Next, we have to start Kafka broker server:

bin/kafka-server-start.sh config/server.properties

And if you see the following text on the console it means it’s up.

2018-06-10 06:38:44,477] INFO Kafka commitId : fdcf75ea326b8e07 (org.apache.kafka.common.utils.AppInfoParser)

[2018-06-10 06:38:44,478] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)

Create Topics

Messages are published in topics. Use this command to create a new topic.

➜ kafka_2.11-1.1.0 bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

Created topic ""test"".

You can also list all available topics by running the following command.

➜ kafka_2.11-1.1.0 bin/kafka-topics.sh --list --zookeeper localhost:2181

test

As you see, it prints, test .

Sending Messages

Next, we have to send messages, producers are used for that purpose. Let’s initiate a producer.

➜ kafka_2.11-1.1.0 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

>Hello

>World

You start the console based producer interface which runs on the port 9092 by default. --topic allows you to set the topic in which the messages will be published. In our case the topic is test

It shows you a > prompt and you can input whatever you want.

Messages are stored locally on your disk. You can learn about the path of it by checking the value of log.dirs in config/server.properties file. By default they are set to /tmp/kafka-logs/

If you list this folder you will find a folder with name test-0 . Upon listing it you will find 3 files: 00000000000000000000.index 00000000000000000000.log 00000000000000000000.timeindex

If you open 00000000000000000000.log in an editor then it shows something like:

^@^@^@^@^@^@^@^@^@^@^@=^@^@^@^@^BÐØR^V^@^@^@^@^@^@^@^@^Acça<9a>o^@^@^Acça<9a>oÿÿÿÿÿÿÿÿÿÿÿÿÿÿ^@^@^@^A^V^@^@^@^A

Hello^@^@^@^@^@^@^@^@^A^@^@^@=^@^@^@^@^BÉJ^B­^@^@^@^@^@^@^@^@^Acça<9f>^?^@^@^Acça<9f>^?ÿÿÿÿÿÿÿÿÿÿÿÿÿÿ^@^@^@^A^V^@^@^@^A

World^@

~

Looks like the encoded data or delimiter separated, I am not sure. If someone knows this format then do let me know.

Anyways, Kafka provides a utility that lets you examine each incoming message.

➜ kafka_2.11-1.1.0 bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/test-0/00000000000000000000.log

Dumping /tmp/kafka-logs/test-0/00000000000000000000.log

Starting offset: 0

offset: 0 position: 0 CreateTime: 1528595323503 isvalid: true keysize: -1 valuesize: 5 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] payload: Hello

offset: 1 position: 73 CreateTime: 1528595324799 isvalid: true keysize: -1 valuesize: 5 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] payload: World

You can see the message with other details like offset , position and CreateTime etc.

Consuming Messages

Messages that are stored should be consumed too. Let’s started a console based consumer.

➜ kafka_2.11-1.1.0 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

If you run, it will dump all the messages from the beginning till now. If you are just interested to consume the messages after running the consumer then you can just omit --from-beginning switch it and run. The reason it does not show the old messages because the offset is updated once the consumer sends an ACK to the Kafka broker about processing messages. You can see the workflow below.

Accessing Kafka in Python

There are multiple Python libraries available for usage:

Kafka-Python — An open-source community-based library.

— An open-source community-based library. PyKafka — This library is maintained by Parsly and it’s claimed to be a Pythonic API. Unlike Kafka-Python you can’t create dynamic topics.

— This library is maintained by Parsly and it’s claimed to be a Pythonic API. Unlike Kafka-Python you can’t create dynamic topics. Confluent Python Kafka:- It is offered by Confluent as a thin wrapper around librdkafka, hence it’s performance is better than the two.

For this post, we will be using the open-source Kafka-Python.

Recipes Alert System in Kafka

In the last post about Elasticsearch, I scraped Allrecipes data. In this post, I am going to use the same scraper as a data source. The system we are going to build is an alert system which will send notification about the recipes if it meets the certain threshold of the calories. There will be two topics:

raw_recipes :- It will be storing the raw HTML of each recipe. The idea is to use this topic as the main source of our data that later can be processed and transformed as per need.

:- It will be storing the raw HTML of each recipe. The idea is to use this topic as the main source of our data that later can be processed and transformed as per need. parsed_recipes:- As the name suggests, this will be parsed data of each recipe in JSON format.

The length of Kafka topic name should not exceed 249.

A typical workflow will look like below:

Install kafka-python via pip

pip install kafka-python

Raw recipe producer

The first program we are going to write is the producer. It will access Allrecpies.com and fetch the raw HTML and store in raw_recipes topic.

This code snippet will extract markup of each recipe and return in list format.

Next, we to create a producer object. Before we proceed further, we will make changes in config/server.properties file. We have to set advertised.listeners to PLAINTEXT://localhost:9092 otherwise you could experience the following error:

Error encountered when producing to broker b'adnans-mbp':9092. Retrying.

We will now add two methods: connect_kafka_producer() that will give you an instance of Kafka producer and publish_message() that will just dump the raw HTML of individual recipes.

The __main__ will look like below:

if __name__ == '__main__':

headers = {

'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',

'Pragma': 'no-cache'

} all_recipes = get_recipes()

if len(all_recipes) > 0:

kafka_producer = connect_kafka_producer()

for recipe in all_recipes:

publish_message(kafka_producer, 'raw_recipes', 'raw', recipe.strip())

if kafka_producer is not None:

kafka_producer.close()

If it runs well, it shows the following output:



Accessing list

Processing..

Processing..

Processing..

Message published successfully.

Message published successfully.

Message published successfully. /anaconda3/anaconda/bin/python /Development/DataScience/Kafka/kafka-recipie-alert/producer-raw-recipies.pyAccessing listProcessing.. https://www.allrecipes.com/recipe/20762/california-coleslaw/ Processing.. https://www.allrecipes.com/recipe/8584/holiday-chicken-salad/ Processing.. https://www.allrecipes.com/recipe/80867/cran-broccoli-salad/ Message published successfully.Message published successfully.Message published successfully. Process finished with exit code 0

I am using a GUI tool, named as Kafka Tool to browse recently published messages. It is available for OSX, Windows and Linux.

KafkaToolKit in action

Recipe Parser

The next script we are going to write will serve as both consumer and producer. First it will consume data from raw_recipes topic, parse and transform data into JSON and then will publish it in parsed_recipes topic. Below is the code that will fetch HTML data from raw_recipes topic, parse and then feed into parsed_recipes topic.

KafkaConsumer accepts a few parameters beside the topic name and host address. By providing auto_offset_reset='earliest' you are telling Kafka to return messages from the beginning. The parameter consumer_timeout_ms helps the consumer to disconnect after the certain period of time. Once disconnected, you can close the consumer stream by calling consumer.close()

After this, I am using same routines to connect producers and publish parsed data in the new topic. KafaTool browser gives glad tidings about newly stored messages.

So far so good. We stored recipes in both raw and JSON format for later use. Next, we have to write a consumer that will connect with parsed_recipes topic and generate alert if certain calories critera meets.

The JSON is decoded and then check the calories count, a notification is issued once the criteria meet.

Conclusion

Kafka is a scalable, fault-tolerant, publish-subscribe messaging system that enables you to build distributed applications. Due to its high performance and efficiency, it’s getting popular among companies that are producing loads of data from various external sources and want to provide real-time findings from it. I have just covered the gist of it. Do explore the docs and existing implementation and it will help you to understand how it could be the best fit for your next system.

The code is available on Github.

This article originally published here.

Click here to subscribe my newsletter for future posts.","['raw', 'system', 'apache', 'consumer', 'message', 'used', 'python', 'published', 'kafka', 'data', 'started', 'topic', 'getting', 'messages']","Additionally, Kafka connects to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library.
Producers can publish raw data from data sources that later can be used to find trends and pattern.
Producers can publish raw data from data sources that later can be used to find trends and pattern.
/anaconda3/anaconda/bin/python /Development/DataScience/Kafka/kafka-recipie-alert/producer-raw-recipies.pyAccessing listProcessing.. https://www.allrecipes.com/recipe/20762/california-coleslaw/ Processing.. https://www.allrecipes.com/recipe/8584/holiday-chicken-salad/ Processing.. https://www.allrecipes.com/recipe/80867/cran-broccoli-salad/ Message published successfully.Message published successfully.Message published successfully.
Process finished with exit code 0I am using a GUI tool, named as Kafka Tool to browse recently published messages.",en,['Adnan Siddiqi'],2018-09-03 18:28:48.386000+00:00,"{'Distributed Systems', 'Data Science', 'Kafka', 'Python', 'Big Data'}","{'https://miro.medium.com/max/60/1*p3omv-7mRFzPA_ruCAJ6Mg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*GmHD3GdjHV8ad2AzCIA_WA.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/max/768/1*e14VH3__6t7yGr-r-3djZQ.png', 'https://miro.medium.com/max/2138/1*kQXkMQTrMrG4VJ3KZehaqA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1284/1*Pp5vDC3T6OVWMHeWLdiIqA.png', 'https://miro.medium.com/max/1536/1*e14VH3__6t7yGr-r-3djZQ.png', 'https://miro.medium.com/max/4028/1*GmHD3GdjHV8ad2AzCIA_WA.png', 'https://miro.medium.com/max/60/1*kQXkMQTrMrG4VJ3KZehaqA.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/max/60/1*lESE2igQKfpd-QmhG39p0w.png?q=20', 'https://miro.medium.com/max/60/1*48ck-bvatKzEpVapVa4Mag.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Pp5vDC3T6OVWMHeWLdiIqA.png?q=20', 'https://miro.medium.com/max/60/1*e14VH3__6t7yGr-r-3djZQ.png?q=20', 'https://miro.medium.com/max/1244/1*48ck-bvatKzEpVapVa4Mag.png', 'https://miro.medium.com/max/4088/1*p3omv-7mRFzPA_ruCAJ6Mg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1310/1*lESE2igQKfpd-QmhG39p0w.png'}",2020-03-05 00:19:48.063730,2.514219045639038
https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e,Derivative of the Sigmoid function,"Okay, so let’s start deriving the sigmoid function!

So, we want the value of

Step 1

In the above step, I just expanded the value formula of the sigmoid function from (1)

Next, let’s simply express the above equation with negative exponents,

Step 2

Next, we will apply the reciprocal rule, which simply says

Reciprocal Rule

Applying the reciprocal rule, takes us to the next step

Step 3

To clearly see what happened in the above step, replace u(x) in the reciprocal rule with (1 + e^(-x)) .

Next, we need to apply the rule of linearity, which simply says

Rule of Linearity

Applying the rule of linearity, we get

Step 4

Okay, that was simple, now let’s derive each of them one by one.

Now, derivative of a constant is 0, so we can write the next step as

Step 5

And adding 0 to something doesn’t effects so we will be removing the 0 in the next step and moving with the next derivation for which we will require the exponential rule, which simply says

Exponential Rule

Applying the exponential rule we get,

Step 6

Again, to better understand you can simply replace e^u(x) in the exponential rule with e^(-x)

Next, by the rule of linearity we can write

Step 7

Derivative of the differentiation variable is 1, applying which we get

Step 8

Now, we can simply open the second pair of parenthesis and applying the basic rule -1 * -1 = +1 we get

Step 9

which can be written as

Step 10

Okay, we are complete with the derivative!!","['exponential', 'function', 'lets', 'reciprocal', 'derivative', 'simply', 'sigmoid', 'value', 'rule', 'step', 'linearity', 'getstep']","Okay, so let’s start deriving the sigmoid function!
So, we want the value ofStep 1In the above step, I just expanded the value formula of the sigmoid function from (1)Next, let’s simply express the above equation with negative exponents,Step 2Next, we will apply the reciprocal rule, which simply saysReciprocal RuleApplying the reciprocal rule, takes us to the next stepStep 3To clearly see what happened in the above step, replace u(x) in the reciprocal rule with (1 + e^(-x)) .
Next, we need to apply the rule of linearity, which simply saysRule of LinearityApplying the rule of linearity, we getStep 4Okay, that was simple, now let’s derive each of them one by one.
Now, derivative of a constant is 0, so we can write the next step asStep 5And adding 0 to something doesn’t effects so we will be removing the 0 in the next step and moving with the next derivation for which we will require the exponential rule, which simply saysExponential RuleApplying the exponential rule we get,Step 6Again, to better understand you can simply replace e^u(x) in the exponential rule with e^(-x)Next, by the rule of linearity we can writeStep 7Derivative of the differentiation variable is 1, applying which we getStep 8Now, we can simply open the second pair of parenthesis and applying the basic rule -1 * -1 = +1 we getStep 9which can be written asStep 10Okay, we are complete with the derivative!",en,[],2018-07-17 13:00:39.724000+00:00,"{'Data Science', 'Sigmoid', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*2s6GP20LU3ntOnAJQw_wVA.png?q=20', 'https://miro.medium.com/max/1600/1*WGY27ugdjQDMimq_TUIuvQ.jpeg', 'https://miro.medium.com/max/3104/1*KVFF0cIhDIFD8NcmGuhNpA.png', 'https://miro.medium.com/fit/c/96/96/1*raW4DYx6WRnCb1vPBRuiFA.jpeg', 'https://miro.medium.com/max/60/1*YB6AMLYoatOCnh6S5uZHKg.png?q=20', 'https://miro.medium.com/max/800/1*WGY27ugdjQDMimq_TUIuvQ.jpeg', 'https://miro.medium.com/max/60/1*ESDZomVsM9om5nJnjkmg0w.png?q=20', 'https://miro.medium.com/max/60/1*WGY27ugdjQDMimq_TUIuvQ.jpeg?q=20', 'https://miro.medium.com/max/1898/1*I2tGxczJXJkN_P2Bpu9SzQ.png', 'https://miro.medium.com/max/4000/1*JHWL_71qml0kP_Imyx4zBg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1828/1*sqsb9Gd0Pm1FCHF_2_K1gw.png', 'https://miro.medium.com/max/2572/1*kzIjrr0ut_7q2Cy53xv5pQ.png', 'https://miro.medium.com/max/1694/1*35EmrnLtXXmJ6lNyKU5_Qw.png', 'https://miro.medium.com/max/3076/1*1KjMcnmXuP2qP0XMZfq1QA.png', 'https://miro.medium.com/max/3140/1*rbg5bUDhQVW2rA_38xweVw.png', 'https://miro.medium.com/max/1828/1*dKz7wC7Qy17NoG2GVbWvAQ.png', 'https://miro.medium.com/max/60/1*VkPVA5t_QQ8qDiqDEkxgaA.png?q=20', 'https://miro.medium.com/max/4384/1*6A3A_rt4YmumHusvTvVTxw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*RMPuOCUKkBv5BUjGgNXcbw.png?q=20', 'https://miro.medium.com/max/60/1*WEw1O995na59wF6XyUyuxA.png?q=20', 'https://miro.medium.com/max/1628/1*Ccw6o37QgyAm6gVnPTqpQQ.png', 'https://miro.medium.com/max/60/1*JHWL_71qml0kP_Imyx4zBg.png?q=20', 'https://miro.medium.com/max/2528/1*RMPuOCUKkBv5BUjGgNXcbw.png', 'https://miro.medium.com/max/2460/1*2ox-uk3GL-Mtpp61gg8sgg.png', 'https://miro.medium.com/max/60/1*dKz7wC7Qy17NoG2GVbWvAQ.png?q=20', 'https://miro.medium.com/max/2608/1*2s6GP20LU3ntOnAJQw_wVA.png', 'https://miro.medium.com/max/1996/1*VkPVA5t_QQ8qDiqDEkxgaA.png', 'https://miro.medium.com/max/60/1*sqsb9Gd0Pm1FCHF_2_K1gw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*kzIjrr0ut_7q2Cy53xv5pQ.png?q=20', 'https://miro.medium.com/max/60/1*asRaIGQgirrBZb9pp-gvOg.png?q=20', 'https://miro.medium.com/max/60/1*KVFF0cIhDIFD8NcmGuhNpA.png?q=20', 'https://miro.medium.com/max/60/1*35EmrnLtXXmJ6lNyKU5_Qw.png?q=20', 'https://miro.medium.com/max/60/1*1KjMcnmXuP2qP0XMZfq1QA.png?q=20', 'https://miro.medium.com/max/60/1*3D0OqIYc045nHH-IfDWkIQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*raW4DYx6WRnCb1vPBRuiFA.jpeg', 'https://miro.medium.com/max/60/1*2ox-uk3GL-Mtpp61gg8sgg.png?q=20', 'https://miro.medium.com/max/60/1*EpIPa-0TKokJZELpI5ne_w.png?q=20', 'https://miro.medium.com/max/1118/1*WEw1O995na59wF6XyUyuxA.png', 'https://miro.medium.com/max/2514/1*wnWXRiM_0H6X4u9wQdZ57w.png', 'https://miro.medium.com/max/3248/1*ESDZomVsM9om5nJnjkmg0w.png', 'https://miro.medium.com/max/60/1*_q48UXewWgfW0cBfQUyG2g.png?q=20', 'https://miro.medium.com/max/3068/1*3D0OqIYc045nHH-IfDWkIQ.png', 'https://miro.medium.com/max/60/1*6A3A_rt4YmumHusvTvVTxw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1422/1*YB6AMLYoatOCnh6S5uZHKg.png', 'https://miro.medium.com/max/3516/1*EpIPa-0TKokJZELpI5ne_w.png', 'https://miro.medium.com/max/3314/1*asRaIGQgirrBZb9pp-gvOg.png', 'https://miro.medium.com/max/60/1*rbg5bUDhQVW2rA_38xweVw.png?q=20', 'https://miro.medium.com/max/1762/1*_q48UXewWgfW0cBfQUyG2g.png', 'https://miro.medium.com/max/60/1*Ccw6o37QgyAm6gVnPTqpQQ.png?q=20', 'https://miro.medium.com/max/60/1*wnWXRiM_0H6X4u9wQdZ57w.png?q=20', 'https://miro.medium.com/max/60/1*I2tGxczJXJkN_P2Bpu9SzQ.png?q=20'}",2020-03-05 00:19:55.540317,7.4765870571136475
https://towardsdatascience.com/a-practical-guide-to-build-an-enterprise-knowledge-graph-for-investment-analysis-3a15363098b7,A Practical Guide to Build an Enterprise Knowledge Graph for Investment Analysis,"This is an application paper about how to solve challenges when developing an Enterprise Knowledge Graph (EKG) service, which incorporates information about 40,000,000 companies. I find this paper is quite practical useful if someone wants to build an EKG for real business. So I write this summary to save you time. If you want to know the detail, I recommend reading the paper directly. PDF is here.

This EKG service is sold to securities companies. Because the securities companies provide investment bank services and investment consulting services, they have to know the information about the small and medium-sized enterprises. So the product can help securities companies to know and to approach target companies better and quicker.

There are two kinds of challenges in this project, the technology challenges, and business challenges.

Business challenges

There are two challenges on the business side.

- Data Privacy: how to provide deep and useful analysis services without violating the privacies of a company and its employees.

- Killer Services on the Graph: EKG is complex and huge, how to make the graph easy to use is a challenge.

The solution for two challenges.

Data Privacy:

- Transform the original data into the rank form or the ratio form instead of using real accurate values (rank form or the ratio form?)

- Obscure critical nodes (e.g., person-related information) which should not be shown when visualizing the EKG as a graph

Killer Services on the Graph:

- Deliver services that directly meet the business requirements of users. For example, the service finding an enterprise’s real controllers tells the investors from investment banks who are the real owner of a company, and the service enterprise path discovery provides hints on how the investors could reach the enterprises they want to invest in.

Technology challenges

Technology challenges arise from the diversity and the scale of the data sources.

- Constructing problems such as transforming the databases to RDF (D2R), representing and querying difficulties when meta properties and n-ary relations are involved

- Performance issues since the KG contain more than one billion triples

Before introducing challenges in detail, let’s see the whole workflow to build the EKG first.

At the first stage of our project, we mainly utilize relational databases (RDBs) from CSAIC.

Secondly, we supplement the EKG with bidding information from the Chinese Government Procurement Network (CGPN) and stock information from Eastern Wealth Network (EWN).

Then the EKG is fused with the patent information extracted from the Patent Search and Analysis Network of State Intellectual Property Office (PASN-SIPO) in another project.

At last, the competitor relations and acquisition events are added to the EKG. This information is extracted from encyclopedia sites, namely Wikipedia, Baidu Baike and Hudong Baike.

The following challenges are encountered during the above process:

Data Model (Complex data types): meta property (property of relations, or property graph) and event (n-ary relation). But no existing mature solutions on representing and querying meta properties and events in an efficient way.

D2R Mapping: using D2R tools (e.g., D2RQ9) to map RDBs from CSAIC into RDF has the following challenges: a) Mapping of meta property. b) Data in the same column of RDBs map to different classes in RDF. c) Data in the same RDB tables may map to different classes having subClass relations.

Information Extraction: Extract useful relation from various types, like “competitive”, “acquisition” and so on. Entity extraction becomes difficult when there are abbreviations of company names in encyclopedic sites.

Query Performance: We encounter performance bottlenecks since the number of triples of our EGK has reached billions. Furthermore, there are more complex query patterns when the EKG usage scenarios increase: a) When users query the KG on an IPC code, we should find all the

subClasses of the IPC code recursively, then find the patents belong to the subClasses. b) Query all properties of an instance. The problem arises since different properties of the same instance may store as different triples in graph store. c) The queries on meta properties and n-ary relations.

The authors carefully select the most suitable methods and adapt them to the above problems.

First, we split the original tables into atomic tables and complex tables, then we use D2RQ tools to handle mappings on atomic tables. At last, we develop programs to process ad hoc mappings on complex tables. We use multi-strategy learning methods in [16]( Bootstrapping yahoo! finance by Wikipedia for competitor mining) to extract competitor relations and acquisition events from various data sources of encyclopedic sites. We adopt a graph-based entity linking algorithms in [2] (Graph ranking for collective named entity disambiguation) to accomplish the task of entity linking. We design our own storage structure to fully optimize the performance of miscellaneous queries in EKG. We use a hybrid storage solution composed of multiple kinds of databases. For large-scale data, we use NoSQL database namely Mongodb as the underlying storage. For high-frequency query data, we use a memory database to store data.

Approach Overview

Data Sources and Related Tasks to Construct the EKG

Building EGK from multiple sources :Aluminum Corporation of China Limited Example

This graph is an example to show the process of extracting the information of Aluminum Corporation of China Limited Example from multiple sources.

First they use the Aluminum Corporation of China Limited Example data in CSAIC as the basic KG. Then they transform RDBs into RDF to form the basic enterprise KG and get triples like (Aluminum Corporation of China Limited, director, Weiping Xiong).

Secondly, they extract the patent information from a patent website and build a patent KG. The basic enterprise KG is transformed from CSAIC. The two KGs serve different users. So they use data fusion algorithms to link the two KGs with companies and persons.

Finally, they extract stock code from a stock website, corporate executives from infobox of Baidu Baike and Wikipedia, acquisition events from free texts of encyclopedia sites.

Building Knowledge Graphs

Data-driven KG constructing process

There are 5 major steps in the whole constructing process: Schema Design, D2R Transformation, Information Extraction, Data Fusion with Instance Matching, Storage Design and Query Optimization.

1. Schema Design

While most general KGs such as DBpedia and YAGO are built in a bottom-up manner to ensure wide coverage of cross-domain data, the authors adopt a top-down approach in EKG construction to ensure the data quality and stricter schema.

At the first iteration, the EKG includes four basic concepts, namely “Company”, “Person”, “Credit” and “Litigation”. Major relations include “subsidiary”, “shareholder”, and “executive”. The concepts in patent KG only include “Patent”. Major relation is “applicant”. At the second iteration, we add “ListedCompany”, “Stock”, “Bidding” and “Investment” to the EKG.

2. D2R Transformation

The authors take three steps to transform RDBs to RDF, namely table splitting, basic D2R transformation by D2RQ and post-processing.

Table Splitting: As shown in Figure 4, the original table Person Information also contains enterprise information. We divide the table into Person_P, Enterprise_E and Person Enterprise_PE. The Enterprise_E table is furthered merged with the original Enterprise Information table because the two tables share similar information about enterprises.

Basic D2R Transformation by D2RQ: We write a customized mapping file in D2RQ to map fields related to atomic entity tables and atomic relation tables into RDF format. We map table names into classes, columns of the table into properties, and cell values of each record as the corresponding property values for a given entity.

Post Processing: a) Meta property Mapping. The program gives a self-increasing ID annotation to the fact which has meta properties. The meta properties will then be properties of this n-ary relation identify by this ID. Thus we get some new triples(e.g., ). b) Conditional taxonomy mapping. Our program determines whether the entity maps to the subClass according to whether the entity appears in the table related to the subClass. For example, if a company exists in the relation table of company and stock, it implies that the company is a listed company, so we add a triple

3. Information Extraction

The authors adopt a multi-strategy learning method to extract multiple types of data from various data sources. The whole process is as follows:

Entities and attribute value pairs of patent, stock and bidding information are extracted from PSAN-SIPO, EWN and CGPN respectively by using HTML wrappers.

Attribute value pairs (e.g., the chairman of an enterprise ) of enterprises are extracted from infoboxes of encyclopedic sites by using HTML wrappers.

Binary relations, events and synonyms identification on free texts require seeds annotation in sentences to learn patterns.

For the problem with abbreviations of company names, the author use entity linking algorithm to link a company mentioned in the text to companies in the basic EKG. They adopt a graph-based method to accomplish the task of entity linking in two steps:

Candidate detection: finding candidate entities in the KB that are referred by each mention. It deletes the suffix (Corp. Co. Ltd, Group) to calculate the similarity between the core work of the mention and the core word of the entity in KB.

finding candidate entities in the KB that are referred by each mention. It deletes the suffix (Corp. Co. Ltd, Group) to calculate the similarity between the core work of the mention and the core word of the entity in KB. Disambiguation: selecting the most possible candidate to link. Here, we use the disambiguation algorithm proposed in the literature (Graph ranking for collective named entity disambiguation)

4. Data Fusion with Instance Matching

The problem is simple for instance matching of companies. However, the problem is tough for instance matching of persons. While there are personal ID numbers for every person, there are no such IDs in the patent data sources. The authors use a simple heuristic rule to match the person in the patent KG to that person in the basic KG. If the name of the patent inventor and the applicant equals the name of the person and company in the basic KG respectively, they say the patent inventor matches the person name in the basic KG.

5. Storage Design and Query Optimization

The authors use MongoDB as main storage for its large install bases, good query performance, mass data storage, and scalability with clustering support.

Deployment and Usage Scenarios

This section talks about how to make the graph easy to use. The authors pre-defined some queries for some use cases.

Different structures for a person to control an enterprise

Finding an enterprise’s real controllers. The person who owns the biggest equity share is the real decision-maker. But there are many patterns, see the above figure.

Innovative enterprise analysis. See the company’s patent.

Enterprise path discovery. Securities companies would like to know whether there are paths to reach their new customers, and they also want to know whether their potential customers have paths to their competitors.

Multidimensional relationship discovery. Given two companies, there might vary relationships between them.

Finally, I highly recommend reading the paper if you have the need to build a KG for real business.

Reference

Other Knowledge graph-related posts:","['graph', 'information', 'company', 'person', 'companies', 'ekg', 'investment', 'entity', 'practical', 'kg', 'knowledge', 'enterprise', 'data', 'build', 'analysis', 'patent', 'guide']","This is an application paper about how to solve challenges when developing an Enterprise Knowledge Graph (EKG) service, which incorporates information about 40,000,000 companies.
- Killer Services on the Graph: EKG is complex and huge, how to make the graph easy to use is a challenge.
Secondly, they extract the patent information from a patent website and build a patent KG.
Table Splitting: As shown in Figure 4, the original table Person Information also contains enterprise information.
The authors use a simple heuristic rule to match the person in the patent KG to that person in the basic KG.",en,['Xu Liang'],2019-12-02 01:22:41.126000+00:00,"{'Deep Learning', 'Data Science', 'Machine Learning', 'Big Data', 'NLP'}","{'https://miro.medium.com/max/60/1*gGyZdmvO94hUygu40p5Ykw.png?q=20', 'https://miro.medium.com/max/60/1*t8uv4_1hx7UXmIXsqQrFFA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1722/1*ZtBNumIvPNR5ltR9YfJrNw.png', 'https://miro.medium.com/fit/c/160/160/1*pklGZuloGD-bWm7Qb4H2ww.png', 'https://miro.medium.com/max/60/1*1rt-qDA7iYGYppXxcKICWA.jpeg?q=20', 'https://miro.medium.com/max/1812/1*t8uv4_1hx7UXmIXsqQrFFA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*A1LMvdwZnVxEp2oceYSsVg.png?q=20', 'https://miro.medium.com/max/60/1*ZtBNumIvPNR5ltR9YfJrNw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1500/0*UHnmMf2e8U9VIV24.png', 'https://miro.medium.com/max/1200/1*1rt-qDA7iYGYppXxcKICWA.jpeg', 'https://miro.medium.com/max/1738/1*gGyZdmvO94hUygu40p5Ykw.png', 'https://miro.medium.com/max/7046/1*1rt-qDA7iYGYppXxcKICWA.jpeg', 'https://miro.medium.com/max/60/1*7BoiA4MhyOj_OCDy31cPYQ.png?q=20', 'https://miro.medium.com/max/60/0*UHnmMf2e8U9VIV24.png?q=20', 'https://miro.medium.com/max/1704/1*7BoiA4MhyOj_OCDy31cPYQ.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*pklGZuloGD-bWm7Qb4H2ww.png', 'https://miro.medium.com/max/1710/1*A1LMvdwZnVxEp2oceYSsVg.png'}",2020-03-05 00:19:57.845722,2.3034019470214844
https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1,Kafka-Python explained in 10 lines of code,"Although it’s not the newest library Python has to offer, it’s hard to find a comprehensive tutorial on how to use Apache Kafka with Python. By means of approximately ten lines of code, I will explain the foundations of Kafka and it’s interaction with Kafka-Python.

Setting up the environment

First of all you want to have installed Kafka and Zookeeper on your machine. For Windows there is an excellent guide by Shahrukh Aslam, and they definitely exist for other OS’s as well.

Next install Kafka-Python. You can do this using pip or conda, if you’re using an Anaconda distribution.

pip install kafka-python conda install -c conda-forge kafka-python

Don’t forget to start your Zookeeper server and Kafka broker before executing the example code below. In this example we assume that Zookeeper is running default on localhost:2181 and Kafka on localhost:9092.

We are also using a topic called numtest in this example, you can create a new topic by opening a new command prompt, navigating to …/kafka/bin/windows and execute:

kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic numtest

What is Kafka?

Simply put, Kafka is a distributed publish-subscribe messaging system that maintains feeds of messages in partitioned and replicated topics. In the simplest way there are three players in the Kafka ecosystem: producers, topics (run by brokers) and consumers.

Producers produce messages to a topic of their choice. It is possible to attach a key to each message, in which case the producer guarantees that all messages with the same key will arrive to the same partition.

Topics are logs that receive data from the producers and store them across their partitions. Producers always write new messages at the end of the log. In our example we can make abstraction of the partitions, since we’re working locally.

Consumers read the messages of a set of partitions of a topic of their choice at their own pace. If the consumer is part of a consumer group, i.e. a group of consumers subscribed to the same topic, they can commit their offset. This can be important if you want to consume a topic in parallel with different consumers.

The offset is the position in the log where the consumer last consumed or read a message. The consumer can then commit this offset to make the reading ‘official’. Offset committing can be done automatically in the background or explicitly. In our example we will commit automatically in the background.

Let’s code

In our example we’ll create a producer that emits numbers from 1 to 1000 and send them to our Kafka broker. Then a consumer will read the data from the broker and store them in a MongoDb collection.

The advantage of using Kafka is that, if our consumer breaks down, the new or fixed consumer will pick up reading where the previous one stopped. This is a great way to make sure all the data is fed into the database without duplicates or missing data.

Create a new Python script named producer.py and start with importing json, time.sleep and KafkaProducer from our brand new Kafka-Python library.

from time import sleep

from json import dumps

from kafka import KafkaProducer

Then initialize a new Kafka producer. Note the following arguments:

bootstrap_servers=[‘localhost:9092’]: sets the host and port the producer should contact to bootstrap initial cluster metadata. It is not necessary to set this here, since the default is localhost:9092.

value_serializer=lambda x: dumps(x).encode(‘utf-8’): function of how the data should be serialized before sending to the broker. Here, we convert the data to a json file and encode it to utf-8.

producer = KafkaProducer(bootstrap_servers=['localhost:9092'],

value_serializer=lambda x:

dumps(x).encode('utf-8'))

Now, we want to generate numbers from one till 1000. This can be done with a for-loop where we feed each number as the value into a dictionary with one key: number. This is not the topic key, but just a key of our data. Within the same loop we will also send our data to a broker.

This can be done by calling the send method on the producer and specifying the topic and the data. Note that our value serializer will automatically convert and encode the data. To conclude our iteration,we take a 5 second break. If you want to make sure the message is received by the broker, it’s advised to include a callback.

for e in range(1000):

data = {'number' : e}

producer.send('numtest', value=data)

sleep(5)

If you want to test the code, it’s advised to create a new topic and send the data to this new topic. This way, you’ll avoid duplicates and possible confusion in the numtest topic when we’re later testing the producer and consumer together.

Consuming the data

Before we start coding our consumer, create a new file consumer.py and import json.loads, the KafkaConsumer class and MongoClient from pymongo. I won’t dig any deeper in the PyMongo code, since that’s outside the scope of this article.

Furthermore, you can replace the mongo code with any other code. This can be code to feed the data into another database, code to process the data or anything else you can think of. For more information about PyMongo and MongoDb, please consult the documentation.

from kafka import KafkaConsumer

from pymongo import MongoClient

from json import loads

Let’s create our KafkaConsumer and take a closer look at the arguments.

The first argument is the topic, numtest in our case.

bootstrap_servers=[‘localhost:9092’]: same as our producer

auto_offset_reset=’earliest’: one of the most important arguments. It handles where the consumer restarts reading after breaking down or being turned off and can be set either to earliest or latest. When set to latest, the consumer starts reading at the end of the log. When set to earliest, the consumer starts reading at the latest committed offset. And that’s exactly what we want here.

enable_auto_commit=True: makes sure the consumer commits its read offset every interval.

auto_commit_interval_ms=1000ms: sets the interval between two commits. Since messages are coming in every five second, committing every second seems fair.

group_id=’counters’: this is the consumer group to which the consumer belongs. Remember from the introduction that a consumer needs to be part of a consumer group to make the auto commit work.

The value deserializer deserializes the data into a common json format, the inverse of what our value serializer was doing.

consumer = KafkaConsumer(

'numtest',

bootstrap_servers=['localhost:9092'],

auto_offset_reset='earliest',

enable_auto_commit=True,

group_id='my-group',

value_deserializer=lambda x: loads(x.decode('utf-8')))

The code below connects to the numtest collection (a collection is similar to a table in a relational database) of our MongoDb database.

client = MongoClient('localhost:27017')

collection = client.numtest.numtest

We can extract the data from our consumer by looping through it (the consumer is an iterable). The consumer will keep listening until the broker doesn’t respond anymore. A value of a message can be accessed with the value attribute. Here, we overwrite the message with the message value.

The next line inserts the data into our database collection. The last line prints a confirmation that the message was added to our collection. Note that it is possible to add callbacks to all the actions in this loop.

for message in consumer:

message = message.value

collection.insert_one(message)

print('{} added to {}'.format(message, collection))

Testing

Let’s test our two scripts. Open a command prompt and go to the directory where you saved producer.py and consumer.py. Execute producer.py and open a new command prompt. Launch consumer.py and look how it reads all the messages, including the new ones.

Now interrupt the consumer, remember at which number it was (or check it in the database) and restart the consumer. Notice that the consumer picks up all the missed messages and then continues listening for new ones.

Note that if you turn off the consumer within 1 second after reading the message, the message will be retrieved again upon restart. Why? Because our auto_commit_interval is set to 1 second, remember that if the offset is not committed, the consumer will read the message again (if auto_offset_reset is set to earliest).

— Please feel free to bring any inconsistencies or mistakes to my attention in the comments or by leaving a private note. —

Acknowledgements

This article is by no means a complete guide to Kafka or Kafka-Python, but rather a comprehensive teaser that will familiarize you with essential Kafka concepts and how to transform these in useful Python code.

For more advanced topics reading the documentation is advised. If you want to deploy code, it is probably a good idea to take a look at Confluent-Kafka and this post by Russell Jurney.

Sources

Kafka-Python documentation

Consume JSON Messages From Kafka using Kafka-Python’s Deserializer

Apache Kafka documentation

Cloudera Kafka documentation

Putting Apache Kafka To Use: A Practical Guide to Building a Streaming Platform

Introducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client","['reading', 'set', 'consumer', 'message', 'offset', 'kafka', 'lines', 'data', 'kafkapython', 'topic', 'explained', 'messages', 'code']","By means of approximately ten lines of code, I will explain the foundations of Kafka and it’s interaction with Kafka-Python.
pip install kafka-python conda install -c conda-forge kafka-pythonDon’t forget to start your Zookeeper server and Kafka broker before executing the example code below.
Furthermore, you can replace the mongo code with any other code.
This can be code to feed the data into another database, code to process the data or anything else you can think of.
SourcesKafka-Python documentationConsume JSON Messages From Kafka using Kafka-Python’s DeserializerApache Kafka documentationCloudera Kafka documentationPutting Apache Kafka To Use: A Practical Guide to Building a Streaming PlatformIntroducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client",en,['Steven Van Dorpe'],2018-08-13 13:56:11.917000+00:00,"{'Producer', 'Kafka', 'Python', 'Consumer', 'Kafka Python'}","{'https://miro.medium.com/max/60/1*ipMuwhEg-LO6wBCy1jlkpg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1210/1*lLlVG5cFS5rchgPqEN_7bg.png', 'https://miro.medium.com/max/1550/1*bIIzkEI5HSaVkYyxxohK2g.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/775/1*bIIzkEI5HSaVkYyxxohK2g.png', 'https://miro.medium.com/max/60/1*lLlVG5cFS5rchgPqEN_7bg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*DfxV3ExsCTqce22TF3kofQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*bIIzkEI5HSaVkYyxxohK2g.png?q=20', 'https://miro.medium.com/max/1590/1*ipMuwhEg-LO6wBCy1jlkpg.png', 'https://miro.medium.com/fit/c/96/96/1*DfxV3ExsCTqce22TF3kofQ.jpeg'}",2020-03-05 00:20:04.981643,7.135920286178589
https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718,Custom Named Entity Recognition Using spaCy,"Figure 1: Source

What is Named Entity Recognition (NER)?

Named entity recognition (NER) is a sub-task of information extraction (IE) that seeks out and categorises specified entities in a body or bodies of texts. NER is also simply known as entity identification, entity chunking and entity extraction. NER is used in many fields in Artificial Intelligence (AI) including Natural Language Processing (NLP) and Machine Learning.

spaCy for NER

SpaCy is an open-source library for advanced Natural Language Processing in Python. It is designed specifically for production use and helps build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning. Some of the features provided by spaCy are- Tokenization, Parts-of-Speech (PoS) Tagging, Text Classification and Named Entity Recognition.

SpaCy provides an exceptionally efficient statistical system for NER in python, which can assign labels to groups of tokens which are contiguous. It provides a default model which can recognize a wide range of named or numerical entities, which include person, organization, language, event etc. Apart from these default entities, spaCy also gives us the liberty to add arbitrary classes to the NER model, by training the model to update it with newer trained examples.

Getting Started

Installation

SpaCy can be installed using a simple pip install. You will also need to download the language model for the language you wish to use spaCy for.

pip install -U spacy

python -m spacy download en

Let’s begin!

Dataset

The dataset which we are going to work on can be downloaded from here. We will be using the ner_dataset.csv file and train only on 260 sentences.

Figure 2: NER Dataset

The dataset consists of the following tags-

geo = Geographical Entity

org = Organization

per = Person

gpe = Geopolitical Entity

tim = Time indicator

art = Artifact

eve = Event

nat = Natural Phenomenon

The dataset follows a BIO type tagging.

Data Preprocessing

SpaCy requires the training data to be in the the following format-

Figure 3: spaCy Format Training Data (Source)

So we have to convert our data which is in .csv format to the above format. (There are also other forms of training data which spaCy accepts. Refer the documentation for more details.) We first drop the columns Sentence # and POS as we don’t need them and then convert the .csv file to .tsv file. Next, we have to run the script below to get the training data in .json format.

Now the data should look like,

Figure 4: Training Data in Json Format

The next step is to convert the above data into format needed by spaCy. It can be done using the following script-

Now we have the the data ready for training! Let’s train a NER model by adding our custom entities.

Training spaCy NER with Custom Entities

SpaCy NER already supports the entity types like- PERSON People, including fictional. NORP Nationalities or religious or political groups. FAC Buildings, airports, highways, bridges, etc. ORG Companies, agencies, institutions, etc. GPE Countries, cities, states, etc.

Our aim is to further train this model to incorporate for our own custom entities present in our dataset. To do this we have to go through the following steps-

Load the model, or create an empty model using spacy.blank with the ID of desired language. If a blank model is being used, we have to add the entity recognizer to the pipeline. If an existing model is being used, we have to disable all other pipeline components during training using nlp.disable_pipes . This way, only the entity recognizer gets trained.

# Setting up the pipeline and entity recognizer. if model is not None:

nlp = spacy.load(model) # load existing spacy model

print(""Loaded model '%s'"" % model)

else:

nlp = spacy.blank('en') # create blank Language class

print(""Created blank 'en' model"") if 'ner' not in nlp.pipe_names:

ner = nlp.create_pipe('ner')

nlp.add_pipe(ner)

else:

ner = nlp.get_pipe('ner')

2. Add the new entity label to the entity recognizer using the add_label method.

# Add new entity labels to entity recognizer for i in LABEL:

ner.add_label(i) # Inititalizing optimizer if model is None:

optimizer = nlp.begin_training()

else:

optimizer = nlp.entity.create_optimizer()

3. Loop over the examples and call nlp.update , which steps through the words of the input. At each word, it makes a prediction. It then consults the annotations, to see whether it was right. If it was wrong, it adjusts its weights so that the correct action will score higher next time.

# Get names of other pipes to disable them during training to train # only NER and update the weights other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

with nlp.disable_pipes(*other_pipes): # only train NER

for itn in range(n_iter):

random.shuffle(TRAIN_DATA)

losses = {}

batches = minibatch(TRAIN_DATA,

size=compounding(4., 32., 1.001))

for batch in batches:

texts, annotations = zip(*batch)

# Updating the weights

nlp.update(texts, annotations, sgd=optimizer,

drop=0.35, losses=losses)

print('Losses', losses) nlp.update(texts, annotations, sgd=optimizer,

drop=0.35, losses=losses)

print('Losses', losses)

4. Save the trained model using nlp.to_disk .

# Save model

if output_dir is not None:

output_dir = Path(output_dir)

if not output_dir.exists():

output_dir.mkdir()

nlp.meta['name'] = new_model_name # rename model

nlp.to_disk(output_dir)

print(""Saved model to"", output_dir)

5. Test the model to make sure the new entity is recognized correctly.

# Test the saved model

print(""Loading from"", output_dir)

nlp2 = spacy.load(output_dir)

doc2 = nlp2(test_text)

for ent in doc2.ents:

print(ent.label_, ent.text)

Use this script to train and test the model-

Execution instructions-

python spacy_ner_custom_entities.py \

-m=en \

-o=path/to/output/directory \

-n=1000

Results

When tested for the queries- ['John Lee is the chief of CBSE', 'Americans suffered from H5N1'] , the model identified the following entities-

John Lee is the chief of CBSE. B-per John I-per Lee B-org CBSE

Americans suffered from H5N1 virus in 2002. B-gpe Americans B-nat H5N1 B-tim 2002

Conclusion

I hope you have now understood how to train your own NER model on top of the spaCy NER model. Thanks for reading! 😊","['ner', 'recognition', 'custom', 'train', 'entity', 'spacy', 'model', 'data', 'named', 'training', 'language', 'recognizer', 'using']","Some of the features provided by spaCy are- Tokenization, Parts-of-Speech (PoS) Tagging, Text Classification and Named Entity Recognition.
Training spaCy NER with Custom EntitiesSpaCy NER already supports the entity types like- PERSON People, including fictional.
Add the new entity label to the entity recognizer using the add_label method.
# Add new entity labels to entity recognizer for i in LABEL:ner.add_label(i) # Inititalizing optimizer if model is None:optimizer = nlp.begin_training()else:optimizer = nlp.entity.create_optimizer()3.
B-gpe Americans B-nat H5N1 B-tim 2002ConclusionI hope you have now understood how to train your own NER model on top of the spaCy NER model.",en,['Kaustumbh Jaiswal'],2019-04-18 13:30:15.047000+00:00,"{'Spacy', 'Named Entity Recognition', 'Naturallanguageprocessing'}","{'https://miro.medium.com/max/1200/1*HTtQseukwrBiREJf8MSVcA.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*TZWuuZjHDMWArQUCohXWGg.png?q=20', 'https://miro.medium.com/max/2400/1*HTtQseukwrBiREJf8MSVcA.jpeg', 'https://miro.medium.com/max/60/1*SyROr8n7L3Z45UiiRqGUPQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/0*eCsgoxTDmUdwalb8.jpg', 'https://miro.medium.com/max/1216/1*TZWuuZjHDMWArQUCohXWGg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1590/1*SyROr8n7L3Z45UiiRqGUPQ.png', 'https://miro.medium.com/max/1412/1*QXIMTMpUCUmS4CIjHQoM-Q.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*HTtQseukwrBiREJf8MSVcA.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*QXIMTMpUCUmS4CIjHQoM-Q.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*eCsgoxTDmUdwalb8.jpg'}",2020-03-05 00:20:07.300642,2.3179991245269775
https://medium.com/vectrconsulting/build-your-own-knowledge-graph-975cf6dde67f,Build your own Knowledge Graph,"Build your own Knowledge Graph

From unstructured dark data to valuable business insights

Do you have a lot of text documents stored on hard disks or in the cloud, and you don't use its textual information directly in your business? Then this article is for you. Learn how you can leverage artificial intelligence to use that dark data and turn it into valuable business insights, using a Knowledge Graph.

Categorisation of data

Many organisations have large amounts of information contained in free-text documents. Processing these documents often entails categorising the information contained in them. Humans read the documents and label them. At the same time, some metadata is usually added to the documents.

Labels and metadata are then stored in a database, together with a link to the original document. The documents themselves tend to sleep and only kept alive for reference.

After time, when business changes, these documents can not be used in a new context unless they are relabelled and reprocessed, which is cumbersome in a manual procedure.

Automation to the rescue.

Using Natural Language Processing (NLP) techniques, such as topic modelling and named-entity recognition, one can quickly find the important topics and entities buried in the texts.

The list of topics found needs to be labeled to have any meaning. This becomes the basis of a taxonomy [1] or ontology [2] for the business.

According to Wikipedia [2], ""an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains.""

A human with domain knowledge is needed to do this labeling properly and to create an ontology. But it's a quick procedure, the human only judges the found topics, and does not have to read through all the documents.

As an example, we examined a few thousand research applications for government subsidies by public institutions. The figure below lists the top 50 topics found using algorithms implementing term frequency — inverse document frequency (TF-IDF)[3] and non-negative matrix factorisation (NMF)[4].

Some found topics are manually labeled as non-topic because they describe the document or its contexts itself, instead of the business domain that the content is about. Such 'topics' should be further ignored.

Machine Learning

Once we have the topics—the ontology—each new document now can be automatically classified. No need to read through all the documents anymore! It's all automated.

Well, not quite. The topics were found using a limited data set, the training set. For those data we know the topics are OK, because we have human-labeled them via the found topics. However, there is no guarantee the training set is representative of new documents.

The system must continuously learn new topics and slowly changing contexts.

So what do we do if we want to introduce a new topic? Finding topics is one thing, but defining new topics through the ontology is something else. The system now has to learn that certain documents should be labeled with the new label.

We want to avoid introducing a human again.

So let’s start with a training set obtained from somewhere else, say a Wikipedia page about the new topic. In our example case we are looking for all applications in the energy sector.

We start with the Energy page from Wikipedia and define it as the training set. We use a linear support vector machine (SVM) [5] for the model training.

We train the model on N applications and for each application we have a similarity score with the training set. We rank these, and pick out the first M applications, with M < N.

The M applications most similar to the Wikipedia page are then examined by a human, who has to decide whether Energy is indeed a good label or not. This produces a new training set of M applications. This training set is part of the business domain and should therefore be more accurate than the generic Wikipedia page.

This technique reduces the cold start period for machine learning. In fact, our dataset shows that 90% of all Energy-labeled applications are correctly found by the algorithm after just 19% of the documents labeled, when using the Wikipedia page as the initial training set. If Wikipedia is not used, we have to label 27% of the documents before 90% of all Energy-labeled applications are correctly found.

At this point, many projects halt, because the goal of categorisation and labelling is achieved. We take it a step further, though, and link the information retrieved from the unstructured data to the domain model and other data.

This results in what is called a Knowledge Graph.

What is a Knowledge Graph?

Basically, a Knowledge Graph is a bunch of interrelated information, usually limited to a specific business domain, and managed as a graph. The interrelations provide new insights into the business domain.

A more formal definition is given by Paulheim[6]: a Knowledge Graph

describes real-world entities and their interrelations; defines possible classes and relations of entities in a schema; allows for potentially interrelating arbitrary entities with each other; covers various topical domains.

This article shows how you can build a Knowledge Graph for your business domain using existing unstructured data.

Building the Knowledge Graph

All the entities and metadata that belong to the documents can now be linked to the ontology describing the business domain. A natural way to represent these relations is in a graph.

Entities can also be linked to information obtained from elsewhere: legacy databases, open data, etc. This way, the information contained in the documents is augmented by other data.

In our example data, we have millions of nodes and relationships in the graph. We use the well-known native graph database Neo4J [7] to hold the data.

What is shown in this graph is just a small part of the data. The nodes are colour-coded as follows:

Blue: the document as a whole, represented by a unique ID Red: the topics found using the topic modelling Grey: business defined labels that group topics into broader fields Green: the research institutions involved in the research described in the document Yellow: the year a specific document was issued

As one can see, a giant network connecting the nodes exists. Each relationship has a meaning. This becomes clearer when we zoom in on a particular part of the graph.

Here we asked the business question which research institutions from the city of Leuven have applied for subsidies for research related to ""Bacteria"" in the year 2015, and which other topics are related to this research.

Similarly, one can ask questions like which research related to ""Energy"" is a collaboration between the Universities of Ghent and Antwerp. Etcetera.

Having a Knowledge Graph allows business insights that are otherwise hard to get to, with a focus on relationships.

A Generic Solution

Starting with unstructured textual data, applying topic-modelling and NLP techniques, together with machine-learning algorithms results in the building bricks for a full Knowledge Graph.

The ontology is domain-dependent, but the techniques around it are generic by nature, summarised in the picture below.

Team work

This article is the result of a NLP Hackathon, hosted by the Flemish Government 10 & 17 October 2018 and is a collective effort.

Thanks to all who participated in and around the team and all others who supported us and were crucial for the inspiration, which made us win the hackaton in the start-up class.

References","['graph', 'information', 'set', 'topics', 'wikipedia', 'documents', 'knowledge', 'data', 'build', 'business', 'training', 'applications']","Learn how you can leverage artificial intelligence to use that dark data and turn it into valuable business insights, using a Knowledge Graph.
Basically, a Knowledge Graph is a bunch of interrelated information, usually limited to a specific business domain, and managed as a graph.
This article shows how you can build a Knowledge Graph for your business domain using existing unstructured data.
Having a Knowledge Graph allows business insights that are otherwise hard to get to, with a focus on relationships.
A Generic SolutionStarting with unstructured textual data, applying topic-modelling and NLP techniques, together with machine-learning algorithms results in the building bricks for a full Knowledge Graph.",en,['Ignaz Wanders'],2018-10-18 07:49:31.524000+00:00,"{'Artificial Intelligence', 'Vectrconsulting', 'Naturallanguageprocessing', 'Knowledge Graph', 'Neo4j'}","{'https://miro.medium.com/max/2812/1*VKefeG9hWcZCOHdae9OL1A.png', 'https://miro.medium.com/fit/c/96/96/2*fWfvRuD0NOzjdSc3pm22Dg.png', 'https://miro.medium.com/max/3330/1*22vxtjoJ1C39tWd9S0DEzA.png', 'https://miro.medium.com/max/3840/1*RHYMzVHBc33Oamn1grO9Qw.png', 'https://miro.medium.com/max/60/1*pE-wKs0ZNAjmF_V73T7khQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*YdQ9E3fRgqq91JIQ9Gs78Q.jpeg', 'https://miro.medium.com/max/140/1*4BclckZgzwo2r0lrYgcQEQ.jpeg', 'https://miro.medium.com/max/1200/1*pE-wKs0ZNAjmF_V73T7khQ.png', 'https://miro.medium.com/max/3466/1*ImYkwA84RnUzH2jVOorYLg.png', 'https://miro.medium.com/max/60/1*VKefeG9hWcZCOHdae9OL1A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ck_sFTlvHqIaRCezkqQ_OQ.png', 'https://miro.medium.com/fit/c/160/160/2*fWfvRuD0NOzjdSc3pm22Dg.png', 'https://miro.medium.com/max/60/1*22vxtjoJ1C39tWd9S0DEzA.png?q=20', 'https://miro.medium.com/max/60/1*UKzlmPJYIp0fKg6UME6oEg.png?q=20', 'https://miro.medium.com/max/60/1*ImYkwA84RnUzH2jVOorYLg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*7_MLRemIc3RO4qZ3yVFybA.jpeg', 'https://miro.medium.com/max/2892/1*O5gNhtSh9OBTs3Y707uKVQ.png', 'https://miro.medium.com/fit/c/80/80/2*fWfvRuD0NOzjdSc3pm22Dg.png', 'https://miro.medium.com/max/2804/1*UKzlmPJYIp0fKg6UME6oEg.png', 'https://miro.medium.com/max/3056/1*bpjwW87r1W3o9FS-ykPlZg.png', 'https://miro.medium.com/max/3692/1*pE-wKs0ZNAjmF_V73T7khQ.png', 'https://miro.medium.com/max/60/1*RHYMzVHBc33Oamn1grO9Qw.png?q=20', 'https://miro.medium.com/max/60/1*O5gNhtSh9OBTs3Y707uKVQ.png?q=20', 'https://miro.medium.com/max/60/1*bpjwW87r1W3o9FS-ykPlZg.png?q=20'}",2020-03-05 00:20:08.592636,1.2909965515136719
https://medium.com/@marcovillarreal_40011/creating-a-spark-standalone-cluster-with-docker-and-docker-compose-ba9d743a157f,Creating a Spark Standalone Cluster with Docker and docker-compose,"In this post we will cover the necessary steps to create a spark standalone cluster with Docker and docker-compose.

Project Structure

We will be using a very basic project structure as follows:

Project basic structure

The base Images

We will be using some base images to get the job done, these are the images used to create the cluster:

spark-base:2.3.1: A base image based on java:alpine-jdk-8 wich ships scala, python3 and spark 2.3.1, basically we need to download spark and scala packages and configure them in the path, also we need to add python 3(for pyspark).

spark-master:2.3.1: A image based on the previously created spark image, used to create a spark master containers. We just need to configure the basics of the master, configure the master and web UI ports and use a bootstrap script that starts the spark-master service.

spark-worker:2.3.1: A image based on the previously created spark image, used to create spark worker containers. In the same way as the master image we just need to configure the ports to be exposed and the master UI, please notice that we configured spark://spark-master:7077(this is because docker will resolve this name for you internally)

spark-submit:2.3.1: A image based on the previously created spark image, used to create spark submit containers(run, deliver driver and die gracefully).

The compose file

The compose file will contain four services (containers) :

spark-master

spark-worker-1

spark-worker-2

spark-worker-3

These containers are wired by a custom network (10.5.0.0/16), every container has a static ip address from this network to make things easier.

One important thing to notice is that every container has two volume mounts:

/mnt/spark-apps:/opt/spark-apps: It will be used to make application code and configuration available on every worker and master alike.

/mnt/spark-data:/opt/spark-data: It will be used to make application input and output files available on every worker and master alike.

Booth mounts will simulate a distributed file system emulated with Docker volume mounts, it come in handy to make available your application code and files without any effort.

Compile the Images

Before running the compose we need to compile every custom image as follows:

Run the docker-compose

The final step to create your test cluster will be to run the compose file: docker-compose up

Validate your cluster

To validate your cluster just access the spark UI on each worker & master URL.

Spark Master

http://10.5.0.2:8080/

Spark Worker 1

http://10.5.0.3:8081/

Spark Worker 2

http://10.5.0.4:8081/

Spark Worker 3

http://10.5.0.5:8081/

Getting the source

You can get the source and a step by step tutorial on how to deploy the cluster from the scratch.","['dockercompose', 'create', 'cluster', 'used', 'docker', 'need', 'image', 'worker', 'master', 'compose', 'spark', 'creating', 'standalone', 'configure', 'containers']","In this post we will cover the necessary steps to create a spark standalone cluster with Docker and docker-compose.
spark-master:2.3.1: A image based on the previously created spark image, used to create a spark master containers.
We just need to configure the basics of the master, configure the master and web UI ports and use a bootstrap script that starts the spark-master service.
spark-worker:2.3.1: A image based on the previously created spark image, used to create spark worker containers.
Spark Masterhttp://10.5.0.2:8080/Spark Worker 1http://10.5.0.3:8081/Spark Worker 2http://10.5.0.4:8081/Spark Worker 3http://10.5.0.5:8081/Getting the sourceYou can get the source and a step by step tutorial on how to deploy the cluster from the scratch.",en,['Marco Villarreal'],2018-09-23 18:15:15.163000+00:00,"{'Spark', 'Containers', 'Big Data', 'Docker'}","{'https://miro.medium.com/max/2598/1*Iu9JLrWwPiwZsua1915F-g.png', 'https://miro.medium.com/max/60/1*HHQnkdz6Ktoad8weN4-mcg.png?q=20', 'https://miro.medium.com/max/60/1*Pa7PO1v7bANI7C-eHMS_PQ.png?q=20', 'https://miro.medium.com/max/752/1*Pa7PO1v7bANI7C-eHMS_PQ.png', 'https://miro.medium.com/max/2566/1*6XkAXlUoPbv2pwPakGLfHA.png', 'https://miro.medium.com/fit/c/160/160/1*22LNL9E7GZT7fI-M_91tOQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*qeEOGQHypnjz7AR5os_-ew.jpeg', 'https://miro.medium.com/max/376/1*Pa7PO1v7bANI7C-eHMS_PQ.png', 'https://miro.medium.com/max/60/1*Iu9JLrWwPiwZsua1915F-g.png?q=20', 'https://miro.medium.com/max/2610/1*HHQnkdz6Ktoad8weN4-mcg.png', 'https://miro.medium.com/max/1766/1*QHJhTnHIb_T6BbHpWe9RYQ.png', 'https://miro.medium.com/fit/c/96/96/1*22LNL9E7GZT7fI-M_91tOQ.jpeg', 'https://miro.medium.com/max/60/1*2RqAgh4HoR0fiKqvXHoheg.png?q=20', 'https://miro.medium.com/max/2542/1*2RqAgh4HoR0fiKqvXHoheg.png', 'https://miro.medium.com/fit/c/80/80/0*G6BnSvlrlSCiRBCV.jpg', 'https://miro.medium.com/max/60/1*6XkAXlUoPbv2pwPakGLfHA.png?q=20', 'https://miro.medium.com/max/672/1*glD7bNJG3SlO0_xNmSGPcQ.png', 'https://miro.medium.com/max/60/1*glD7bNJG3SlO0_xNmSGPcQ.png?q=20', 'https://miro.medium.com/max/60/1*QHJhTnHIb_T6BbHpWe9RYQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*R2E0EQINvNPTJ8TV-4J4aw.jpeg'}",2020-03-05 00:20:10.104638,1.511000633239746
https://medium.com/@achilleus/learn-docker-to-get-started-with-spark-dd8468e9de5b,Learn Docker to get started with Spark,"A practical introduction to the nuances of Docker.

What is Docker?

Docker is a containerization platform that eases the development, deployment of applications relatively. To put it in a very simplistic way, docker is software that you run to simulate the same environment that your code would run in PROD in your local machine.

Docker on Spark

Before we get started, we need to understand some Docker terminologies.

Registry: It's like the central repo for all your docker images from where you can download the docker image. Docker Hub is one such example. We can also set up a private Docker registry which won’t be publicly available. We can pull ie download an image from or push an image to the registry. image: It is basically a blueprint on what constitutes your Docker container. For example, to deploy a Spark cluster you might wanna start with base Linux, install java and stuff like that. All of these requirements are baked into as an image that can be pulled from the registry or created locally from your Dockerfile. container: Well, as per Docker’s documentation it is A standardized unit of software. It is an instance of an image. Basically, Container is like a lightweight, isolated virtual machine(not exactly but a good analogy). There are a couple of cool features of Linux ie namespace and cgroups that Docker utilizes to provide us an isolated environment to run our Light-weight images. Dockerfile: Its a text file like a script which contains detailed instructions of commands you wanna run, things that you wanna download and stuff like that. We will be writing 1 of these by the end of this article.

Now that we know, some basic definitions. It’s time we ask the main question! Why do I care?

There are many reasons you might wanna use Docker. I will give my perspective on why I started to learn about Docker.

I had to test my Kafka producers and consumers locally instead of deploying my code in DEV/QA even before I was sure things are working fine but also be sure that the same code, when deployed in other environments, should behave the same.

docker run --rm -it -p 2181:2181 -p 3030:3030 -p 8081:8081 -p 8082:8082 -p 8083:8083 -p 9092:9092 -e ADV_HOST=127.0.0.1 landoop/fast-data-dev

Don’t worry about the specifics, we will get into this in the later part of the blog. But a 1 liner will spin up Kafka broker on my local machine with UI. Isn’t it awesome!!

PS: We need the Docker to be installed on our machine obviously...

No more of this …

In this article, we shall try to scratch the surface of your journey about understanding Docker.

Writing your first Docker file.

Let’s try to create a simple Docker image which will have an isolated environment to run your Spark application. This base image can be used to create a multi-node cluster as well.

#ARG ubuntu_version=18.04

#FROM ubuntu:${ubuntu_version} #Use ubuntu 18:04 as your base image

FROM ubuntu:18.04 #Any label to recognise this image.

LABEL image=Spark-base-image ENV SPARK_VERSION=2.4.1

ENV HADOOP_VERSION=2.7 #Run the following commands on my Linux machine

#install the below packages on the ubuntu image RUN apt-get update -qq && \

apt-get install -qq -y gnupg2 wget openjdk-8-jdk scala #Download the Spark binaries from the repo

WORKDIR /

RUN wget --no-verbose http://www.gtlib.gatech.edu/pub/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz # Untar the downloaded binaries , move them the folder name spark and add the spark bin on my class path RUN tar -xzf /spark-2.4.1-bin-hadoop2.7.tgz && \

mv spark-2.4.1-bin-hadoop2.7 spark && \

echo ""export PATH=$PATH:/spark/bin"" >> ~/.bashrc #Expose the UI Port 4040

EXPOSE 4040

So this is a sample docker file. When we build this docker file, it creates an image. We can spin up containers using this image. Also, note that each instruction here creates an intermediate layer of the image and reuses the same layer.

FROM: This is the first line of your docker file(unless if you ARGs). This specifies what is the base image that you would be starting from. Here we say we are going to use vanilla Ubuntu and install our binaries on top of it.

ARG: This can be used if we wanna set any variable when we build the image.

docker build --build-arg ubuntu_version = 18.04

LABEL: As the name suggests, these are just the labels of the image that would be created.

ENV: This is used to set the environment variables within the containers created from this image. Here we use it to just set the SPARK_VERSION.

RUN: This will run the commands specified to build that image. Here, we use this instruction to perform various operations. Also, it is a good practice to have multiline(using \ ) related RUN instructions.

EXPOSE: This will expose the port within the network of the docker containers but won’t be available outside of the network of Docker containers(Multiple docker containers can be hooked up together and EXPOSE can be used for the inter-container communication).EXPOSE does not make the ports of the container accessible to the host. This comes in handy say when we need to open up ports between Spark Master and Workers in a Spark cluster.

We will have to use docker run -p option when we run our container(More on this later).

docker run -p <HOST_PORT>:<CONTAINER:PORT> IMAGE_NAME docker run — rm -dit -p 14040:4040 — name test001 spark-base-image

Note that we are exposing 4040 port of the docker container as 14040 and will be accessible at http://localhost:14040

To build a Docker image

docker build -t spark-base-image ~/home/myDockerFileFo/

This will create an image and tags it as spark-base-image from the above Dockerfile. If we don’t tag it with a specific name, it will be untagged and will have an ImageId as per the build output. Something like Successfully built 955016ee2387

To list all the images available

docker images --format ""table {{.ID}}\t{{.Repository}}\t{{.Tag}}"" IMAGE ID REPOSITORY TAG

d1b494784ab8 <none> <none>

ed7532b3f781 spark-base 2.4

ba3684c184e1 spark spark-docker

b1fc416d936f openjdk latest

c842abf5149c openjdk 12-jdk-oraclelinux7

94e814e2efa8 ubuntu 18.04

We can use docker images command to list all the images available on your machine. We can sort of query on the type of images we need. We can filter, format and lot more using this.

To run a docker image

There are a ton of things that we can do when we run a docker.

docker run --rm -dit -p 14040:4040 — name mySpark spark-base

What we are telling the docker is that run a container called mySpark using the spark-base that we just created. Some of the flags that we pass here are --rm this will remove the docker container when we stop the container, -dit is kind of an important one as if we don’t pass this if our container will just start and stop. -d indicates to run this container in detached mode and -i specifies that the STDIN should be kept open so that we can use it later.

To get inside the container, we can use:

user@myMac:~/home $ docker exec -it test001 bin/bash

root@7b5d8dcbd265:/# ls

bin boot dev etc home lib lib64 media mnt opt proc root run sbin spark spark-2.4.1-bin-hadoop2.7.tgz srv sys tmp usr var

To check all the containers running:

docker ps // List containers running currently

docker ps -a // This will list even the stopped containers

To stop and remove the running container:

docker stop mySpark

docker rm mySpark

To remove an image:

docker rmi imagename

These are some of the basics to get started with Docker. I will write a follow-up article on some more involved concepts in Docker. Happy learning! As always, Thanks for reading! Please do share the article, if you liked it. Any comments or suggestions are welcome! Check out my other articles here.","['wanna', 'docker', 'p', 'image', 'run', 'container', 'learn', 'images', 'build', 'started', 'spark', 'containers']","Registry: It's like the central repo for all your docker images from where you can download the docker image.
docker run --rm -it -p 2181:2181 -p 3030:3030 -p 8081:8081 -p 8082:8082 -p 8083:8083 -p 9092:9092 -e ADV_HOST=127.0.0.1 landoop/fast-data-devDon’t worry about the specifics, we will get into this in the later part of the blog.
Let’s try to create a simple Docker image which will have an isolated environment to run your Spark application.
We will have to use docker run -p option when we run our container(More on this later).
To run a docker imageThere are a ton of things that we can do when we run a docker.",en,[],2019-04-11 04:33:43.987000+00:00,"{'DevOps', 'Programming', 'Data', 'Docker', 'Technology'}","{'https://miro.medium.com/fit/c/80/80/2*UCh_gX_VuUGhBLge9kUu5w.jpeg', 'https://miro.medium.com/fit/c/96/96/1*ocu8-PTrPD6FCI71du9D4Q.jpeg', 'https://miro.medium.com/max/50/1*vY6sm4lZyL_FPUJtVjrFhA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*R_SaUHX5IGCthlpxMOnrmw.png', 'https://miro.medium.com/max/1000/1*wspCxNX29e0zSY5BdVjgxQ.png', 'https://miro.medium.com/max/1892/1*vY6sm4lZyL_FPUJtVjrFhA.png', 'https://miro.medium.com/max/54/1*wspCxNX29e0zSY5BdVjgxQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*qeEOGQHypnjz7AR5os_-ew.jpeg', 'https://miro.medium.com/fit/c/160/160/1*ocu8-PTrPD6FCI71du9D4Q.jpeg', 'https://miro.medium.com/max/946/1*vY6sm4lZyL_FPUJtVjrFhA.png'}",2020-03-05 00:20:11.341669,1.2360327243804932
https://medium.com/@avinash.pasupulate/distributed-data-analysis-environment-with-jupyter-and-apache-spark-cluster-on-docker-b68792e67fae,Distributed Data Analysis Environment with Jupyter and Apache Spark Cluster on Docker,"It’s now time to get our hands dirty. . .

Creating the Dockerfile

We will start by creating our Dockerfile which describes our base image that would be deployed as a container,

We would be using debian as a base for building our spark image

FROM debian:latest

We would then start by setting up on the required tools / packages on the base image

RUN apt-get update && \

apt-get install — no-install-recommends -y python3.5 \

curl \

default-jre \

scala && \

curl

easy_install pip && \ rm -rf /var/lib/apt/lists* && \curl https://bootstrap.pypa.io/ez_setup.py -o — | python3.5 && \easy_install pip && \ pip uninstall tornado && \

pip install tornado==5.1.1\

py4j==0.10.7 \

jupyter \

pyspark

The following lines would install the required dependencies, and the tools required to set them up. And purge the downloaded packages when done.

Note: I have repeated parts of code blocks to explain them

RUN apt-get update && \

apt-get install — no-install-recommends -y python3.5 \

curl \

default-jre \

scala && \

rm -rf /var/lib/apt/lists* && \

We would then instal the required python packages. Remember that pyspark depends on the py4j python package to transfer the tasks to the underlying jvm. Currently the only py4j version 0.10.7 is supported by pyspark. In the latest version of Jupyter (5.7.4), the package tornado (version 6.0) causes issues with working on a notebook, so downgrading tornado to version 5.1.1

Also installing Jupyter to interact with the spark cluster through pyspark, it is not required that we install Jupyter on the spark master and worker images.



easy_install pip && \

pip uninstall tornado && \

pip install tornado==5.1.1\

pip install py4j==0.10.7 \

jupyter \

pyspark curl https://bootstrap.pypa.io/ez_setup.py -o — | python3.5 && \easy_install pip && \pip uninstall tornado && \pip install tornado==5.1.1\pip install py4j==0.10.7 \jupyter \pyspark

Using the below lines we would be downloading and installing spark

#setting up spark RUN rm /bin/sh && ln -s /bin/bash /bin/sh && \

curl -O

tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz && \

mv spark-2.0.0-bin-hadoop2.7 /spark && \

rm spark-2.0.0-bin-hadoop2.7.tgz rm /bin/sh && ln -s /bin/bash /bin/sh && \curl -O http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz && \tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz && \mv spark-2.0.0-bin-hadoop2.7 /spark && \rm spark-2.0.0-bin-hadoop2.7.tgz

Setting up the environment variables to create a reference point for the installed dependencies, remember to set these right.

#setting up environment variables ENV SPARK_HOME=’/spark’

ENV PATH=$SPARK_HOME:$PATH

ENV PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

ENV JAVA_HOME=’/usr/lib/jvm/java-8-openjdk-amd64'

We would then be sourcing the bashrc file for the environmental variables to take effect, also we would create a working directory to launch Jupyter notebook from (creating a working directory is not mandatory). This folder would also be used to mount the shared bind volume.

#sourcing the environment variables RUN source ~/.bashrc && \



#setting working directory mkdir /home/jupyter && \

mkdir /home/jupyter/notebooks WORKDIR /home/jupyter/

Sourcing the environment variables in bashrc file for the environment variable to take effect

#sourcing the environment variables RUN source ~/.bashrc && \

To reduce the size of the resulting docker image, we would be clearing the downloaded packages from using apt-get install

RUN apt-get clean

We would be running Jupyter by default when the container is started, although /bin/sh can be used to run the shell when starting the container to run spark-master and spark-worker nodes

The below line could be excluded and the command to start spark or Jupyter can be run once the container is started in a shell

#Running jupyter by default CMD [“sh”,”-c”,”jupyter notebook --ip 0.0.0.0 --allow-root --no-browser”]

Save the file as “Dockerfile” (without an extension in a folder)

Building Image and Launching Containers

Build the image with a tag of avipasup (username) / jupyter-spark (image name) : latest (image tag) don’t forget the . at the end

Create a directory (I’ve used the dir name “dvol01”) on your host, which would be mounted as a shared bind volume on all containers to store the raw data with access to all the nodes.

Use the terminal to run the below commands

(bash on terminal; user$ — host terminal , sh#-container terminal)

user$ docker build -t avipasup/jupyter-spark:latest .

Create a network to launch the containers into,

user$ docker network create spark_network

Launching Containers

Before launching the containers create a docker network to launch the clusters into a local network.

Jupyter Container

Once the image is built, it can be run to create a container with port mapping

user$ docker run --rm -it -p 8888:8888 --network spark_network -v dvol01:/home/jupyter/notebooks/ avipasup/jupyter-spark:latest

The above command would launch the container and run Jupyter notebook by default, use the resulting url to open the Jupyter notebook environment.

The below line can be append, launch directly into the shell

/bin/sh

Spark Master Container

Launching the image as a container directly into a shell

user$ docker run --rm -it -p 8080:8080 -p 7077:7077 --name spark-master --hostname spark-master --network spark_network -v dvol01:/home/jupyter/notebooks/ avipasup/jupyter-spark:latest /bin/sh

run the below lines on the container terminal

sh.# /spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --webui-port 8080 --port 7077

Note the `` around hostname

After running the code, the below message would appear on the terminal

You can also check its web ui by visiting https://localhost:8080

Spark Worker Container

Launching the image into container directly into a shell

user$ docker run --rm -it --name spark-worker01 --hostname spark-worker01 --network spark_network -v dvol01:/home/jupyter/notebooks/ avipasup/jupyter-spark:latest /bin/sh

Mention the spark-master ip to register the spark-worker

sh.# /spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8080 spark://spark-master:7077

Multiple spark-workers can be launched using the above code, just remember to change the hostname and name to something more suitable e.g. “spark-worker02"" when launching the container.

If there are issues with launching the spark worker check if the network is mentioned before launching the container to communicate with the master node.

Once the clusters are launched, you will find them registered on the spark-master console or at http://localhost:8080

Jupyter Notebooks

Create a new notebook and run the below python code to create a spark session,

(python code)

[1]: from pyspark.sql import SparkSession [2]: spark = SparkSession\

.builder\

.appName(‘pyspark_01’)\

.master(‘spark://spark-master:7077’)\

.config\

(‘spark.submit.deployMode’,’cluster’)\

.getOrCreate()

If the containers have been setup properly, there should be no resulting errors, and the application name (in this case ‘pyspark_01’) would appear on the master ip http://localhost:8080

Also keep an eye on the container console for more specific error messages which would be helpful in debugging.

Now that we have created a spark session let us import other methods from the pyspark mllib

Remember to restart the kernel and clear the output before running the entire notebook again and also kill unfinished applications from the spark-master web ui

NLP using Spark ML lib

For this example we would be working with the SMS spam collection dataset from the UCI Machine Learning Repository.

After importing pyspark and initiating the SparkSession, follow the below instructions,

Note: This is not an NLP article, this is a basic working implementation on spark. Also the code blocks have been modified for readability.

(Jupyter notebook python code)

[3]: ! curl -O https://raw.githubusercontent.com/

avinashpasupulate/pyspark_cluster_and_classifier/

master/SMSSpamCollection [4]: #listing the files in the dir, including the downloaded file

import os

os.listdir(os.getcwd())

Importing the downloaded raw file

[5]: df = spark.read.format('csv').load('SMSSpamCollection',

inferSchema = true, sep = '\t')



#use .show() to display the first 20 lines of the imported data [6]: df.show() #now let us rename the columns to something more understandable [7]: df = df.withColumnRenamed('_c0', 'class')\

.withColumnRenamed('_c1', 'text')

Now that we have the working dataset ready for processing, let us try to extract some basic features from it,

[8]: df = df.withColumn('length', length(df['text'])) #grouping the data to get the average length of 'spam' and 'ham' messages [9]: df.groupBy('class').mean().show()

This gives an average length of spam messages as ~138 characters and ham messages as ~71 characters, which is quite significant.

We would now process the text using some of spark ml text processing methods,

[10]: from pyspark.ml.feature import Tokenizer, StopWordsRemover,

CountVectorizer, IDF, StrignIndexer

#tokenizing and removing stop words from the text [11]: tokenizer = Tokenizer(inputCol = 'text', outputCol =

'token_text') [12]: stopword_remove = StopWordsRemover(inputCol = 'token_text',

outputCol = 'stop_token')

#extracting features from the text [13]: count_vec = CountVectorizer(inputCol = 'stop_token',

outputCol = 'count_vec') [14]: idf = IDF(inputCol = 'count_vec', outputCol = 'tf_idf')

#encoding 'class' (string) variable to a numeric [15]: class_to_numeric = StringIndexer(inputCol = 'class',

outputCol = 'label') [16]: assembler = VectorAssembler(inputCols = ['tf_idf',

'length'], outputCol='features')

Creating a pipeline to run the above processing tasks,

[17]: from pyspark.ml import Pipeline #creating a data cleaning pipelines from all of the above defined stages [18]: proc_pipe = Pipeline(stages = [tokenizer, stopword_remove,

count_vec, idf, class_to_numeric, assembler]) #fitting/transforming the data to the pipeline [19]: clean = proc_pipe.fit(df) [20]: clean_data = clean.transform(df) #selecting only the required columns from the cleaned / transformed dataset [21]: clean_data = clean_data.select(['label', 'features']) [22]: clean_data.show()

We would now create the training and the test datasets,

[23]: train_data, test_data = clean_data.randomSplit([0.7, 0.3])

Now to run a NaiveBayes classifier on the text as an example for classifying spam and non-spam text messages.

[24]: from pyspark.ml.classification import NaiveBayes [25]: nb = NaiveBayes() [26]: spam_classifier = nb.fit(train_data)

Validating using the test dataset,

[27]: test_results = spam_classifier.transform(test_data)

We would now use an evaluator to get the accuracy of the model prediction on the test dataset,

[28]: from pyspark.ml.evaluation import

MulticlassClassificationEvaluator [29]: eval_acc = MulticlassClassificationEvaluator() [30]: acc = eval_acc.evaluate(test_results)

The resulting classification accuracy was ~91% for this sample dataset, this far from what is common in real world scenarios, but since this a test case I would leave it at that. There are other classification methods available in spark MLlib library which can also be used instead of the NaiveBayes method.

You can check the status of different tasks run and resource metrics for the application, by clicking on the application name in the spark-master web ui.

The End. . .

Running docker containers on a laptop may not be ideal for production level deployments, but are great for learning, testing and creating PoCs.

Another great python library that is good for processing large datasets is Dask, which similar to spark uses lazy loading for processing data.

Feedbacks are greatly appreciated, they would help me build further on the above methods and learn more.

You can find the files used in this article on the github repository provided below,

Further Steps to Explore

Use docker swarm to auto scale containers for testing High Availability solutions Deploy containers and run multiple analytics applications on a cluster of SBCs (Odroid XU4s and a Raspberry Pi), which although not as powerful would act as our analytics servers.

A sneak peak of the sbc cluster : https://youtu.be/Y7pvDkZN69I

Reference

— — — —

— — — —

Avinash Pasupulate

https://www.linkedin.com/in/avinashpasupulate/","['rm', 'apache', 'distributed', 'cluster', 'install', 'pip', 'environment', 'docker', 'image', 'notebook', 'container', 'data', 'run', 'spark', 'jupyter', 'analysis']","#setting up environment variables ENV SPARK_HOME=’/spark’ENV PATH=$SPARK_HOME:$PATHENV PYTHONPATH=$SPARK_HOME/python:$PYTHONPATHENV JAVA_HOME=’/usr/lib/jvm/java-8-openjdk-amd64'We would then be sourcing the bashrc file for the environmental variables to take effect, also we would create a working directory to launch Jupyter notebook from (creating a working directory is not mandatory).
Use the terminal to run the below commands(bash on terminal; user$ — host terminal , sh#-container terminal)user$ docker build -t avipasup/jupyter-spark:latest .
Jupyter ContainerOnce the image is built, it can be run to create a container with port mappinguser$ docker run --rm -it -p 8888:8888 --network spark_network -v dvol01:/home/jupyter/notebooks/ avipasup/jupyter-spark:latestThe above command would launch the container and run Jupyter notebook by default, use the resulting url to open the Jupyter notebook environment.
(Jupyter notebook python code)[3]: !
Another great python library that is good for processing large datasets is Dask, which similar to spark uses lazy loading for processing data.",en,['Avinash Pasupulate'],2019-03-06 17:11:04.221000+00:00,"{'Cluster', 'Apache Spark', 'Distributed Computing', 'Jupyter Notebook', 'Docker'}","{'https://miro.medium.com/max/60/0*91ZSk5cTwb7UrniA?q=20', 'https://miro.medium.com/max/60/0*m2-VHv--wjU72DBX.jpg?q=20', 'https://miro.medium.com/max/1936/0*pqNt29tGoRr3ZHLV', 'https://miro.medium.com/fit/c/160/160/1*_ylX3Mu9HEv3TdnxGxsZ0A.png', 'https://miro.medium.com/fit/c/80/80/1*qeEOGQHypnjz7AR5os_-ew.jpeg', 'https://miro.medium.com/max/60/1*kehWrfKAUD3cH26UXUBKHA.png?q=20', 'https://miro.medium.com/max/60/0*wWClmhs_qTJVpeRZ?q=20', 'https://miro.medium.com/max/60/0*VxMi5J2vhSaLxe4m?q=20', 'https://miro.medium.com/max/2000/0*lZ-FXyrUtnBMajro', 'https://miro.medium.com/max/2000/0*Uy47PVpOOlPMHrEr', 'https://miro.medium.com/max/2584/1*kehWrfKAUD3cH26UXUBKHA.png', 'https://miro.medium.com/max/1756/0*TZSoyQLV_thYJi0I', 'https://miro.medium.com/max/2000/0*wWClmhs_qTJVpeRZ', 'https://miro.medium.com/max/60/1*G2X6MSBWk85ARzEEBR5htA.png?q=20', 'https://miro.medium.com/max/2560/0*m2-VHv--wjU72DBX.jpg', 'https://miro.medium.com/max/2000/0*vg8OiDVHhFcR1v6k', 'https://miro.medium.com/max/60/0*Gzv2LJC7IJ6KunZF?q=20', 'https://miro.medium.com/max/3706/1*G2X6MSBWk85ARzEEBR5htA.png', 'https://miro.medium.com/fit/c/80/80/2*iZeOs5YAr0IoXQR6Q-ulpA.jpeg', 'https://miro.medium.com/max/60/0*TZSoyQLV_thYJi0I?q=20', 'https://miro.medium.com/max/2000/0*91ZSk5cTwb7UrniA', 'https://miro.medium.com/max/60/0*vg8OiDVHhFcR1v6k?q=20', 'https://miro.medium.com/max/2000/0*VxMi5J2vhSaLxe4m', 'https://miro.medium.com/max/60/0*pqNt29tGoRr3ZHLV?q=20', 'https://miro.medium.com/fit/c/80/80/1*4DftZU6XNUAH5bwKRJIePA.png', 'https://miro.medium.com/max/1000/0*Uy47PVpOOlPMHrEr', 'https://miro.medium.com/fit/c/96/96/1*_ylX3Mu9HEv3TdnxGxsZ0A.png', 'https://miro.medium.com/max/60/0*Uy47PVpOOlPMHrEr?q=20', 'https://miro.medium.com/max/1600/0*Gzv2LJC7IJ6KunZF', 'https://miro.medium.com/max/60/0*lZ-FXyrUtnBMajro?q=20'}",2020-03-05 00:20:13.656642,2.313988208770752
https://towardsdatascience.com/text-summarization-in-python-76c0a41f0dc4,Text Summarization with Python,"The is the Simple guide to understand Text Summarization problem with Python Implementation.

Table of content

Motivation

Why text summarization is important?

What is Summarization?

Extractive Method

Abstractive Method

Text Summarization using Python

Further Reading

Motivation

To take the appropriate action, we need the latest information.

But on the contrary, the amount of information is more and more growing.

There are many categories of information (economy, sports, health, technology…) and also there are many sources (news site, blog, SNS…).So to make an automatically & accurate summaries feature will helps us to understand the topics and shorten the time to do it.

Why text summarization is important?

Moved by the cutting edge advancement and Innovation, Data is to this century what oil was to the last one. Today, our reality is parachuted by the gathering and dissemination of huge amounts of data.

In fact, the International Data Corporation that the total amount of digital data circulating annually around the world would sprout from 4.4 zettabytes in 2013 to hit 180 zettabytes in 2025. That’s a lot of data!

With such a huge amount of data circulating in the digital space, there is a need to develop algorithms that can automatically shorten large huge texts and summaries that information that can fluently pass the intended messages.

So, What is Summarization?

Basically, we can regard the “summarization” as the “function” its input is document and output is summary. And its input & output type helps us to categorize the multiple summarization tasks.

Single document summarization

[ summary = summarize(document)]

2. Multi-document summarization

[summary = summarize(document_1, document_2, …) ]

We can take the query to add the viewpoint of summarization.

3. Query focused summarization

summary = summarize(document, query)

This type of summarization is called “Query focused summarization” on the contrary to the “Generic summarization”. Especially, a type that set the viewpoint to the “difference” (update) is called “Update summarization”.

5. Update summarization

summary = summarize(document, previous_document_or_summary)

And the “summary” itself has some variety.

6. Indicative summary

It looks like a summary of the book. This summary describes what kinds of the story, but not tell all of the stories especially its ends (so indicative summary has only partial information).

7. Informative summary

In contrast to the indicative summary, the informative summary includes full information of the document.

8. Keyword summary

Not the text, but the words or phrases from the input document.

9. Headline summary

Only one line summary.

Approach

There are mainly two ways to make the summary. Extractive and Abstractive.

Extractive Method

Select relevant phrases of the input document and concatenate them to form a summary (like “copy-and-paste”).

Pros: They are quite robust since they use existing natural-language phrases that are taken straight from the input.

Cons: But they lack in flexibility since they cannot use novel words or connectors. They also cannot paraphrase like people sometimes do.

Now I show some categories of extractive summarization.

Graph Base

The graph base model makes the graph from the document, then summarize it by considering the relation between the nodes (text-unit). TextRank is the typical graph-based method.

TextRank

TextRank is based on PageRank algorithm that is used on Google Search Engine. It's base concept is “The linked page is good, much more if it from many linked page”. The links between the pages are expressed by matrix (like Round-robin table). We can convert this matrix to the transition probability matrix by dividing the sum of links in each page. And the page surfer moves the page according to this matrix.

Feature Base

The feature base model extracts the features of the sentence, then evaluate its importance. Here is the representative research.

Sentence Extraction Based Single Document Summarization

Following features are used in the above method.

Position of the sentence in the input document

Presence of the verb in the sentence

Length of the sentence

Term frequency

Named entity tag NE

Font style

…etc. All the features are accumulated as the score.

The No.of coreferences are the number of pronouns to the previous sentence. It is simply calculated by counting the pronouns occurred in the first half of the sentence. So the Score represents the reference to the previous sentence.

Now we can evaluate each sentence. Next is selecting the sentence to avoid the duplicate of the information. In this paper, the same word between the new and selected sentence is considered. And the refinement to connect the selected sentences are executed.

Luhn’s Algorithm is also feature base. It evaluates the “significance” of the word that is calculated from the frequency.

You can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).

Topic Base

The topic base model calculates the topic of the document and evaluates each sentence by what kinds of topics are included (the “main” topic is highly evaluated when scoring the sentence).

Latent Semantic Analysis (LSA) is usually used to detect the topic. It’s based on SVD (Singular Value Decomposition).

The following paper is a good starting point to overview the LSA(Topic) base summarization.

Text summarization using Latent Semantic Analysis

The simple LSA base sentence selection

There are many variations the way to calculate & select the sentence according to the SVD value. To select the sentence by the topic(=V, eigenvectors/principal axes) and its score is most simple method.

If you want to use LSA, gensim supports it.

Grammer Base

The grammar base model parses the text and constructs a grammatical structure, then select/reorder substructures.

Title Generation with Quasi-Synchronous Grammar

This model can produce meaningful “paraphrase” based on the grammatical structure.

Abstractive Method

Generate a summary that keeps original intent. It’s just like humans do.

Pros: They can use words that were not in the original input. It enables to make more fluent and natural summaries.

Cons: But it is also a much harder problem as you now require the model to generate coherent phrases and connectors.

Extractive & Abstractive is not conflicting ways. You can use both to generate the summary. And there is a way to collaborate with the human.

Aided Summarization

Combines automatic methods with human input.

The computer suggests important information from the document, and the human decide to use it or not. It uses information retrieval, and text mining way.

Encoder-Decoder Model

The encoder-decoder model is composed of encoder and decoder like its name. The encoder converts an input document to a latent representation (vector), and the decoder generates a summary by using it.

Nowadays, the encoder-decoder model that is one of the neural network models is mainly used in machine translation. So this model is also widely used in abstractive summarization model. If you want to try the encoder-decoder summarization model, tensorflow offers the basic model.

Combination Approach

Not only one side of extractive or abstractive, combine them to generate summaries.

Pointer-Generator Network

Combine the extractive and abstractive model by switching probability.

Summarization with Pointer-Generator Networks

This is a hybrid network that can choose to copy words from the source via pointing while retaining the ability to generate words from the fixed vocabulary.

you can find it’s Implementation here

Deep Reinforced Model

It’s a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL).

Text Summarization using Python

Gensim

gensim.summarization offers TextRank summarization

from gensim.summarization.summarizer import summarize

print(summarize(text))

gensim models.lsimodel offers topic model

from gensim.test.utils import common_dictionary, common_corpus

from gensim.models import LsiModel

model = LsiModel(common_corpus, id2word=common_dictionary)

vectorized_corpus = model[common_corpus]

TextTeaser

TextTeaser is an automatic summarization algorithm that combines the power of natural language processing and machine learning to produce good results.

>>> from textteaser import TextTeaser

>>> tt = TextTeaser()

>>> tt.summarize(title, text)

PyTeaser

PyTeaser takes any news article and extracts a brief summary from it

Summaries are created by ranking sentences in a news article according to how relevant they are to the entire text. The top 5 sentences are used to form a “summary”. Each sentence is ranked by using four criteria:

Relevance to the title

Relevance to keywords in the article

The position of the sentence

Length of the sentence

>>> from pyteaser import SummarizeUrl

>>> url = 'http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html'

>>> summaries = SummarizeUrl(url)

>>> print (summaries)

Textrank

pytextrank is the Python implementation of TextRank.

In this Notebook you can find the complete implementation of pytextrank

The core model is the traditional sequence-to-sequence model with attention. It is customized (mostly inputs/outputs) for the text summarization task. The model has been trained on Gigaword dataset and achieved state-of-the-art results (as of June 2016).

Further Reading","['information', 'summarization', 'used', 'python', 'model', 'sentence', 'input', 'document', 'summary', 'text', 'base']","The is the Simple guide to understand Text Summarization problem with Python Implementation.
Why text summarization is important?
Query focused summarizationsummary = summarize(document, query)This type of summarization is called “Query focused summarization” on the contrary to the “Generic summarization”.
You can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).
It is customized (mostly inputs/outputs) for the text summarization task.",en,['Umer Farooq'],2019-03-28 21:32:20.655000+00:00,"{'Text Summarization', 'Artificial Intelligence', 'Python', 'Naturallanguageprocessing', 'Machine Learning'}","{'https://miro.medium.com/max/1890/1*JeH8IkX4iU7oWMt6nnUVFQ.png', 'https://miro.medium.com/max/60/1*rNzXDP8NU-EpGO6ecboVRA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*SIvyiAhcoI6yPrMNasgEbw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*tiQVZEZxHMPcnVmEmN7UtA.jpeg', 'https://miro.medium.com/max/1218/1*h7a1PDkFU8uI8SHKPU3Txw.png', 'https://miro.medium.com/fit/c/160/160/1*EKCh17nCHYK5QbUqJ2zxVQ.jpeg', 'https://miro.medium.com/max/60/1*ScYUIWaj7cog3m24--d8Yg.png?q=20', 'https://miro.medium.com/max/1286/1*ScYUIWaj7cog3m24--d8Yg.png', 'https://miro.medium.com/fit/c/96/96/1*EKCh17nCHYK5QbUqJ2zxVQ.jpeg', 'https://miro.medium.com/max/609/1*h7a1PDkFU8uI8SHKPU3Txw.png', 'https://miro.medium.com/max/1286/1*1BwMlWYa5ewAt96Z-gJ8Yg.png', 'https://miro.medium.com/max/60/1*L9cX64ZzcGqApQXEunX6rw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*6h3MepuhxCvgGIe2vlCcAg.jpeg', 'https://miro.medium.com/max/60/1*1BwMlWYa5ewAt96Z-gJ8Yg.png?q=20', 'https://miro.medium.com/max/60/1*h7a1PDkFU8uI8SHKPU3Txw.png?q=20', 'https://miro.medium.com/max/60/1*JeH8IkX4iU7oWMt6nnUVFQ.png?q=20', 'https://miro.medium.com/max/1402/1*L9cX64ZzcGqApQXEunX6rw.png', 'https://miro.medium.com/max/812/1*m-5TtzA4mtpSPo3wLtfCUw.png', 'https://miro.medium.com/max/50/1*m-5TtzA4mtpSPo3wLtfCUw.png?q=20', 'https://miro.medium.com/max/1338/1*rNzXDP8NU-EpGO6ecboVRA.png'}",2020-03-05 00:20:21.659901,8.002258062362671
https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70,Understand Text Summarization and create your own summarizer in python,"Let’s look at it in action.

The complete text from an article titled Microsoft Launches Intelligent Cloud Hub To Upskill Students In AI & Cloud Technologies

In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, ""With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow."" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.

(source: analyticsindiamag.com)

and the summarized text with 2 lines as an input is

Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning. According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, ""With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset.

As you can see, it does a pretty good job. You can further customized it to reduce to number to character instead of lines.

It is important to understand that we have used textrank as an approach to rank the sentences. TextRank does not rely on any previous training data and can work with any arbitrary piece of text. TextRank is a general purpose graph-based ranking algorithm for NLP.

There are much-advanced techniques available for text summarization. If you are new to it, you can start with an interesting research paper named Text Summarization Techniques: A Brief Survey

Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation is a much more detailed research paper which you can go through for better understanding.

Hope this would have given you a brief overview of text summarization and sample demonstration of code to summarize the text. You can start with the above research papers for advance knowledge and approaches to solve this problem.

The code shown here is available on my GitHub. You can download and play around with it.","['create', 'services', 'program', 'understand', 'microsoft', 'summarization', 'intelligent', 'cloud', 'tools', 'python', 'hub', 'ai', 'build', 'text', 'summarizer']","The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry.
Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public.
This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.
The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.
Hope this would have given you a brief overview of text summarization and sample demonstration of code to summarize the text.",en,['Praveen Dubey'],2018-12-23 03:36:58.520000+00:00,"{'Natural Language', 'Python', 'Machine Learning', 'Towards Data Science', 'Natural Language Process'}","{'https://miro.medium.com/max/60/0*C7rxXyLdAc6H5TLf?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2000/0*C7rxXyLdAc6H5TLf', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*tQfCbYOLk-rcmowmznl_6w.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1000/0*C7rxXyLdAc6H5TLf', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/1*4Tto28edZ_mJzBbuY_JPNQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*4Tto28edZ_mJzBbuY_JPNQ.jpeg', 'https://miro.medium.com/max/1504/1*tQfCbYOLk-rcmowmznl_6w.png'}",2020-03-05 00:20:25.549539,3.8886027336120605
https://towardsdatascience.com/pandas-groupby-aggregate-transform-filter-c95ba3444bbb,Pandas’ groupby explained in detail,"To demonstrate some advanced grouping functionalities, we will use the simplest version of the apply step (and count the rows in each group) via the size method. We do this so that we can focus on the groupby operations.

We will go into much more detail regarding the apply methods in section 2 of the article.

Count rows in each group

IN:

grouped.size() OUT:

Sales Rep

Aaron Hendrickson 292

Adam Sawyer 324

Adele Kimmel 115

Adrian Daugherty 369

Adrianna Shelton 37

...

Willie Lin 44

Willie Rau 95

Willie Sanchez 309

Yvonne Jones 74

Yvonne Lindsey 67

Length: 499, dtype: int64

Group by the first name of sales rep

The following is the first example where we group by a variation of one of the existing columns. I find this is a vast improvement over creating helper columns all the time. It just keeps the data cleaner.

In this example, we use a string accessor to retrieve the first name. You can read up on accessors here.

IN:

df.groupby(

df['Sales Rep'].str.split(' ').str[0]

).size() OUT:

Sales Rep

Aaron 292

Adam 324

Adele 115

Adrian 369

Adrianna 37

...

Wesley 144

Wilbert 213

William 1393 # Plenty of Williams

Willie 448

Yvonne 141

Length: 318, dtype: int64

Grouping by whether or not there is a “William” in the name of the rep

We saw that there seem to be a lot of Williams, lets group all sales reps who have William in their name together.

IN:

df.groupby(

df['Sales Rep'].apply(lambda x: 'William' in x)

).size() OUT:

Sales Rep

False 97111

True 2889

dtype: int64

Group by random series (for illustrative purposes only)

This example is — admittedly — silly, but it illustrates the point that you can group by arbitrary series quite well.

IN:

df.groupby(

pd.Series(np.random.choice(list('ABCDG'),len(df)))

).size() OUT:

A 19895

B 20114

C 19894

D 20108

G 19989

dtype: int64

Grouping by three evenly cut “Val” buckets

In the following example, we apply qcut to a numerical column first. qcut allocates the data equally into a fixed number of bins.

IN:

df.groupby(

pd.qcut(

x=df['Val'],

q=3,

labels=['low','mid','high']

)

).size() OUT:

Val

low 33339

mid 33336

high 33325

dtype: int64

Grouping by custom-sized “Val” buckets

Like in the previous example, we allocate the data to buckets. This time, however, we also specify the bin boundaries.

IN:

df.groupby(

pd.cut(

df['Val'],

[0,3000,5000,7000,10000]

)

).size() OUT:

Val

(0, 3000] 29220

(3000, 5000] 19892

(5000, 7000] 20359

(7000, 10000] 30529

dtype: int64

pd.Grouper

pd.Grouper is important! This one took me way too long to learn, as it is incredibly helpful when working with time-series data.

Grouping by year

In the following example, we are going to use pd.Grouper(key=<INPUT COLUMN>, freq=<DESIRED FREQUENCY>) to group our data based on the specified frequency for the specified column. In our case, the frequency is 'Y' and the relevant column is 'Date' .

IN:

df.groupby(

pd.Grouper(

key='Date',

freq='Y'

)

).size() OUT:

Date

2014-12-31 19956

2015-12-31 20054

2016-12-31 20133

2017-12-31 20079

2018-12-31 19778

Freq: A-DEC, dtype: int64

Grouping by quarter or other frequencies

Instead of 'Y' we can use different standard frequencies like 'D','W','M', or 'Q' . For a list of less common usable frequencies, check out the documentation.

I found 'SM' for semi-month end frequency (15th and end of the month) to be an interesting one.

IN:

df.groupby(pd.Grouper(key='Date',freq='Q')).size() OUT:

Date

2014-03-31 4949

2014-06-30 4948

2014-09-30 4986

2014-12-31 5073

2015-03-31 4958

2015-06-30 4968

2015-09-30 5109

2015-12-31 5019

2016-03-31 5064

2016-06-30 4892

2016-09-30 5148

2016-12-31 5029

2017-03-31 4959

2017-06-30 5102

2017-09-30 5077

2017-12-31 4941

2018-03-31 4889

2018-06-30 4939

2018-09-30 4975

2018-12-31 4975

Freq: Q-DEC, dtype: int64

Grouping by multiple columns

So far, we have only grouped by one column or transformation. The same logic applies when we want to group by multiple columns or transformations. All we have to do is to pass a list to groupby .

IN:

df.groupby(['Sales Rep','Company Name']).size() OUT:

Sales Rep Company Name

Aaron Hendrickson 6-Foot Homosexuals 20

63D House'S 27

Angular Liberalism 28

Boon Blish'S 18

Business-Like Structures 21

..

Yvonne Jones Entry-Limiting Westinghouse 20

Intractable Fairgoers 18

Smarter Java 17

Yvonne Lindsey Meretricious Fabrication 28

Shrill Co-Op 39

Length: 4619, dtype: int64

Random names, I swear!","['column', 'pandas', 'int64grouping', 'frequency', 'dtype', 'example', 'groupby', 'outsales', 'data', 'following', 'group', 'explained', 'william']","We do this so that we can focus on the groupby operations.
IN:df.groupby(pd.Series(np.random.choice(list('ABCDG'),len(df)))).size() OUT:A 19895B 20114C 19894D 20108G 19989dtype: int64Grouping by three evenly cut “Val” bucketsIn the following example, we apply qcut to a numerical column first.
Grouping by yearIn the following example, we are going to use pd.Grouper(key=<INPUT COLUMN>, freq=<DESIRED FREQUENCY>) to group our data based on the specified frequency for the specified column.
IN:df.groupby(pd.Grouper(key='Date',freq='Y')).size() OUT:Date2014-12-31 199562015-12-31 200542016-12-31 201332017-12-31 200792018-12-31 19778Freq: A-DEC, dtype: int64Grouping by quarter or other frequenciesInstead of 'Y' we can use different standard frequencies like 'D','W','M', or 'Q' .
All we have to do is to pass a list to groupby .",en,['Fabian Bosler'],2019-11-13 18:45:33.720000+00:00,"{'Data Science', 'Data Analytics', 'Python', 'Towards Data Science', 'Technology'}","{'https://miro.medium.com/max/60/1*Drwub3OL7kekJ-V921Svkw.png?q=20', 'https://miro.medium.com/max/60/1*MwTlm5i9vL0uNmbgP86eJQ.png?q=20', 'https://miro.medium.com/max/60/1*3yQ0e29L1-ZUy3xJXK9hSw.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*Xw3AiPCiQa72cQEA.', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*xdVezf0wUdENHwU3oFK3lw.png?q=20', 'https://miro.medium.com/max/1212/1*3yQ0e29L1-ZUy3xJXK9hSw.png', 'https://miro.medium.com/max/60/1*ybLDn8JZ0u8ysrIY8pAyDw.jpeg?q=20', 'https://miro.medium.com/max/12000/1*ybLDn8JZ0u8ysrIY8pAyDw.jpeg', 'https://miro.medium.com/max/9216/1*XBETIXH-vrY2xgTxvoSDMQ.jpeg', 'https://miro.medium.com/max/60/1*6d5dw6dPhy4vBp2vRW6uzw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2896/1*wfVSE2X5pc0JUVJjeeBcCg.png', 'https://miro.medium.com/max/60/1*XBETIXH-vrY2xgTxvoSDMQ.jpeg?q=20', 'https://miro.medium.com/max/8512/0*UlhZFIMA35p62wGP', 'https://miro.medium.com/max/1200/1*6d5dw6dPhy4vBp2vRW6uzw.png', 'https://miro.medium.com/max/60/1*lDZgurT8zja-u77DW758KQ.png?q=20', 'https://miro.medium.com/max/3556/1*WTlC_t-5RQqeqS0tcbkCKQ.png', 'https://miro.medium.com/max/3052/1*Drwub3OL7kekJ-V921Svkw.png', 'https://miro.medium.com/max/3548/1*lDZgurT8zja-u77DW758KQ.png', 'https://miro.medium.com/max/60/1*vis_mI0UDZ8elAoSPnApww.png?q=20', 'https://miro.medium.com/max/60/0*UlhZFIMA35p62wGP?q=20', 'https://miro.medium.com/max/60/1*WTlC_t-5RQqeqS0tcbkCKQ.png?q=20', 'https://miro.medium.com/max/60/1*fxdhr6i-jlzeex7f5RkIlw.png?q=20', 'https://miro.medium.com/max/3100/1*fxdhr6i-jlzeex7f5RkIlw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*wfVSE2X5pc0JUVJjeeBcCg.png?q=20', 'https://miro.medium.com/max/2952/1*xdVezf0wUdENHwU3oFK3lw.png', 'https://miro.medium.com/max/2736/1*vis_mI0UDZ8elAoSPnApww.png', 'https://miro.medium.com/fit/c/160/160/0*Xw3AiPCiQa72cQEA.', 'https://miro.medium.com/max/1296/1*MwTlm5i9vL0uNmbgP86eJQ.png', 'https://miro.medium.com/max/2550/1*6d5dw6dPhy4vBp2vRW6uzw.png'}",2020-03-05 00:20:32.417725,6.868186712265015
https://towardsdatascience.com/workflow-tools-for-model-pipelines-45030a93e9e0,Workflow Tools for ML Pipelines,"Workflow Tools for ML Pipelines

Chapter 5 excerpt of “Data Science in Production”

Airflow is becoming the industry standard for authoring data engineering and model pipeline workflows. This chapter of my book explores the process of taking a simple pipeline that runs on a single EC2 instance to a fully-managed Kubernetes ecosystem responsible for scheduling tasks. This posts omits the sections on the fully-managed solutions with GKE and Cloud Composer.

Model pipelines are usually part of a broader data platform that provides data sources, such as lakes and warehouses, and data stores, such as an application database. When building a pipeline, it’s useful to be able to schedule a task to run, ensure that any dependencies for the pipeline have already completed, and to backfill historic data if needed. While it’s possible to perform these types of tasks manually, there are a variety of tools that have been developed to improve the management of data science workflows.

In this chapter, we’ll explore a batch model pipeline that performs a sequence of tasks in order to train and store results for a propensity model. This is a different type of task than the deployments we’ve explored so far, which have focused on serving real-time model predictions as a web endpoint. In a batch process, you perform a set of operations that store model results that are later served by a different application. For example, a batch model pipeline may predict which users in a game are likely to churn, and a game server fetches predictions for each user that starts a session and provides personalized offers.

When building batch model pipelines for production systems, it’s important to make sure that issues with the pipeline are quickly resolved. For example, if the model pipeline is unable to fetch the most recent data for a set of users due to an upstream failure with a database, it’s useful to have a system in place that can send alerts to the team that owns the pipeline and that can rerun portions of the model pipeline in order to resolve any issues with the prerequisite data or model outputs.

Workflow tools provide a solution for managing these types of problems in model pipelines. With a workflow tool, you specify the operations that need to be completed, identify dependencies between the operations, and then schedule the operations to be performed by the tool. A workflow tool is responsible for running tasks, provisioning resources, and monitoring the status of tasks. There’s a number of open source tools for building workflows including AirFlow, Luigi, MLflow, and Pentaho Kettle. We’ll focus on Airflow, because it is being widely adopted across companies and cloud platforms and are also providing fully-managed versions of Airflow.

In this chapter, we’ll build a batch model pipeline that runs as a Docker container. Next, we’ll schedule the task to run on an EC2 instance using cron, and then explore a managed version of cron using Kubernetes. In the third section, we’ll use Airflow to define a graph of operations to perform in order to run our model pipeline, and explore a cloud offering of Airflow.

5.1 SKLearn Workflow

A common workflow for batch model pipelines is to extract data from a data lake or data warehouse, train a model on historic user behavior, predict future user behavior for more recent data, and then save the results to a data warehouse or application database. In the gaming industry, this is a workflow I’ve seen used for building likelihood to purchase and likelihood to churn models, where the game servers use these predictions to provide different treatments to users based on the model predictions. Usually libraries like sklearn are used to develop models, and languages such as PySpark are used to scale up to the full player base.

It is typical for model pipelines to require other ETLs to run in a data platform before the pipeline can run on the most recent data. For example, there may be an upstream step in the data platform that translates json strings into schematized events that are used as input for a model. In this situation, it might be necessary to rerun the pipeline on a day that issues occurred with the json transformation process. For this section, we’ll avoid this complication by using a static input data source, but the tools that we’ll explore provide the functionality needed to handle these issues.

There’s typically two types of batch model pipelines that can I’ve seen deployed in the gaming industry:

Persistent: A separate training workflow is used to train models from the one used to build predictions. A model is persisted between training runs and loaded in the serving workflow.

A separate training workflow is used to train models from the one used to build predictions. A model is persisted between training runs and loaded in the serving workflow. Transient: The same workflow is used for training and serving predictions, and instead of saving the model as a file, the model is rebuilt for each run.

In this section we’ll build a transient batch pipeline, where a new model is retrained with each run. This approach generally results in more compute resources being used if the training process is heavyweight, but it helps avoid issues with model drift, which we’ll discuss in Chapter 11. We’ll author a pipeline that performs the following steps:

Fetches a dataset from GitHub Trains a logistic regression model Applies the regression model Saves the results to BigQuery

The pipeline will execute as a single Python script that performs all of these steps. For situations where you want to use intermediate outputs from steps across multiple tasks, it’s useful to decompose the pipeline into multiple processes that are integrated through a workflow tool such as Airflow.

We’ll build this script by first writing a Python script that runs on an EC2 instance, and then Dockerize the script so that we can use the container in workflows. To get started, we need to install a library for writing a Pandas data frame to BigQuery:

pip install --user pandas_gbq

Next, we’ll create a file called pipeline.py that performs the four pipeline steps identified above.. The script shown below performs these steps by loading the necessary libraries, fetching the CSV file from GitHub into a Pandas data frame, splits the data frame into train and test groups to simulate historic and more recent users, builds a logistic regression model using the training data set, creates predictions for the test data set, and saves the resulting data frame to BigQuery.

import pandas as pd

import numpy as np

from google.oauth2 import service_account

from sklearn.linear_model import LogisticRegression

from datetime import datetime

import pandas_gbq # fetch the data set and add IDs

gamesDF = pd.read_csv(""https://github.com/bgweber/Twitch/raw/

master/Recommendations/games-expand.csv"")

gamesDF['User_ID'] = gamesDF.index

gamesDF['New_User'] = np.floor(np.random.randint(0, 10,

gamesDF.shape[0])/9) # train and test groups

train = gamesDF[gamesDF['New_User'] == 0]

x_train = train.iloc[:,0:10]

y_train = train['label']

test = gameDF[gamesDF['New_User'] == 1]

x_test = test.iloc[:,0:10] # build a model

model = LogisticRegression()

model.fit(x_train, y_train)

y_pred = model.predict_proba(x_test)[:, 1] # build a predictions data frame

resultDF = pd.DataFrame({'User_ID':test['User_ID'], 'Pred':y_pred})

resultDF['time'] = str(datetime. now()) # save predictions to BigQuery

table_id = ""dsp_demo.user_scores""

project_id = ""gameanalytics-123""

credentials = service_account.Credentials.

from_service_account_file('dsdemo.json')

pandas_gbq.to_gbq(resultDF, table_id, project_id=project_id,

if_exists = 'replace', credentials=credentials)

To simulate a real-world data set, the script assigns a User_ID attribute to each record, which represents a unique ID to track different users in a system. The script also splits users into historic and recent groups by assigning a New_User attribute. After building predictions for each of the recent users, we create a results data frame with the user ID, the model predictIon, and a timestamp. It’s useful to apply timestamps to predictions in order to determine if the pipeline has completed successfully. To test the model pipeline, run the following statements on the command line:

export GOOGLE_APPLICATION_CREDENTIALS=

/home/ec2-user/dsdemo.json

python3 pipeline.py

If successfully, the script should create a new data set on BigQuery called dsp_demo , create a new table called user_users , and fill the table with user predictions. To test if data was actually populated in BigQuery, run the following commands in Jupyter:

from google.cloud import bigquery

client = bigquery.Client() sql = ""select * from dsp_demo.user_scores""

client.query(sql).to_dataframe().head()

This script will set up a client for connecting to BigQuery and then display the result set of the query submitted to BigQuery. You can also browse to the BigQuery web UI to inspect the results of the pipeline, as shown in Figure 5.1. We now have a script that can fetch data, apply a machine learning model, and save the results as a single process.

FIGURE 5.1: Querying the uploaded predictions in BigQuery.

With many workflow tools, you can run Python code or bash scripts directly, but it’s good to set up isolated environments for executing scripts in order to avoid dependency conflicts for different libraries and runtimes. Luckily, we explored a tool for this in Chapter 4 and can use Docker with workflow tools. It’s useful to wrap Python scripts in Docker for workflow tools, because you can add libraries that may not be installed on the system responsible for scheduling, you can avoid issues with Python version conflicts, and containers are becoming a common way of defining tasks in workflow tools.

To containerize our workflow, we need to define a Dockerfile, as shown below. Since we are building out a new Python environment from scratch, we’ll need to install Pandas, sklearn, and the BigQuery library. We also need to copy credentials from the EC2 instance into the container so that we can run the export command for authenticating with GCP. This works for short term deployments, but for longer running containers it’s better to run the export in the instantiated container rather than copying static credentials into images. The Dockerfile lists out the Python libraries needed to run the script, copies in the local files needed for execution, exports credentials, and specifies the script to run.

FROM ubuntu:latest

MAINTAINER Ben Weber RUN apt-get update \

&& apt-get install -y python3-pip python3-dev \

&& cd /usr/local/bin \

&& ln -s /usr/bin/python3 python \

&& pip3 install pandas \

&& pip3 install sklearn \

&& pip3 install pandas_gbq



COPY pipeline.py pipeline.py

COPY /home/ec2-user/dsdemo.json dsdemo.json RUN export GOOGLE_APPLICATION_CREDENTIALS=/dsdemo.json ENTRYPOINT [""python3"",""pipeline.py""]

Before deploying this script to production, we need to build an image from the script and test a sample run. The commands below show how to build an image from the Dockerfile, list the Docker images, and run an instance of the model pipeline image.

sudo docker image build -t ""sklearn_pipeline"" .

sudo docker images

sudo docker run sklearn_pipeline

After running the last command, the containerized pipeline should update the model predictions in BigQuery. We now have a model pipeline that we can run as a single bash command, which we now need to schedule to run at a specific frequency. For testing purposes, we’ll run the script every minute, but in practice models are typically executed hourly, daily, or weekly.

5.2 Cron

A common requirement for model pipelines is running a task at a regular frequency, such as every day or every hour. Cron is a utility that provides scheduling functionality for machines running the Linux operating system. You can Set up a scheduled task using the crontab utility and assign a cron expression that defines how frequently to run the command. Cron jobs run directly on the machine where cron is utilized, and can make use of the runtimes and libraries installed on the system.

There are a number of challenges with using cron in production-grade systems, but it’s a great way to get started with scheduling a small number of tasks and it’s good to learn the cron expression syntax that is used in many scheduling systems. The main issue with the cron utility is that it runs on a single machine, and does not natively integrate with tools such as version control. If your machine goes down, then you’ll need to recreate your environment and update your cron table on a new machine.

A cron expression defines how frequently to run a command. It is a sequence of 5 numbers that define when to execute for different time granularities, and it can include wildcards to always run for certain time periods. A few sample expresions are shown in the snippet below:

# run every minute

* * * * * # Run at 10am UTC everyday

0 10 * * * # Run at 04:15 on Saturday

15 4 * * 6

When getting started with cron, it’s good to use tools to validate your expressions. Cron expressions are used in Airflow and many other scheduling systems.

We can use cron to schedule our model pipeline to run on a regular frequency. To schedule a command to run, run the following command on the console:

crontab -e

This command will open up the cron table file for editing in vi . To schedule the pipeline to run every minute, add the following commands to the file and save.

# run every minute

* * * * * sudo docker run sklearn_pipeline

After exiting the editor, the cron table will be updated with the new command to run. The second part of the cron statement is the command to run. when defining the command to run, it’s useful to include full file paths. With Docker, we just need to define the image to run. To check that the script is actually executing, browse to the BigQuery UI and check the time column on the user_scores model output table.

We now have a utility for scheduling our model pipeline on a regular schedule. However, if the machine goes down then our pipeline will fail to execute. To handle this situation, it’s good to explore cloud offerings with cron scheduling capabilities.

5.3 Workflow Tools

Cron is useful for simple pipelines, but runs into challenges when tasks have dependencies on other tasks which can fail. To help resolve this issue, where tasks have dependencies and only portions of a pipeline need to be rerun, we can leverage workflow tools. Apache Airflow is currently the most popular tool, but other open source projects are available and provide similar functionality including Luigi and MLflow.

There are a few situations where workflow tools provide benefits over using cron directly:

Dependencies: Workflow tools define graphs of operations, which makes dependencies explicit.

Workflow tools define graphs of operations, which makes dependencies explicit. Backfills: It may be necessary to run an ETL on old data, for a range of different dates.

It may be necessary to run an ETL on old data, for a range of different dates. Versioning: Most workflow tools integrate with version control systems to manage graphs.

Most workflow tools integrate with version control systems to manage graphs. Alerting: These tools can send out emails or generate PageDuty alerts when failures occur.

Workflow tools are particularly useful in environments where different teams are scheduling tasks. For example, many game companies have data scientists that schedule model pipelines which are dependent on ETLs scheduled by a seperate engineering team.

In this section, we’ll schedule our task to run an EC2 instance using hosted Airflow, and then explore a fully-managed version of Airflow on GCP.

5.3.1 Apache Airflow

Airflow is an open source workflow tool that was originally developed by Airbnb and publically released in 2015. It helps solve a challenge that many companies face, which is scheduling tasks that have many dependencies. One of the core concepts in this tool is a graph that defines the tasks to perform and the relationships between these tasks.

In Airflow, a graph is referred to as a DAG, which is an acronym for directed acyclic graph. A DAG is a set of tasks to perform, where each task has zero or more upstream dependencies. One of the constraints is that cycles are not allowed, where two tasks have upstream dependencies on each other.

DAGs are set up using Python code, which is one of the differences from other workflow tools such as Pentaho Kettle which is GUI focused. The Airflow approach is called “configuration as code”, because a Python script defines the operations to perform within a workflow graph. Using code instead of a GUI to configure workflows is useful because it makes it much easier to integrate with version control tools such as GitHub.

To get started with Airflow, we need to install the library, initialize the service, and run the scheduler. To perform these steps, run the following commands on an EC2 instance or your local machine:

export AIRFLOW_HOME=~/airflow

pip install --user apache-airflow

airflow initdb

airflow scheduler

Airflow also provides a web frontend for managing DAGs that have been scheduled. To start this service, run the following command in a new terminal on the same machine.

airflow webserver -p 8080

This command tells Airflow to start the web service on port 8080. You can open a web browser at this port on your machine to view the web frontend for Airflow, as shown in Figure 5.3.

FIGURE 5.3: The Airflow web app running on an EC2 instance.

Airflow comes preloaded with a number of example DAGs. For our model pipeline we’ll create a new DAG and then notify Airflow of the update. We’ll create a file called sklearn.py with the following DAG definition:

from airflow import DAG

from airflow.operators.bash_operator import BashOperator

from datetime import datetime, timedelta default_args = {

'owner': 'Airflow',

'depends_on_past': False,

'email': 'bgweber@gmail.com',

'start_date': datetime(2019, 11, 1),

'email_on_failure': True,

} dag = DAG('games', default_args=default_args,

schedule_interval=""* * * * *"") t1 = BashOperator(

task_id='sklearn_pipeline',

bash_command='sudo docker run sklearn_pipeline',

dag=dag)

There’s a few steps in this Python script to call out. The script uses a Bash operator to define the action to perform. The Bash operator is defined as the last step in the script, which specifies the command to perform. The DAG is instantiated with a number of input arguments that define the workflow settings, such as who to email when the task fails. A cron expression is passed to the DAG object to define the schedule for the task, and the DAG object is passed to the Bash operator to associate the task with this graph of operations.

Before adding the DAG to airflow, it’s useful to check for syntax errors in your code. We can run the following command from the terminal to check for issues with the DAG:

python3 sklearn.py

This command will not run the DAG, but will flag any syntax errors present in the script. To update Airflow with the new DAG file, run the following command:

airflow list_dags -------------------------------------------------------------------

DAGS

-------------------------------------------------------------------

games

This command will add the DAG to the list of workflows in Airflow. To view the list of DAGs, navigate to the Airflow web server, as shown in Figure 5.4. The web server will show the schedule of the DAG, and provide a history of past runs of the workflow. To check that the DAG is actually working, browse to the BigQuery UI and check for fresh model outputs.

FIGURE 5.4: The sklearn DAG scheduled on Airflow.

We now have an Airflow service up and running that we can use to monitor the execution of our workflows. This setup enables us to track the execution of workflows, backfill any gaps in data sets, and enable alerting for critical workflows.

Airflow supports a variety of operations, and many companies author custom operators for internal usage. In our first DAG, we used the Bash operator to define the task to execute, but other options are available for running Docker images, including the Docker operator. The code snippet below shows how to change our DAG to use the Docker operator instead of the Bash operator.

from airflow.operators.docker_operator import DockerOperator t1 = DockerOperator(

task_id='sklearn_pipeline',

image='sklearn_pipeline',

dag=dag)

The DAG we defined does not have any dependencies, since the container performs all of the steps in the model pipeline. If we had a dependency, such as running a sklearn_etl container before running the model pipeline, we can use the set_upstrean command as shown below. This configuration sets up two tasks, where the pipeline task will execute after the etl task completes.

t1 = BashOperator(

task_id='sklearn_etl',

bash_command='sudo docker run sklearn_etl',

dag=dag) t2 = BashOperator(

task_id='sklearn_pipeline',

bash_command='sudo docker run sklearn_pipeline',

dag=dag) t2.set_upstream(t1)

Airflow provides a rich set of functionality and we’ve only touched the surface of what the tool provides. While we were already able to schedule the model pipeline with hosted and managed cloud offerings, it’s useful to schedule the task through Airflow for improved monitoring and versioning. The landscape of workflow tools will change over time, but many of the concepts of Airflow will translate to these new tools.

5.4 Conclusion

In this chapter we explored a batch model pipeline for applying a machine learning model to a set of users and storing the results to BigQuery. To make the pipeline portable, so that we can execute it in different environments, we created a Docker image to define the required libraries and credentials for the pipeline. We then ran the pipeline on an EC2 instance using batch commands, cron, and Airflow. We also used GKE and Cloud Composer to run the container via Kubernetes.

Workflow tools can be tedious to set up, especially when installing a cluster deployment, but they provide a number of benefits over manual approaches. One of the key benefits is the ability to handle DAG configuration as code, which enables code reviews and version control for workflows. It’s useful to get experience with configuration as code, because it is an introduction to another concept called “infra as code” that we’ll explore in Chapter 10.","['tools', 'script', 'workflow', 'ml', 'cron', 'model', 'data', 'airflow', 'run', 'pipelines', 'dag', 'pipeline']","Workflow Tools for ML PipelinesChapter 5 excerpt of “Data Science in Production”Airflow is becoming the industry standard for authoring data engineering and model pipeline workflows.
Workflow tools provide a solution for managing these types of problems in model pipelines.
It’s useful to wrap Python scripts in Docker for workflow tools, because you can add libraries that may not be installed on the system responsible for scheduling, you can avoid issues with Python version conflicts, and containers are becoming a common way of defining tasks in workflow tools.
There are a few situations where workflow tools provide benefits over using cron directly:Dependencies: Workflow tools define graphs of operations, which makes dependencies explicit.
The landscape of workflow tools will change over time, but many of the concepts of Airflow will translate to these new tools.",en,['Ben Weber'],2019-11-11 17:12:46.084000+00:00,"{'Programming', 'Machine Learning', 'Python', 'Data Science'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*ydFRQ_f2OauuinRCvVXgEw.png?q=20', 'https://miro.medium.com/max/2732/1*UR5cfIZTo7IK2iXiDBnETQ.png', 'https://miro.medium.com/max/60/1*iWqdUjkY_LOdWsxd3nxWpA.png?q=20', 'https://miro.medium.com/max/2568/1*BMf9Y9wV7U9sXVEo_9yigg.png', 'https://miro.medium.com/max/2170/1*ydFRQ_f2OauuinRCvVXgEw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*UR5cfIZTo7IK2iXiDBnETQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*k5U0JTbKUqtoJBMU.', 'https://miro.medium.com/fit/c/96/96/0*k5U0JTbKUqtoJBMU.', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2606/1*iWqdUjkY_LOdWsxd3nxWpA.png', 'https://miro.medium.com/max/60/1*BMf9Y9wV7U9sXVEo_9yigg.png?q=20', 'https://miro.medium.com/max/1200/1*BMf9Y9wV7U9sXVEo_9yigg.png'}",2020-03-05 00:20:34.554195,2.135468006134033
https://towardsdatascience.com/everything-you-need-to-know-about-regular-expressions-8f622fe10b03,Everything you need to know about Regular Expressions,"After reading this article you will have a solid understanding of what regular expressions are, what they can do, and what they can’t do.

You’ll be able to judge when to use them and — more importantly — when not to.

Let’s start at the beginning.

What is a Regular Expression?

On an abstract level a regular expression, regex for short, is a shorthand representation for a set. A set of strings.

Say we have a list of all valid zip codes. Instead of keeping that long and unwieldy list around, it’s often more practical to have a short and precise pattern that completely describes that set. Whenever you want to check whether a string is a valid zip code, you can match it against the pattern. You’ll get a true or false result indicating whether the string belongs to the set of zip codes the regex pattern represents.

Let’s expand on the set of zip codes. A list of zip codes is finite, consists of rather short strings, and is not particularly challenging computationally.

What about the set of strings that end in .csv ? Can be quite useful when looking for data files. This set is infinite. You can’t make a list up front. And the only way to test for membership is to go to the end of the string and compare the last four characters. Regular expressions are a way of encoding such patterns in a standardized way.

Regular expressions match sets of strings. Photo by Kristian Strand on Unsplash

The following is a regular expression pattern that represents our set of strings ending in .csv

^.*\.csv$

Let’s leave the mechanics of this particular pattern aside, and look at practicalities: a regex engine can test a pattern against an input string to see if it matches. The above pattern matches foo.csv , but does not match bar.txt or my_csv_file .

Before you use regular expressions in your code, you can test them using an online regex evaluator, and experiment with a friendly UI.

I like regex101.com: you can pick the flavor of the regex engine, and patterns are nicely decomposed for you, so you get a good understanding of what your pattern actually does. Regex patterns can be cryptic.

I’d recommend you open regex101.com in another window or tab and experiment with the examples presented in this article interactively. You’ll get a much better feel for regex patterns this way, I promise.

debugging regular expressions

What are Regular Expressions used for?

Regular expressions are useful in any scenario that benefits from full or partial pattern matches on strings. These are some common use cases:

verify the structure of strings

extract substrings form structured strings

search / replace / rearrange parts of the string

split a string into tokens

All of these come up regularly when doing data preparation work.

The Building Blocks of a Regular Expression

A regular expression pattern is constructed from distinct building blocks. It may contain literals, character classes, boundary matchers, quantifiers, groups and the OR operator.

Let’s dive in and look at some examples.

Literals

The most basic building block in a regular expression is a character a.k.a. literal. Most characters in a regex pattern do not have a special meaning, they simply match themselves. Consider the following pattern:

I am a harmless regex pattern

None of the characters in this pattern has special meaning. Thus each character of the pattern matches itself. Therefore there is only one string that matches this pattern, and it is identical to the pattern string itself.

matching a simple pattern

Escaping Literal Characters

What are the characters that do have special meaning? The following list shows characters that have special meaning in a regular expression. They must be escaped by a backslash if they are meant to represent themselves.

characters with special meaning in regular expressions

Consider the following pattern:

\+21\.5

The pattern consists of literals only — the + has special meaning and has been escaped, so has the . — and thus the pattern matches only one string: +21.5

Matching non-printable Characters

Sometimes it’s necessary to refer to some non-printable character like the tab character ⇥ or a newline ↩

It’s best to use the proper escape sequences for them:

If you need to match a line break, they usually come in one of two flavors:



often referred to as the unix-style newline

often referred to as the unix-style newline \r

often referred to as the windows-style newline

To catch both possibilities you can match on \r?

which means: optional \r followed by



matching a newline

Matching any Unicode Character

Sometimes you have to match characters that are best expressed by using their Unicode index. Sometimes a character simply cannot be typed— like control characters such as ASCII NUL , ESC , VT etc.

Sometimes your programming language simply does not support putting certain characters into patterns. Characters outside the BMP, such as 𝄞 or emojis are often not supported verbatim.

In many regex engines — such as Java, JavaScript, Python, and Ruby — you can use the \uHexIndex escape syntax to match any character by its Unicode index. Say we want to match the symbol for natural numbers: ℕ - U+2115

The pattern to match this character is: \u2115

matching a unicode symbol

Other engines often provide an equivalent escape syntax. In Go, you would use \x{2115} to match ℕ

Unicode support and escape syntax varies across engines. If you plan on matching technical symbols, musical symbols, or emojis — especially outside the BMP — check the documentation of the regex engine you use to be sure of adequate support for your use-case.

Escaping Parts of a Pattern

Sometimes a pattern requires consecutive characters to be escaped as literals. Say it’s supposed to match the following string: +???+

The pattern would look like this:

\+\?\?\?\+

The need to escape every character as literal makes it harder to read and to understand.

Depending on your regex engine, there might be a way to start and end a literal section in your pattern. Check your docs. In Java and Perl sequences of characters that should be interpreted literally can be enclosed by \Q and \E . The following pattern is equivalent to the above:

\Q+???+\E

Escaping parts of a pattern can also be useful if it is constructed from parts, some of which are to be interpreted literally, like user-supplied search words.

If your regex engine does not have this feature, the ecosystem often provides a function to escape all characters with special meaning from a pattern string, such as lodash escapeRegExp.

The OR Operator

The pipe character | is the selection operator. It matches alternatives. Suppose a pattern should match the strings 1 and 2

The following pattern does the trick:

1|2

The patterns left and right of the operator are the allowed alternatives.

The following pattern matches William Turner and Bill Turner

William Turner|Bill Turner

The second part of the alternatives is consistently Turner . Would be convenient to put the alternatives William and Bill up front, and mention Turner only once. The following pattern does that:

(William|Bill) Turner

It looks more readable. It also introduces a new concept: Groups.

Groups

You can group sub-patterns in sections enclosed in round brackets. They group the contained expressions into a single unit. Grouping parts of a pattern has several uses:

simplify regex notation, making intent clerer

apply quantifiers to sub-expressions

extract sub-strings matching a group

replace sub-strings matching a group

Let’s look at a regex with a group: (William|Bill) Turner

Groups are sometimes referred to as “capturing groups” because in case of a match, each group’s matched sub-string is captured, and is available for extraction.

How captured groups are made available depends on the API you use. In JavaScript, calling ""my string"".match(/pattern/) returns an array of matches. The first item is the entire matched string and subsequent items are the sub-strings matching pattern groups in order of appearance in the pattern.

Accessing sub-strings in captured in groups

Example: Chess Notation

Consider a string identifying a chess board field. Fields on a chess board can be identified as A1-A8 for the first column, B1-B8 for the second column and so on until H1-H8 for the last column. Suppose a string containing this notation should be validated and the components (the letter and the digit) extracted using capture groups. The following regular expression would do that.

(A|B|C|D|E|F|G|H)(1|2|3|4|5|6|7|8)

While the above regular expression is valid and does the job, it is somewhat clunky. This one works just as well, and it is a bit more concise:

([A-H])([1-8])

This sure looks more concise. But it introduces a new concept: Character Classes.

Character Classes

Character classes are used to define a set of allowed characters. The set of allowed characters is put in square brackets, and each allowed character is listed. The character class [abcdef] is equivalent to (a|b|c|d|e|f) . Since the class contains alternatives, it matches exactly one character.

The pattern [ab][cd] matches exactly 4 strings ac , ad , bc , and bd . It does not match ab , the first character matches, but the second character must be either c or d .

Suppose a pattern should match a two digit code. A pattern to match this could look like this:

[0123456789][0123456789]

This pattern matches all 100 two digit strings in the range from 00 to 99 .

Ranges

It is often tedious and error-prone to list all possible characters in a character class. Consecutive characters can be included in a character class as ranges using the dash operator: [0-9][0-9]

matching two characters in range 0–9

Characters are ordered by a numeric index— in 2019 that is almost always the Unicode index. If you’re working with numbers, Latin characters and basic punctuation, you can instead look at the much smaller historical subset of Unicode: ASCII.

The digits zero through nine are encoded sequentially through code-points: U+0030 for 0 to code point U+0039 for 9 , so a character set of [0–9] is a valid range.

Lower case and upper case letters of the Latin alphabet are encoded consecutively as well, so character classes for alphabetic characters are often seen too. The following character set matches any lower case Latin character:

[a-z]

You can define multiple ranges within the same character class. The following character class matches all lower case and upper case Latin characters:

[A-Za-z]

You might get the impression that the above pattern could be abbreviated to:

[A-z]

That is a valid character class, but it matches not only A-Z and a-z, it also matches all characters defined between Z and a, such as [ , \ , and ^ .

the range A-z includes unexpected characters [ and ]

If you’re tearing your hair out cursing the stupidity of the people who defined ASCII and introduce this mind-boggling discontinuity, hold your horses for a bit. ASCII was defined at a time where computing capacity was much more precious than today. Look at A hex: 0x41 bin: 0100 0001 and a hex: 0x61 bin: 0110 0001 How do you convert between upper and lower case? You flip one bit. That is true for the entire alphabet. ASCII is optimized to simplify case conversion. The people defining ASCII were very thoughtful. Some desirable qualities had to be sacrificed for others. You’re welcome.

You might wonder how to put the - character into a character class. After all, it is used to define ranges. Most engines interpret the - character literally if placed as the first or last character in the class: [-+0–9] or [+0–9-] . Some few engines require escaping with a backslash: [\-+0–9]

Negations

Sometimes it’s useful to define a character class that matches most characters, except for a few defined exceptions. If a character class definition begins with a ^ , the set of listed characters is inverted. As an example, the following class allows any character as long as it’s neither a digit nor an underscore.

[^0-9_]

looking for three consecutive non-digit and non-underscore characters

Please note that the ^ character is interpreted as a literal if it is not the first character of the class, as in [f^o] , and that it is a boundary matcher if used outside character classes.

Predefined Character Classes

Some character classes are used so frequently that there are shorthand notations defined for them. Consider the character class [0–9] . It matches any digit character and is used so often that there is a mnemonic notation for it: \d .

The following list shows character classes with most common shorthand notations, likely to be supported by any regex engine you use.

Most engines come with a comprehensive list of predefined character classes matching certain blocks or categories of the Unicode standard, punctuation, specific alphabets, etc. These additional character classes are often specific to the engine at hand, and not very portable.

The Dot Character Class

The most ubiquitous predefined character class is the dot, and it deserves a small section on its own. It matches any character except for line terminators like \r and

.

The following pattern matches any three character string ending with a lower case x:

..x

the dot matches anything except newline characters

In practice the dot is often used to create “anything might go in here” sections in a pattern. It is frequently combined with a quantifier and .* is used to match “anything” or “don’t care” sections.

matching anything between 1 and 2

Please note that the . character loses its special meaning, when used inside a character class. The character class [.,] simply matches two characters, the dot and the comma.

Depending on the regex engine you use you may be able to set the dotAll execution flag in which case . will match anything including line terminators.

Boundary Matchers

Boundary matchers — also known as “anchors” — do not match a character as such, they match a boundary. They match the positions between characters, if you will. The most common anchors are ^ and $ . They match the beginning and end of a line respectively. The following table shows the most commonly supported anchors.

Anchoring to Beginnings and Endings

Consider a search operation for digits on a multi-line text. The pattern [0–9] finds every digit in the text, no matter where it is located. The pattern ^[0–9] finds every digit that is the first character on a line.

matching digits at the beginning of a line

The same idea applies to line endings with $ .

matching digits at the end of a line

The \A and \Z or \z anchors are useful matching multi-line strings. They anchor to the beginning and end of the entire input. The upper case \Z variant is tolerant of trailing newlines and matches just before that, effectively discarding any trailing newline in the match.

The \A and \Z anchors are supported by most mainstream regex engines, with the notable exception of JavaScript.

Suppose the requirement is to check whether a text is a two-line record specifying a chess position. This is what the input strings looks like:

Column: F

Row: 7

The following pattern matches the above structure:

\AColumn: [A-H]\r?

Row: [1-8]\Z

Using /A and /Z to anchor to beginning and end of input

Whole Word Matches

The \b anchor matches the edge of any alphanumeric sequence. This is useful if you want to do “whole word” matches. The following pattern looks for a standalone upper case I .

\bI\b

the \b anchor matches on transitions between “words”

The pattern does not match the first letter of Illinois because there is no word boundary to the right. The next letter is a word letter — defined by the character class \w as [a-zA-Z0–9_] —and not a non-word letter, which would constitute a boundary.

Let’s replace Illinois with I!linois . The exclamation point is not a word character, and thus constitutes a boundary.

Misc Anchors

The somewhat esoteric non-word boundary \B is the negation of \b . It matches any position that is not matched by \b . It matches every position between characters within white space and alphanumeric sequences.

Some regex engines support the \G boundary matcher. It is useful when using regular expressions programmatically, and a pattern is applied repeatedly to a string, trying to find pattern all matches in a loop. It anchors to the position of the last match found.

Quantifiers

Any literal or character group matches the occurrence of exactly one character. The pattern [0–9][0–9] matches exactly two digits. Quantifiers help specifying the expected number of matches of a pattern. They are notated using curly braces. The following is equivalent to [0–9][0–9]

[0-9]{2}

The basic notation can be extended to provide upper and lower bounds. Say it’s necessary to match between two and six digits. The exact number varies, but it must be between two and six. The following notation does that:

[0-9]{2,6}

sequences of 2–6 digits are matched

The upper bound is optional, if omitted any number of occurrences equal to or greater than the lower bound is acceptable. The following sample matches two or more consecutive digits.

[0-9]{2,}

There are some predefined shorthands for common quantifiers that are very frequently used in practice.

The ? quantifier

The ? quantifier is equivalent to {0, 1} , which means: optional single occurrence. The preceding pattern may not match, or match once.

Let’s find integers, optionally prefixed with a plus or minus sign: [-+]?\d{1,}

finding integers with optional sign

The + quantifier

The + quantifier is equivalent to {1,} , which means: at least one occurrence.

We can modify our integer matching pattern from above to be more idiomatic by replacing {1,} with + and we get: [-+]?\d+

finding integers with optional sign again

The * quantifier

The * quantifier is equivalent to {0,} , which means: zero or more occurrences. You’ll see it very often in conjunction with the dot as .* , which means: any character don’t care how often.

Let’s match an comma separated list of integers. Whitespace between entries is not allowed, and at least one integer must be present: \d+(,\d+)*

We’re matching an integer followed by any number of groups containing a comma followed by an integer.

Greedy by Default

Suppose the requirement is to match the domain part from a http URL in a capture group. The following seems like a good idea: match the protocol, then capture the domain, then an optional path. The idea translates roughly to this:

http://(.*)/?

If you’re using an engine that uses /regex/ notation like JavaScript, you have to escape the forward slashes: http:\/\/(.*)\/?.*

It matches the protocol, captures what comes after the protocol as domain and it allows for an optional slash and some arbitrary text after that, which would be the resource path.

greedy capture matches too much

Strangely enough, the following is captured by the group given some input strings:

unexpected portion of url captured by group

The results are somewhat surprising, as the pattern was designed to capture the domain part only, but it seems to be capturing everything till the end of the URL.

This happens because each quantifier encountered in the pattern tries to match as much of the string as possible. The quantifiers are called greedy for this reason.

Let’s check the of matching behaviour of: http://(.*)/?.*

The greedy * in the capturing group is the first encountered quantifier. The . character class it applies to matches any character, so the quantifier extends to the end of the string. Thus the capture group captures everything. But wait, you say, there’s the /?.* part at the end. Well, yes, and it matches what’s left of the string — nothing, a.k.a the empty string — perfectly. The slash is optional, and is followed by zero or more characters. The empty string fits. The entire pattern matches just fine.

Alternatives to Greedy Matching

Greedy is the default, but not the only flavor of quantifiers. Each quantifier has a reluctant version, that matches the least possible amount of characters. The greedy versions of the quantifiers are converted to reluctant versions by appending a ? to them.

The following table gives the notations for all quantifiers.

The quanfier {n} is equvalent in both greedy and reluctant versions. For the others the number of matched characters may vary. Let’s revisit the example from above and change the capture group to match as little as possible, in the hopes of getting the domain name captured properly.

http://(.*?)/?.*

capturing empty strings with reluctant quantifiers

Using this pattern, nothing — more precisely the empty string — is captured by the group. Why is that? The capture group now captures as little as possible: nothing. The (.*?) captures nothing, the /? matches nothing, and the .* matches the entirety of what’s left of the string. So again, this pattern does not work as intended.

So far the capture group matches too little or too much. Let’s revert back to the greedy quantifier, but disallow the slash character in the domain name, and also require that the domain name be at least one character long.

http://([^/]+)/?.*

This pattern greedily captures one or more non slash characters after the protocol as the domain, and if finally any optional slash occurs it may be followed by any number of characters in the path.

Quantifier Performance

Both greedy and reluctant quantifiers imply some runtime overhead. If only a few such quantifiers are present, there are no issues. But if mutliple nested groups are each quantified as greedy or reluctant, determining the longest or shortest possible matches is a nontrivial operation that implies running back and forth on the input string adjusting the length of each quantifier’s match to determine whether the expression as a whole matches.

Pathological cases of catastrophic backtracking may occur. If performance or malicious input is a concern, it’s best to prefer reluctant quantifiers and also have a look at a third kind of quantifiers: possessive quantifiers.

Possessive Quantifiers: Never Giving Back

Possessive quantifiers, if supported by your engine, act much like greedy quantifiers, with the distinction that they do not support backtracking. They try to match as many characters as possible, and once they do, they never yield any matched characters to accommodate possible matches for any other parts of the pattern.

They are notated by appending a + to the base greedy quantifier.

They are a fast performing version of “greedy-like” quantifiers, which makes them a good choice for performance sensitive operations.

Let’s look at them in the PHP engine. First, let’s look at simple greedy matches. Let’s match some digits, followed by a nine: ([0–9]+)9

Matched against the input string: 123456789 the greedy quantifier will first match the entire input, then give back the 9 , so the rest of the pattern has a chance to match.

greedy match

Now, when we replace the greedy with the possessive quantifier, it will match the entire input, then refuse to give back the 9 to avoid backtracking, and that will cause the entire pattern to not match at all.

possessive quantifer causing a non-match

When would you want possessive behaviour? When you know that you always want the longest conceivable match.

Let’s say you want to extract the filename part of filesystem paths. Let’s assume / as the path separator. Then what we effectively want is the last bit of the string after the last occurrence of a / .

A possessive pattern works well here, because we always want to consume all folder names before capturing the file name. There is no need for the part of the pattern consuming folder names to ever give characters back. A corresponding pattern might look like this:

\/?(?:[^\/]+\/)++(.*)

Note: using PHP /regex/ notation here, so the forward slashes are escaped.

We want to allow absolute paths, so we allow the input to start with an optional forward slash. We then possessively consume folder names consisting of a series of non-slash characters followed by a slash. I’ve used a non-capturing group for that — so it’s notated as (?:pattern) instead of just (pattern) . Anything that is left over after the last slash is what we capture into a group for extraction.

example of posessive matching

Non-Capturing Groups

Non-capturing groups match exactly the way normal groups do. However, they do not make their matched content available. If there’s no need to capture the content, they can be used to improve matching performance. Non-capturing groups are written as: (?:pattern)

Suppose we want to verify that a hex-string is valid. It needs to consist of an even number of hexadecimal digits each between 0–9 or a-f . The following expression does the job using a group:

([0-9a-f][0-9a-f])+

Since the point of the group in the pattern is to make sure that the digits come in pairs, and the digits actually matched are not of any relevance, the group may just as well be replaced with the faster performing non-capturing group:

(?:[0-9a-f][0-9a-f])+

matching sequences of hex bytes

Atomic Groups

There is also a fast-performing version of a non-capturing group, that does not support backtracking. It is called the “independent non-capturing group” or “atomic group”.

It is written as (?>pattern)

An atomic group is a non-capturing group that can be used to optimize pattern matching for speed. Typically it is supported by regex engines that also support possessive quantifiers.

Its behavior is also similar to possessive quantifiers: once an atomic group has matched a part of the string, that first match is permanent. The group will never try to re-match in another way to accommodate other parts of the pattern.

a(?>bc|b)c matches abcc but it does not match abc .

The atomic group’s first successful match is on bc and it stays that way. A normal group would re-match during backtracking to accommodate the c at the end of the pattern for a successful match. But an atomic group’s first match is permanent, it won’t change.

This is useful if you want to match as fast as possible, and don’t want any backtracking to take place anyway.

Say we’re matching the file name part of a path. We can match an atomic group of any characters followed by a slash. Then capture the rest:

(?>.*\/)(.*)

Note: using PHP /regex/ notation here, so the forward slashes are escaped.

atomic group matching

A normal group would have done the job just as well, but eliminating the possibility of backtracking improves performance. If you’re matching millions of inputs against non-trivial regex patterns, you’ll start noticing the difference.

It also improves resilience against malicious input designed to DoS-Attack a service by triggering catastrophic backtracking scenarios.

Back References

Sometimes it’s useful to refer to something that matched earlier in the string. Suppose a string value is only valid if it starts and ends with the same letter. The words “alpha”, “radar”, “kick”, “level” and “stars” are examples. It is possible to capture part of a string in a group and refer to that group later in the pattern pattern: a back reference.

Back references in a regex pattern are notated using

syntax, where n is the number of the capture group. The numbering is left to right starting with 1. If groups are nested, they are numbered in the order their opening parenthesis is encountered. Group 0 always means the entire expression.

The following pattern matches inputs that have at least 3 characters and start and end with the same letter:

([a-zA-Z]).+\1

In words: an lower or upper case letter — that letter is captured into a group — followed by any non-empty string, followed by the letter we captured at the beginning of the match.

on-letter back references

Let’s expand a bit. An input string is matched if it contains any alphanumeric sequence — think: word — more then once. Word boundaries are used to ensure that whole words are matched.

\b(\w+)\b.*\b\1\b

Search and Replace with Back References

Regular expressions are useful in search and replace operations. The typical use case is to look for a sub-string that matches a pattern and replace it with something else. Most APIs using regular expressions allow you to reference capture groups from the search pattern in the replacement string.

These back references effectively allow to rearrange parts of the input string.

Consider the following scenario: the input string contains an A-Z character prefix followed by an optional space followed by a 3–6 digit number. Strings like A321 , B86562 , F 8753 , and L 287 .

The task is to convert it to another string consisting of the number, followed by a dash, followed by the character prefix.

Input Output

A321 321-A

B86562 86562-B

F 8753 8753-F

L 287 287-L

The first step to transform one string to the other is to capture each part of the string in a capture group. The search pattern looks like this:

([A-Z])\s?([0-9]{3,6})

It captures the prefix into a group, allows for an optional space character, then captures the digits into a second group. Back references in a replacement string are notated using $n syntax, where n is the number of the capture group. The replacement string for this operation should first reference the group containing the numbers, then a literal dash, then the first group containing the letter prefix. This gives the following replacement string:

$2-$1

Thus A321 is matched by the search pattern, putting A into $1 and 312 into $2 . The replacement string is arranged to yield the desired result: The number comes first, then a dash, then the letter prefix.

Please note that, since the $ character carries special meaning in a replacement string, it must be escaped as $$ if it should be inserted as a character.

This kind of regex-enabled search and replace is often offered by text editors. Suppose you have a list of paths in your editor, and the task at hand is to prefix the file name of each file with an underscore. The path /foo/bar/file.txt should become /foo/bar/_file.txt

With all we learned so far, we can do it like this:

example regex-enabled search and replace in VS Code

Look, but don’t touch: Lookahead and Lookbehind

It is sometimes useful to assert that a string has a certain structure, without actually matching it. How is that useful?

Let’s write a pattern that matches all words that are followed by a word beginning with an a

Let’s try \b(\w+)\s+a it anchors to a word boundary, and matches word characters until it sees some space followed by an a.

first attempt at matching words that are followed by words beginning with an a

In the above example, we match love , swat , fly , and to , but fail to capture the an before ant . This is because the a starting an has been consumed as part of the match of to . We’ve scanned past that a , and the word an has no chance of matching.

Would be great if there was a way to assert properties of the first character of the next word without actually consuming it.

Constructs asserting existence, but not consuming the input are called “lookahead” and “lookbehind”.

Lookahead

Lookaheads are used to assert that a pattern matches ahead. They are written as (?=pattern)

Let’s use it to fix our pattern:

\b(\w+)(?=\s+a)

We’ve put the space and initial a of the next word into a lookahead, so when scanning a string for matches, they are checked but not consumed.

look ahead asserts a pattern matches ahead, but does not consume it

A negative lookahead asserts that its pattern does not match ahead. It is notated as (?!pattern)

Let’s find all words not followed by a word that starts with an a .

\b(\w+)\b(?!\s+a)

We match whole words which are not followed by some space and an a .

negative look ahead asserts that its pattern does not match ahead

Lookbehind

The lookbehind serves the same purpose as the lookahead, but it applies to the left of the current position, not to the right. Many regex engines limit the kind of pattern you can use in a lookbehind, because applying a pattern backwards is something that they are not optimized for. Check your docs!

A lookbehind is written as (?<=pattern)

It asserts the existence of something before the current position. Let’s find all words that come after a word ending with an r or t .

(?<=[rt]\s)(\w+)

We assert that there is an r or t followed by a space, then we capture the sequence of word characters that follows.

a lookbehind to capture certain words

There’s also a negative lookbehind asserting the non-existence of a pattern to the left. It is written as (?<!pattern)

Let’s invert the words found: We want to match all words that come after words not ending with r or t .

(?<![rt]\s)\b(\w+)

We match all words by \b(\w+) , and by prepending (?<![rt]\s) we ensure that any words we match are not preceded by a word ending in r or t .

matching with a negative lookbehind

Split patterns

If you’re working with an API that allows you to split a string by pattern, it is often useful to keep lookaheads and lookbehinds in mind.

A regex split typically uses the pattern as a delimiter, and removes the delimiter from the parts. Putting lookahead or lookbehind sections in a delimiter makes it match without removing the parts that were merely looked at.

Suppose you have a string delimited by : , in which some of the parts are labels consisting of alphabetic characters, and some are time stamps in the format HH:mm .

Let’s look at input string time_a:9:23:time_b:10:11

If we just split on : , we get the parts: [time_a, 9, 32, time_b, 10, 11]

splitting on delimiter

Let’s say we want to improve by splitting only if the : has a letter on either side. The delimiter is now [a-z]:|:[a-z]

including adjacent letters in the match treats them as part of the delimiter

We get the parts: [time_, 9:32, ime_, 10:11] We’ve lost the adjacent characters, since they were part of the delimiter.

If we refine the delimiter to use lookahead and lookbehind for the adjacent characters, their existence will be verified, but they won’t match as part of the delimiter: (?<[a-z]):|:(?=[a-z])

putting the characters into lookbehind and lookahead does not consume them

Finally we get the parts we want: [time_a, 9:32, time_b, 10:11]

Regex Modifiers

Most regex engines allow setting flags or modifiers to tune aspects of the pattern matching process. Be sure to familiarise yourself with the way your engine of choice handles such modifiers.

They often make the difference between a impractically complex pattern and a trival one.

You can expect to find case (in-)sensitivity modifiers, anchoring options, full match vs. patial match mode, and a dotAll mode which lets the . character class match anything including line terminators.

JavaScript, Python, Java, Ruby, .NET

Let’s look at JavaScript, for example. If you want case insensitive mode and only the first match found, you can use the i modifier, and make sure to omit the g modifier.

case insensitive match, with only the first match found

Limitations of Regular Expressions

Arriving at the end of this article you may feel that all possible string parsing problems can be dealt with, once you get regular expressions under your belt.

Well, no.

This article introduces regular expressions as a shorthand notation for sets of strings. If you happen to have the exact regular expression for zip codes, you have a shorthand notation for the set of all strings representing valid zip codes. You can easily test an input string to check if it is an element of that set. There is a problem however.

There are many meaningful sets of strings for which there is no regular expression!

The set of valid JavaScript programs has no regex representation, for example. There will never be a regex pattern that can check if a JavaScript source is syntactically correct.

This is mostly due to regex’ inherent inability to deal with nested structures of arbitrary depth. Regular expressions are inherently non-recursive. XML and JSON are nested structures, so is the source code of many programming languages. Palindromes are another example— words that read the same forwards and backwards like racecar — are a very simple form of nested structure. Each character is opening or closing a nesting level.

If yor input can arbitrarily nest like JavaScript, you can’t validate it with Regular Expressions alone. Photo by Christopher Robin Ebbinghaus on Unsplash

You can construct patterns that will match nested structures up to a certain depth but you can’t write a pattern that matches arbitrary depth nesting.

Nested structures often turn out to be not regular. If you’re interested in computation theory and classifications of languages — that is, sets of strings — have a glimpse at the Chomsky Hiararchy, Formal Grammars and Formal Languages.

Know when to reach for a different Hammer

Let me conclude with a word of caution. I sometimes see attempts trying to use regular expressions not only for lexical analysis — the identification and extraction of tokens from a string — but also for semantic analysis trying to interpret and validate each token’s meaning as well.

While lexical analysis is a perfectly valid use case for regular expressions, attempting semantic validation more often than not leads towards creating another problem.

The plural of “regex” is “regrets”

Let me illustrate with an example.

Suppose a string shall be an IPv4 address in decimal notation with dots separating the numbers. A regular expression should validate that an input string is indeed an IPv4 address. The first attempt may look something like this:

([0-9]{1,3})\.([0-9]{1,3})\.([0-9]{1,3})\.([0-9]{1,3})

It matches four groups of one to three digits separated by a dot. Some readers may feel that this pattern falls short. It matches 111.222.333.444 for example, which is not a valid IP address.

If you now feel the urge to change the pattern so it tests for each group of digits that the encoded number be between 0 and 255 — with possible leading zeros — then you’re on your way to creating the second problem, and regrets.

Trying to do that leads away from lexical analysis — identifying four groups of digits — to a semantic analysis verifying that the groups of digits translate to admissible numbers.

This yields a dramatically more complex regular expression, examples of which is found here. I’d recommend solving a problem like this by capturing each group of digits using a regex pattern, then converting captured items to integers and validating their range in a separate logical step.

Use regular expressions — like all your tools — wisely. Photo by Todd Quackenbush on Unsplash

When working with regular expressions, the trade-off between complexity, maintainability, performance, and correctness should always be a conscious decision. After all, a regex pattern is as “write-only” as computing syntax can get. It is difficult to read regular expression patterns correctly, let alone debug and extend them.

My advice is to embrace them as a powerful string processing tool, but to neither overestimate their possibilities, nor the ability of human beings to handle them.

When in doubt, consider reaching for another hammer in the box.","['know', 'pattern', 'matches', 'characters', 'expressions', 'need', 'character', 'regex', 'match', 'string', 'group', 'following', 'regular']","Regular expressions are useful in any scenario that benefits from full or partial pattern matches on strings.
The Building Blocks of a Regular ExpressionA regular expression pattern is constructed from distinct building blocks.
Therefore there is only one string that matches this pattern, and it is identical to the pattern string itself.
The following pattern matches William Turner and Bill TurnerWilliam Turner|Bill TurnerThe second part of the alternatives is consistently Turner .
This is what the input strings looks like:Column: FRow: 7The following pattern matches the above structure:\AColumn: [A-H]\r?",en,['Slawomir Chodnicki'],2019-11-16 17:36:08.635000+00:00,"{'Regex', 'Patterns', 'Learning', 'Programming', 'Data'}","{'https://miro.medium.com/max/60/1*rNFkMW-1Gb-X4CvShvXznw.png?q=20', 'https://miro.medium.com/max/1802/1*sB4s8nLwGSjitT-qh0VHeA.png', 'https://miro.medium.com/freeze/max/60/1*2HdN1eq9X6UcmnDKtNOjMg.gif?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/1*UPjNDXvsbEaFNhHax3gCog.png', 'https://miro.medium.com/max/60/1*vh20E2SOawPrG1SaYOv6rg.png?q=20', 'https://miro.medium.com/max/60/1*P1eRFcCOnZynsD7G041T4A.png?q=20', 'https://miro.medium.com/max/1800/1*nqKph_QLcCXtz30-mCblcw.png', 'https://miro.medium.com/max/1828/1*7mbjHmnVrAJwiR0DlsDMSA.png', 'https://miro.medium.com/max/1794/1*hP82WIRLQuav64jO8r_aPg.png', 'https://miro.medium.com/max/11326/0*yQMbA_vzrxsdOYNB', 'https://miro.medium.com/max/1806/1*ffwA8QSYMZR1PBBZ07htQQ.png', 'https://miro.medium.com/max/60/1*HA91EZDCYNSo0bSL31hOkg.png?q=20', 'https://miro.medium.com/max/1004/1*pAorPhRzksREjpyPvaBBAQ.png', 'https://miro.medium.com/max/1802/1*0qMHOszaxexjjFSJfa7IkQ.png', 'https://miro.medium.com/max/60/1*tPGFpz2xqqkaC5KefRQMBw.png?q=20', 'https://miro.medium.com/max/1836/1*M4umtRGpikeFJuSwX91NVg.png', 'https://miro.medium.com/max/1806/1*GvEXd5B8oXb2G-pkzJEzvg.png', 'https://miro.medium.com/max/1802/1*2HdN1eq9X6UcmnDKtNOjMg.gif', 'https://miro.medium.com/max/60/1*UF84hEao2eeSiOYVPeofEw.png?q=20', 'https://miro.medium.com/max/60/1*9DIrV3NsjJ_9iJ153-57LQ.png?q=20', 'https://miro.medium.com/max/60/1*0qMHOszaxexjjFSJfa7IkQ.png?q=20', 'https://miro.medium.com/max/60/1*FwioUb7Ip1wml7EZIj6L8w.png?q=20', 'https://miro.medium.com/max/60/1*pAorPhRzksREjpyPvaBBAQ.png?q=20', 'https://miro.medium.com/max/60/1*mmq2km_LOf3gV3zevfl_6w.png?q=20', 'https://miro.medium.com/max/1816/1*FwioUb7Ip1wml7EZIj6L8w.png', 'https://miro.medium.com/max/60/0*yQMbA_vzrxsdOYNB?q=20', 'https://miro.medium.com/max/60/1*B3sV5YpgWTbdIcdowdu4kA.png?q=20', 'https://miro.medium.com/max/1812/1*j24J--XlW4peIZCRkL-pdA.png', 'https://miro.medium.com/max/60/1*nqKph_QLcCXtz30-mCblcw.png?q=20', 'https://miro.medium.com/max/60/1*r3X9AEtRz5M791nrOZNOCg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/9724/0*zyJfVFCraVQXD4IX', 'https://miro.medium.com/max/1916/1*1tjSaKkybq7omLHW0wHcLw.png', 'https://miro.medium.com/max/60/1*7mbjHmnVrAJwiR0DlsDMSA.png?q=20', 'https://miro.medium.com/max/60/1*hP82WIRLQuav64jO8r_aPg.png?q=20', 'https://miro.medium.com/max/1940/1*UF84hEao2eeSiOYVPeofEw.png', 'https://miro.medium.com/max/1798/1*3DhaPFAxh3_efWbzKG0A9w.png', 'https://miro.medium.com/max/60/1*qqgO53z7bVI80trJ2dsgcg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1804/1*7rY4q1UbMvW424zVjpwLQQ.png', 'https://miro.medium.com/max/1804/1*VlS07_GkcaATDvtLt7Yp7w.png', 'https://miro.medium.com/max/60/1*GvEXd5B8oXb2G-pkzJEzvg.png?q=20', 'https://miro.medium.com/max/1834/1*qqgO53z7bVI80trJ2dsgcg.png', 'https://miro.medium.com/max/1808/1*HA91EZDCYNSo0bSL31hOkg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*3DhaPFAxh3_efWbzKG0A9w.png?q=20', 'https://miro.medium.com/max/1824/1*r3X9AEtRz5M791nrOZNOCg.png', 'https://miro.medium.com/max/1808/1*rNFkMW-1Gb-X4CvShvXznw.png', 'https://miro.medium.com/max/60/1*j24J--XlW4peIZCRkL-pdA.png?q=20', 'https://miro.medium.com/max/1912/1*4Hjqa6luYuEleYEfz__6GA.png', 'https://miro.medium.com/max/1804/1*mb-wKexWXT9Rzz-yQ9Lllg.png', 'https://miro.medium.com/max/13440/1*uvRLCts9a2k_8ptbolMgOg.jpeg', 'https://miro.medium.com/max/60/0*zyJfVFCraVQXD4IX?q=20', 'https://miro.medium.com/max/60/1*Ov25rq3Ge3CHeMKaoY6ePg.png?q=20', 'https://miro.medium.com/max/60/1*iV4GKHeCnBhrNrkIjp-pog.png?q=20', 'https://miro.medium.com/max/60/1*4Hjqa6luYuEleYEfz__6GA.png?q=20', 'https://miro.medium.com/max/1914/1*4vn1LlR6P1_PkyJQXjVTEQ.png', 'https://miro.medium.com/max/60/1*hKXrH9Ujh8zQ7Jp4VGBV4w.png?q=20', 'https://miro.medium.com/max/60/1*7RSMpCCvNHI7Qbct7fCmxg.png?q=20', 'https://miro.medium.com/max/60/1*0ij35h4_6DhvDXMuP-_kuQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*UPjNDXvsbEaFNhHax3gCog.png', 'https://miro.medium.com/max/1810/1*0ij35h4_6DhvDXMuP-_kuQ.png', 'https://miro.medium.com/max/60/1*tByYqkMhUCBXabiQsAuhwA.png?q=20', 'https://miro.medium.com/max/1800/1*1-09HnIO6IoEN1z5PsRK2g.png', 'https://miro.medium.com/max/1812/1*j-MiFHwOk5PLs0UsLo-A1A.png', 'https://miro.medium.com/max/60/1*UkTZyxLEVU3cbezhae4FNg.png?q=20', 'https://miro.medium.com/max/1940/1*Ov25rq3Ge3CHeMKaoY6ePg.png', 'https://miro.medium.com/max/1816/1*7RSMpCCvNHI7Qbct7fCmxg.png', 'https://miro.medium.com/max/60/0*5rXe_WZ9C4WDzr_a?q=20', 'https://miro.medium.com/max/1792/1*p9QA4dcYRYZI0BzjbUq7aA.png', 'https://miro.medium.com/max/60/1*bpNK1PYlXbd0jkcRjGlCvQ.png?q=20', 'https://miro.medium.com/max/1806/1*tByYqkMhUCBXabiQsAuhwA.png', 'https://miro.medium.com/max/60/1*1-09HnIO6IoEN1z5PsRK2g.png?q=20', 'https://miro.medium.com/max/60/1*7rY4q1UbMvW424zVjpwLQQ.png?q=20', 'https://miro.medium.com/max/60/1*1tjSaKkybq7omLHW0wHcLw.png?q=20', 'https://miro.medium.com/max/1856/1*vh20E2SOawPrG1SaYOv6rg.png', 'https://miro.medium.com/max/1808/1*9DIrV3NsjJ_9iJ153-57LQ.png', 'https://miro.medium.com/max/1802/1*7AAu-Pba_Ds6pSuFlhm6SQ.png', 'https://miro.medium.com/max/1032/1*tPGFpz2xqqkaC5KefRQMBw.png', 'https://miro.medium.com/max/1862/1*B3sV5YpgWTbdIcdowdu4kA.png', 'https://miro.medium.com/max/60/1*sB4s8nLwGSjitT-qh0VHeA.png?q=20', 'https://miro.medium.com/max/60/1*0A_R3Wp5t2wvTNAayAFXoQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1854/1*UkTZyxLEVU3cbezhae4FNg.png', 'https://miro.medium.com/max/1200/1*uvRLCts9a2k_8ptbolMgOg.jpeg', 'https://miro.medium.com/max/1816/1*mmq2km_LOf3gV3zevfl_6w.png', 'https://miro.medium.com/max/60/1*j-MiFHwOk5PLs0UsLo-A1A.png?q=20', 'https://miro.medium.com/max/60/1*ffwA8QSYMZR1PBBZ07htQQ.png?q=20', 'https://miro.medium.com/max/1028/1*bpNK1PYlXbd0jkcRjGlCvQ.png', 'https://miro.medium.com/max/60/1*M4umtRGpikeFJuSwX91NVg.png?q=20', 'https://miro.medium.com/max/60/1*uvRLCts9a2k_8ptbolMgOg.jpeg?q=20', 'https://miro.medium.com/max/8544/0*5rXe_WZ9C4WDzr_a', 'https://miro.medium.com/max/1840/1*hKXrH9Ujh8zQ7Jp4VGBV4w.png', 'https://miro.medium.com/max/1804/1*P1eRFcCOnZynsD7G041T4A.png', 'https://miro.medium.com/max/1804/1*iV4GKHeCnBhrNrkIjp-pog.png', 'https://miro.medium.com/max/60/1*mb-wKexWXT9Rzz-yQ9Lllg.png?q=20', 'https://miro.medium.com/max/60/1*4vn1LlR6P1_PkyJQXjVTEQ.png?q=20', 'https://miro.medium.com/max/60/1*VlS07_GkcaATDvtLt7Yp7w.png?q=20', 'https://miro.medium.com/max/1842/1*0A_R3Wp5t2wvTNAayAFXoQ.png', 'https://miro.medium.com/max/60/1*7AAu-Pba_Ds6pSuFlhm6SQ.png?q=20', 'https://miro.medium.com/max/60/1*p9QA4dcYRYZI0BzjbUq7aA.png?q=20'}",2020-03-05 00:20:44.126748,9.572553396224976
https://towardsdatascience.com/automated-hyper-parameter-optimization-in-sagemaker-492f4c5205b4,Automated Hyper-Parameter Optimization in SageMaker,"Note: The code examples render on my blog much better than on Medium.

What is Hyper-Parameter Optimization (HPO)?

So you’ve built your model and are getting sensible results, and are now ready to squeeze out as much performance as possible. One possibility is doing Grid Search, where you try every possible combination of hyper-parameters and choose the best one. That works well if your number of choices are relatively small, but what if you have a large number of hyper-parameters, and some are continuous values that might span several orders of magnitude? Random Search works pretty well to explore the parameter space without committing to exploring all of it, but is randomly groping in the dark the best we can do?

Of course not. Bayesian Optimization is a technique for optimizing a function when making sequential decisions. In this case, we’re trying to maximize performance by choosing hyper-parameter values. This sequential decision framework means that the hyper-parameters you choose for the next step will be influenced by the performance of all the previous attempts. Bayesian Optimization makes principled decisions about how to balance exploring new regions of the parameter space vs exploiting regions that are known to perform well. This is all to say that it’s generally much more efficient to use Bayesian Optimization than alternatives like Grid Search and Random Search.

How to do it

The good news is that SageMaker makes this very easy because the platform takes care of the following:

Implementing the Bayesian Optimization algorithm to handle categorical, integer, and float hyper-parameters Orchestrating the training and evaluation of models given a set of hyper-parameters from the HPO service Integrate the training jobs and the HPO service, which communicates the selected hyper-parameter values and reports performance back once the job is complete

Prerequisites

The code below will assume we’re working with a TensorFlow Estimator model, but the HPO-relevant parts should extend to any SageMaker Estimator. To run code in the way this example presents, you’ll need the following:

Some understanding of how SageMaker works. If you’d like some examples of that, there are several official notebook examples in this repo. You might find the TensorFlow HPO example particularly relevant.

Have SageMaker’s Python SDK

Have configured the necessary API permissions, or are running in a SageMaker Notebook Instance

Step 1 — Create an Estimator

A key requirement to run HPO with SageMaker is that your model needs to both:

Expect the hyper-parameters to be passed from SageMaker Write performance metrics to the logs

For built-in algorithms, this has already been completed for you. In the case of using SageMaker to build arbitrary TensorFlow models, this means configuring things correctly in the model.py file, a.k.a. the “entry point”. This is the file that SageMaker uses to build your TensorFlow model, and it expects certain functions to be defined that adhere to a particular input/output scheme. (See the TensorFlow README for more details about the functions you need to specify.)

Get your model ready to accept hyper-parameters from SageMaker

To dynamically specify parameter values, your model code needs to accept, parse, and utilize them. In TensorFlow, you allow for hyper-parameters to be specified by SageMaker via the addition of the hyperparameters argument to the functions you need to specify in the entry point file. For example, for a hyper-parameter needed in your model_fn :

DEFAULT_LEARNING_RATE = 1e-3

def model_fn(features, labels, mode, hyperparameters=None):

if hyperparameters is None:

hyperparameters = dict()

# Extract parameters

learning_rate = hyperparameters.get('learning_rate', DEFAULT_LEARNING_RATE)

...

You might also want a hyper-parameter in the train_input_fn , e.g. to specify the number of training epochs:

def train_input_fn(training_dir, hyperparameters=None):

# Extract parameters

if not hyperparameters:

hyperparameters = dict() epochs = hyperparameters.get('epochs', None)

...

These examples extract the parameter if it’s specified, but use a default if not.

Write performance metrics to logs

The second requirement of writing performance metrics to the logs is an implementation detail of SageMaker: it gets the model performance of the run by extracting it from the training logs text. These are the values that are sent back to the HPO engine.

For TensorFlow, metrics that are specified in the EstimatorSpec are written to the logs by default. For example, this code exists as part of my model_fn :

def model_fn(features, labels, model, hyperparameters=None)

...

if mode == tf.estimator.ModeKeys.EVAL:

eval_metric_ops = {

""roc_auc"": tf.metrics.auc(

labels,

predictions,

summation_method='careful_interpolation'),

""pr_auc"": tf.metrics.auc(

labels,

predictions,

summation_method='careful_interpolation',

curve='PR'),

}

else:

# e.g. in ""training"" mode

eval_metric_ops = {} return tf.estimator.EstimatorSpec(

mode=mode,

loss=loss,

train_op=train_op,

eval_metric_ops=eval_metric_ops,

)

During training, the model will periodically stop and evaluate the test set (the details of this process can be configured by you). The logs for these events will look something like the following:

2018–10–02 17:23:40,657 INFO — tensorflow — Saving dict for global step 101: global_step = 101, loss = 0.45420808, pr_auc = 0.36799875, roc_auc = 0.6891242

This is what SageMaker will use to measure the performance of any particular training job.

Build the estimator

An Estimator is normally used to kick off a single training job. This enables you to tell SageMaker where to store the outputs, which instances to use for training…etc. Now that the functions in the entry point file have been properly configured to accept hyperparameters and write performance metrics to the logs, you can create the TensorFlow Estimator:

from sagemaker.tensorflow import TensorFlow # The parameters that are constant and will not be tuned

shared_hyperparameters = {

'number_layers': 5,

} tf_estimator = TensorFlow(

entry_point='my/tensorflow/model.py',

role='<sagemaker_role_arn>',

train_instance_count=1,

train_instance_type='ml.p3.2xlarge',

training_steps=10000,

hyperparameters=shared_hyperparameters,

)

Step 2 — Define the performance metrics

In this step, we need to tell SageMaker how to extract the performance information from the logs. This is done by specifying a RegEx expression and assigning it to a metric name. Although you can specify multiple expressions (which are automatically gathered in AWS CloudWatch for easy plotting/monitoring), one of them needs to be singled out as the optimization objective of the HPO. You also need to specify whether you want to maximize or minimize the number. Note that while the RegEx expression will likely match multiple log entries, it’s the last instance in the logs that’s returned as the final performance value.

objective_metric_name = 'PR-AUC'

objective_type = 'Maximize'

metric_definitions = [

{'Name': 'ROC-AUC', 'Regex': 'roc_auc = ([0-9\\.]+)'},

{'Name': 'PR-AUC', 'Regex': 'pr_auc = ([0-9\\.]+)'}

]

Step 3 — Define the hyper-parameter search space

We now need to specify what our hyper-parameters are called, what type they are (continuous, integer, or categorical), and what their possible values are. Below is an example:

from sagemaker.tuner import (

IntegerParameter, CategoricalParameter,

ContinuousParameter, HyperparameterTuner) hyperparameter_ranges = {

""learning_rate"": ContinuousParameter(1e-5, 1e-1),

""number_nodes"": IntegerParameter(32, 512),

""optimizer"": CategoricalParameter(['Adam', 'SGD'])

}

Step 4 — Specify the number of optimization iterations

Finally, we need to decide how to run the HPO job. If you run many jobs in parallel, then you can explore a large part of the space simultaneously. However, if you just ran a million jobs in parallel, you would effectively be doing Random Search. It’s the sequential nature of Bayesian Optimization that allows future runs to be informed by the results of the previous runs.

We therefore need to decide how many total jobs to run, and how many to run in parallel at any given time. For instance, we might run 100 jobs, 5 in parallel. That would be 20 total sequential iterations, exploring 5 points at a time. The choices here will depend on the size of your parameter space and your budget.

Now you have everything you need to ask SageMaker to run HPO:

tuner = HyperparameterTuner(

tf_estimator,

objective_metric_name,

hyperparameter_ranges,

metric_definitions,

max_jobs=100,

max_parallel_jobs=5,

objective_type=objective_type

) # The data configuration

channel = {

'training': 's3://<bucket_name>/my/training_file.csv',

'test': 's3://<bucket_name>/my/test_file.csv',

} tuner.fit(inputs=channel)

Final Performance

You can use the sdk for pinging SageMaker for status reports, or getting the stats of the best job from the HPO run. You can also do this from the AWS SageMaker console, which nicely presents a summary of all the jobs’ performance along with the HPO job configuration.

As mentioned above, you can go to CloudWatch in the console, click on Browse Metrics , and find the metrics you defined in the Name field of metric_definitions from Step 2.

And once everything is complete, you deploy the best model by simply issuing the following command:","['hyperparameters', 'tensorflow', 'automated', 'hpo', 'optimization', 'need', 'hyperparameter', 'performance', 'run', 'model', 'sagemaker', 'training']","What is Hyper-Parameter Optimization (HPO)?
To run code in the way this example presents, you’ll need the following:Some understanding of how SageMaker works.
You might find the TensorFlow HPO example particularly relevant.
In the case of using SageMaker to build arbitrary TensorFlow models, this means configuring things correctly in the model.py file, a.k.a.
This is the file that SageMaker uses to build your TensorFlow model, and it expects certain functions to be defined that adhere to a particular input/output scheme.",en,['Zak Jost'],2018-10-16 21:48:05.092000+00:00,"{'Sagemaker', 'Data Science', 'Python', 'Hpo', 'Machine Learning'}","{'https://miro.medium.com/fit/c/160/160/1*man2lUmAqYq7Jk0hHdndYA.jpeg', 'https://miro.medium.com/max/60/1*0RuFKK3VBOa3xh3UOuBfRw.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/1*man2lUmAqYq7Jk0hHdndYA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/960/1*0RuFKK3VBOa3xh3UOuBfRw.jpeg', 'https://miro.medium.com/max/1920/1*0RuFKK3VBOa3xh3UOuBfRw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:20:51.757401,7.628649711608887
https://towardsdatascience.com/turn-your-previous-python-projects-into-awesome-tools-with-tkinter-2e61f2241e29,Turn your previous Python projects into awesome tools — with Tkinter,"Where are the buttons?!

Yes, it’s time to get the buttons to show up in the app. We already took care of the function to exit the app. The command is called “destroy” which sounds super creepy and scary, but that’s what it does: it destroys the window! So, our close_app function is a single line, window.destroy(). The run_app function we created before will simply print “run” in the console, so we will know it is working if we get a text saying “run”.

This part can go directly below the last snippet, and the mainloop() will take care of the proper execution. So you can pack widgets, and then create a few more and pack them below. The important part is to keep track of the order you are using. Otherwise, it can get messy with more complex scripts.

The Button widget is easily mapped with a function in the argument “command=”. Notice how I direct one to the run_app function and the other to close_app. Oh, and the master for these buttons is the bottom_frame.

And voilá! The interface is complete. It looks rough, and the alignment could be better (to say the least), but hey… it’s your app, so go ahead and improve it!

finally, something that looks like an actual tool!

This should give you the basics to build simple apps for your scripts, but we still need to link this project to our Flight Scraper script and redo the run_app function to activate it with the “Start” button. Here’s how the run_app function should look in the end. This function uses the method .get() on the entries and it will initiate our bot with Flight_Bot(). It then passes the user inputs to the function start_kayak that is part of the bot. You should be able to recognize its name from the previous article.

please note: in order to start the bot, you need to import it in the initial imports!! Check it out in the fourth video

I did this by adapting the script from the previous article (in a jupyter notebook) to a py file named flight_scraper_bot.py, and placing it in the same folder as the one we have been working on. From there, we can import it and use it within our program. That py file is available here. The line we need to add to our tkinter file is: from flight_scraper_bot import Flight_Bot

That is already included in the final and compiled version of our script and I placed it here for you to grab it. It’s easier than copy and pasting multiple snippets throughout the article. You’re welcome!

I go over it in more detail in the videos, so definitely check them out if you need help. And like I mentioned in the beginning, your comments are really important to help me improve the quality of the articles and videos, so feel free to send your feedback and questions!

This book had everything I needed to know whenever I was struggling with a particular widget. If you want to master Tkinter, I’m sure it can help you with that. And if you got interested in the Web Scraping part, this is the book I used to learn Selenium and others. (If you use this link, I receive a small fee at no extra cost to you. I do need a lot of coffee to write these articles!)","['turn', 'file', 'bot', 'import', 'function', 'previous', 'tools', 'script', 'awesome', 'python', 'need', 'run_app', 'working', 'projects', 'help', 'tkinter', 'app']","The run_app function we created before will simply print “run” in the console, so we will know it is working if we get a text saying “run”.
Notice how I direct one to the run_app function and the other to close_app.
Here’s how the run_app function should look in the end.
please note: in order to start the bot, you need to import it in the initial imports!!
I go over it in more detail in the videos, so definitely check them out if you need help.",en,['Fábio Neves'],2020-03-04 16:48:23.397000+00:00,"{'Web Development', 'Data Science', 'Python', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/1024/0*L_4twG6eRsMSHMVI.jpg', 'https://miro.medium.com/max/60/0*L_4twG6eRsMSHMVI.jpg?q=20', 'https://miro.medium.com/max/60/1*6RahoGcjWaFsIiBaJ-IurQ.png?q=20', 'https://miro.medium.com/max/1200/1*RZzMRaaL7Lv1-aiRG71cxg.jpeg', 'https://miro.medium.com/max/700/0*2JqBA4nGkG8fRvUv.jpg', 'https://miro.medium.com/max/60/0*4zyXUn1z1vo17ef0.png?q=20', 'https://miro.medium.com/max/60/1*M8-AlH7dBIxcaTptULB_ug.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1948/1*M8-AlH7dBIxcaTptULB_ug.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1940/1*6RahoGcjWaFsIiBaJ-IurQ.png', 'https://miro.medium.com/max/60/1*M0pcg9mRhDj53v7IeIaDnA.jpeg?q=20', 'https://miro.medium.com/max/60/1*pNDuoXKsRm4K6gYWzX54tQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1950/1*7waWEFgVcqeFHJQKtvOmXQ.png', 'https://miro.medium.com/max/44/0*2JqBA4nGkG8fRvUv.jpg?q=20', 'https://miro.medium.com/max/676/1*M0pcg9mRhDj53v7IeIaDnA.jpeg', 'https://miro.medium.com/max/1946/1*pNDuoXKsRm4K6gYWzX54tQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/160/160/0*ekAsOkhALaEuuLgx.', 'https://miro.medium.com/max/60/1*RZzMRaaL7Lv1-aiRG71cxg.jpeg?q=20', 'https://miro.medium.com/max/60/1*7waWEFgVcqeFHJQKtvOmXQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/0*4zyXUn1z1vo17ef0.png', 'https://miro.medium.com/max/11326/1*RZzMRaaL7Lv1-aiRG71cxg.jpeg', 'https://miro.medium.com/fit/c/96/96/0*ekAsOkhALaEuuLgx.'}",2020-03-05 00:20:54.219176,2.461775302886963
https://towardsdatascience.com/bayesian-nightmare-how-to-start-loving-bayes-1622741fa960,Bayesian nightmare. Solved!,"Who has not heard that Bayesian statistics are difficult, computationally slow, cannot scale-up to big data, the results are subjective; and we don’t need it at all? Do we really need to learn a lot of math and a lot of classical statistics first before approaching Bayesian techniques. Why do the most popular books about Bayesian statistics have over 500 pages?

Bayesian nightmare is real or myth?

Someone once compared Bayesian approach to the kitchen of a Michelin star chef with high-quality chef knife, a stockpot and an expensive sautee pan; while Frequentism is like your ordinary kitchen, with banana slicers and pasta pots. People talk about Bayesianism and Frequentism as if they were two different religions. Which cook are you? Does Bayes really put more burden on the data scientist to use her brain at the outset because Bayesianism is a religion for the brightest of the brightest? Is “Bayesian” the right word after all?

The building blocks of Bayesian data analysis

The essential characteristic of Bayesian methods is their explicit use of probability for quantifying uncertainty in inferences based on statistical data analysis. Recently, an increased emphasis has been placed on interval estimation rather than hypothesis testing. This provides a strong drive to the Bayesian viewpoint, because it seems likely that most users of standard confidence intervals give them Bayesian interpretation by common-sense. We recommend to go through our article below as a refresher about probabilities if needed.

The Bayesian approach to data analysis typically requires data, a generative model and priors. In the classical approach, data is used to fit a linear regression line for example, in order to estimate the most suitable intercept and slope that best describe a linear trend. There is no direct way to include our prior belief about those parameters we are estimating. Bayesian approach allows us to make a prior good guess of the intercept and slope, based on our real-life domain knowledge and common sense. Additionally we can also make statements about how uncertain this guess is. We can say for example that, from experience, the slope is drawn from a normal distribution with mean μ and standard deviation σ, while typical intercept values will be normally distributed with mean θ and standard deviation ρ. Given the distributions, which describe our prior belief, we can generate simulated data using a generative model, as depicted in the image below.

Estimating signup rate with Bayesian approach

Let’s consider a real life example, where we are interested in estimating the percentage of leads conversion in a sales funnel: how many visitors of a website turn into paying customers. Typically marketing would conduct a campaign to attract an audience and encourage them to sign-up. If we observe that out of 16 visitors, 6 of them sign-up, would that mean that the signup rate θ is 6/16=38%? How uncertain is this percentage, especially given the small set of data? Bayesian data analysis helps finding answers to those questions.

Prior belief

We start by looking for priors, a belief from the sales department about the typical rate of sign-up they observed from experience or given the current state of the industry. Let’s assume that, according to sales, any signup rate between 0% and 100% is equally likely. Therefore we can put a prior on the signup rate as a uniform distribution.

Samples generation

Next, we build a generative model that simulates a high number of marketing campaigns for any randomly chosen signup rate from the prior distribution. For example, if 55% is chosen as signup rate, it is like asking 16 potential customers, where the chance of each customer signing up is 55%. Therefore, the generative model is a binomial distribution with parameters 16 and θ. We simulate the generative model 100,000 times. Each time, we draw a random θ from the uniform distribution and we run the generative model by creating 16 “fake” customers from the binomial distribution. At the end of this process, we have 100,000 samples.

Posterior distribution

Now, it is time to bring in our data that is telling us, what the true marketing campaign had achieved, this is 6/16=38% signup rate. Therefore, we filter out simulated samples where the simulated signup rate is not 6/16 and keep only those simulated samples where the simulated signup rate is 6/16. The process is visualized in the image below. Note that, the same prior draw (e.g. 21%) can generate different signup rates from the binomial distribution (e.g. 4/16 and 6/16).

The image below illustrates the Bayesian sampling and rejection process.

Now, we consider all the 100,000 draws of θ from the uniform distribution on one side, and on the other side, we look at the θ we kept after filtering those which did not produce samples with 6/16 signup rate. If we count the frequency of each value in both buckets, then we end up with the histograms below.

Results interpretation

The blue plot shows the so-called posterior distribution of signup rate. It is the answer we were looking for. As we can see, the signup rate is pretty uncertain, and we cannot say with 100% confidence that it is 38% as marketing found out. It is likely to be between 20% and 60%. Using the posterior distribution, we can deliver statements about uncertainty. The parameter value with the maximum likelihood to generate the data we observed will be the signup rate with the highest probability in the posterior distribution: 38%. This is also called maximum likelihood estimator, which is one of the most common ways of estimating unknown parameters in classical statistics. This is why the Bayesian approach can be seen as an extension of maximum likelihood estimation. We can also compute the mean of all probabilities, the posterior mean as a best guest of signup rate: 39%. It is also common to compute the shortest interval that contains 90% of the probabilities from the posterior distribution, called credible interval, which in this case goes between 30% and 40%. Therefore, we can say that, there is 90% probability that the signup rate is between 30% and 40%.

6 signups out of 16 leads does NOT ALWAYS mean 38% signup rate!

Bayes Theorem

How did we go from the prior to the posterior? Let’s look at it with an example, where we draw θ=35% from the uniform distribution, with probability P(35%). In order to not throw θ away, it has to allow us to simulate data which match the data from marketing. In other words, with the probability P(6|35%) of having 6 signups given θ=35%. By multiplying both quantities, we obtain the probability of drawing θ=35% and simulating data that match the signup rate that we observed. This value will be proportional to the probability of 35% being the best parameter value that results in 6 observed signups. If we divide that quantity by the total probability of generating the data for all possible parameter values, we obtain the exact probability of having a signup rate of 35% given the data P(35%|6). When we repeat this procedure for all the signup rates, drawn from the prior distribution, we obtain all probabilities needed to draw the blue histogram, i.e. the posterior distribution. This process is illustrated in the image below.

The exercise above can be extended to multiple parameters Θ, to any generative model and to any dataset D. This generalization is illustrated by the equation below, commonly called Bayes Theorem.

Scaling Bayesian data analysis

In order to illustrate the generalization of Bayesian data analysis, let’s consider that the marketing department actually ran two campaigns. In the first, they got 6/16 signups, while the second resulted in 10/16 signups. Furthermore, the sales department came up with a prior belief that signup rate has never been higher than 20% and it uses to be between 5% and 15%. Now, we have two parameters θ1 and θ2 and two generative models. We are also given an informative prior that is not uniform, which we can represent by a Beta distribution with parameters 2 and 25, as illustrated below.

We can now sample 100,000 samples from both generative models, retain those matching marketing data and build two posterior distributions, one for each campaign. The posterior distributions can be used to draw conclusions about the effectiveness of the campaigns, when taking the prior belief of salespeople into consideration. We can also compare both campaigns by calculating the difference between the two posterior distributions.

Bayesian linear regression

After we feel more comfortable with the Bayesian philosophy, we are going to perform Bayesian linear regression on a more extended marketing data set, which describes leads conversion (y) versus time (x). In the plot below, x represents the number of weeks before and after launching a new company website at time x=0, with x=-10 being 10 weeks before launch, x=+10 being 10 weeks after launch. y represents the signup rate for positive values of y, or the churn rate for negative values of y. Churn happens when the website looses visitors. Marketing is interested in estimating the uncertainty around what signup rate or churn rate to expect in the future.

As introduced in the previous sections, Bayesian statistics is a mathematical method that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data. Bayes’ theorem describes the conditional probability of an event based on data as well as prior information or beliefs about the event or conditions related to the event. Bayes’ theorem can be used to calculate the posterior probability (which is the revised probability of an event occurring after taking into consideration new information). The posterior probability is calculated by updating the prior probability. The prior probability is the probability of an outcome based on the current knowledge before an experiment.

Bayesian linear regression might allow a useful mechanism to deal with insufficient data, or poor distributed data as it appeared to be the case in the plot above. It allows us to put a prior on the parameters and on the noise so that in the absence of data, the priors can take over.

Generative model

In the Bayesian viewpoint, we formulate linear regression using probability distributions. The response y is not estimated as a single value, but is assumed to be drawn from a probability distribution.

However, according to the linear regression equation, we believe that the most probable value for y is as follow.

Even though μ is the most probable value for y, y can also include some error or noise. Accordingly, we model such errors ϵ in the observations by adjusting the variance term σ to compensate for the deviations of y from μ.

Priors

Not only is the response generated from a probability distribution, but the model parameters are assumed to come from a distribution as well. The noise on y is assumed normally distributed, and we also include priors for the slope and intercept as follows.

Posterior distribution

The posterior probability of the model parameters is conditional upon the training inputs and outputs:

The equation above reflects again the Bayes theorem as we learnt before in a simpler example. The specific computation method we used so far for calculation the posterior probabilities by generating 100,000 samples and excluding those which do not match the data works only in rare cases. It is called Approximate Bayesian Computation. Although it is conceptually simple, it can be incredibly slow and scale horribly with large dataset. There are faster methods, mostly within the so-called Markov chain Monte Carlo (MCMC) family of algorithms. Specific popular examples include Hamiltonian Monte Carlo and Metropolis–Hastings.

Below we implement MCMC to find the posterior distribution of the model parameters using the Python library PyMC3.

The Bayesian Model provides more opportunities for interpretation than the ordinary least squares regression because it provides a posterior distribution. Rather than a single point estimate of the model weights, Bayesian linear regression will give us a posterior distribution for the model weights. We can use this distribution to find the most likely single value as well as the entire range of likely values for our model parameters.

PyMC3 has many built-in tools for visualizing and inspecting model runs. These let us see the distributions and provide estimates with a level of uncertainty, which should be a necessary part of any model. Below we see the trace for all model parameters. The trace plots tend to be normally distributed around the true parameters, what is a good sign that the samples converged towards the target distribution.

pm.traceplot(linear_trace, figsize = (12, 12))

Results interpretation

The posterior distribution of model parameters is given below. Marketing can expect the signup rate (slope) to increase by 4.7% per year on average. There is 94% probability that the signup will increase by a rate between 4.4.% and 5.1% in the future. The signup rate at the moment of website launch (intercept at week 0) was between 4.6% and 7.2% with 94% probability. The standard deviation of signup rate is expected to be 6.6% on average.

pm.plot_posterior(linear_trace, figsize = (12, 10), text_size = 20);

We can also visualize the credibility interval of model parameters. The rate of change in signups (slope) has less uncertainty than the signup rate at week 0.

pm.forestplot(linear_trace);

Benefits of Bayesian data analysis

We can also generate predictions of the linear regression line using the model results. The following plot shows 100 different estimates of the regression line drawn from the posterior. The distribution of the lines gives an estimate of the uncertainty in the estimate. Bayesian Linear Regression has the benefit that it gives us a posterior distribution rather than a single point estimate in the frequentist ordinary least squares regression (OLS).

Conclusion

In this article we provided a gentle introduction the Bayesian approach to statistics and machine learning. Especially we quantified uncertainty around point estimates of signup rate in a marketing campaign. Bayesian concepts such as priors, generative models, posterior distribution, Bayes theorem, credible intervals were introduced under the light of practical examples and illustrations. We also presented a Python implementation of linear regression using a Bayesian approach and compared it to the classical ordinary least squares method. The article offers few take-out on PyMC3, an easy to use Python library for Bayesian analysis.

Rather than trying to cram too much into one article, you might redirect to books, such as Bayesian Data Analysis from Andrew Gelman et. al.

This article was inspired by materials from Rasmus Baath, who instructs about Bayesian data analysis in a very intuitive way. I highly recommend his course at DataCamp about Fundamentals of Bayesian Data Analysis in R and his videos on YouTube.

Well, this article has somehow already motivated readers to start loving Bayes. I would understand if you still prefer a non-Bayesian approach to machine learning. In this case you will get served by my article below.","['probability', 'nightmare', 'rate', 'prior', 'signup', 'bayesian', 'posterior', 'data', 'model', 'distribution', 'parameters', 'solved']","The building blocks of Bayesian data analysisThe essential characteristic of Bayesian methods is their explicit use of probability for quantifying uncertainty in inferences based on statistical data analysis.
Therefore, we filter out simulated samples where the simulated signup rate is not 6/16 and keep only those simulated samples where the simulated signup rate is 6/16.
Scaling Bayesian data analysisIn order to illustrate the generalization of Bayesian data analysis, let’s consider that the marketing department actually ran two campaigns.
This article was inspired by materials from Rasmus Baath, who instructs about Bayesian data analysis in a very intuitive way.
I highly recommend his course at DataCamp about Fundamentals of Bayesian Data Analysis in R and his videos on YouTube.",en,['Michel Kana'],2020-02-06 18:22:56.141000+00:00,"{'Data Science', 'Statistics', 'Machine Learning', 'Towards Data Science', 'Mathematics'}","{'https://miro.medium.com/max/60/1*U0v38-wuSm1z8S00iE6tyg.png?q=20', 'https://miro.medium.com/max/700/1*vDvaFxKrySmFXsxJ63BbGg.png', 'https://miro.medium.com/max/1060/1*dxZtKHvzYYnZkH2I3_0Q8g.png', 'https://miro.medium.com/max/1000/1*RSIg799FVASZt3v5-Iu0uQ.png', 'https://miro.medium.com/max/60/1*VB2v_VJMfr-dP0Ws34MVWw.png?q=20', 'https://miro.medium.com/max/1200/1*4Hm6fKMZ2srtn7_7Y_J2ZQ.png', 'https://miro.medium.com/max/342/1*XFrJ5As_JNmhN7EcdBM-zw.png', 'https://miro.medium.com/max/800/1*q8y7mfTdmMwnIoVyoI3jWQ.png', 'https://miro.medium.com/max/800/1*qVdV6QN-qY_mwHaPvOnaDQ.png', 'https://miro.medium.com/max/60/1*XFrJ5As_JNmhN7EcdBM-zw.png?q=20', 'https://miro.medium.com/max/60/1*q8y7mfTdmMwnIoVyoI3jWQ.png?q=20', 'https://miro.medium.com/max/306/1*ahs6P8czEWya82LbelpS0A.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*xlvZ1opH6zOFJpDSearjqg.png?q=20', 'https://miro.medium.com/max/1200/1*uaob1oqwle_YiL279ZY1rA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/46/1*wlRlzLsxD_MH7QjwHoShyg.png?q=20', 'https://miro.medium.com/max/1306/1*rSRd0_YgkfIA1YbWsBLWmA.png', 'https://miro.medium.com/max/1200/1*xlvZ1opH6zOFJpDSearjqg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1540/1*SE4qw8VWdwwtGq5xsNOLJQ.png', 'https://miro.medium.com/max/60/1*SE4qw8VWdwwtGq5xsNOLJQ.png?q=20', 'https://miro.medium.com/max/410/1*m09Tvvipp9pe3P-LK5m2LQ.png', 'https://miro.medium.com/max/60/1*4Hm6fKMZ2srtn7_7Y_J2ZQ.png?q=20', 'https://miro.medium.com/max/56/1*vDvaFxKrySmFXsxJ63BbGg.png?q=20', 'https://miro.medium.com/max/60/1*RSIg799FVASZt3v5-Iu0uQ.png?q=20', 'https://miro.medium.com/max/1844/1*U0v38-wuSm1z8S00iE6tyg.png', 'https://miro.medium.com/max/60/1*MFHARCD-ygXq74db8zSc5g.png?q=20', 'https://miro.medium.com/max/60/1*qVdV6QN-qY_mwHaPvOnaDQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*uaob1oqwle_YiL279ZY1rA.png?q=20', 'https://miro.medium.com/max/60/1*wblcsHBuX3_IrIsQqH8lsg.png?q=20', 'https://miro.medium.com/max/1000/1*MFHARCD-ygXq74db8zSc5g.png', 'https://miro.medium.com/max/60/1*ahs6P8czEWya82LbelpS0A.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*aVyIg7GQbGFxFphcjUqVAw.jpeg', 'https://miro.medium.com/max/1200/1*T9Am845x5UP3UNXGtJX41g.jpeg', 'https://miro.medium.com/fit/c/160/160/1*aVyIg7GQbGFxFphcjUqVAw.jpeg', 'https://miro.medium.com/max/60/1*rSRd0_YgkfIA1YbWsBLWmA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/600/1*wlRlzLsxD_MH7QjwHoShyg.png', 'https://miro.medium.com/max/1164/1*VB2v_VJMfr-dP0Ws34MVWw.png', 'https://miro.medium.com/max/60/1*T9Am845x5UP3UNXGtJX41g.jpeg?q=20', 'https://miro.medium.com/max/800/1*o1CjS0EsLylmNR1xrQQ0Mw.png', 'https://miro.medium.com/max/60/1*o1CjS0EsLylmNR1xrQQ0Mw.png?q=20', 'https://miro.medium.com/max/60/1*dxZtKHvzYYnZkH2I3_0Q8g.png?q=20', 'https://miro.medium.com/max/60/1*m09Tvvipp9pe3P-LK5m2LQ.png?q=20', 'https://miro.medium.com/max/1200/1*wblcsHBuX3_IrIsQqH8lsg.png', 'https://miro.medium.com/max/8194/1*T9Am845x5UP3UNXGtJX41g.jpeg'}",2020-03-05 00:21:00.682312,6.462134122848511
https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f,The Math Behind A/B Testing with Example Python Code,"Animation by Gal Shir

While taking the A/B testing course by Google on Udacity, I had some questions about some of the mathematical steps that were not clearly covered by the course. This is understandable because the course was intended to be a compressed and concise overview. To resolve my questions, I turned to other sources on the web and decided to summarize what I learned in this article.

Outline for A/B Tests

Set up the experiment. Run the test and record the success rate for each group. Plot the distribution of the difference between the two samples. Calculate the statistical power. Evaluate how sample size affects A/B tests.

1. Set Up The Experiment

We will run an A/B test for a hypothetical company that is trying to increase the amount of users that sign up for a premium account. The goal of running an A/B test is to evaluate if a change in a website will lead to improved performance in a specific metric. You may decide to test very simple alternatives such as changing the look of a single button on a webpage or testing different layouts and headlines. You could also run an A/B test on multi-step processes which may have many differences. Examples of this include the steps required in signing up a new user or processing the sale on an online marketplace. A/B testing is a huge subject and there are many techniques and rules on setting up an experiment. In addition to the Udacity course, here are a few other useful resources below:

For this article, I will keep it simple so that we can just focus on the math.

Baseline Conversion Rate and Lift

Before running the test, we will know the baseline conversion rate and the desired lift or increase in signups that we would like to test. The baseline conversion rate is the current rate at which we sign up new users under the existing design. For our example, we want to use our test to confirm that the changes we make to our signup process will result in at least a 2% increase in our sign up rate. We currently sign up 10 out of 100 users who are offered a premium account.

# code examples presented in Python

bcr = 0.10 # baseline conversion rate

d_hat = 0.02 # difference between the groups

Control Group (A) and Test Group (B)

Typically, the total number of users participating in the A/B test make up a small percentage of the total amount of users. Users are randomly selected and assigned to either a control group or a test group. The sample size that you decide on will determine how long you might have to wait until you have collected enough. For example, websites with large audiences may be able to collect enough data very quickly, while other websites may have to wait a number of weeks. There are some events that happen rarely even for high-traffic websites, so determining the necessary sample size will inform how soon you can assess your experiment and move on to improving other metrics.

Initially, we will collect 1000 users for each group and serve the current signup page to the control group and a new signup page to the test group.

# A is control; B is test

N_A = 1000

N_B = 1000

2. Run the Test

Because this is a hypothetical example, we will need “fake” data to work on. I wrote a function that will generate data for our simulation. The script can be found at my Github repo here.

ab_data = generate_data(N_A, N_B, bcr, d_hat)

The generate_data function returned the table on the left. Only the first five rows are shown. The converted column indicates whether a user signed up for the premium service or not with a 1 or 0, respectively. The A group will be used for our control group and the B group will be our test group.

Let’s look at a summary of the results using the pivot table function in Pandas.

ab_summary = ab_data.pivot_table(values='converted', index='group', aggfunc=np.sum) # add additional columns to the pivot table

ab_summary['total'] = ab_data.pivot_table(values='converted', index='group', aggfunc=lambda x: len(x))

ab_summary['rate'] = ab_data.pivot_table(values='converted', index='group')

It looks like the difference in conversion rates between the two groups is 0.028 which is greater than the lift we initially wanted of 0.02. This is a good sign but this is not enough evidence for us to confidently go with the new design. At this point we have not measured how confident we are in this result. This can be mitigated by looking at the distributions of the two groups.

3. Compare the Two Groups

We can compare the two groups by plotting the distribution of the control group and calculating the probability of getting the result from our test group. We can assume that the distribution for our control group is binomial because the data is a series of Bernoulli trials, where each trial only has two possible outcomes (similar to a coin flip).

fig, ax = plt.subplots(figsize=(12,6))

x = np.linspace(A_converted-49, A_converted+50, 100)

y = scs.binom(A_total, A_cr).pmf(x)

ax.bar(x, y, alpha=0.5)

ax.axvline(x=B_cr * A_total, c='blue', alpha=0.75, linestyle='--')

plt.xlabel('converted')

plt.ylabel('probability')

The distribution for the control group is shown in red and the result from the test group is indicated by the blue dashed line. We can see that the probability of getting the result from the test group was very low. However, the probability does not convey the confidence level of the results. It does not take the sample size of our test group into consideration. Intuitively, we would feel more confident in our results as our sample sizes grow larger. Let’s continue and plot the test group results as a binomial distribution and compare the distributions against each other.

Binomial Distribution

fig, ax = plt.subplots(figsize=(12,6))

xA = np.linspace(A_converted-49, A_converted+50, 100)

yA = scs.binom(A_total, p_A).pmf(xA)

ax.bar(xA, yA, alpha=0.5)

xB = np.linspace(B_converted-49, B_converted+50, 100)

yB = scs.binom(B_total, p_B).pmf(xB)

ax.bar(xB, yB, alpha=0.5)

plt.xlabel('converted')

plt.ylabel('probability')

Binomial distributions for the control (red) and test (blue) groups

We can see that the test group converted more users than the control group. We can also see that the peak of the test group results is lower than the control group. How do we interpret the difference in peak probability? We should focus instead on the conversion rate so that we have an apples-to-apples comparison. In order to calculate this, we need to standardize the data and compare the probability of successes, p, for each group.

Bernoulli Distribution and the Central Limit Theorem

To do this, first, consider the Bernoulli distribution for the control group.

where p is the conversion probability of the control group.

According to the properties of the Bernoulli distribution, the mean and variance are as follows:

According to the central limit theorem, by calculating many sample means we can approximate the true mean of the population, 𝜇, from which the data for the control group was taken. The distribution of the sample means, p, will be normally distributed around the true mean with a standard deviation equal to the standard error of the mean. The equation for this is given as:

Therefore, we can represent both groups as a normal distribution with the following properties:

The same can be done for the test group. So, we will have two normal distributions for p_A and p_B .

# standard error of the mean for both groups

SE_A = np.sqrt(p_A * (1-p_A)) / np.sqrt(A_total)

SE_B = np.sqrt(p_B * (1-p_B)) / np.sqrt(B_total) # plot the null and alternative hypothesis

fig, ax = plt.subplots(figsize=(12,6)) x = np.linspace(0, .2, 1000) yA = scs.norm(p_A, SE_A).pdf(x)

ax.plot(xA, yA)

ax.axvline(x=p_A, c='red', alpha=0.5, linestyle='--') yB = scs.norm(p_B, SE_B).pdf(x)

ax.plot(xB, yB)

ax.axvline(x=p_B, c='blue', alpha=0.5, linestyle='--') plt.xlabel('Converted Proportion')

plt.ylabel('PDF')

Control (red) and test (blue) groups as normal distributions for the proportion of successes

The dashed lines represent the mean conversion rate for each group. The distance between the red dashed line and the blue dashed line is equal to mean difference between the control and test group. d_hat is the distribution of the difference between random variables from the two groups.

Variance of the Sum

Recall that the null hypothesis states that the difference in probability between the two groups is zero. Therefore, the mean for this normal distribution will be at zero. The other property we will need for the normal distribution is the standard deviation or the variance. (Note: The variance is the standard deviation squared.) The variance of the difference will be dependent on the variances of the probability for both groups.

A basic property of variance is that the variance of the sum of two random independent variables is the sum of the variances.

This means that the null hypothesis and alternative hypothesis will have the same variance which will be the sum of the variances for the control group and the test group.

The standard deviation can then be calculated as:

If we put this equation in terms of standard deviation for the Bernoulli distribution, s:

and we get the Satterthwaite approximation for pooled standard error. If we calculate the pooled probability and use the pooled probability to calculate the standard deviation for both groups, we get:

where:

This is the same equation used in the Udacity course. Both equations for pooled standard error will give you very similar results.

With that, we now have enough information to construct the distributions for the null hypothesis and the alternative hypothesis.

Compare the Null Hypothesis vs. the Alternative Hypothesis

Let’s start off by defining the null hypothesis and the alternative hypothesis.

The null hypothesis is the position that the change in the design made for the test group would result in no change in the conversion rate.

in the conversion rate. The alternative hypothesis is the opposing position that the change in the design for the test group would result in an improvement (or reduction) in the conversion rate.

According to the Udacity course, the null hypothesis will be a normal distribution with a mean of zero and a standard deviation equal to the pooled standard error.

The null hypothesis

The alternative hypothesis has the same standard deviation as the null hypothesis, but the mean will be located at the difference in the conversion rate, d_hat . This makes sense because we can calculate the difference in the conversion rates directly from the data, but the normal distribution represents the possible values our experiment could have given us.

The alternative hypothesis

Now that we understand the derivation of the pooled standard error, we can just directly plot the null and alternative hypotheses for future experiments. I wrote a script for quickly plotting the null and alternative hypotheses, abplot , which can be found here.

# define the parameters for abplot()

# use the actual values from the experiment for bcr and d_hat

# p_A is the conversion rate of the control group

# p_B is the conversion rate of the test group n = N_A + N_B

bcr = p_A

d_hat = p_B - p_A

abplot(n, bcr, d_hat)

Null hypothesis (red) vs. alternative hypothesis (blue)

Visually, the plot for the null and alternative hypothesis looks very similar to the other plots above. Fortunately, both curves are identical in shape, so we can just compare the distance between the means of the two distributions. We can see that the alternative hypothesis curve suggests that the test group has a higher conversion rate than the control group. This plot also can be used to directly determine the statistical power.

4. Statistical Power and Significance Level

I think it is easier to define statistical power and significance level by first showing how they are represented in the plot of the null and alternative hypothesis. We can return a visualization of the statistical power by adding the parameter show_power=True .

abplot(N_A, N_B, bcr, d_hat, show_power=True)

Statistical power shown in green

The green shaded area represents the statistical power, and the calculated value for power is also displayed on the plot. The gray dashed lines in the plot above represent the confidence interval (95% for the plot above) for the null hypothesis. Statistical power is calculated by finding the area under the alternative hypothesis distribution and outside of the confidence interval of the null hypothesis.

After running our experiment, we get a resulting conversion rate for both groups. If we calculate the difference between the conversion rates, we end up with one result, the difference or the effect of the design change. Our task is to determine which population this result came from, the null hypothesis or the alternative hypothesis.

The area under the alternative hypothesis curve is equal to 1. If the alternative design is truly better, the power is the probability that we accept the alternative hypothesis and reject the null hypothesis and is equal to the area shaded green (true positive). The opposite area under the alternative curve is the probability that we accept the null hypothesis and reject the alternative hypothesis (false negative). This is referred to as beta in A/B testing or hypothesis testing and is shown below.

abplot(N_A, N_B, bcr, d_hat, show_beta=True)

Beta shown in green

The gray dashed line that divides the area under the alternative curve into two also directly segments the area associated with the significance level, often denoted with the greek letter alpha.

The green shaded area has an area equal to 0.025, which represents alpha.

If the null hypothesis is true and there truly is no difference between the control and test groups, then the significance level is the probability that we would reject the null hypothesis and accept the alternative hypothesis (false positive). A false positive is when we mistakenly conclude that the new design is better. This value is low because we want to limit this probability.

Oftentimes, a problem will be given with a desired confidence level instead of the significance level. A typical 95% confidence level for an A/B test corresponds to a significance level of 0.05.

Significance Level (alpha) and Confidence Level

It might be helpful to refer to a confusion matrix when you are evaluating the results of an A/B test and the different probability of outcomes.

Experiments are typically set up for a minimum desired power of 80%. If our new design is truly better, we want our experiment to show that there is at least an 80% probability that this is the case. Unfortunately, our current experiment only has a power of 0.594. We know that if we increase the sample size for each group, we will decrease the pooled variance for our null and alternative hypothesis. This will make our distributions much narrower and may increase the statistical power. Let’s take a look at how sample size will directly affect our results.

5. Sample Size

If we run our test again with a sample size of 2000 instead of 1000 for each group, we get the following results.

abplot(2000, 2000, bcr, d_hat, show_power=True)

Our curves for the null and alternative hypothesis have become more narrow and more of the area under the alternative curve is located on the right of the gray dashed line. The result for power is greater than 0.80 and meets our benchmark for statistical power. We can now say that our results are statistically significant.

A problem you may encounter is determining the minimum sample size you will need for your experiment. This is a common interview question and it is useful to know because it is directly related to how quickly you can complete your experiments and deliver statistically significant results to your design team. You can use the calculators available on the web, such as the ones below:

You will need the baseline conversion rate (bcr) and the minimum detectable effect, which is the minimum difference between the control and test group that you or your team will determine to be worth the investment of making the design change in the first place.

I wanted to write a script that would do the same calculation but needed to find the equation that was being used. After much searching, I found and tested this equation from this Stanford Lecture. (Warning: link opens Powerpoint download.)

Equation for minimum sample size

Many people calculate Z from tables such as those shown here and here. However, I am more of a visual learner and I like to refer to the plot of the Z-distribution from which the values are derived.

Plot for typical significance level of 0.05 or confidence level of 0.95 (z = 1.96)

Typical z-score for power level of 0.80 (z = 0.842

The code for these z-plots can be found in my Github repo here.

Here is the Python code that performs the same calculation for minimum sample size:

I can demonstrate that this equation returns a correct answer by running another A/B experiment with the sample size that results from the equation.

min_sample_size(bcr=0.10, mde=0.02)

Out: 3842.026 abplot(3843, 3843, 0.10, 0.02, show_power=True)

The calculated power for this sample size was approximately 0.80. Therefore, if our design change had an improvement in conversion of about 2 percent, we would need at least 3843 samples in each group for a statistical power of at least 0.80.

That was a very long but basic walkthrough of A/B tests. Once you have developed an understanding and familiarity with the procedure, you will probably be able to run an experiment and go directly to the plots for the null and alternative hypothesis to determine if your results achieved enough power. By calculating the minimum sample size you need prior to the experiment, you can determine how long it will take to get the results back to your team for a final decision.","['sample', 'test', 'conversion', 'ab', 'null', 'alternative', 'control', 'math', 'python', 'example', 'testing', 'hypothesis', 'distribution', 'group', 'power', 'code']","Users are randomly selected and assigned to either a control group or a test group.
The A group will be used for our control group and the B group will be our test group.
We can also see that the peak of the test group results is lower than the control group.
With that, we now have enough information to construct the distributions for the null hypothesis and the alternative hypothesis.
Compare the Null Hypothesis vs. the Alternative HypothesisLet’s start off by defining the null hypothesis and the alternative hypothesis.",en,['Nguyen Ngo'],2020-01-17 22:49:24.055000+00:00,"{'Product Development', 'Data Science', 'Data Visualization', 'Ab Testing', 'Data Analysis'}","{'https://miro.medium.com/max/2348/1*X4gMDtilM8XzX90bxA75pA.png', 'https://miro.medium.com/max/60/0*UdyX7UZPSCwis9PJ?q=20', 'https://miro.medium.com/max/60/1*A4MPteliElcxGZb4WPS5jQ.png?q=20', 'https://miro.medium.com/max/4836/1*9AhVDgPJa2PE47pdC45Sdw.png', 'https://miro.medium.com/max/190/0*QHZWNK4YOhNqq3Kw', 'https://miro.medium.com/max/60/1*Zd3heVFmHwp81v26LsBU6A.png?q=20', 'https://miro.medium.com/max/60/0*ABRMQFby2W7eAMHl?q=20', 'https://miro.medium.com/max/60/0*cqi5yST1Y40nHOv3?q=20', 'https://miro.medium.com/max/60/0*E8eVrIN-_GjClf5v?q=20', 'https://miro.medium.com/max/2466/1*WHsGzLwuGM1dreVIingizg.png', 'https://miro.medium.com/max/2324/1*HMIhlIQgNJXrbFWKOlj89w.png', 'https://miro.medium.com/max/3288/1*x6DINjpdiQK948bxPHAYBA.png', 'https://miro.medium.com/max/534/0*E8eVrIN-_GjClf5v', 'https://miro.medium.com/max/60/1*833KqTUt5jFfG5BwiBAdSA.png?q=20', 'https://miro.medium.com/max/360/0*CovoN2E2MA4LvWMn', 'https://miro.medium.com/max/3182/1*BPP6QYVIx4yqXrFfzUgTBg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*pdsB3rgXcdmQuYmjbTE6dg.png?q=20', 'https://miro.medium.com/max/60/1*cqnWsQJ00dQNWXifl8K9cg.png?q=20', 'https://miro.medium.com/max/3468/1*Zd3heVFmHwp81v26LsBU6A.png', 'https://miro.medium.com/max/60/0*U7xkH_7xwFySX9Gz?q=20', 'https://miro.medium.com/max/1600/1*sQDMVPZPBa32shY-JZChRg.gif', 'https://miro.medium.com/max/60/0*XRg36EwD4jQQfQ2z?q=20', 'https://miro.medium.com/max/54/1*EDGfsj6kuUjohTUtJ-9hpg.png?q=20', 'https://miro.medium.com/max/298/0*ABRMQFby2W7eAMHl', 'https://miro.medium.com/max/2834/1*8LVDtWvkXvuK2CXdF_zk-Q.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*HMIhlIQgNJXrbFWKOlj89w.png?q=20', 'https://miro.medium.com/max/2406/1*LoA3ApxTM2IwsHGb4gXaMQ.png', 'https://miro.medium.com/max/60/1*WHsGzLwuGM1dreVIingizg.png?q=20', 'https://miro.medium.com/max/4896/1*ecbIjZjjYgsVo_GUJw2fwg.png', 'https://miro.medium.com/max/60/0*oXCR83ZaQPYMxtIt?q=20', 'https://miro.medium.com/max/60/1*T838jBxHMddyN9EM3Nl0Iw.png?q=20', 'https://miro.medium.com/max/60/0*CovoN2E2MA4LvWMn?q=20', 'https://miro.medium.com/max/60/0*pwtp3EtwYDMqcRd1?q=20', 'https://miro.medium.com/max/60/0*vMthUKVgFOFDVHS_?q=20', 'https://miro.medium.com/max/60/1*8LVDtWvkXvuK2CXdF_zk-Q.png?q=20', 'https://miro.medium.com/max/2538/1*A4MPteliElcxGZb4WPS5jQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*qXqLi-RAHb_c4lDv?q=20', 'https://miro.medium.com/max/310/0*6LZDIZ8QlLd4D9bE', 'https://miro.medium.com/max/160/0*oXCR83ZaQPYMxtIt', 'https://miro.medium.com/max/700/0*XRg36EwD4jQQfQ2z', 'https://miro.medium.com/max/384/0*dnZDh2NqhbsXo8sx', 'https://miro.medium.com/max/2404/1*833KqTUt5jFfG5BwiBAdSA.png', 'https://miro.medium.com/max/2376/1*pdsB3rgXcdmQuYmjbTE6dg.png', 'https://miro.medium.com/max/936/1*T838jBxHMddyN9EM3Nl0Iw.png', 'https://miro.medium.com/max/60/1*1ZNedvFV_X44PCBzeKI2qw.png?q=20', 'https://miro.medium.com/max/60/0*dnZDh2NqhbsXo8sx?q=20', 'https://miro.medium.com/max/606/0*cqi5yST1Y40nHOv3', 'https://miro.medium.com/max/60/0*QHZWNK4YOhNqq3Kw?q=20', 'https://miro.medium.com/max/376/0*pwtp3EtwYDMqcRd1', 'https://miro.medium.com/max/60/0*6LZDIZ8QlLd4D9bE?q=20', 'https://miro.medium.com/max/1132/0*qXqLi-RAHb_c4lDv', 'https://miro.medium.com/max/60/0*DF4Ikn26llR4Bfgo?q=20', 'https://miro.medium.com/max/2004/1*1ZNedvFV_X44PCBzeKI2qw.png', 'https://miro.medium.com/fit/c/160/160/1*tC4-bfix5QcCAnczR9JDDg.jpeg', 'https://miro.medium.com/max/60/1*9AhVDgPJa2PE47pdC45Sdw.png?q=20', 'https://miro.medium.com/max/60/1*X4gMDtilM8XzX90bxA75pA.png?q=20', 'https://miro.medium.com/max/60/0*7z_Wx76kN_6v8sfL?q=20', 'https://miro.medium.com/max/580/1*EDGfsj6kuUjohTUtJ-9hpg.png', 'https://miro.medium.com/max/156/0*vMthUKVgFOFDVHS_', 'https://miro.medium.com/freeze/max/60/1*sQDMVPZPBa32shY-JZChRg.gif?q=20', 'https://miro.medium.com/max/282/0*U7xkH_7xwFySX9Gz', 'https://miro.medium.com/max/60/1*LoA3ApxTM2IwsHGb4gXaMQ.png?q=20', 'https://miro.medium.com/max/448/0*7z_Wx76kN_6v8sfL', 'https://miro.medium.com/max/270/0*DF4Ikn26llR4Bfgo', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/800/1*sQDMVPZPBa32shY-JZChRg.gif', 'https://miro.medium.com/max/60/1*x6DINjpdiQK948bxPHAYBA.png?q=20', 'https://miro.medium.com/max/60/1*BPP6QYVIx4yqXrFfzUgTBg.png?q=20', 'https://miro.medium.com/max/60/1*ecbIjZjjYgsVo_GUJw2fwg.png?q=20', 'https://miro.medium.com/max/2210/1*cqnWsQJ00dQNWXifl8K9cg.png', 'https://miro.medium.com/max/536/0*UdyX7UZPSCwis9PJ', 'https://miro.medium.com/fit/c/96/96/1*tC4-bfix5QcCAnczR9JDDg.jpeg'}",2020-03-05 00:21:07.392101,6.708747386932373
https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f,A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning,"Hyperparameter Optimization

The aim of hyperparameter optimization in machine learning is to find the hyperparameters of a given machine learning algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine learning engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. I like to think of hyperparameters as the model settings to be tuned.)

Hyperparameter optimization is represented in equation form as:

Here f(x) represents an objective score to minimize— such as RMSE or error rate— evaluated on the validation set; x* is the set of hyperparameters that yields the lowest value of the score, and x can take on any value in the domain X. In simple terms, we want to find the model hyperparameters that yield the best score on the validation set metric.

The problem with hyperparameter optimization is that evaluating the objective function to find the score is extremely expensive. Each time we try different hyperparameters, we have to train a model on the training data, make predictions on the validation data, and then calculate the validation metric. With a large number of hyperparameters and complex models such as ensembles or deep neural networks that can take days to train, this process quickly becomes intractable to do by hand!

Grid search and random search are slightly better than manual tuning because we set up a grid of model hyperparameters and run the train-predict -evaluate cycle automatically in a loop while we do more productive things (like feature engineering). However, even these methods are relatively inefficient because they do not choose the next hyperparameters to evaluate based on previous results. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.

For example, if we have the following graph with a lower score being better, where does it make sense to concentrate our search? If you said below 200 estimators, then you already have the idea of Bayesian optimization! We want to focus on the most promising hyperparameters, and if we have a record of evaluations, then it makes sense to use this information for our next choice.

Random and grid search pay no attention to past results at all and would keep searching across the entire range of the number of estimators even though it’s clear the optimal answer (probably) lies in a small region!

Bayesian Optimization

Bayesian approaches, in contrast to random or grid search, keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function:

In the literature, this model is called a “surrogate” for the objective function and is represented as p(y | x). The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function. In other words:

Build a surrogate probability model of the objective function Find the hyperparameters that perform best on the surrogate Apply these hyperparameters to the true objective function Update the surrogate model incorporating the new results Repeat steps 2–4 until max iterations or time is reached

The aim of Bayesian reasoning is to become “less wrong” with more data which these approaches do by continually updating the surrogate probability model after each evaluation of the objective function.

At a high-level, Bayesian optimization methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential compared to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, Bayesian methods can find better model settings than random search in fewer iterations.

If there’s one thing to take away from this article it’s that Bayesian model-based methods can find better hyperparameters in less time because they reason about the best set of hyperparameters to evaluate based on past trials.

As a good visual description of what is occurring in Bayesian Optimization take a look at the images below (source). The first shows an initial estimate of the surrogate model — in black with associated uncertainty in gray — after two evaluations. Clearly, the surrogate model is a poor approximation of the actual objective function in red:

The next image shows the surrogate function after 8 evaluations. Now the surrogate almost exactly matches the true function. Therefore, if the algorithm selects the hyperparameters that maximize the surrogate, they will likely yield very good results on the true evaluation function.

Bayesian methods have always made sense to me because they operate in much the same way we do: we form an initial view of the world (called a prior) and then we update our model based on new experiences (the updated model is called a posterior). Bayesian hyperparameter optimization takes that framework and applies it to finding the best value of model settings!","['machine', 'conceptual', 'hyperparameters', 'set', 'function', 'bayesian', 'learning', 'hyperparameter', 'model', 'search', 'objective', 'explanation', 'optimization', 'score', 'surrogate']","Hyperparameter OptimizationThe aim of hyperparameter optimization in machine learning is to find the hyperparameters of a given machine learning algorithm that return the best performance as measured on a validation set.
If you said below 200 estimators, then you already have the idea of Bayesian optimization!
At a high-level, Bayesian optimization methods are efficient because they choose the next hyperparameters in an informed manner.
As a good visual description of what is occurring in Bayesian Optimization take a look at the images below (source).
Clearly, the surrogate model is a poor approximation of the actual objective function in red:The next image shows the surrogate function after 8 evaluations.",en,['Will Koehrsen'],2018-07-02 20:53:28.248000+00:00,"{'Bayesian Machine Learning', 'Machine Learning', 'Towards Data Science', 'Computer Science', 'Education'}","{'https://miro.medium.com/max/60/1*luY6Ahh7uttR4quIcgOCBw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/840/1*luY6Ahh7uttR4quIcgOCBw.png', 'https://miro.medium.com/max/60/1*QR4_VOfAAWLVe2I0nqwtTg.png?q=20', 'https://miro.medium.com/max/1246/1*ybiePL_8lKNouHlSb5OSgQ.png', 'https://miro.medium.com/max/2614/1*RQ-pAwQ88yC904QppChGPQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*E0_THdPH2NfKB37JUQB8Eg.png?q=20', 'https://miro.medium.com/max/512/1*4D1QpDZzWpBOl7ANBhsSJA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*YfoPLKK8_WXIsRaQ7zcSjg.png?q=20', 'https://miro.medium.com/max/2014/1*E0_THdPH2NfKB37JUQB8Eg.png', 'https://miro.medium.com/max/60/1*e6cIETdFd1rzD9ivofNJqw.png?q=20', 'https://miro.medium.com/max/60/1*RQ-pAwQ88yC904QppChGPQ.png?q=20', 'https://miro.medium.com/max/2628/1*bSLAe1LCj3mMKfaZsQWCrw.png', 'https://miro.medium.com/max/1120/1*MiNXGrkk5BbjfkNAXZQSNA.png', 'https://miro.medium.com/max/614/1*u00KlxHhd1fz6-Jaeou6PA.png', 'https://miro.medium.com/max/790/1*e6cIETdFd1rzD9ivofNJqw.png', 'https://miro.medium.com/max/60/0*aBsprZzniYMB0KWc.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1120/1*H5pyf3G115WGJwPpg65yaQ.png', 'https://miro.medium.com/max/582/1*idWxsGylqq2ZaMGpHmbxDg.png', 'https://miro.medium.com/max/60/1*bSLAe1LCj3mMKfaZsQWCrw.png?q=20', 'https://miro.medium.com/max/60/1*6SH5O_ail54karro8j0NGg.png?q=20', 'https://miro.medium.com/max/60/1*u00KlxHhd1fz6-Jaeou6PA.png?q=20', 'https://miro.medium.com/max/1160/1*6SH5O_ail54karro8j0NGg.png', 'https://miro.medium.com/max/60/1*MiNXGrkk5BbjfkNAXZQSNA.png?q=20', 'https://miro.medium.com/max/796/1*YfoPLKK8_WXIsRaQ7zcSjg.png', 'https://miro.medium.com/max/60/1*CtJD4zJr6PNxbUZMwZxPKA.jpeg?q=20', 'https://miro.medium.com/max/60/1*4D1QpDZzWpBOl7ANBhsSJA.png?q=20', 'https://miro.medium.com/max/60/1*H5pyf3G115WGJwPpg65yaQ.png?q=20', 'https://miro.medium.com/max/716/1*ebsqjhOTSGKBbIR_RLkjSQ.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/780/1*QR4_VOfAAWLVe2I0nqwtTg.png', 'https://miro.medium.com/max/60/1*ybiePL_8lKNouHlSb5OSgQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*ebsqjhOTSGKBbIR_RLkjSQ.png?q=20', 'https://miro.medium.com/max/1200/1*CtJD4zJr6PNxbUZMwZxPKA.jpeg', 'https://miro.medium.com/max/3840/1*CtJD4zJr6PNxbUZMwZxPKA.jpeg', 'https://miro.medium.com/max/60/1*idWxsGylqq2ZaMGpHmbxDg.png?q=20', 'https://miro.medium.com/max/960/0*aBsprZzniYMB0KWc.png'}",2020-03-05 00:21:10.128460,2.735360860824585
https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a,Automated Machine Learning Hyperparameter Tuning in Python,"Now, let’s define the entire domain:

Here we use a number of different domain distribution types:

choice : categorical variables

: categorical variables quniform : discrete uniform (integers spaced evenly)

: discrete uniform (integers spaced evenly) uniform : continuous uniform (floats spaced evenly)

: continuous uniform (floats spaced evenly) loguniform : continuous log uniform (floats spaced evenly on a log scale)

(There are other distributions as well listed in the documentation.)

There is one important point to notice when we define the boosting type:

Here we are using a conditional domain which means the value of one hyperparameter depends on the value of another. For the boosting type ""goss"" , the gbm cannot use subsampling (selecting only a subsample fraction of the training observations to use on each iteration). Therefore, the subsample ratio is set to 1.0 (no subsampling) if the boosting type is ""goss"" but is 0.5–1.0 otherwise. This is implemented using a nested domain.

Conditional nesting can be useful when we are using different machine learning models with completely separate parameters. A conditional lets us use different sets of hyperparameters depending on the value of a choice .

Now that our domain is defined, we can draw one example from it to see what a typical sample looks like. When we sample, because subsample is initially nested, we need to assign it to a top-level key. This is done using the Python dictionary get method with a default value of 1.0.

{'boosting_type': 'gbdt',

'class_weight': 'balanced',

'colsample_bytree': 0.8111305579351727,

'learning_rate': 0.16186471096789776,

'min_child_samples': 470.0,

'num_leaves': 88.0,

'reg_alpha': 0.6338327001528129,

'reg_lambda': 0.8554826167886239,

'subsample_for_bin': 280000.0,

'subsample': 0.6318665053932255}

(This reassigning of nested keys is necessary because the gradient boosting machine cannot deal with nested hyperparameter dictionaries).

Optimization Algorithm

Although this is the most conceptually difficult part of Bayesian Optimization, creating the optimization algorithm in Hyperopt is a single line. To use the Tree Parzen Estimator the code is:

from hyperopt import tpe # Algorithm

tpe_algorithm = tpe.suggest

That’s all there is to it! Hyperopt only has the TPE option along with random search, although the GitHub page says other methods may be coming. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.

Result History

Keeping track of the results is not strictly necessary as Hyperopt will do this internally for the algorithm. However, if we want to find out what is going on behind the scenes, we can use a Trials object which will store basic training information and also the dictionary returned from the objective function (which includes the loss and params ). Making a trials object is one line:

from hyperopt import Trials # Trials object to track progress

bayes_trials = Trials()

Another option which will allow us to monitor the progress of a long training run is to write a line to a csv file with each search iteration. This also saves all the results to disk in case something catastrophic happens and we lose the trials object (speaking from experience). We can do this using the csv library. Before training we open a new csv file and write the headers:

and then within the objective function we can add lines to write to the csv on every iteration (the complete objective function is in the notebook):

Writing to a csv means we can check the progress by opening the file while training (although not in Excel because this will cause an error in Python. Use tail out_file.csv from bash to view the last rows of the file).

Optimization

Once we have the four parts in place, optimization is run with fmin :

Each iteration, the algorithm chooses new hyperparameter values from the surrogate function which is constructed based on the previous results and evaluates these values in the objective function. This continues for MAX_EVALS evaluations of the objective function with the surrogate function continually updated with each new result.

Results

The best object that is returned from fmin contains the hyperparameters that yielded the lowest loss on the objective function:

{'boosting_type': 'gbdt',

'class_weight': 'balanced',

'colsample_bytree': 0.7125187075392453,

'learning_rate': 0.022592570862044956,

'min_child_samples': 250,

'num_leaves': 49,

'reg_alpha': 0.2035211643104735,

'reg_lambda': 0.6455131715928091,

'subsample': 0.983566228071919,

'subsample_for_bin': 200000}

Once we have these hyperparameters, we can use them to train a model on the full training data and then evaluate on the testing data (remember we can only use the test set once, when we evaluate the final model). For the number of estimators, we can use the number of estimators that returned the lowest loss in cross validation with early stopping. Final results are below:

The best model scores 0.72506 AUC ROC on the test set.

The best cross validation score was 0.77101 AUC ROC.

This was achieved after 413 search iterations.

As a reference, 500 iterations of random search returned a model that scored 0.7232 ROC AUC on the test set and 0.76850 in cross validation. A default model with no optimization scored 0.7143 ROC AUC on the test set.

There are a few important notes to keep in mind when we look at the results:

The optimal hyperparameters are those that do best in cross validation and not necessarily those that do best on the testing data. When we use cross validation, we hope that these results generalize to the testing data. Even using 10-fold cross-validation, the hyperparameter tuning overfits to the training data. The best score from cross-validation is significantly higher than that on the testing data. Random search may return better hyperparameters just by sheer luck (re-running the notebook can change the results). Bayesian optimization is not guaranteed to find better hyperparameters and can get stuck in a local minimum of the objective function.

Bayesian optimization is effective, but it will not solve all our tuning problems. As the search progresses, the algorithm switches from exploration — trying new hyperparameter values — to exploitation — using hyperparameter values that resulted in the lowest objective function loss. If the algorithm finds a local minimum of the objective function, it might concentrate on hyperparameter values around the local minimum rather than trying different values located far away in the domain space. Random search does not suffer from this issue because it does not concentrate on any values!

Another important point is that the benefits of hyperparameter optimization will differ with the dataset. This is a relatively small dataset (~ 6000 training observations) and there is a small payback to tuning the hyperparameters (getting more data would be a better use of time!). With all of those caveats in mind, in this case, with Bayesian optimization we can get:

Better performance on the testing set

Fewer iterations to tune the hyperparameters

Bayesian methods can (although will not always) yield better tuning results than random search. In the next few sections, we will examine the evolution of the Bayesian hyperparameter search and compare to random search to understand how Bayesian Optimization works.

Visualizing Search Results

Graphing the results is an intuitive way to understand what happens during the hyperparameter search. Moreover, it’s helpful to compare Bayesian Optimization to random search so we can see how the methods differ. To see how the plots are made and random search is implemented, see the notebook, but here we will go through the results. (As a note, the exact results will change across iterations, so if you run the notebook, don’t be surprised if you get different images. All of these plots are made with 500 iterations).","['machine', 'hyperparameters', 'automated', 'function', 'python', 'learning', 'hyperparameter', 'tuning', 'training', 'search', 'objective', 'random', 'optimization', 'results', 'using']","Hyperopt only has the TPE option along with random search, although the GitHub page says other methods may be coming.
This continues for MAX_EVALS evaluations of the objective function with the surrogate function continually updated with each new result.
As the search progresses, the algorithm switches from exploration — trying new hyperparameter values — to exploitation — using hyperparameter values that resulted in the lowest objective function loss.
Another important point is that the benefits of hyperparameter optimization will differ with the dataset.
Visualizing Search ResultsGraphing the results is an intuitive way to understand what happens during the hyperparameter search.",en,['Will Koehrsen'],2018-07-04 01:16:37.424000+00:00,"{'Data Science', 'Automation', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/1824/1*dw6VrNPpfVQuzSbHpqRSRw.png', 'https://miro.medium.com/max/1690/1*V4NBLoeePElKQeGqur8pFg.png', 'https://miro.medium.com/max/60/1*v7N67eMfFxeabPGBW_-7HQ.png?q=20', 'https://miro.medium.com/max/60/1*dH-n2BtATMKpkMuMcnpyDA.png?q=20', 'https://miro.medium.com/max/60/1*9KuQTZQqbAaiCJ-wH9UixQ.jpeg?q=20', 'https://miro.medium.com/max/2370/1*6pWbEJoqNwonzolxD4KpCw.png', 'https://miro.medium.com/max/60/1*p_SKapLKyyZXqRm437TgKw.png?q=20', 'https://miro.medium.com/max/1758/1*p_SKapLKyyZXqRm437TgKw.png', 'https://miro.medium.com/max/1024/1*kzLTXwKXkywDFUcR3bJeuQ.png', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*1EPxm6czkBcyZNqVd8UCXg.png?q=20', 'https://miro.medium.com/max/10368/1*9KuQTZQqbAaiCJ-wH9UixQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1200/1*9KuQTZQqbAaiCJ-wH9UixQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1036/1*Gm1yXk6qM-3NbYLKew9Vgw.png', 'https://miro.medium.com/max/1678/1*sy-axn8KuB1GC4mss0EmuA.png', 'https://miro.medium.com/max/60/1*Gm1yXk6qM-3NbYLKew9Vgw.png?q=20', 'https://miro.medium.com/max/60/1*kzLTXwKXkywDFUcR3bJeuQ.png?q=20', 'https://miro.medium.com/max/1736/1*pSlIV25n8bsxqPiNwPWzBw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*pSlIV25n8bsxqPiNwPWzBw.png?q=20', 'https://miro.medium.com/max/2324/1*dH-n2BtATMKpkMuMcnpyDA.png', 'https://miro.medium.com/max/60/1*LYrDpsvyfYtM143qIwaXLg.png?q=20', 'https://miro.medium.com/max/2526/1*xn-A948AcHROFSP173D1RA.png', 'https://miro.medium.com/max/1736/1*1EPxm6czkBcyZNqVd8UCXg.png', 'https://miro.medium.com/max/60/1*6pWbEJoqNwonzolxD4KpCw.png?q=20', 'https://miro.medium.com/max/60/1*sy-axn8KuB1GC4mss0EmuA.png?q=20', 'https://miro.medium.com/max/60/1*V4NBLoeePElKQeGqur8pFg.png?q=20', 'https://miro.medium.com/max/1780/1*v7N67eMfFxeabPGBW_-7HQ.png', 'https://miro.medium.com/max/60/1*VNhV0ATCudF890dLo0YIvQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/3406/1*LYrDpsvyfYtM143qIwaXLg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*dw6VrNPpfVQuzSbHpqRSRw.png?q=20', 'https://miro.medium.com/max/60/1*xn-A948AcHROFSP173D1RA.png?q=20', 'https://miro.medium.com/max/1372/1*VNhV0ATCudF890dLo0YIvQ.png'}",2020-03-05 00:21:16.539073,6.409612655639648
https://medium.com/vantageai/bringing-back-the-time-spent-on-hyperparameter-tuning-with-bayesian-optimisation-2e21a3198afb,Using Bayesian Optimization to reduce the time spent on hyperparameter tuning,"Having constructed our train and test sets, our GridSearch / Random Search function and defined our Pipeline, we can now go back and have a closer look at the three core components of Bayesian Optimisation, being 1) the search space to sample from, 2) the objective function and, 3) the surrogate- and selection functions.

1) Choosing the search space

Bayesian Optimisation operates along probability distributions for each parameter that it will sample from. These distributions have to be set by a user. Specifying the distribution for each parameter is one of the subjective parts in the process. One approach to go about this is to start wide and let the optimisation algorithm do the heavy lifting. One could then in a subsequent run opt to focus on specific areas around the optimal parameters retrieved from the previous run.

Parameter domains can be defined along a number of Hyperopt-specific distribution functions. In order to provide a level playing field, let us work along the same minima and maxima we are using in the GridSearch and Random Search parameter domains we previously defined. When it comes to the learning rate, we opt for a continuous log uniform distribution (evenly distributing float values on a logarithmic scale). We do this as we ideally want to tap onto values such 0.01, 0.1 and 1, instead of a regular evenly distributed set of say 0.1, 0.2, 0.3,…,1. hp.loguniform enables us to set up the learning rate distribution accordingly. The hyperparameters max_depth, n_estimators and num_leaves require integers as input. In addition to this requirement, and like the learning rate, we have to make an assumption about their distributions. Generally, it is equitable to assume for these parameters that each value in the specified range has an equal probability. Discrete uniform probability distributions like this can be put in place with hp.quniform in combination with scope.int . The type of gradient boosting technique consists of a list of categories. Parameters in this particular area can be represented by means of hp.choice . Last but not least, we have our regularisation parameter lambda and colsample_bytree, which both require floats. Again, we assume the values of these parameters are equally likely. hp.uniform enables us to employ uniform continuous distributions.

2) Objective function

The objective function (1) serves as the main evaluator of hyperparameter combinations. It simply takes in a set of hyperparameters and outputs a score that indicates how well a set of hyperparameters performs on the validation set. For our classification problem we will use the ‘accuracy’ score as the evaluation metric of choice. In this case, clearly the aim is to maximise the objective function. However, evaluating each set of hyperparameters by means of the objective function can become quite a costly operation when dealing with large hyperparameter grids as we each time need to train a model in order to put it forward for evaluation on the validation set. Hence, calling this evaluation should ideally be restricted to a minimum.

(1) the objective function and (2) the optimal set of parameters that maximise the accuracy score

In a Hyperopt setting, the objective function in its simplest way can be defined as follows:

It takes in a set of hyperparameters, trains a classifier and returns a cross validation score. Note that our function returns a negative score, this as the Hyperopt optimiser requires a value to minimise. Maximising accuracy scores is equal to minimising negative accuracy scores. This score is passed on to the optimiser, which we’ll deal with in a second. The status key returns the parameters evaluated.

3) Surrogate function and selection function

Ideally, we only want to fire the objective function once we are reasonably sure that we have a set of parameters that leads to a decrease in the validation score. Why not construct some kind of a lightweight assistant that tracks parameter sets evaluated thus far and uses this to advise us on the parameters to put forward to the objective function?

This is where the (3) surrogate- and the (4) selection function come in. Both work together to propose those parameters of which it believes will bring the highest accuracy on the objective function.

The surrogate function can be interpreted as an approximation of the objective function. It is used to propose parameter sets to the objective function that likely yield an improvement in terms of accuracy score.

There are different algorithms for the surrogate of the objective function. Hyperopt uses something called the Tree Parzen Estimator (TPE). Alternatively approaches for the surrogate are the use of Gaussian Processes (through GPyOpt) or a Random Forest Regression (by means of SMAC).

In the case of TPE, the surrogate function is a probabilistic model that maps hyperparameters to a probability of a score on the objective function. In short, it is an application of Bayes’ Theorem (4). A prior distribution is updated to a posterior distribution every time more data becomes available. The data in this case is the score we get back from the objective function on the parameter we have asked it to evaluate. With each iteration it becomes a more accurate predictor of validation scores.

The surrogate function

Good old Bayes’ Theorem

The probability of a set of hyperparameters given an accuracy score , p(x|y), has a bit of a twist to it when it comes to TPE. This probability is broken into two separate distributions, being l(x) and g(x). The former defines the distribution of hyperparameters when the (negative) accuracy score is lower than a score threshold, y* (e.g. the top accuracy score achieved thus far). The latter represents the hyper parameter distribution for scores above this same threshold.

l(x) and g(x) each are a Gaussian mixture, or a linear combination of Gaussians. Each time a hyperparameter set is evaluated by the objective function it gets a corresponding Gaussian within mixture l(x) or g(x) depending on the validation score it got back. The mean of this corresponding Gaussian is the point sampled from the prior distribution. Its standard deviation is the greater of the distance to its immediate neighbour on either side. Each mixture is updated after each iteration after which it is used to sample the next hyperparameter combination that maximises Expected Improvement.

The probability of a set of hyperparameters given an accuracy score broken down into two distributions

In Bayesian Optimisation the hyperparameters that are put forward for evaluation by the objective function are selected by applying a criterion to the surrogate function. This criterion is defined by a selection function. A common approach is to use a metric called Expected Improvement. For a TPE-based surrogate function, the Expected Improvement is given by the following equation:

Getting to the hyperparameter set that maximises Expected Improvement

One of the main take-aways of this equation is that one should maximise the ratio l(x) / g(x) in order to maximise the Expected Improvement. To maximise this ratio, hyperparameters values which are more likely under l(x) than under g(x) should be drawn. In other words, you want a high probability of a set of hyperparameters given a negative accuracy score below the threshold y*. This is exactly what TPE does. It samples hyperparameters from l(x), screens them in terms of the ratio l(x) / g(x), and puts forward the set that corresponds to the largest expected improvement.

The threshold y* is defined by the variable γ, which represents the quantile of of the negative accuracy scores (observed thus far) to use as cut-off point. This threshold is set at 15% by default.

The other take-away of the Expected Improvement equation is its ability to trade-off exploitation and exploration. High values of Expected Improvement correspond to points where the surrogate function predicts a high accuracy, whilst they also relate to points where the prediction uncertainty is high. However, the hyperparameter set that maximises Expected Improvement is no guarantee for improved performance versus the best hyperparameter set thus far — predominantly as we’re dealing with hyperparameter probability distributions that are driving Expected Improvement instead of fixed values.

The optimisation process

So how do all these ingredients come together? After setting the initial parameter distributions and putting the process in action Bayesian Optimisation works as follows:

For each iteration:

Find the set of hyperparameter values that maximise the Expected Improvement by optimising the selection function over the surrogate function Hand this hyperparameter combination to the objective function for evaluation — and retrieve the corresponding score Update the surrogate function along the feedback of the objective function by applying Bayes’ theorem.

The surrogate function initially starts off as a weak approximation of the objective function as its probability distributions for l(x) and g(x) are widely defined (i.e. they resemble our initial parameter distributions). However, at each step the feedback retrieved from the objective function is used to update these distributions by means of Bayesian reasoning. Each time l(x) and g(x) become a more accurate reflection of the actual distribution of sets below and above the threshold, *y — and thus a better predictor of validation scores given a hyperparameter set. At each step a new hyperparameter set is chosen that maximises Expected Improvement. This selection is made based on the updated distributions l(x) and g(x).

Setting up the optimisation in Hyperopt

The most complex part of Bayesian Optimisation, setting up the surrogate- and selection function and kicking off the algorithm in Hyperopt, can be done in one single line by using Hyperopt’s fmin function. To use the Tree Parzen Estimator for the surrogate function, simply import tpe . In addition, you put in your objective function, parameter grid and the number of iterations and off you go.

In order to see what is going on under the hood, we add one additional argument, being the Trials function. This function stores basic training information and the dictionary returned from the objective function (being the scores and their corresponding parameters).

Hyperopt taking on GridSearch and Random Search

Let’s break it down and see how Hyperopt compares to GridSearch and Random Search on our dummy dataset. For the sake of ease, we have wrapped our objective function and optimiser in one function, as shown below.

Let us run GridSearch, Random Search and our Hyperopt function on the parameter grids defined above. The latter two require the number of iterations to be specified, which we will set at 75 for both so that we have a level playing field.

GridSearch takes on all 1,451 parameter combinations and does this in about 4 minutes with an accuracy score on the test set of 79%. As expected, Random Search and GridSearch are notably faster as they solely have to evaluate 75 hyperparameter combinations. Hyperopt ends up a bit slower than Random Search, but note the significantly lower number of iterations it took to get to the optimum. Also, it manages to get to a relatively better score on the test set. This is why you would want to use Hyperopt. However, do keep in mind that Hyperopt not always ends up on top. Random Search could bump into the optimal set right at the start just by luck.","['hyperparameters', 'set', 'expected', 'function', 'parameter', 'spent', 'bayesian', 'tuning', 'hyperparameter', 'reduce', 'objective', 'optimization', 'score', 'surrogate', 'accuracy', 'using']","The surrogate function can be interpreted as an approximation of the objective function.
However, the hyperparameter set that maximises Expected Improvement is no guarantee for improved performance versus the best hyperparameter set thus far — predominantly as we’re dealing with hyperparameter probability distributions that are driving Expected Improvement instead of fixed values.
However, at each step the feedback retrieved from the objective function is used to update these distributions by means of Bayesian reasoning.
In addition, you put in your objective function, parameter grid and the number of iterations and off you go.
For the sake of ease, we have wrapped our objective function and optimiser in one function, as shown below.",en,['Mike Kraus'],2019-03-28 07:58:22.694000+00:00,"{'Hyperparameter Tuning', 'Machine Learning', 'Bayesian Optimization', 'Data Science'}","{'https://miro.medium.com/fit/c/80/80/1*0bLxcaXrpHTV4W0Qeu0RfA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*DXGWN5McvYvdZTlefl02nA.png', 'https://miro.medium.com/max/60/1*LOTqTaovGIIkt9eYuETHZQ.png?q=20', 'https://miro.medium.com/max/60/1*xkpqnZyps6o3pH1vupq8kQ.jpeg?q=20', 'https://miro.medium.com/max/2976/1*XSECVm2Z_k2_v0fMK2RV_g.png', 'https://miro.medium.com/fit/c/80/80/2*l3gtLYjbHli-lcrjPZqVpA.png', 'https://miro.medium.com/max/916/1*LOTqTaovGIIkt9eYuETHZQ.png', 'https://miro.medium.com/max/60/1*gBlw88GHHSESrtKNoCsJ4g.png?q=20', 'https://miro.medium.com/max/60/1*OGACb_1A6GUzhGRWgV3HeQ.png?q=20', 'https://miro.medium.com/max/4104/1*-GYJL-9v34RNlCwdSO0QXA.png', 'https://miro.medium.com/fit/c/96/96/2*l3gtLYjbHli-lcrjPZqVpA.png', 'https://miro.medium.com/fit/c/80/80/0*tBvp6e4RX7ESQyBq', 'https://miro.medium.com/max/2620/1*x-D2-RP0663U-3HozDRuNA.png', 'https://miro.medium.com/max/1004/1*sLJJuL6Knqq0fAP5Oy5m1A.png', 'https://miro.medium.com/max/60/1*x-D2-RP0663U-3HozDRuNA.png?q=20', 'https://miro.medium.com/max/2704/1*tg5ynmF8aaIAH7rnMcHfMg.png', 'https://miro.medium.com/max/12000/1*xkpqnZyps6o3pH1vupq8kQ.jpeg', 'https://miro.medium.com/max/2572/1*OGACb_1A6GUzhGRWgV3HeQ.png', 'https://miro.medium.com/fit/c/160/160/2*l3gtLYjbHli-lcrjPZqVpA.png', 'https://miro.medium.com/max/142/1*acaCOzeFw04ZNw8-s99Dtg.png', 'https://miro.medium.com/max/918/1*14tVf99gCCuVKwUnCxZ6EQ.png', 'https://miro.medium.com/max/60/1*14tVf99gCCuVKwUnCxZ6EQ.png?q=20', 'https://miro.medium.com/max/60/1*XSECVm2Z_k2_v0fMK2RV_g.png?q=20', 'https://miro.medium.com/max/3108/1*gBlw88GHHSESrtKNoCsJ4g.png', 'https://miro.medium.com/max/60/1*-GYJL-9v34RNlCwdSO0QXA.png?q=20', 'https://miro.medium.com/max/1200/1*xkpqnZyps6o3pH1vupq8kQ.jpeg', 'https://miro.medium.com/max/60/1*sLJJuL6Knqq0fAP5Oy5m1A.png?q=20', 'https://miro.medium.com/max/60/1*tg5ynmF8aaIAH7rnMcHfMg.png?q=20'}",2020-03-05 00:21:18.062174,1.522139310836792
https://medium.com/spikelab/hyperparameter-optimization-using-bayesian-optimization-f1f393dcd36d,Hyperparameter Optimization using bayesian optimization,"Hyperparameter Optimization using bayesian optimization

How to find the best hyperparameters for your machine learning model without losing your mind

Hyperparameters Optimization

Choosing the right parameters for your machine learning algorithm is a hard and crucial task, since it can make a big difference on the performance of a model. These parameters can be tuned in a manual or automatic way.

The manual way implies training and testing models, manually changing the parameters at each step. This could end up being a time consuming task and maybe you’ll never find the optimal parameters. On the other hand, we could use algorithms that start with a potential set of hyperparameters, and try to optimize them automatically.

Almost all machine learning libraries and frameworks include some automatic hyperparameters optimization algorithms, Here, we’ll talk about two of these: RandomSearch and GridSearch.

In a GridSearch, the key idea is to define a set of parameter values and train the model for all possible combinations and then save the best one. This method is pretty good if you have a simple model, but if your model takes some time to train (like in almost all Deep Learning models) or if your hyperparameter space is too big, this approach could not be the best one, because of the time required to do it.

The RandomSearch algorithm is pretty similar, but instead of using all possible combinations, it randomly assigns a value (within a defined range) for each hyperparameter, so the required time could decrease significantly. However, it might not find the optimal set.

Bayesian Optimization is an alternative way to efficiently get the best hyperparameters for your model, and we’ll talk about this next.

Bayesian Optimization

As Fernando Nogueira explains in his amazing python package bayesian-optimization:

Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not

We can see this in the image below

As you iterate over and over, the algorithm balances its needs of exploration and exploitation taking into account what it knows about the target function. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored.

Using Bayesian Optimization, we can explore the parameter space in a smarter way, and thus reduce the time required to do this process.

You can learn more about Bayesian Optimization here

Using Bayesian Optimization with H2O.ai

I’m going to use H2O.ai and the python package bayesian-optimization developed by Fernando Nogueira. The goal is to optimize the hyperparameters of a regression model using GBM as our machine learning algorithm.

Show me the data!

I chose the red wine quality dataset on Kaggle because it’s a simple dataset (and I live in Chile, we love wine!!) which you can use to train regression or classification models. I always use it when I want to play with a new machine learning algorithm.

The dataset contains a set of features that determine the quality of wine like: pH, citric acidity, sulphates, alcohol, etc. The data looks like this:

Red wine quality dataset

So our model will try to predict the quality of the wine.

Let’s go to the code

First, import h2o and bayesian-optimization, then start a H2O’s server:

import h2o

from h2o.estimators.gbm import H2OGradientBoostingEstimator

from bayes_opt import BayesianOptimization

h2o.init()

h2o.remove_all()

Let’s load our dataset into a H2O’s frame, we are going to split our dataset into train and test, 70% will be used to train. Internally H2O’s uses cross-validation. So we can save the test data just to validate our final model.

Our target will be the quality of the wine.

data = h2o.upload_file(“data/winequality-red.csv”)

train_cols = [x for x in data.col_names if x not in [‘quality’]]

target = ""quality""

train, test = data.split_frame(ratios=[0.7])

The bayesian optimization package requires a function to optimize, and this function must return a number. In our case this number will be the metric (or cost function) that we want to minimize. I choose to minimize the root mean squared error (remember, we are going to train a regression model), so the function returns this value.

def train_model(max_depth,

ntrees,

min_rows,

learn_rate,

sample_rate,

col_sample_rate):

params = {

'max_depth': int(max_depth),

'ntrees': int(ntrees),

'min_rows': int(min_rows),

'learn_rate':learn_rate,

'sample_rate':sample_rate,

'col_sample_rate':col_sample_rate

}

model = H2OGradientBoostingEstimator(nfolds=5,**params)

model.train(x=train_cols, y=target, training_frame=train)

return -model.rmse()

The function returns -model.rmse() because, as we will see soon, the optimizer by default is designed to maximize functions.

Now, we have to define the parameter space:

bounds = {

'max_depth':(5,10),

'ntrees': (100,500),

'min_rows':(10,30),

'learn_rate':(0.001, 0.01),

'sample_rate':(0.5,0.8),

'col_sample_rate':(0.5,0.8)

}

With this done, it’s time to define our optimizer. This receives a python function and the hyperparameter space. Then we can set the number of initial points, and how many iterations we want. The number of iterations will be equal to how many models we are going to train.

optimizer = BayesianOptimization(

f=train_model,

pbounds=bounds,

random_state=1,

)

optimizer.maximize(init_points=10, n_iter=50)

Runing this, we get an output at each step:

Output of our optimization at each step

Finally we can get the best hyperparameters for our model

optimizer.max {'target': -0.35322220505969215,

'params': {'col_sample_rate': 0.8,

'learn_rate': 0.01,

'max_depth': 10.0,

'min_rows': 10.0,

'ntrees': 300.0,

'sample_rate': 0.8}}

Conclusions

We can use Bayesian Optimization for efficiently tuning hyperparameters of our model. As we saw in our example, this just involves defining a few helper functions. We considered a machine learning example, but Bayesian Optimization can be used to optimize a wide variety of black box problems.

We can integrate the package developed by Fernando Nogueira with almost all popular machine learning libraries like h2o, sklearn, tensorflow, XGboost, CatBoost, etc.

You can find the full example here in GitHub","['machine', 'hyperparameters', 'function', 'train', 'bayesian', 'learning', 'hyperparameter', 'model', 'quality', 'optimization', 'best', 'using']","Almost all machine learning libraries and frameworks include some automatic hyperparameters optimization algorithms, Here, we’ll talk about two of these: RandomSearch and GridSearch.
Bayesian Optimization is an alternative way to efficiently get the best hyperparameters for your model, and we’ll talk about this next.
Using Bayesian Optimization, we can explore the parameter space in a smarter way, and thus reduce the time required to do this process.
You can learn more about Bayesian Optimization hereUsing Bayesian Optimization with H2O.aiI’m going to use H2O.ai and the python package bayesian-optimization developed by Fernando Nogueira.
We considered a machine learning example, but Bayesian Optimization can be used to optimize a wide variety of black box problems.",en,['Matias Aravena Gamboa'],2019-04-01 15:50:30.489000+00:00,"{'Ia', 'ML', 'Artificial Intelligence', 'Machine Learning', 'Bayesian Optimization'}","{'https://miro.medium.com/max/2204/1*1_PdcS7tBUi2LTDkzeLRuQ.png', 'https://miro.medium.com/max/4308/1*oN15nE6G4Oq1wumsE6e3NA.png', 'https://miro.medium.com/fit/c/96/96/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/max/60/1*oolaSv2UFiSin97ASWnWSA.png?q=20', 'https://miro.medium.com/max/3516/1*oolaSv2UFiSin97ASWnWSA.png', 'https://miro.medium.com/max/72/1*jmI1clbffK5X9gbWJt0EJw.png', 'https://miro.medium.com/fit/c/160/160/1*jJQAwYdaaVuSNMPhX0GfzA.png', 'https://miro.medium.com/max/1102/1*1_PdcS7tBUi2LTDkzeLRuQ.png', 'https://miro.medium.com/fit/c/80/80/2*pIhE1Xo0j7pItNNPMU55Og.jpeg', 'https://miro.medium.com/fit/c/80/80/2*r1Xiqq17iPdp3Lk2RJEzCw.png', 'https://miro.medium.com/fit/c/80/80/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/max/60/1*oN15nE6G4Oq1wumsE6e3NA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/max/60/1*1_PdcS7tBUi2LTDkzeLRuQ.png?q=20'}",2020-03-05 00:21:18.802851,0.7396755218505859
https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0,Simple Transformers — Named Entity Recognition with Transformer Models,"Preface

The Simple Transformers library was conceived to make Transformer models easy to use. Transformers are incredibly powerful (not to mention huge) deep learning models which have been hugely successful at tackling a wide variety of Natural Language Processing tasks. Simple Transformers enabled the application of Transformer models to Sequence Classification tasks (binary classification initially, but with multiclass classification added soon after) with only three lines of code.

I am delighted to announce that Simple Transformers now supports Named Entity Recognition, another common NLP task, alongside Sequence Classification.

Links to other capabilities:

The Simple Transformers library is built on top of the excellent Transformers library by Hugging Face. The Hugging Face Transformers library is the library for researchers and other people who need extensive control over how things are done. It is also the best choice when you need to stray off the beaten path, do things differently, or do new things altogether. Simple Transformers is, well, a lot simpler.

Introduction

You want to try out that brilliant idea, you want to roll up your sleeves and get to work but the thousands of lines of code full of cryptic (but cool) looking stuff can be intimidating even to a veteran NLP researcher. The core idea behind Simple Transformers is that using Transformers doesn’t need to be difficult (or frustrating).

Simple Transformers abstracts away all the complicated setup code while retaining flexibility and room for configuration as far as possible. A Transformer model can be used in just three lines of code, one line for initializing, one for training, and one for evaluation.

This post demonstrates how to perform NER using Simple Transformers.

All source code is available on the Github Repo. If you have any issues or questions, that’s the place to resolve them. Please do check it out!

Installation

Install Anaconda or Miniconda Package Manager from here Create a new virtual environment and install the required packages.

conda create -n transformers python pandas tqdm

conda activate transformers

If using cuda:

conda install pytorch cudatoolkit=10.0 -c pytorch

else:

conda install pytorch cpuonly -c pytorch

conda install -c anaconda scipy

conda install -c anaconda scikit-learn

pip install transformers

pip install tensorboardx

pip install seqeval Install simpletransformers.

pip install simpletransformers

Usage

To demonstrate Named Entity Recognition, we’ll be using the CoNLL Dataset. Getting hold of this dataset can be a little tricky, but I found a version of it on Kaggle that works for our purpose.

Data Preparation

Download the dataset from Kaggle. Extract the text files to the data/ directory. (It should contain 3 text files train.txt, valid.txt, test.txt . We’ll be using the train and test files. You can use the valid file to perform hyperparameter tuning to improve model performance.

Simple Transformers’ NER model can be used with either .txt files or with pandas DataFrames . For a usage example with DataFrames , please refer to the minimal start example for NER in the repo docs.

When using your own datasets, the input text files should follow the CoNLL format. Each line in the file should contain one word and its related tags separated by a single space each. Simple Transformers assumes the first “word” in a line is the actual word, and that the last “word” in a line is its assigned label. To denote a new sentence, an empty line is added between the last word of the previous sentence and the first word of the next sentence. However, it may be easier to use the DataFrame approach when using custom datasets.

The NERModel

We create a NERModel that can be used for training, evaluation, and prediction in NER tasks. The full parameter list for a NERModel object is given below.

model_type : The type of model (bert, roberta)

: The type of model (bert, roberta) model_name : Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).

: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin). labels (optional): A list of all Named Entity labels. If not given, [“O”, “B-MISC”, “I-MISC”, “B-PER”, “I-PER”, “B-ORG”, “I-ORG”, “B-LOC”, “I-LOC”] will be used.

(optional): A list of all Named Entity labels. If not given, [“O”, “B-MISC”, “I-MISC”, “B-PER”, “I-PER”, “B-ORG”, “I-ORG”, “B-LOC”, “I-LOC”] will be used. args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.

(optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args. use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.

To load a model a previously saved model instead of a default model, you can change the model_name to the path to a directory which contains a saved model.

model = NERModel(‘bert’, ‘path_to_model/’)

A NERModel contains a python dict args with many attributes that provide control over hyperparameters. A detailed description of each is provided in the repo docs. The default values are shown below.

Any of these attributes can be modified when creating a NERModel or when calling its train_model method by simply passing in a dict containing the key-value pairs to be updated. An example is given below.

Training the Model

As promised, training can be done in a single line of code.

The train_model method will create a checkpoint (save) of the model at every nth step where n is self.args['save_steps'] . Upon completion of training, the final model will be saved to self.args['output_dir'] .

Loading a saved model is shown below.

Evaluating the Model

Again, evaluation is just a single line of code.

Here, the three return values are:

result : Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score)

: Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score) model_outputs : List of raw model outputs

: List of raw model outputs preds_list : List of predicted tags

The evaluation results I obtained are given here for reference.

{'eval_loss': 0.10684790916955669, 'precision': 0.9023580786026201, 'recall': 0.9153082919914954, 'f1_score': 0.9087870525112148}

Not too shabby for a single run with default hyperparameter values!

Putting it all together

Prediction and Testing

In real-world applications, we often have no idea what the true labels should be. To perform predictions on arbitrary examples, you can use the predict method. This method is fairly similar to the eval_model method except that this takes in a list of text and returns a list of predictions and a list of model outputs.

predictions, raw_outputs = model.predict([""Some arbitary sentence""])

Wrapping Up

Simple Transformers provides a quick and easy way to perform Named Entity Recognition (and other token level classification tasks). To steal a line from the man behind BERT himself, Simple Transformers is “conceptually simple and empirically powerful”.","['models', 'recognition', 'transformer', 'install', 'transformers', 'entity', 'simple', 'list', 'containing', 'default', 'model', 'named', 'line', 'using']","I am delighted to announce that Simple Transformers now supports Named Entity Recognition, another common NLP task, alongside Sequence Classification.
Links to other capabilities:The Simple Transformers library is built on top of the excellent Transformers library by Hugging Face.
The core idea behind Simple Transformers is that using Transformers doesn’t need to be difficult (or frustrating).
model_type : The type of model (bert, roberta): The type of model (bert, roberta) model_name : Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).
: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).",en,['Thilina Rajapakse'],2019-10-29 15:24:09.890000+00:00,"{'Simple Transformers', 'Data Science', 'Artificial Intelligence', 'Python', 'NLP'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*EBWanyh_UMq96bYshlcsPw.jpeg', 'https://miro.medium.com/max/10368/1*EBWanyh_UMq96bYshlcsPw.jpeg', 'https://miro.medium.com/max/60/1*EBWanyh_UMq96bYshlcsPw.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/96/96/2*oBM4rR-W3VS_mtPKrbcIpA.jpeg', 'https://miro.medium.com/fit/c/160/160/2*oBM4rR-W3VS_mtPKrbcIpA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:21:28.143390,9.339549541473389
https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0,An Introductory Example of Bayesian Optimization in Python with Hyperopt,"Optimization Example in Hyperopt

Formulating an optimization problem in Hyperopt requires four parts:

Objective Function: takes in an input and returns a loss to minimize Domain space: the range of input values to evaluate Optimization Algorithm: the method used to construct the surrogate function and choose the next values to evaluate Results: score, value pairs that the algorithm uses to build the model

Once we know how to specify these four parts, they can be applied to any optimization problem. For now, we will walk through a basic problem.

Objective Function

The objective function can be any function that returns a real value that we want to minimize. (If we have a value that we want to maximize, such as accuracy, then we just have our function return the negative of that metric.)

Here we will use polynomial function with the code and graph shown below:

This problem is 1-D because we are optimizing over a single value, x. In Hyperopt, the objective function can take in any number of inputs but must return a single loss to minimize.

Domain Space

The domain space is the input values over which we want to search. As a first try, we can use a uniform distribution over the range that our function is defined:

from hyperopt import hp # Create the domain space

space = hp.uniform('x', -5, 6)

To visualize the domain, we can draw samples from the space and plot the histogram:

Uniform domain space

If we have an idea where the best values are, then we can make a smarter domain that places more probability in higher scoring regions. (See the notebook for an example of using a normal distribution on this problem.)

Optimization Algorithm

While this is technically the most difficult concept, in Hyperopt creating an optimization algorithm only requires one line. We are using the Tree-structured Parzen Estimator model, and we can have Hyperopt configure it for us using the suggest method.

from hyperopt import tpe # Create the algorithm

tpe_algo = tpe.suggest

There’s a lot of theory going on behind the scenes we don’t have to worry about! In the notebook, we also use a random search algorithm for comparison.

Results (Trials)

This is not strictly necessary as Hyperopt keeps track of the results for the algorithm internally. However, if we want to inspect the progression of the alogorithm, we need to create a Trials object that will record the values and the scores:

from hyperopt import Trials # Create a trials object

tpe_trials = Trials()

Optimization

Now that the problem is defined, we can minimize our objective function! To do so, we use the fmin function that takes the four parts above, as well as a maximum number of trials:

{'x': 4.878208088771056}

For this run, the algorithm found the best value of x (the one which minimizes the loss) in just under 1000 trials. The best object only returns the input value that minimizes the function. While this is what we are looking for, it doesn’t give us much insight into the method. To get more details, we can get the results from the trials object:

Visualizations are useful for an intuitive understanding of what is occurring. For example, let’s plot the values of x evaluated in order:

Over time, the input values cluster around the optimal indicated by the red line. This is a simple problem, so the algorithm does not have much trouble finding the best value of x.

To contrast with what a naive search looks like, if we run the same problem with random search we get the following figure:

The random search basically tries values, well, at random! The differences between the values become even more apparent when we look at the histogram of values for x of the TPE algorithm and random search:","['algorithm', 'function', 'python', 'example', 'hyperopt', 'introductory', 'bayesian', 'problem', 'search', 'random', 'optimization', 'values', 'trials', 'value', 'domain']","In Hyperopt, the objective function can take in any number of inputs but must return a single loss to minimize.
Optimization AlgorithmWhile this is technically the most difficult concept, in Hyperopt creating an optimization algorithm only requires one line.
In the notebook, we also use a random search algorithm for comparison.
To contrast with what a naive search looks like, if we run the same problem with random search we get the following figure:The random search basically tries values, well, at random!
The differences between the values become even more apparent when we look at the histogram of values for x of the TPE algorithm and random search:",en,['Will Koehrsen'],2018-06-28 18:07:33.255000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/1300/1*Qfk1dEE_Ipfx8v9MLsdmfA.jpeg', 'https://miro.medium.com/max/60/1*cF4PRvXaz0oeDn9r1aANOg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/1306/1*iNiCVxg59vMryDRouqUzoA.png', 'https://miro.medium.com/max/60/1*2qDZxQkRoP28CidZtoT-gQ.png?q=20', 'https://miro.medium.com/max/60/1*VtPt7tt6Ya4QsYnxSewb_Q.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1118/1*2qDZxQkRoP28CidZtoT-gQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*rNcXXToCHfFvN6zTEgX5mg.png?q=20', 'https://miro.medium.com/max/526/1*cF4PRvXaz0oeDn9r1aANOg.png', 'https://miro.medium.com/max/60/1*iNiCVxg59vMryDRouqUzoA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/650/1*Qfk1dEE_Ipfx8v9MLsdmfA.jpeg', 'https://miro.medium.com/max/60/1*fqs6C_hylKgFV1SOtDlBpg.png?q=20', 'https://miro.medium.com/max/60/1*FPBWcAEGPUAK81KWyqpNAA.png?q=20', 'https://miro.medium.com/max/60/1*xfcy8-0DU1LBUuzTVnpqGA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/884/1*xfcy8-0DU1LBUuzTVnpqGA.png', 'https://miro.medium.com/max/1126/1*rNcXXToCHfFvN6zTEgX5mg.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*Qfk1dEE_Ipfx8v9MLsdmfA.jpeg?q=20', 'https://miro.medium.com/max/1126/1*FPBWcAEGPUAK81KWyqpNAA.png', 'https://miro.medium.com/max/1134/1*fqs6C_hylKgFV1SOtDlBpg.png', 'https://miro.medium.com/max/1116/1*VtPt7tt6Ya4QsYnxSewb_Q.png'}",2020-03-05 00:21:34.759276,6.614847421646118
https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a,Cryptocurrency Price Prediction Using Deep Learning,"Where is the code?

Without much ado, let’s get started with the code. The complete project on github can be found here.

I started with loading all the libraries and dependencies required.

import json

import requests

from keras.models import Sequential

from keras.layers import Activation, Dense, Dropout, LSTM

import matplotlib.pyplot as plt

import numpy as np

import pandas as pd

import seaborn as sns

from sklearn.metrics import mean_absolute_error

%matplotlib inline

I have used Canadian exchange rate and stored the real time data into a pandas data-frame. I used to_datetime() method to convert string Date time into Python Date time object. This is necessary as Date time objects in the file are read as a string object. Performing operations like time difference on a string rather a Date Time object is much easy.

endpoint = 'https://min-api.cryptocompare.com/data/histoday'

res = requests.get(endpoint + '?fsym=BTC&tsym=CAD&limit=500')

hist = pd.DataFrame(json.loads(res.content)['Data'])

hist = hist.set_index('time')

hist.index = pd.to_datetime(hist.index, unit='s')

target_col = 'close'

Let’s see how the dataset looks like with all the trading features like price, volume, open, high, low.

hist.head(5)

Next, I split the data into two sets — training set and test set with 80% and 20% data respectively. The decision made here is just for the purpose of this tutorial. In real projects, you should always split your data into training, validation, testing (like 60%, 20%, 20%).

def train_test_split(df, test_size=0.2):

split_row = len(df) - int(test_size * len(df))

train_data = df.iloc[:split_row]

test_data = df.iloc[split_row:]

return train_data, test_data train, test = train_test_split(hist, test_size=0.2)

Now let’s plot the cryptocurrency prices in Canadian dollars as a function of time using the below code:

def line_plot(line1, line2, label1=None, label2=None, title='', lw=2):

fig, ax = plt.subplots(1, figsize=(13, 7))

ax.plot(line1, label=label1, linewidth=lw)

ax.plot(line2, label=label2, linewidth=lw)

ax.set_ylabel('price [CAD]', fontsize=14)

ax.set_title(title, fontsize=16)

ax.legend(loc='best', fontsize=16) line_plot(train[target_col], test[target_col], 'training', 'test', title='')

We can observe that there is a clear dip in prices between December 2018 and April 2019. The prices keep on increasing from April 2019 to August 2019 with fluctuations happening in the months of July and August. From September 2019 onward prices are constantly decreasing. The interesting thing to be noted from this price fluctuation is that the prices are low in winter and it increases in the summer. Although this can’t be generalized as the dataset under consideration is just a small sample that is for a year. Also with cryptocurrency it’s hard to generalize anything.

Next, I made a couple of functions to normalize the values. Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.

def normalise_zero_base(df):

return df / df.iloc[0] - 1



def normalise_min_max(df):

return (df - df.min()) / (data.max() - df.min())

Next, I made a function to extract data of windows which are of size 5 each as shown in the code below:

def extract_window_data(df, window_len=5, zero_base=True):

window_data = []

for idx in range(len(df) - window_len):

tmp = df[idx: (idx + window_len)].copy()

if zero_base:

tmp = normalise_zero_base(tmp)

window_data.append(tmp.values)

return np.array(window_data)

I continued with making a function to prepare the data in a format to be later fed into the neural network. I used the same concept of splitting the data into two sets — training set and test set with 80% and 20% data respectively as shown in the code below:

def prepare_data(df, target_col, window_len=10, zero_base=True, test_size=0.2):

train_data, test_data = train_test_split(df, test_size=test_size)

X_train = extract_window_data(train_data, window_len, zero_base)

X_test = extract_window_data(test_data, window_len, zero_base)

y_train = train_data[target_col][window_len:].values

y_test = test_data[target_col][window_len:].values

if zero_base:

y_train = y_train / train_data[target_col][:-window_len].values - 1

y_test = y_test / test_data[target_col][:-window_len].values - 1



return train_data, test_data, X_train, X_test, y_train, y_test

LSTM

It works by using special gates to allow each LSTM layer to take information from both previous layers and the current layer. The data goes through multiple gates (like forget gate, input gate, etc.) and various activation functions (like the tanh function, relu function) and is passed through the LSTM cells. The main advantage of this is that it allows each LSTM cell to remember patterns for a certain amount of time. The thing to be noted is that LSTM can remember important information and at the same time forget irrelevant information. The LSTM architectures is shown below:

LSTM architecture

Now let’s build the model. Sequential model is used for stacking all the layers (input, hidden and output). The neural network comprises of a LSTM layer followed by 20% Dropout layer and a Dense layer with linear activation function. I complied the model using Adam as the optimizer and Mean Squared Error as the loss function.

def build_lstm_model(input_data, output_size, neurons=100, activ_func='linear', dropout=0.2, loss='mse', optimizer='adam'):

model = Sequential()

model.add(LSTM(neurons, input_shape=(input_data.shape[1], input_data.shape[2])))

model.add(Dropout(dropout))

model.add(Dense(units=output_size))

model.add(Activation(activ_func))

model.compile(loss=loss, optimizer=optimizer)

return model

Next, I set up some of the parameters to be used later. These parameters are — random number seed, length of the window, test set size, number of neurons in LSTM layer, epochs, batch size, loss, dropouts and optimizer.

np.random.seed(42)

window_len = 5

test_size = 0.2

zero_base = True

lstm_neurons = 100

epochs = 20

batch_size = 32

loss = 'mse'

dropout = 0.2

optimizer = 'adam'

Now let’s train the model using inputs x_train and labels y_train .

train, test, X_train, X_test, y_train, y_test = prepare_data(

hist, target_col, window_len=window_len, zero_base=zero_base, test_size=test_size) model = build_lstm_model(

X_train, output_size=1, neurons=lstm_neurons, dropout=dropout, loss=loss,

optimizer=optimizer)

history = model.fit(

X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)

Let us take a look at snapshot during model training for 20 epochs.

Training of the neural network

I used Mean Absolute Error (MAE) as the evaluation metric. The reason behind choosing MAE over Root Mean Squared Error (RMSE) is that MAE is more interpretable. RMSE does not describe average error alone and hence is much more difficult to understand. Since we want the model to be readily explained even to the non technical audience, MAE looks like a better choice.

Mean Absolute Error

It measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between actual and predicted observations where all individual differences have equal weight.

targets = test[target_col][window_len:]

preds = model.predict(X_test).squeeze()

mean_absolute_error(preds, y_test)

# 0.027955859325876943

The MAE value obtained looks good. Finally, let’s plot the actual and predicted prices using the below code:","['y_train', 'set', 'function', 'cryptocurrency', 'prediction', 'used', 'lstm', 'price', 'learning', 'prices', 'deep', 'data', 'model', 'test', 'layer', 'using']","hist.head(5)Next, I split the data into two sets — training set and test set with 80% and 20% data respectively.
and various activation functions (like the tanh function, relu function) and is passed through the LSTM cells.
The neural network comprises of a LSTM layer followed by 20% Dropout layer and a Dense layer with linear activation function.
These parameters are — random number seed, length of the window, test set size, number of neurons in LSTM layer, epochs, batch size, loss, dropouts and optimizer.
np.random.seed(42)window_len = 5test_size = 0.2zero_base = Truelstm_neurons = 100epochs = 20batch_size = 32loss = 'mse'dropout = 0.2optimizer = 'adam'Now let’s train the model using inputs x_train and labels y_train .",en,['Abhinav Sagar'],2019-12-03 18:49:38.914000+00:00,"{'Neural Networks', 'Deep Learning', 'Artificial Intelligence', 'Machine Learning', 'Technology'}","{'https://miro.medium.com/fit/c/160/160/2*ks56lissSz1lWjizk263EQ.jpeg', 'https://miro.medium.com/max/1560/1*XkYcLrD87EZNdp4uWUnWJA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/2*ks56lissSz1lWjizk263EQ.jpeg', 'https://miro.medium.com/max/1598/1*-VXdg4PezpmCSzQGDop6gg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*-VXdg4PezpmCSzQGDop6gg.png?q=20', 'https://miro.medium.com/max/1200/1*Tz18n1AReRLLMtUbr0jR0A.jpeg', 'https://miro.medium.com/freeze/max/60/1*CAveNR7cueMjbFLcOGNRgg.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/56/1*XkYcLrD87EZNdp4uWUnWJA.png?q=20', 'https://miro.medium.com/max/60/1*jhat36DbocuR43tlQ93VOw.png?q=20', 'https://miro.medium.com/max/60/1*dqNZXivRrYtXnWuFQqzDcg.png?q=20', 'https://miro.medium.com/max/1592/1*dqNZXivRrYtXnWuFQqzDcg.png', 'https://miro.medium.com/max/1592/1*jhat36DbocuR43tlQ93VOw.png', 'https://miro.medium.com/max/630/1*CAveNR7cueMjbFLcOGNRgg.gif', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/7262/1*Tz18n1AReRLLMtUbr0jR0A.jpeg', 'https://miro.medium.com/max/60/1*Tz18n1AReRLLMtUbr0jR0A.jpeg?q=20', 'https://miro.medium.com/max/4466/1*D9X8_4icjB1HYmW08xmVUA.png', 'https://miro.medium.com/max/60/1*D9X8_4icjB1HYmW08xmVUA.png?q=20'}",2020-03-05 00:21:36.884280,2.1240034103393555
https://towardsdatascience.com/how-to-draw-insights-from-cryptocurrencies-with-machine-learning-52ef318c4c1f,How to draw insights from cryptocurrencies with machine learning,"How to draw insights from cryptocurrencies with machine learning

A handy tutorial that summarizes what you can do to load, process and make useful models out of cryptocurrency datasets.

Source: Pixabay

The cryptocurrency craze has spawned a wide variety of use cases and an interesting amount of data. As many of the blockchains that power these cryptocurrencies are open and public by default, there are vast amounts of data being generated across different blockchains. Since each data point creates economic value, there have been a variety of projects and actors looking into the technology, from tax regulators to those looking to predict future cryptocurrency prices.

I was one of the first to write about new cryptocurrencies and the use of Bitcoin in remittance payments for TechCrunch and VentureBeat. I have a familiarity with the subject, and I also am a data science lover who has helped Springboard work on its machine learning bootcamp with job guarantee, so I’m always looking for ways to combine both the vast data generated by cryptocurrencies and the machine learning projects you can build on them with real-world implications.

Fortunately, Google BigQuery makes it easy to process data for free from some of the world’s leading Bitcoin-based cryptocurrencies, from Bitcoin to Zcash.

Google was able to build a simple classifier that detected whether or not a transaction came from mining pools. You are able to import a library in iPython notebook and get started using SQL to query that massive dataset. Let’s have some fun with some sample data queries, and see what we can do with that data with the Bitcoin data Google has on hand.

There are two main tables to examine here in the schema: a “blocks” table and a “transactions” table, which are joinable.

Here’s the schema for the blocks table:

Source: Author screenshot

Here, we need to know a few basic definitions. A blockchain is composed of a series of blocks that contain transaction data and which are confirmed sequentially after a flow of time. Using the query of block_id, we can actually narrow down to a particular block in a blockchain.

Let’s play around with the basic concepts of the blockchain with this explorer. Looking at the first block, we see that there were 50 Bitcoin output for the first block. The block height (1) indicates that it is the first block mined and validated in the Bitcoin mainnet blockchain, which is the in-production version of Bitcoin that people use to transact financial value between one another (in contrast to testnets, which are for staging purposes only).

You can query for the previous block_id in the blockchain using the previous_block variable. In this case, with a block height of 1, there is no previous_block reference. But if you looked at the block height of 2, previous_block would then refer to the genesis block/the block with the block height of 1.

The Merkle root is a hash of all the hashed transactions within a block that allows you to verify the entire block’s integrity with much lower computational cost.

Each block also has a timestamp that has a relatively accurate metric for when it was validated and which serves as an additional layer of authentication against blockchain attackers. The difficulty target in each block is a measure of how much computational power it takes to mine a block: the difficulty is adjusted every 2016 blocks to ensure a continual balanced rate of about one block being mined every 10 minutes. Since Bitcoin is a proof-of-work system, mining blocks is what allows the system to arrive to a consensus on its current state, no matter the distributed amount of nodes and transactions present.

Each block has a nonce that satisfies the network’s requirements and means that the block can be mined. There is a version marker that talks about the version of the core configuration of Bitcoin code, and can be used as a marker of what conditions and specific rules a block was mined under.

Work_terahash is then meant as an expression of how much computational power was required to contribute and mine a block. In the case of Bitcoin now, serious miners will run terahashes (trillions of attempted Bitcoin problem solutions) — up to 13 per second. This metric can serve as a proxy (along with difficulty level) of how many computational resources it takes to mine a block.

You can play around with different queries on the BigQuery explorer to test them out, though you will be rate-limited, especially with larger data queries that return gigabytes of data. This limit is quite easy to bump against with rich blockchain data.

Google BigQuery has a specific function where you need to use backdashes (`) surrounding table names for it to be a valid query as well.

As a first query, we can look at the first block in Bitcoin, which is the genesis block. We see that the Merkle root for that is “0e3e2357e806b6cdb1f70b54c3a3a17b6714ee1f0e68bebb44a74b1efd512098”. Selecting the block_id for that Merkle root gets us the block_id of the genesis block, 00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048. When we check the blockchain explorer for that block_id, we see that it holds the block height of 1, confirming that this is, in fact, the first block mined in Bitcoin.

Source: Author screenshot

After all the block variables are analyzed, we can go onto transaction-level variables — the amounts of transactions between different Bitcoin users.

Through the primary key of the transaction table, transaction_id, we can unlock information on individual transactions within a block: everything from the code output to the amount of satoshis (100 million satoshis equals one Bitcoin) in each transaction within each block. Blocks will have a number of transactions that are the lifeblood of Bitcoin, the transfer of value between different accounts.

The coinbase also contains code and output that doesn’t have to do with the satoshis transferred — this can be used to do things like identify mining pools that aggregate computing power and then distribute the proceeds of cryptocurrency mining among its individual contributors, who explicitly place their signature as an output on a block coinbase to identify themselves.

After playing around with different SQL queries and the different concepts presented with the Google BigQuery interface, we can then import directly into iPython notebooks using Kaggle’s BigQuery client tool and its kernel interface, and then also do aggregate functions, subfunctions, and everything SQL offers to source exact amounts of data on blocks and the transactions they contain.

Let’s do a count of every block contained in the Bitcoin blockchain data and play around with the interface in iPython and the Kaggle kernel with a few different queries, where we can easily obtain data via their integration with BigQuery.

Source: Author screenshot

What are some things we can do in machine learning with this ability to source rich data on the Bitcoin blockchain and different cryptocurrencies?

Classify whether or not a miner is part of a mining pool

Google released a sample project that was able to classify whether or not a given transaction was generated by a mining pool, an effort to aggregate computing power together across several entities and individuals in order to try to successfully mine Bitcoin and other cryptocurrencies, given their difficulty level.

The following queries led to the 26 feature vectors from output given to a certain address and idle times. Using a simple random forest classifier and miner classifications in the coinbase, Google was able to train and label a model that helped determine whether transactions were likely to be generated as part of a mining pool.

Correlate and predict cryptocurrency interest and difficulty to price levels

You can take factors such as the difficulty level and the amount of hash power dedicated to a blockchain, as well as the amount of transactional activity on the chain, and correlate that with historical price data to see if you can predict future prices based on time-series analysis and on-chain factors — potentially a very lucrative venture these days.

Classify different blockchains based on their attributes

You can source different attributes from different blockchains such as the Gini coefficient of miner rewards, as was done in this analysis of Ethereum Classic on Kaggle, and then use that to analyze the similarity between different blockchains based on different traits, such as the concentration of mining rewards, or the difficulty level or version history changes — or anything else at all that you can find.","['machine', 'bitcoin', 'different', 'transactions', 'queries', 'cryptocurrencies', 'blocks', 'block', 'difficulty', 'insights', 'learning', 'data', 'blockchain', 'mining', 'draw']","How to draw insights from cryptocurrencies with machine learningA handy tutorial that summarizes what you can do to load, process and make useful models out of cryptocurrency datasets.
Let’s have some fun with some sample data queries, and see what we can do with that data with the Bitcoin data Google has on hand.
You can play around with different queries on the BigQuery explorer to test them out, though you will be rate-limited, especially with larger data queries that return gigabytes of data.
This limit is quite easy to bump against with rich blockchain data.
Source: Author screenshotWhat are some things we can do in machine learning with this ability to source rich data on the Bitcoin blockchain and different cryptocurrencies?",en,['Roger Huang'],2019-09-24 01:26:01.413000+00:00,"{'Blockchain', 'Cryptocurrency', 'Machine Learning', 'Bigquery', 'Bitcoin'}","{'https://miro.medium.com/max/2560/1*kUgCHWo7bNuAqipOn3OxVQ.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2472/0*xSD5PglZJKaDMIVn', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/0*mqDEqHI8EVgClgha.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*xSD5PglZJKaDMIVn?q=20', 'https://miro.medium.com/max/60/0*AYC11fixE3KU9A4w?q=20', 'https://miro.medium.com/max/1200/1*kUgCHWo7bNuAqipOn3OxVQ.jpeg', 'https://miro.medium.com/max/3200/0*AYC11fixE3KU9A4w', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/0*mqDEqHI8EVgClgha.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3108/1*v6D-MlMxQKMUZ_T6mvjOYg.png', 'https://miro.medium.com/max/60/1*kUgCHWo7bNuAqipOn3OxVQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*v6D-MlMxQKMUZ_T6mvjOYg.png?q=20'}",2020-03-05 00:21:40.105401,3.220122814178467
https://towardsdatascience.com/cryptocurrency-data-analysis-in-python-using-rest-api-8c28234e5fd,Cryptocurrency Data Analysis In Python Using REST Api,"Hi folks, I hope you all are doing well. In today’s edition we will introduce Cryptocurrency Data Analysis using Python via REST Api. The aim of this blog is to get a quick overview as to how to work with various data types in python. What variables are and when to use them. How to connect to a RESTful Api. How to write data to Excel files. How to sort and analyse data with Python. So let us dive in knowing the approach.

Installations Required:

Python, you can download it by clicking here.

PyCharm click here to install.

Once you have installed the above mentioned prerequisites, you need to open a new project in PyCharm and name your project as per you concerns and a .py file. After that you need to visit the “coinmarketcap api” Cick Here and use the Example API link copy that and paste it in your .py file and create a new variable name api and give the link to the variable api.

api = ‘https://api.coinmarketcap.com/v2/ticker/1/’

Further, we need to import a package name requests to do so you need to click on FILE in your PyCharm IDE the go to SETTINGS go to PROJECT select Project Interpreter and click on plus(+) sign and search requests and the install the package.

After that you need to code in you .py file and get the details from the coinmarketcap api or rather fetch from there.

You can initialize the code by using the variable to call the data from the api by using the following code:

raw_data = requests.get(api).json

After that we will try to fetch all the currency name , its price and also the percent changes based on price in 1 hour, 24 hour and 7 days and also we will put them into tabular format using the package named PrettyTable.

Kindly view the code attached below:

The above Api was the public Api. We can also create our private api by using the professional API on this LINK.

Ahead click on GET YOUR API KEY NOW button and register your self by signing up. You can also see the free version by click on Pricing on this LINK and select the starter api for practice.After that you need to copy your API key and past in your program.

Key = ‘Your Private Key’

api=‘https://proapi.coinmarketcap.com/v1/cryptocurrency/map?CMC_PRO_API_KEY=’

This Key link can be achieved by visiting the API DOUMENTATION on this LINK .

Now you need to replace the map keyword in the link by “listing/latest” so the new key link becomes as follows:

api=‘https://proapi.coinmarketcap.com/v1/cryptocurrency/listing/latest?CMC_PRO_API_KEY=’

then you need to concatenate the key to your URL code , this can be done by adding a code line as follows:

api +=key

Moving ahead the few structure of programming code changes have to done and whole code file you can have a view below:

Now we will learn how to write the fetched data to excel sheets

For this we need a package extension named OPENPYXL, this can be done as follows:

from openpyxl import Workbook

import datetime

After that we will create sheet and append the data to the newly created sheet by using the sheet.append() method.

And will save the data file using method file.save() as file.save(‘data.xlsx’) which will be stored in your project file.

The sample code is mentioned below do have a look on it:

Adding bingo bonus I’m attaching one more code that is worked on by connecting to BINANCE REST Api, you can view the code below.

I hope the above collection of stuff is knowledgeable and would have given you a glance about the topic and on this note, I would like to sign off for today. Do follow me to get updates regarding all my blogs on Medium & LinkedIn. If you really like the above stuffs then do show your love by banging the Claps Button below because learning has no limits .

Thank you for Reading…","['file', 'cryptocurrency', 'key', 'rest', 'project', 'link', 'python', 'need', 'click', 'data', 'api', 'analysis', 'using', 'code']","In today’s edition we will introduce Cryptocurrency Data Analysis using Python via REST Api.
We can also create our private api by using the professional API on this LINK.
Ahead click on GET YOUR API KEY NOW button and register your self by signing up.
Key = ‘Your Private Key’api=‘https://proapi.coinmarketcap.com/v1/cryptocurrency/map?CMC_PRO_API_KEY=’This Key link can be achieved by visiting the API DOUMENTATION on this LINK .
And will save the data file using method file.save() as file.save(‘data.xlsx’) which will be stored in your project file.",en,['Madhav Mishra'],2018-08-16 12:20:46.429000+00:00,"{'Cryptocurrency', 'Python', 'Rest Api', 'Science', 'Technology'}","{'https://miro.medium.com/fit/c/96/96/1*9Xhc4ABXHO9ZvkEvSKNfWg.jpeg', 'https://miro.medium.com/max/1800/1*4_SIPZkvLEQVyEAosWD9Kw.gif', 'https://miro.medium.com/freeze/max/60/1*tm1kHzmzVgVDWyh4HqXw0w.gif?q=20', 'https://miro.medium.com/freeze/max/900/1*4_SIPZkvLEQVyEAosWD9Kw.gif', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/768/1*tm1kHzmzVgVDWyh4HqXw0w.gif', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*9Xhc4ABXHO9ZvkEvSKNfWg.jpeg', 'https://miro.medium.com/freeze/max/60/1*4_SIPZkvLEQVyEAosWD9Kw.gif?q=20'}",2020-03-05 00:21:48.112647,8.006246328353882
https://towardsdatascience.com/cryptocurrency-price-prediction-using-lstms-tensorflow-for-hackers-part-iii-264fcdbccd3f,Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III),"TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2. Use the model to predict the future Bitcoin price.

Complete source code in Google Colaboratory Notebook

This time you’ll build a basic Deep Neural Network model to predict Bitcoin price based on historical data. You can use the model however you want, but you carry the risk for your actions.

You might be asking yourself something along the lines:

Can I still get rich with cryptocurrency?

Of course, the answer is fairly nuanced. Here, we’ll have a look at how you might build a model to help you along the crazy journey.

Or you might be having money problems? Here is one possible solution:

Here is the plan:

Cryptocurrency data overview Time Series Data preprocessing Build and train LSTM model in TensorFlow 2 Use the model to predict future Bitcoin price

Data Overview

Our dataset comes from Yahoo! Finance and covers all available (at the time of this writing) data on Bitcoin-USD price. Let’s load it into a Pandas dataframe:

Note that we sort the data by Date just in case. Here is a sample of the data we’re interested in:

We have a total of 3201 data points representing Bitcoin-USD price for 3201 days (~9 years). We’re interested in predicting the closing price for future dates.

Of course, Bitcoin made some people really rich and for some went really poor. The question remains though, will it happen again? Let’s have a look at what one possible model thinks about that. Shall we?

Our dataset is somewhat different from our previous examples. The data is sorted by time and recorded at equal intervals (1 day). Such a sequence of data is called Time Series.

Time Series

Temporal datasets are quite common in practice. Your energy consumption and expenditure (calories in, calories out), weather changes, stock market, analytics gathered from the users for your product/app and even your (possibly in love) heart produce Time Series.

You might be interested in a plethora of properties regarding your Time Series — stationarity, seasonality and autocorrelation are some of the most well known.

Autocorrelation is the correlation of data points separated by some interval (known as lag).

Seasonality refers to the presence of some cyclical pattern at some interval (no, it doesn’t have to be every spring).

A time series is said to be stationarity if it has constant mean and variance. Also, the covariance is independent of the time.

One obvious question you might ask yourself while watching at Time Series data is: “Does the value of the current time step affects the next one?” a.k.a. Time Series forecasting.

There are many approaches that you can use for this purpose. But we’ll build a Deep Neural Network that does some forecasting for us and use it to predict future Bitcoin price.

Modeling

All models we’ve built so far do not allow for operating on sequence data. Fortunately, we can use a special class of Neural Network models known as Recurrent Neural Networks (RNNs) just for this purpose. RNNs allow using the output from the model as a new input for the same model. The process can be repeated indefinitely.

One serious limitation of RNNs is the inability of capturing long-term dependencies in a sequence (e.g. Is there a dependency between today`s price and that 2 weeks ago?). One way to handle the situation is by using an Long short-term memory (LSTM) variant of RNN.

The default LSTM behavior is remembering information for prolonged periods of time. Let’s see how you can use LSTM in Keras.

Data preprocessing

First, we’re going to squish our price data in the range [0, 1]. Recall that this will help our optimization algorithm converge faster:

We’re going to use the MinMaxScaler from scikit learn:

The scaler expects the data to be shaped as (x, y), so we add a dummy dimension using reshape before applying it.

Let’s also remove NaNs since our model won’t be able to handle them well:

We use isnan as a mask to filter out NaN values. Again we reshape the data after removing the NaNs.

Making sequences

LSTMs expect the data to be in 3 dimensions. We need to split the data into sequences of some preset length. The shape we want to obtain is:

[batch_size, sequence_length, n_features]

We also want to save some data for testing. Let’s build some sequences:

The process of building sequences works by creating a sequence of a specified length at position 0. Then we shift one position to the right (e.g. 1) and create another sequence. The process is repeated until all possible positions are used.

We save 5% of the data for testing. The datasets look like this:

(2945, 99, 1)

(156, 99, 1)

Our model will use 2945 sequences representing 99 days of Bitcoin price changes each for training. We’re going to predict the price for 156 days in the future (from our model POV).

Building LSTM model

We’re creating a 3 layer LSTM Recurrent Neural Network. We use Dropout with a rate of 20% to combat overfitting during training:

You might be wondering about what the deal with Bidirectional and CuDNNLSTM is?

Bidirectional RNNs allows you to train on the sequence data in forward and backward (reversed) direction. In practice, this approach works well with LSTMs.

CuDNNLSTM is a “Fast LSTM implementation backed by cuDNN”. Personally, I think it is a good example of leaky abstraction, but it is crazy fast!

Our output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.

Training

We’ll use Mean Squared Error as a loss function and Adam optimizer.

Note that we do not want to shuffle the training data since we’re using Time Series.

After a lightning-fast training (thanks Google for the free T4 GPUs), we have the following training loss:

Predicting Bitcoin price

Let’s make our model predict Bitcoin prices!

We can use our scaler to invert the transformation we did so the prices are no longer scaled in the [0, 1] range.

Our rather succinct model seems to do well on the test data. Care to try it on other currencies?

Conclusion

Congratulations, you just built a Bidirectional LSTM Recurrent Neural Network in TensorFlow 2. Our model (and preprocessing “pipeline”) is pretty generic and can be used for other datasets.

Complete source code in Google Colaboratory Notebook

One interesting direction of future investigation might be analyzing the correlation between different cryptocurrencies and how would that affect the performance of our model.","['bitcoin', 'network', 'tensorflow', 'iii', 'cryptocurrency', 'prediction', 'lstms', 'predict', 'series', 'lstm', 'price', 'neural', 'hackers', 'data', 'model', 'sequence', 'using']","TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2.
Complete source code in Google Colaboratory NotebookThis time you’ll build a basic Deep Neural Network model to predict Bitcoin price based on historical data.
Fortunately, we can use a special class of Neural Network models known as Recurrent Neural Networks (RNNs) just for this purpose.
Data preprocessingFirst, we’re going to squish our price data in the range [0, 1].
After a lightning-fast training (thanks Google for the free T4 GPUs), we have the following training loss:Predicting Bitcoin priceLet’s make our model predict Bitcoin prices!",en,['Venelin Valkov'],2019-06-29 21:41:07.847000+00:00,"{'Deep Learning', 'Cryptocurrency', 'Artificial Intelligence', 'Machine Learning', 'TensorFlow'}","{'https://miro.medium.com/max/2124/0*JRmh_mIo55mwfjk5.png', 'https://miro.medium.com/max/1388/1*rKV8k6l39n_mOe62-RCe7Q.png', 'https://miro.medium.com/max/60/1*I7zdCfxw2RTX8k1etPSX2g.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*nL9votCH7vGZeIgq.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/0*JRmh_mIo55mwfjk5.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*OoQjeo1aWgiGKub_5QxwvA.jpeg', 'https://miro.medium.com/max/1750/0*93u6ddc2iv0zutpg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*I7zdCfxw2RTX8k1etPSX2g.jpeg', 'https://miro.medium.com/max/1770/0*nL9votCH7vGZeIgq.png', 'https://miro.medium.com/max/60/0*93u6ddc2iv0zutpg.png?q=20', 'https://miro.medium.com/max/1738/0*zfQdQnWLpbMRsP0A.png', 'https://miro.medium.com/fit/c/96/96/2*OoQjeo1aWgiGKub_5QxwvA.jpeg', 'https://miro.medium.com/max/60/0*zfQdQnWLpbMRsP0A.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*rKV8k6l39n_mOe62-RCe7Q.png?q=20', 'https://miro.medium.com/max/5720/1*I7zdCfxw2RTX8k1etPSX2g.jpeg'}",2020-03-05 00:21:51.077791,2.9651436805725098
https://towardsdatascience.com/demystifying-cryptocurrency-price-prediction-5fb2b504a110,CryptoCurrency Price Prediction with Python,"Ever since Bitcoin’s price began to skyrocket, there has been constant hype surrounding the crytocurrency market. Alternate coins keep popping up everyday- some are scams, some make it to the top coin list in months. The topic comes up everywhere, whether it’s on the radio, Twitter, Facebook, or at the Thanksgiving dinner table with your grandfather. The people involved are usually fueled by speculation, hoping to come into a windfall from the surging market. Just when you think the hype can’t get any more severe, this project is an intersection between crytocurrencies and the glorious maCHEEN learning- the magical word all tech CEOs use to hype up their products. So, I hope this gets you just as hyped as I am. This project uses historical price, cointegration, and macroeconomic covariates to predict a coin future price. The out-of-sample prediction was acceptable for the first ~100 hours. Let’s dig into it.

When talking about classical time series analysis, we believe that an observed time series is a combination of a pattern and some random variations. Using this approach, future values are predicted based on its historical data. This method works well in most cases, but what if we’re looking at a time series that is more random than pattern-like? What if a time series is purely speculative and heavily based on current events rather than some rhythmic components? You know… something we all have heard of- crypocurrency prices.

So if not just some simple pattern, what drives cryptocurrencies prices? Speculation? Innovation? Legal issues? Public opinion? Supply/demand? Bitcoin popularity? Some rich (wo)man decided to buy a million coins last night? Okay, enough speculating on speculations (haha). I’ll stop talking and show you the data now.

So what affects crypto prices?

1Most people say Bitcoin is the answer. Blockchain technology is a decentralized database system that was first implemented by Bitcoin. Created by a mysterious person (or group), Blockchain has a very high tendency to transform modern day business operation models. As Bitcoin gains more traction, people keep coming up with alternate coins that are also based on Blockchain technology. So pretty much Bitcoin is the mother of all cryptocurrencies because they came up with the cool technology first. That’s why I think it makes sense that when Bitcoin spikes, every other coin spikes. When it drops, every other coin drops. The following plot shows scaled rolling averages of Bitcoin (green) and Ethereum (Blue).

As you can see, the claim looks pretty accurate. But we’re not going to rush into conclusion without statistical methods. Later on, we’re going to talk more about identifying a “cointegrated pair” using the Granger Causality Test.

2 It’s also pretty obvious that current events such as legal issues or technological game changers also play a role. Remember when China banned crypto back in September? The price dropped rapidly and everything was in chaos. In an attempt to capture important events like this one, Google News search frequency data is obtained from Pytrends API. The plot below shows spikes in search frequency of the word “cryptocurrency” (red) as cryto prices drop.

Interesting right? The search terms used in this project are selected using the Google Keyword Tool. Not only does the tool let you know how popular a search term is, it also suggests a list of related keywords. Using the list provided and the Pytrend API, search frequency data of seven different keywords is obtained. I’ll elaborate more about these terms in a later section.

3Another factor that stands out to me is public perception. The more buy-ins, the more demand, and thus the higher the price. Capturing this data is a bit painful. The paid Twitter API is everything I need, but I’m a student so I’d rather save the money for groceries. Instead, the site redditmetrics.com plots out historical subscription growth data of just about any subreddit in the world. So web scraping it is!!! The plot below compares Nem subreddit subscription growth (orange) with Nem historical price (blue).

Just as expected, the subscription growth and the price stick together through the highs and lows. How cute. I wish someone love me the way price loves subscription growth.

I hope these visuals are interesting to you. This is just to get you started with some domain knowledge and to introduce you to the problem we’re trying to solve. If the intro didn’t scratch your itch, please check out the complete EDA available on my GitHub here. Next I’m going to jump right into the statistical methods used to build a model that predicts a coin’s future price.

How did I build the model?

In this section, we’re going to dig into the methodology. It will be just a summary of each step. If you want to dig deeper into the code, please refer to my GitHub repository here. It’s going to be technical and I’m going to try my best to make it fun and easy to digest. But if you’re not interested in the technical stuff, feel free to skip right to the TLDR section.

1. Identifying the Cointegrated Pair

A total of 12 top coins’ historical prices over a three month period are obtained through Cryptocompare API. But before we can do anything with the time series, we have to make sure that the time series is stationary. To meet the stationary requirements, a time series must have constant mean, constant variance, and constant autocorrelation. Sounds like a lot to ask for, right? In reality, no time series is perfectly stationary. But worry not my friends, Dickey & Fuller have your back!

Augmented Dickey-Fuller Test of Stationary. This is a statistical test that allows you to check if the expectations of stationarity are met. It’s a unit root test that determines how strongly a time series is defined by a trend. The test uses an autoregressive model and optimizes an information criterion across different lag values. The null hypothesis is that the time series can be represented by a unit root (means it’s not stationary). Some statistician came up with the magic threshold of 0.05 and a lot of people agreed with that number. So if your p-value is less than 0.05, you could reject the null hypothesis. But then again, the result should be interpreted for a given problem to be meaningful. It turned out that, assuming a threshold of 0.05, the historical prices from all 12 coins don’t pass the stationary test (surprise!). In that case, we’ll have to stationarize the time series and re-test them.

Differencing. This is a popular method used to stationarize time series. It can be used to remove trends and seasonality. Taking the difference of consecutive observations (lag 1) is used for this project. If a time series had a seasonal component, the lag value should be the period of the seasonality. In our case, there is no obvious seasonal component. The box plot below shows how Ethereum hourly average is relatively constant throughout 24 hour of the day. The variance varies, but there is no obvious pattern.

After we performed lag-1 difference on the time series, all twelve time series passed the Dickey-Fuller stationary test! Phew.

Granger Causality Test. This is a statistical hypothesis test for determining whether one time series is useful in forecasting another. A time series A “Granger-cause” time series B if lag values of A provided statistically significant information about future values of B. In this project, we’re using this test to identify a cointegrated pair- a pair of cryptocurrencies in which one coin’s lag values can be used to predict the other coin’s future values.

Now that the historical price data of all twelve coins is stationary, we constructed a total of 132 dataframes, each of which is a permutation pair (not to be confused with combination pair) of the twelve coins’ historical prices. Yikes, that’s really confusing. So, for example, let’s say I had ETH, BTC, and LTC historical price data. Then I would need to make six data frames: ETH & BTC historical prices, ETH & LTC historical prices, BTC & ETH historical prices, BTC & LTC historical prices, LTC & ETH historical prices, and LTC & ETH historical prices. Notice that ETH & BTC and BTC & ETH aren’t the same thing! This is just to prepare our data for the StatsModels Granger Causality Test. The test’s null hypothesis is that the time series in the 2nd column does not Granger-cause the time series in the 1st column. We need to test whether ETC causes BTH or BTH causes ETC. This is why we need permutation pairs and not combination pairs! After performing 132 tests, the DASH & BCH pair was previously selected as the cointegrated pair because of the strongest correlation. However, with further research, it turns out that the strong correlation is due to the surge in Korean trading. Since this is not a normal condition, we instead pick XEM and IOT (Nem and Iota) as our cointegrated pair since it has the strongest correlation under normal conditions. For the purpose of this project, the IOT historical price will be used as one of the XEM future price predictors.

2. Feature Selection

Querying Data. The following is the acquired data and it’s sources:

Cryptocompare API: XEM and IOT historical prices in hour frequency

Pytrends API: Google News search frequency of the phrase “cryptocurrency”

Scraping redditmetrics.com: Subreddit “CryptoCurrency,” “Nem,” and “Iota” subscription growth

Pytrends API: Google search frequency for the phrases “Nem wallet download,” “Iota wallet download,” “Nem price,” Iota price,” Bitcoin price,” “GPU for mining”- these keywords are selected using Google Keyword Tool

Yahoo Financials API: AMD and Nvidia stock prices- these are the top two semiconductor companies used for coin mining

ElasticNet Regularization. As you might have already noticed, the queried data is somewhat related. For example, the Google search frequency of the phrases “Bitcoin price” and “CryptoCurrency” might contain very similar information. Building a model using related features like these creates redundancy and that’s bad! Just like how humans get emotionally unstable (a.k.a irritated) from redundancy, machines freak out and become unstable due to multicollinearity too! Thank God ElasticNet is a thing. The algorithm linearly combines the L1 and L2 penalties of LASSO and Ridge methods. It “shrinks” the redundant predictors’ coefficients to zero and so it becomes an industry standard way of feature selection. Using ElasticNet result, features that have non-zero coefficients are to be selected.

In this project, ElasticNet is performed on 13 predictor variables acquired above, with XEM historical price being the dependent variable. After running the algorithm, we are left with three predictors that have non-zero coefficients. These features will be used to build the final model. Regardless of the ElasticNet result, I’ve also tried building a model using all 13 predictors then compared the result with the model built using the three selected features. It turns out that the difference in performance isn’t significant (MSE = 0.107 for all 15 features and MSE = 0.105 for 3 features). However, I’m going with the three features model still because I’m from LA and they always say “the leaner the better.”

3. Building the Model

For this project, we are going to use the ARIMAX model to predict XEM future price. Just like ARIMA model, ARIMAX produces forecasts based on autoregressive (AR) and moving average (MA) terms. However, ARIMAX includes exogenous variables in the model as well. In this case, the three predictor variables previously selected will be used here.

Data Preprocessing. Since we’ve already talked about stationary and Dickey-Fuller tests in the previous section, I will omit the details here. The data obtained is already standardized prior to ElasticNet, so all we need to do is to perform differencing and then make sure it passes Dickey-Fuller test. After that, the data is cleaned and split into test and train sets.

ACF & PACF. Now that our data is ready, we need to (1) determine if the time series is AR or MA process and (2) determine what order of AR or MA process we need to use in the model. The first question can be answered using ACF. For the AR series, the correlation goes down gradually without a cut-off value. ACF could also be used to determine the lag order of the MA series- it’s the cut-off value. However, if we had an AR series, the PACF cut off value would be used to determine the lag order instead. Plots below are the ACF and PACF of XEM historical price.

As we can see, it’s an AR process since ACF doesn’t have a cut-off value. Then, looking at PACF, it cuts off at lag 1, which would be the parameter we’re using for our ARIMAX model.

ARIMAX. Using AR 1 and 3 exogenous variables, the plot below is the fitted value compared to the actual value.

Using the fitted model, XEM price prediction is obtained. The plot below is the out-of-sample prediction of XEM 600 steps ahead of time.

As expected, the model performs better in the beginning. This is because the prediction errors keep compounding as longer time passed. After around 100 steps, the model doesn’t perform as well. The mean-squared error for steps 1–100 is 0.039 while the mean-squared error for steps 101–600 is 0.119.

TLDR

The Iota historical price combined with other macroeconomic covariates such as Google search frequency of the word “Nem price” and “Nem” subreddit subscription growth data was used to build ARIMAX model to predict Nem price. The out-of-sample prediction performance was acceptable for the first ~100 hours. Anything beyond that is pretty much junk.

This project is my very first data science project, and there is a lot of room for improvements. Getting the paid Twitter data or using a different machine learning model might improve the performance significantly. At that point, I might as well come up with a trading signal algorithm and use it to automate trading. But for now, this project will continue to serve as a portfolio piece. And yes, I’m disappointed that I haven’t become rich from crypto trading by the end of this project. Back to crying now. Adios.

I hope you enjoyed this article just as much as I enjoyed working on it! Leave your comments and let me know what you think.","['cryptocurrency', 'prediction', 'historical', 'series', 'used', 'project', 'python', 'price', 'prices', 'data', 'model', 'test', 'using']","This project uses historical price, cointegration, and macroeconomic covariates to predict a coin future price.
The plot below compares Nem subreddit subscription growth (orange) with Nem historical price (blue).
So, for example, let’s say I had ETH, BTC, and LTC historical price data.
Then I would need to make six data frames: ETH & BTC historical prices, ETH & LTC historical prices, BTC & ETH historical prices, BTC & LTC historical prices, LTC & ETH historical prices, and LTC & ETH historical prices.
For the purpose of this project, the IOT historical price will be used as one of the XEM future price predictors.",en,['Chalita Lertlumprasert'],2019-02-13 01:47:26.842000+00:00,"{'Data Science', 'Cryptocurrency', 'Timeseries', 'Machine Learning', 'Towards Data Science'}","{'https://miro.medium.com/max/1200/1*Ne_qQzFMsZIGfDIFRrt0YQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/808/1*CrYb5cuTOGQBYwfWgQAaaQ.png', 'https://miro.medium.com/max/800/1*mNMb6ANsvnkWmDy7zU53Iw.png', 'https://miro.medium.com/max/60/1*EtDnMC-clLYsJ1fNJo_QQA.png?q=20', 'https://miro.medium.com/max/60/1*mNMb6ANsvnkWmDy7zU53Iw.png?q=20', 'https://miro.medium.com/max/60/1*IGScfizrAQcgHoCxe5FYeA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2560/1*Ne_qQzFMsZIGfDIFRrt0YQ.jpeg', 'https://miro.medium.com/max/60/1*XeFMDrUsWoOBRHLcaZvTtw.png?q=20', 'https://miro.medium.com/max/60/1*CrYb5cuTOGQBYwfWgQAaaQ.png?q=20', 'https://miro.medium.com/max/60/1*fHjAu4Od0W5U5ZiWlNj1lw.png?q=20', 'https://miro.medium.com/max/986/1*IGScfizrAQcgHoCxe5FYeA.png', 'https://miro.medium.com/max/820/1*XeFMDrUsWoOBRHLcaZvTtw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*Ne_qQzFMsZIGfDIFRrt0YQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*vQARtynsbBFKfcElAuw9Aw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ypLjiHbk57HzwYCSn3WCoA.jpeg', 'https://miro.medium.com/fit/c/96/96/1*ypLjiHbk57HzwYCSn3WCoA.jpeg', 'https://miro.medium.com/max/60/1*7Js9a5dJ4nP7XK9QCa2U2g.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/996/1*7Js9a5dJ4nP7XK9QCa2U2g.png', 'https://miro.medium.com/max/996/1*fHjAu4Od0W5U5ZiWlNj1lw.png', 'https://miro.medium.com/max/958/1*vQARtynsbBFKfcElAuw9Aw.png', 'https://miro.medium.com/max/778/1*EtDnMC-clLYsJ1fNJo_QQA.png'}",2020-03-05 00:21:58.594819,7.517027854919434
https://towardsdatascience.com/getting-started-with-data-analytics-using-jupyter-notebooks-pyspark-and-docker-57c1aaab2408,"Getting Started with Data Analytics using Jupyter Notebooks, PySpark, and Docker","Introduction

There is little question, big data analytics, data science, artificial intelligence (AI), and machine learning (ML), a subcategory of AI, have all experienced a tremendous surge in popularity over the last few years. Behind the marketing hype, these technologies are having a significant influence on many aspects of our modern lives. Due to their popularity and potential benefits, commercial enterprises, academic institutions, and the public sector are rushing to develop hardware and software solutions to lower the barriers to entry and increase the velocity of ML and Data Scientists and Engineers.

Many open-source software projects are also lowering the barriers to entry into these technologies. An excellent example of one such open-source project working on this challenge is Project Jupyter. Similar to Apache Zeppelin and the newly open-sourced Netflix’s Polynote, Jupyter Notebooks enables data-driven, interactive, and collaborative data analytics.

This post will demonstrate the creation of a containerized data analytics environment using Jupyter Docker Stacks. The particular environment will be suited for learning and developing applications for Apache Spark using the Python, Scala, and R programming languages. We will focus on Python and Spark, using PySpark.

Featured Technologies

The following technologies are featured prominently in this post.

Jupyter Notebooks

According to Project Jupyter, the Jupyter Notebook, formerly known as the IPython Notebook, is an open-source web application that allows users to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleansing and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. The word, Jupyter, is a loose acronym for Julia, Python, and R, but today, the Jupyter supports many programming languages.

Interest in Jupyter Notebooks has grown dramatically over the last 3–5 years, fueled in part by the major Cloud providers, AWS, Google Cloud, and Azure. Amazon Sagemaker, Amazon EMR (Elastic MapReduce), Google Cloud Dataproc, Google Colab (Collaboratory), and Microsoft Azure Notebooks all have direct integrations with Jupyter notebooks for big data analytics and machine learning.

Jupyter Docker Stacks

To enable quick and easy access to Jupyter Notebooks, Project Jupyter has created Jupyter Docker Stacks. The stacks are ready-to-run Docker images containing Jupyter applications, along with accompanying technologies. Currently, the Jupyter Docker Stacks focus on a variety of specializations, including the r-notebook, scipy-notebook, tensorflow-notebook, datascience-notebook, pyspark-notebook, and the subject of this post, the all-spark-notebook. The stacks include a wide variety of well-known packages to extend their functionality, such as scikit-learn, pandas, Matplotlib, Bokeh, NumPy, and Facets.

Apache Spark

According to Apache, Spark is a unified analytics engine for large-scale data processing. Starting as a research project at the UC Berkeley AMPLab in 2009, Spark was open-sourced in early 2010 and moved to the Apache Software Foundation in 2013. Reviewing the postings on any major career site will confirm that Spark is widely used by well-known modern enterprises, such as Netflix, Adobe, Capital One, Lockheed Martin, JetBlue Airways, Visa, and Databricks. At the time of this post, LinkedIn, alone, had approximately 3.5k listings for jobs that reference the use of Apache Spark, just in the United States.

With speeds up to 100 times faster than Hadoop, Apache Spark achieves high performance for static, batch, and streaming data, using a state-of-the-art DAG (Directed Acyclic Graph) scheduler, a query optimizer, and a physical execution engine. Spark’s polyglot programming model allows users to write applications quickly in Scala, Java, Python, R, and SQL. Spark includes libraries for Spark SQL (DataFrames and Datasets), MLlib (Machine Learning), GraphX (Graph Processing), and DStreams (Spark Streaming). You can run Spark using its standalone cluster mode, Apache Hadoop YARN, Mesos, or Kubernetes.

PySpark

The Spark Python API, PySpark, exposes the Spark programming model to Python. PySpark is built on top of Spark’s Java API and uses Py4J. According to Apache, Py4J, a bridge between Python and Java, enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine (JVM). Data is processed in Python and cached and shuffled in the JVM.

Docker

According to Docker, their technology gives developers and IT the freedom to build, manage, and secure business-critical applications without the fear of technology or infrastructure lock-in. For this post, I am using the current stable version of Docker Desktop Community version for macOS.

Docker Swarm

Current versions of Docker include both a Kubernetes and Swarm orchestrator for deploying and managing containers. We will choose Swarm for this demonstration. According to Docker, the cluster management and orchestration features embedded in the Docker Engine are built using swarmkit. Swarmkit is a separate project which implements Docker’s orchestration layer and is used directly within Docker.

PostgreSQL

PostgreSQL is a powerful, open-source, object-relational database system. According to their website, PostgreSQL comes with many features aimed to help developers build applications, administrators to protect data integrity and build fault-tolerant environments, and help manage data no matter how big or small the dataset.

Demonstration

In this demonstration, we will explore the capabilities of the Spark Jupyter Docker Stack to provide an effective data analytics development environment. We will explore a few everyday uses, including executing Python scripts, submitting PySpark jobs, and working with Jupyter Notebooks, and reading and writing data to and from different file formats and a database. We will be using the latest jupyter/all-spark-notebook Docker Image. This image includes Python, R, and Scala support for Apache Spark, using Apache Toree.

Architecture

As shown below, we will deploy a Docker stack to a single-node Docker swarm. The stack consists of a Jupyter All-Spark-Notebook, PostgreSQL (Alpine Linux version 12), and Adminer container. The Docker stack will have two local directories bind-mounted into the containers. Files from our GitHub project will be shared with the Jupyter application container through a bind-mounted directory. Our PostgreSQL data will also be persisted through a bind-mounted directory. This allows us to persist data external to the ephemeral containers. If the containers are restarted or recreated, the data is preserved locally.

Source Code

All source code for this post can be found on GitHub. Use the following command to clone the project. Note this post uses the v2 branch.



--branch v2 --single-branch --depth 1 --no-tags \

https://github.com/garystafford/pyspark-setup-demo.git git clone \--branch v2 --single-branch --depth 1 --no-tags \

Source code samples are displayed as GitHub Gists, which may not display correctly on some mobile and social media browsers.

Deploy Docker Stack

To start, create the $HOME/data/postgres directory to store PostgreSQL data files.

mkdir -p ~/data/postgres

This directory will be bind-mounted into the PostgreSQL container on line 41 of the stack.yml file, $HOME/data/postgres:/var/lib/postgresql/data . The HOME environment variable assumes you are working on Linux or macOS and is equivalent to HOMEPATH on Windows.

The Jupyter container’s working directory is set on line 15 of the stack.yml file, working_dir: /home/$USER/work . The local bind-mounted working directory is $PWD/work . This path is bind-mounted to the working directory in the Jupyter container, on line 29 of the stack.yml file, $PWD/work:/home/$USER/work . The PWD environment variable assumes you are working on Linux or macOS ( CD on Windows).

By default, the user within the Jupyter container is jovyan . We will override that user with our own local host’s user account, as shown on line 21 of the Docker stack file, NB_USER: $USER . We will use the host’s USER environment variable value (equivalent to USERNAME on Windows). There are additional options for configuring the Jupyter container. Several of those options are used on lines 17–22 of the Docker stack file (gist).

Assuming you have a recent version of Docker installed on your local development machine and running in swarm mode, standing up the stack is as easy as running the following docker command from the root directory of the project.

docker stack deploy -c stack.yml jupyter

The Docker stack consists of a new overlay network, jupyter_demo-net , and three containers. To confirm the stack deployed successfully, run the following docker command.

docker stack ps jupyter --no-trunc

The jupyter/all-spark-notebook Docker image is large, approximately 5 GB. Depending on your Internet connection, if this is the first time you have pulled this image, the stack may take several minutes to enter a running state. Although not required, I usually pull Docker images in advance.

docker pull jupyter/all-spark-notebook:latest

docker pull postgres:12-alpine

docker pull adminer:latest

To access the Jupyter Notebook application, you need to obtain the Jupyter URL and access token. The Jupyter URL and the access token are output to the Jupyter container log, which can be accessed with the following command.

docker logs $(docker ps | grep jupyter_spark | awk '{print $NF}')

You should observe log output similar to the following. Retrieve the complete URL, for example, http://127.0.0.1:8888/?token=f78cbe... , to access the Jupyter web-based user interface.

From the Jupyter dashboard landing page, you should see all the files in the project’s work/ directory. Note the types of files you can create from the dashboard, including Python 3, R, and Scala (using Apache Toree or spylon-kernal) notebooks, and text. You can also open a Jupyter terminal or create a new Folder from the drop-down menu. At the time of this post, the latest jupyter/all-spark-notebook Docker Image runs Spark 2.4.4, Scala 2.11.12, Python 3.7.3, and OpenJDK 64-Bit Server VM, Java 1.8.0 Update 222.

Bootstrap the Environment

Included in the project is a bootstrap script, bootstrap_jupyter.sh. The script will install the required Python packages using pip, the Python package installer, and a requirement.txt file. The bootstrap script also installs the latest PostgreSQL driver JAR, configures Apache Log4j to reduce log verbosity when submitting Spark jobs, and installs htop. Although these tasks could also be done from a Jupyter terminal or from within a Jupyter notebook, using a bootstrap script ensures you will have a consistent work environment every time you spin up the Jupyter Docker stack. Add or remove items from the bootstrap script as necessary (gist).

That’s it, our new Jupyter environment is ready to start exploring.

Running Python Scripts

One of the simplest tasks we could perform in our new Jupyter environment is running Python scripts. Instead of worrying about installing and maintaining the correct versions of Python and multiple Python packages on your own development machine, we can run Python scripts from within the Jupyter container. At the time of this post update, the latest jupyter/all-spark-notebook Docker image runs Python 3.7.3 and Conda 4.7.12. Let’s start with a simple Python script, 01_simple_script.py.

From a Jupyter terminal window, use the following command to run the script.

python3 01_simple_script.py

You should observe the following output.

Kaggle Datasets

To explore more features of the Jupyter and PySpark, we will use a publicly available dataset from Kaggle. Kaggle is an excellent open-source resource for datasets used for big-data and ML projects. Their tagline is ‘Kaggle is the place to do data science projects.’ For this demonstration, we will use the Transactions from a bakery dataset from Kaggle. The dataset is available as a single CSV-format file. A copy is also included in the project.

The ‘Transactions from a bakery’ dataset contains 21,294 rows with 4 columns of data. Although certainly not big data, the dataset is large enough to test out the Spark Jupyter Docker Stack functionality. The data consists of 9,531 customer transactions for 21,294 bakery items between 2016–10–30 and 2017–04–09 (gist).

Submitting Spark Jobs

We are not limited to Jupyter notebooks to interact with Spark. We can also submit scripts directly to Spark from the Jupyter terminal. This is typically how Spark is used in a Production for performing analysis on large datasets, often on a regular schedule, using tools such as Apache Airflow. With Spark, you are load data from one or more data sources. After performing operations and transformations on the data, the data is persisted to a datastore, such as a file or a database, or conveyed to another system for further processing.

The project includes a simple Python PySpark ETL script, 02_pyspark_job.py. The ETL script loads the original Kaggle Bakery dataset from the CSV file into memory, into a Spark DataFrame. The script then performs a simple Spark SQL query, calculating the total quantity of each type of bakery item sold, sorted in descending order. Finally, the script writes the results of the query to a new CSV file, output/items-sold.csv .

Run the script directly from a Jupyter terminal using the following command.

python3 02_pyspark_job.py

An example of the output of the Spark job is shown below.

Typically, you would submit the Spark job using the spark-submit command. Use a Jupyter terminal to run the following command.

$SPARK_HOME/bin/spark-submit 02_pyspark_job.py

Below, we see the output from the spark-submit command. Printing the results in the output is merely for the purposes of the demo. Typically, Spark jobs are submitted non-interactively, and the results are persisted directly to a datastore or conveyed to another system.

Using the following commands, we can view the resulting CVS file, created by the spark job.

ls -alh output/items-sold.csv/

head -5 output/items-sold.csv/*.csv

An example of the files created by the spark job is shown below. We should have discovered that coffee is the most commonly sold bakery item with 5,471 sales, followed by bread with 3,325 sales.

Interacting with Databases

To demonstrate the flexibility of Jupyter to work with databases, PostgreSQL is part of the Docker Stack. We can read and write data from the Jupyter container to the PostgreSQL instance, running in a separate container. To begin, we will run a SQL script, written in Python, to create our database schema and some test data in a new database table. To do so, we will use the psycopg2, the PostgreSQL database adapter package for the Python, we previously installed into our Jupyter container using the bootstrap script. The Python script, 03_load_sql.py, will execute a set of SQL statements contained in a SQL file, bakery.sql, against the PostgreSQL container instance.

The SQL file, bakery.sql.

To execute the script, run the following command.

python3 03_load_sql.py

This should result in the following output, if successful.

Adminer

To confirm the SQL script’s success, use Adminer. Adminer (formerly phpMinAdmin) is a full-featured database management tool written in PHP. Adminer natively recognizes PostgreSQL, MySQL, SQLite, and MongoDB, among other database engines.

Adminer should be available on localhost port 8080. The password credentials, shown below, are located in the stack.yml file. The server name, postgres , is the name of the PostgreSQL Docker container. This is the domain name the Jupyter container will use to communicate with the PostgreSQL container.

Connecting to the new bakery database with Adminer, we should see the transactions table.

The table should contain 21,293 rows, each with 5 columns of data.

pgAdmin

Another excellent choice for interacting with PostgreSQL, in particular, is pgAdmin 4. It is my favorite tool for the administration of PostgreSQL. Although limited to PostgreSQL, the user interface and administrative capabilities of pgAdmin is superior to Adminer, in my opinion. For brevity, I chose not to include pgAdmin in this post. The Docker stack also contains a pgAdmin container, which has been commented out. To use pgAdmin, just uncomment the container and re-run the Docker stack deploy command. pgAdmin should then be available on localhost port 81. The pgAdmin login credentials are in the Docker stack file.

Developing Jupyter Notebooks

The real power of the Jupyter Docker Stacks containers is Jupyter Notebooks. According to the Jupyter Project, the notebook extends the console-based approach to interactive computing in a qualitatively new direction, providing a web-based application suitable for capturing the whole computation process, including developing, documenting, and executing code, as well as communicating the results. Notebook documents contain the inputs and outputs of an interactive session as well as additional text that accompanies the code but is not meant for execution.

To explore the capabilities of Jupyter notebooks, the project includes two simple Jupyter notebooks. The first notebooks, 04_notebook.ipynb, demonstrates typical PySpark functions, such as loading data from a CSV file and from the PostgreSQL database, performing basic data analysis with Spark SQL including the use of PySpark user-defined functions (UDF), graphing the data using BokehJS, and finally, saving data back to the database, as well as to the fast and efficient Apache Parquet file format. Below we see several notebook cells demonstrating these features.

Markdown for Notebook Documentation

Read CSV-Format Files into Spark DataFrame

Load Data from PostgreSQL into Spark DataFrame

Perform Spark SQL Query including use of UDF

Plot Spark SQL Query Results using BokehJS

IDE Integration

Recall, the working directory, containing the GitHub source code for the project, is bind-mounted to the Jupyter container. Therefore, you can also edit any of the files, including notebooks, in your favorite IDE, such as JetBrains PyCharm and Microsoft Visual Studio Code. PyCharm has built-in language support for Jupyter Notebooks, as shown below.

PyCharm 2019.2.5 (Professional Edition)

As does Visual Studio Code using the Python extension.

Visual Studio Code Version: 1.40.2

Using Additional Packages

As mentioned in the Introduction, the Jupyter Docker Stacks come ready-to-run, with a wide variety of Python packages to extend their functionality. To demonstrate the use of these packages, the project contains a second Jupyter notebook document, 05_notebook.ipynb. This notebook uses SciPy, the well-known Python package for mathematics, science, and engineering, NumPy, the well-known Python package for scientific computing, and the Plotly Python Graphing Library. While NumPy and SciPy are included on the Jupyter Docker Image, the bootstrap script uses pip to install the required Plotly packages. Similar to Bokeh, shown in the previous notebook, we can use these libraries to create richly interactive data visualizations.

Plotly

To use Plotly from within the notebook, you will first need to sign up for a free account and obtain a username and API key. To ensure we do not accidentally save sensitive Plotly credentials in the notebook, we are using python-dotenv. This Python package reads key/value pairs from a .env file, making them available as environment variables to our notebook. Modify and run the following two commands from a Jupyter terminal to create the .env file and set you Plotly username and API key credentials. Note that the .env file is part of the .gitignore file and will not be committed to back to git, potentially compromising the credentials.

echo ""PLOTLY_USERNAME=your-username"" >> .env

echo ""PLOTLY_API_KEY=your-api-key"" >> .env

Shown below, we use Plotly to construct a bar chart of daily bakery items sold. The chart uses SciPy and NumPy to construct a linear fit (regression) and plot a line of best fit for the bakery data and overlaying the vertical bars. The chart also uses SciPy’s Savitzky-Golay Filter to plot the second line, illustrating a smoothing of our bakery data.

Plotly also provides Chart Studio Online Chart Maker. Plotly describes Chart Studio as the world’s most modern enterprise data visualization solutions. We can enhance, stylize, and share our data visualizations using the free version of Chart Studio Cloud.

Jupyter Notebook Viewer

Notebooks can also be viewed using nbviewer, an open-source project under Project Jupyter. Thanks to Rackspace hosting, the nbviewer instance is a free service.

Using nbviewer, below, we see the output of a cell within the 04_notebook.ipynb notebook. View this notebook, here, using nbviewer.

Monitoring Spark Jobs

The Jupyter Docker container exposes Spark’s monitoring and instrumentation web user interface. We can review each completed Spark Job in detail.

We can review details of each stage of the Spark job, including a visualization of the DAG (Directed Acyclic Graph), which Spark constructs as part of the job execution plan, using the DAG Scheduler.

We can also review the task composition and timing of each event occurring as part of the stages of the Spark job.

We can also use the Spark interface to review and confirm the runtime environment configuration, including versions of Java, Scala, and Spark, as well as packages available on the Java classpath.

Local Spark Performance

Running Spark on a single node within the Jupyter Docker container on your local development system is not a substitute for a true Spark cluster, Production-grade, multi-node Spark clusters running on bare metal or robust virtualized hardware, and managed with Hadoop YARN, Apache Mesos, or Kubernetes. In my opinion, you should only adjust your Docker resources limits to support an acceptable level of Spark performance for running small exploratory workloads. You will not realistically replace the need to process big data and execute jobs requiring complex calculations on a Production-grade, multi-node Spark cluster.

We can use the following docker stats command to examine the container’s CPU and memory metrics.

docker stats --format ""table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}""

Below, we see the stats from the Docker stack’s three containers showing little or no activity.

Compare those stats with the ones shown below, recorded while a notebook was reading and writing data, and executing Spark SQL queries. The CPU and memory output show spikes, but both appear to be within acceptable ranges.

Linux Process Monitors

Another option to examine container performance metrics is top, which is pre-installed in our Jupyter container. For example, execute the following top command from a Jupyter terminal, sorting processes by CPU usage.

top -o %CPU

We should observe the individual performance of each process running in the Jupyter container.

A step up from top is htop, an interactive process viewer for Unix. It was installed in the container by the bootstrap script. For example, we can execute the htop command from a Jupyter terminal, sorting processes by CPU % usage.

htop --sort-key PERCENT_CPU

With htop , observe the individual CPU activity. Here, the four CPUs at the top left of the htop window are the CPUs assigned to Docker. We get insight into the way Spark is using multiple CPUs, as well as other performance metrics, such as memory and swap.

Assuming your development machine is robust, it is easy to allocate and deallocate additional compute resources to Docker if required. Be careful not to allocate excessive resources to Docker, starving your host machine of available compute resources for other applications.

Notebook Extensions

There are many ways to extend the Jupyter Docker Stacks. A popular option is jupyter-contrib-nbextensions. According to their website, the jupyter-contrib-nbextensions package contains a collection of community-contributed unofficial extensions that add functionality to the Jupyter notebook. These extensions are mostly written in JavaScript and will be loaded locally in your browser. Installed notebook extensions can be enabled, either by using built-in Jupyter commands, or more conveniently by using the jupyter_nbextensions_configurator server extension.

The project contains an alternate Docker stack file, stack-nbext.yml. The stack uses an alternative Docker image, garystafford/all-spark-notebook-nbext:latest , This Dockerfile, which builds it, uses the jupyter/all-spark-notebook:latest image as a base image. The alternate image adds in the jupyter-contrib-nbextensions and jupyter_nbextensions_configurator packages. Since Jupyter would need to be restarted after nbextensions is deployed, it cannot be done from within a running jupyter/all-spark-notebook container.

Using this alternate stack, below in our Jupyter container, we see the sizable list of extensions available. Some of my favorite extensions include ‘spellchecker’, ‘Codefolding’, ‘Code pretty’, ‘ExecutionTime’, ‘Table of Contents’, and ‘Toggle all line numbers’.

Below, we see five new extension icons that have been added to the menu bar of 04_notebook.ipynb. You can also observe the extensions have been applied to the notebook, including the table of contents, code-folding, execution time, and line numbering. The spellchecking and pretty code extensions were both also applied.

Conclusion

In this brief post, we have seen how easy it is to get started learning and performing data analytics using Jupyter notebooks, Python, Spark, and PySpark, all thanks to the Jupyter Docker Stacks. We could use this same stack to learn and perform machine learning using Scala and R. Extending the stack’s capabilities is as simple as swapping out this Jupyter image for another, with a different set of tools, as well as adding additional containers to the stack, such as MySQL, MongoDB, RabbitMQ, Apache Kafka, and Apache Cassandra.","['file', 'analytics', 'spark', 'docker', 'pyspark', 'python', 'notebook', 'stack', 'container', 'data', 'started', 'getting', 'jupyter', 'notebooks', 'using']","This post will demonstrate the creation of a containerized data analytics environment using Jupyter Docker Stacks.
Jupyter Docker StacksTo enable quick and easy access to Jupyter Notebooks, Project Jupyter has created Jupyter Docker Stacks.
docker stack deploy -c stack.yml jupyterThe Docker stack consists of a new overlay network, jupyter_demo-net , and three containers.
Developing Jupyter NotebooksThe real power of the Jupyter Docker Stacks containers is Jupyter Notebooks.
ConclusionIn this brief post, we have seen how easy it is to get started learning and performing data analytics using Jupyter notebooks, Python, Spark, and PySpark, all thanks to the Jupyter Docker Stacks.",en,['Gary A. Stafford'],2020-01-08 17:16:11.593000+00:00,"{'Pyspark', 'Jupyter Notebook', 'Spark', 'Big Data', 'Docker'}","{'https://miro.medium.com/max/60/0*4lCl9AlKtfuSs4Hs.png?q=20', 'https://miro.medium.com/max/60/0*Lr2g0YHWkhqo_OM3?q=20', 'https://miro.medium.com/max/60/0*8MZRv9ar9V-jQ0ye.png?q=20', 'https://miro.medium.com/max/60/0*i9h0iY5-N7JRJRPi?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1958/1*IZU36yfGL8XXB8Uf414plQ.png', 'https://miro.medium.com/max/1240/0*RRG7CEzfV34pHAoH', 'https://miro.medium.com/max/60/0*RRG7CEzfV34pHAoH?q=20', 'https://miro.medium.com/max/60/0*CKf28B44sgLJViwL.png?q=20', 'https://miro.medium.com/max/1224/0*Cy7n7OzTUHt0eT6i.png', 'https://miro.medium.com/max/1224/0*CKf28B44sgLJViwL.png', 'https://miro.medium.com/max/60/0*RC4un5Q5JHVkf4OH.png?q=20', 'https://miro.medium.com/max/2048/0*z23uVHrq1W64BNL6.png', 'https://miro.medium.com/max/60/1*IZU36yfGL8XXB8Uf414plQ.png?q=20', 'https://miro.medium.com/max/620/0*Dr0Wd3dOC9f53FB-', 'https://miro.medium.com/max/1240/0*GyDchQtPCW-t3xWr', 'https://miro.medium.com/max/1240/0*Hj3CAzX14jUtO-rA', 'https://miro.medium.com/max/2048/0*pO3abFHkzPzwhKbt.png', 'https://miro.medium.com/max/60/0*nu8iX68xBLlJuJrH?q=20', 'https://miro.medium.com/max/60/0*5vEc_gAmHESEl8JF.png?q=20', 'https://miro.medium.com/max/60/0*Dr0Wd3dOC9f53FB-?q=20', 'https://miro.medium.com/max/1240/0*xXaUIQfRE0WaZUbK', 'https://miro.medium.com/max/2048/0*EM9qtifLtogn5iJB.png', 'https://miro.medium.com/max/60/0*EM9qtifLtogn5iJB.png?q=20', 'https://miro.medium.com/max/2048/0*5jglf8iIgZV4Y0yU.png', 'https://miro.medium.com/max/1240/0*XZbJKxL26bj6k9Y1', 'https://miro.medium.com/max/2048/0*62-9MqZiNhXh4poX.png', 'https://miro.medium.com/max/60/0*x8dultJk_2EKel6d?q=20', 'https://miro.medium.com/max/1240/0*IKFmYuqPGP0JrR0X', 'https://miro.medium.com/max/60/0*_S62lfe1llO_CihV.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/0*IKFmYuqPGP0JrR0X?q=20', 'https://miro.medium.com/max/5200/0*RtWzlILT3QtxCDqI.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2048/0*5vEc_gAmHESEl8JF.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*5jglf8iIgZV4Y0yU.png?q=20', 'https://miro.medium.com/max/2048/0*_S62lfe1llO_CihV.png', 'https://miro.medium.com/max/60/1*1hmeZI_nlDBZktoOOlW4Zw.png?q=20', 'https://miro.medium.com/max/1240/0*cJNC3XuJnViJNrKV', 'https://miro.medium.com/max/60/0*PsibO5rIffPx31hF.png?q=20', 'https://miro.medium.com/max/60/1*sa4FZp9N68xyhIMSXmuhgw.png?q=20', 'https://miro.medium.com/max/60/0*GyDchQtPCW-t3xWr?q=20', 'https://miro.medium.com/max/60/0*z23uVHrq1W64BNL6.png?q=20', 'https://miro.medium.com/max/2048/0*RNadvOaRrBnTcMQ1.png', 'https://miro.medium.com/max/2048/0*8MZRv9ar9V-jQ0ye.png', 'https://miro.medium.com/max/60/0*IsNIOvkBUzcHKpvd?q=20', 'https://miro.medium.com/max/60/0*FAEnkOc9HuvMQfTN.png?q=20', 'https://miro.medium.com/max/1958/1*sa4FZp9N68xyhIMSXmuhgw.png', 'https://miro.medium.com/max/60/0*47Is-7qI2NM1ofj_.png?q=20', 'https://miro.medium.com/max/60/0*wRciDQBR-ULKMeTn.png?q=20', 'https://miro.medium.com/max/60/0*syg63Sq0NrUmWKS2?q=20', 'https://miro.medium.com/max/60/0*9wSodYZlvSTZFP6w?q=20', 'https://miro.medium.com/max/60/0*tv17_3yBYOYnVj0n.png?q=20', 'https://miro.medium.com/max/60/0*RNadvOaRrBnTcMQ1.png?q=20', 'https://miro.medium.com/max/60/0*TTRtjnle1xp6Vw9b?q=20', 'https://miro.medium.com/max/1240/0*9wSodYZlvSTZFP6w', 'https://miro.medium.com/max/5200/0*RC4un5Q5JHVkf4OH.png', 'https://miro.medium.com/max/60/0*XZbJKxL26bj6k9Y1?q=20', 'https://miro.medium.com/max/2048/0*wRciDQBR-ULKMeTn.png', 'https://miro.medium.com/max/2048/0*FAEnkOc9HuvMQfTN.png', 'https://miro.medium.com/max/2048/0*4lCl9AlKtfuSs4Hs.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1240/0*i9h0iY5-N7JRJRPi', 'https://miro.medium.com/max/60/0*Hj3CAzX14jUtO-rA?q=20', 'https://miro.medium.com/max/60/0*ZP6zQgXmiZ_naRZr.png?q=20', 'https://miro.medium.com/max/60/0*pO3abFHkzPzwhKbt.png?q=20', 'https://miro.medium.com/max/1240/0*fPn-xRiFAnvRGBAA', 'https://miro.medium.com/max/1240/0*Rxv9F8auW1Ry4ZBi', 'https://miro.medium.com/max/60/0*62-9MqZiNhXh4poX.png?q=20', 'https://miro.medium.com/max/1240/0*x8dultJk_2EKel6d', 'https://miro.medium.com/max/4328/0*ZP6zQgXmiZ_naRZr.png', 'https://miro.medium.com/max/60/0*fPn-xRiFAnvRGBAA?q=20', 'https://miro.medium.com/max/4328/0*47Is-7qI2NM1ofj_.png', 'https://miro.medium.com/max/60/0*dPUMtPmeAEYrr0iz.png?q=20', 'https://miro.medium.com/max/2048/0*dPUMtPmeAEYrr0iz.png', 'https://miro.medium.com/max/1240/0*TTRtjnle1xp6Vw9b', 'https://miro.medium.com/max/1906/1*1hmeZI_nlDBZktoOOlW4Zw.png', 'https://miro.medium.com/max/60/0*cJNC3XuJnViJNrKV?q=20', 'https://miro.medium.com/max/1240/0*IsNIOvkBUzcHKpvd', 'https://miro.medium.com/max/60/0*xXaUIQfRE0WaZUbK?q=20', 'https://miro.medium.com/fit/c/96/96/1*X1kPJdug_0qSelVFypdjYQ.jpeg', 'https://miro.medium.com/max/60/0*Rxv9F8auW1Ry4ZBi?q=20', 'https://miro.medium.com/max/1240/0*Lr2g0YHWkhqo_OM3', 'https://miro.medium.com/max/1240/0*nu8iX68xBLlJuJrH', 'https://miro.medium.com/max/60/0*RtWzlILT3QtxCDqI.png?q=20', 'https://miro.medium.com/max/2048/0*PsibO5rIffPx31hF.png', 'https://miro.medium.com/max/60/0*Cy7n7OzTUHt0eT6i.png?q=20', 'https://miro.medium.com/max/2048/0*tv17_3yBYOYnVj0n.png', 'https://miro.medium.com/fit/c/160/160/1*X1kPJdug_0qSelVFypdjYQ.jpeg', 'https://miro.medium.com/max/1240/0*syg63Sq0NrUmWKS2', 'https://miro.medium.com/max/1240/0*Dr0Wd3dOC9f53FB-'}",2020-03-05 00:22:01.206704,2.61088490486145
https://towardsdatascience.com/attention-for-time-series-classification-and-forecasting-261723e0006d,Attention for time series forecasting and classification,"Attention for time series forecasting and classification

Harnessing the most recent advances in NLP for time series forecasting and classification

*Note this article was published on 10/18/19 I have no idea why Medium is saying that it is from 4/9/19. The article contains the most recent and up-to-date results including articles that will appear at Neurips 2019 and preprints that came out just this past August.

Transformers (specifically self-attention) have powered significant recent progress in NLP. They have enabled models like BERT, GPT-2, and XLNet to form powerful language models that can be used to generate text, translate text, answer questions, classify documents, summarize text, and much more. With their recent success in NLP one would expect widespread adaptation to problems like time series forecasting and classification. After all, both involve processing sequential data. However, to this point research on their adaptation to time series problems has remained limited. Moreover, while some results are promising, others remain more mixed. In this article, I will review current literature on applying transformers as well as attention more broadly to time series problems, discuss the current barriers/limitations, and brainstorm possible solutions to (hopefully) enable these models to achieve the same level success as in NLP. This article will assume that you have a basic understanding of soft-attention, self-attention, and transformer architecture. If you don’t please read one of the linked articles. You can also watch my video from the PyData Orono presentation night.

Attention for time series data: Review

The need to accurately forecast and classify time series data spans across just about every industry and long predates machine learning. For instance, in hospitals you may want to triage patients with the highest mortality early-on and forecast patient length of stay; in retail you may want to predict demand and forecast sales; utility companies want to forecast power usage, etc.

Despite the successes of deep learning with respect to computer vision many time series models are still shallow. Particularly, in industry many data scientists still utilize simple autoregressive models instead of deep learning. In some cases, they may even use models like XGBoost fed with manually manufactured time intervals. Usually, the common reasons for choosing these methods remain interpretability, limited data, ease of use, and training cost. While there is no single solution to address all these issues, deep models with attention provide a compelling case. In many cases, they offer overall performance improvements (other vanilla LSTMs/RNNs) with the benefit of interpretability in the form of attention heat maps. Additionally, in many cases, they are faster than using an RNN/LSTM (particularly with some of the techniques we will discuss).

Several papers have studied using basic and modified attention mechanisms for time series data. LSTNet is one of the first papers that proposes using an LSTM + attention mechanism for multivariate forecasting time series. Temporal Pattern Attention for Multivariate Time Series Forecasting by Shun-Yao Shih et al. focused on applying attention specifically attuned for multivariate data. This mechanism aimed at resolving issues including noisy variables in the multivariate time series and introducing a better method than a simple average. Specifically,

The attention weights on rows select those variables that are helpful for forecasting. Since the context vector vt is now the weighted sum of the row vectors containing the information across multiple time steps, it captures temporal information.

Simply speaking, this aims to select the useful information across the various feature time series data for predicting the target time series. First, they utilize a 2dConvolution on the row vectors of the RNNs hidden states. This is followed by a scoring function. Finally, they use a sigmoid activation instead of softmax since they expect multiple variables to be relevant for prediction. The rest follows a fairly standard attention practice.

Code for the temporal pattern attention mechanism. Notice that the authors choose to use 32 filters.

Diagram from the paper

In terms of results, the model outperforms (using relative absolute error) other methods including a standard auto-regressive model and LSTNet on forecasting solar energy and electricity demand, traffic and exchange rate.

Even though this article doesn’t use self-attention I think this is a really interesting and well-thought-out use of attention. A lot of time-series research seems to focus on univariate time series data. Moreover, the ones that do study multivariate time series often solely expand the dimensions of the attention mechanism rather than apply it horizontally across the feature time-series. It might make sense to see if a modified self-attention mechanism could select the relevant source time series data for predicting the target. The full code for this paper is publicly accessible on GitHub.

What is really going on with self-attention?

Lets first briefly review a couple of specifics of self-attention before we delve into the time series portion. For a more detailed examination please see this article on mathematics of attention or the Illustrated Transformer. For self-attention recall that we generally have query, key, value vectors that are formed via simple matrix multiplication of the embedding by the weight matrix. What a lot of explanatory articles don’t mention is that query, key, and value can often come from different sources depending on the task and vary based on whether it is the encoder or the decoder layer. So for instance, if the task is machine translation the query, key and value vectors in the encoder would come from the source language but the query, key, and value vectors in the decoder would come from the target language. In the unsupervised language modeling case however they are all generally formed from the source sequence. Later on we will see that many self-attention time series models modify how these values are formed.

Secondly, self-attention generally requires positional encodings as it has no knowledge of sequence order. It usually incorporates this positional information via addition to the word or time step embedding rather than concatenation. This is somewhat odd as you would assume that adding positional encodings directly to the word embedding would hurt it. However according to this Reddit response due to the high dimensionality of the word embeddings the positional encodings you get approximate orthogonality (i.e. the positional encodings and word embeddings already occupy different spaces). Moreover, the poster argues that sine and cosine help to give nearby word similar positional embeddings.

But in the end this still leaves a lingering question: wouldn’t straightforward concatenation work better in this respect? This is something that I don’t have a direct answer for at the moment. There are however some good recent papers on creating better positional embeddings. Transformer-XL (the basis for XLNet) has its own specific relational embeddings. Also the NIPs 2019 paper, Self-attention with Functional Time Representation Learning, examines creating more effective positional representations through a functional feature map (though paper is not currently on arxiv at the moment).

A number of recent studies have analyzed what actually happens in models like BERT. Although geared entirely towards NLP these studies can help us to understand how to effectively utilize these architectures for time series data as well as anticipate possible problems.

In “What Does BERT Look At? An Analysis of BERT’s Attention” the authors analyze the attention of BERT and investigate linguistic relations. This paper is a great illustration of how self-attention (or any type of attention really) naturally lends itself to interpretability. As we can use the attention weights to visualize the relevant parts of focus.

Figure 5 from the paper. This technique of illustrating attention weights is highly useful for interpretability purposes and cracking open the “black box” of deep learning. Similar methods of analyzing specific attention weights could show which time steps or time series a model focuses on when predicting.

Also interesting is the fact that the authors find the following:

We find that most heads put little attention on the current token. However, there are heads that specialize to attending heavily on the next or previous token, especially in earlier layers of the network

Obviously in time-series data attention heads “attending to the next token” is problematic. Hence, when dealing with time series we will have to apply some sort of mask. Secondly, it is hard to tell if this is solely a product of the language data BERT was trained on or if this is likely to occur with multi-headed attention more broadly speaking. For forming language representations focusing on the closest word makes a lot of sense. However, this is much more variable with time series data, in certain time series sequences causality can come from steps much further back (for instance for some rivers it can take 24+ hours for heavy rainfall to raise the river).

Are Sixteen Heads Really Better than One

In this article, the authors found that pruning several attention heads had a limited effect on performance. Generally, performance only significantly fell when more than 20% of attention heads were pruned. This is particularly relevant for time series data as often we are dealing with long dependencies. Especially only ablating a single attention head seems to have almost no impact on score and in some cases results in better performance.

From p. 6 of the article. Using fewer attention heads may serve as an effective strategy for reducing the computational burden of self-attention for time series data. There seems to be a substantial amount of overlap of certain heads. In general it might make sense to train on more data (when available) rather than have more heads.

Visualizing the Geometry of BERT

Umap visualization of the different semantic sub-spaces of the word “run.” Visualization made using context atlas https://storage.googleapis.com/bert-wsd-vis/demo/index.html?#word=run . It would be interesting to create Umap visualization of different time series representations from a large scale trained transformer model.

This paper explores the geometrical structures found within the BERT model. They conclude that BERT seems to have geometric representations of parse trees internally. They also discover there are semantically meaningful subspaces within the larger embedding space. Although this probe is obviously linguistically focused, the main question it raises is if BERT learns these linguistically meaningful patterns then would it learn similar temporally relevant patterns. For instance, if we large scale trained a transformer time series, what would we discover in the embedding space? Would for instance we see similar patient trajectories clustered together or if we trained on many different streams for flood forecasting would it group dam fed streams together with similar release cycles, etc… Large scale training of a transformer on thousands of different time series could prove insightful and enhance our understanding of the data as well. The authors include two cool GitHub pages with interactive visualizations that you can use to explore further.

Another fascinating research work that came out of ICLR 2019 was Pay Less Attention with Lightweight and Dynamic Convolutions. This work investigates both why self-attention works and proposes dynamic convolutions as an alternative. The main advantage of dynamic convolutions are that they are computationally simpler and more parallelizable than self-attention. The authors find that these dynamic convolutions preform roughly equivalent to self-attention. The authors also employ weight sharing which further reduces the parameters required overall. Interestingly, despite the potential speed improvements I haven’t seen any time series forecasting research adopt this methodology (at least not yet).

Self-attention for time series

There have been only a few research papers that use self-attention on time series data with varying degrees of success. If you know of any additional ones please let me know. Additionally, huseinzol05 on GitHub has implemented a vanilla version of attention is all you need for stock forecasting.

Attend and Diagnose leverages self attention on medical time series data. This time series data is multivariate and contains information like a patient’s heart rate, SO2, blood pressure, etc.

The architecture for attend and diagnose

Their architecture starts with a 1-D convolution across each clinical factor which they use to achieve preliminary embeddings. Recall that a 1D Conv will utilize a kernel of a specific length and process it a set number of times. It is important to note that here the 1-D convolution is not applied across the time series steps as is typical. Therefore if the initial time series contains 100 steps it will still contain 100 steps. Rather it is instead applied to create a multi-dimensional representation of each time step. For more information on 1-D convolutions for time series data refer to this great article. After the 1-D convolution step the authors then use positional encodings:

The encoding is performed by mapping time step t to the same randomized lookup table during both training and prediction.

This is different than standard self-attention which uses cosine and sine functions to capture the position of words. The positional encodings are joined (likely added although…the authors do not indicate exactly how) to each respective output from the 1D Conv layer.

Next comes the self-attention operation. This is mostly the same as the standard type of multi-headed attention operation, however it has a few subtle differences. First as mentioned above since this is time series data the self-attention mechanism cannot incorporate the entire sequence. It can only incorporate timesteps up to the time step being considered. To accomplish this the authors appear to use a masking mechanism that also masks timestamps too far in the past. Unfortunately, the authors are very non-specific on the actual formula for this, however, if I had to guess I would assume it is roughly analogous to the masking operation shown by the authors in overcoming the transformer bottleneck.

After the multi-headed attention, the now transformed embeddings still need to have additional steps taken before they are useful. Typically in standard self-attention, we have an addition and layer normalization component. The layer normalization will normalize the output of the self-attention and the original embedding (see here for more information on this), however, the authors instead chooses to Dense Interpolation. This means that embeddings outputted from the multi-headed-attention module are taken and used in a manner that is useful for capturing syntactic and structural information.

After the dense interpolation algorithm, there is a linear layer followed by a softmax, sigmoid or relu layer (depending on the task). The model itself is multitasking so it aims to forecast length of stay, the diagnosis code, the risk of decompensation, the length of stay and the mortality rate.

Altogether I thought this paper was good demonstration of using self-attention on multivariate time series data. The results were state of the art at the time it was released, they have now been surpassed by TimeNet. However, this is primarily due to the effectiveness of transfer-learning based pretraining rather than the architecture. If I had to guess similar pre-training with SAND would result in better performance.

My main critcism of this paper is primarily from reproducibility standpoint as no code is provided and various hyperparameters such as the kernal size are either not included or only vaguely hinted at. Other concepts are not discussed clearly enough such as the masking mechanism. I’m currently working on trying to reimplement in PyTorch and will post the code here when I’m more sure about its realiability.

Another recent paper that is fairly interesting is “CDSA: Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation” by Jiawei Ma et al. This article focuses on imputing (estimating) missing time series values. Effective data imputation is important for many real world applications as sensors often have periods where they malfunction causing missing data. This creates problems when trying to forecast or classify data as the missing or null values will impact the forecast. The authors setup their model to use a cross attention mechanism that works by utilizing data in different dimensions such as time location and the measurement.

This is another fascinating example of modifying the standard self-attention mechanism to work across multi dimensional data. In particular as stated above the value vector is meant to capture contextual information. Oddly this is in contrast to what we will see later where the query and key vectors utilize contextual information but not the value vector.

The authors evaluate their results on several traffic-forecasting and air-quality datasets. They evaluate with respect to both forecasting and imputation. For testing imputation, they discard a certain percentage of the values and attempt to impute them using the model. They compare these for with the actual values. Their model outperforms other RNN and statistical imputation methods on all missing data rates. In terms of forecasting, the model also achieves the best performance.

Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting by Shiyang Li et al.

This is a recent article that will appear at NIPS in 2019. It focuses on several of the problems with applying the transformer to time series data. The authors basically argue that classical self attention does fully leverage the contextual data. They argue that this particularly causes problems with dynamic time series data that varies with seasonality (for instance forecasting sales around the holidays vs. the rest of the year or forecasting extreme weather patterns). To remedy this they introduce a new method of generating the query and value vectors.

We propose convolutional self attention [mechanism] by employing causal convolutions to produce queries and keys in the self attention layer. Query-key matching aware of local context, e.g. shapes, can help the model achieve lower training error and further improve its forecasting accuracy.

From p. 3 of article. Essentially with other versions of multi-headed attention query, value, and key vectors are created off a single time-step, whereas a larger kernel size allows it to create the key and query vectors from multiple time-steps. This allows the model to be able to understand a greater degree of context.

Part two of the article focuses on solutions related to the memory use of the transformer model. Self-attention is very memory intensive particularly with respect to very long sequences (specifically it is O(L²)). The authors propose a new attention mechanism that is O(L(log L)²). With this self-attention mechanism, cells can only attend to previous cells with an exponential step size. So for instance cell five would attend to cell four and cell two. They also introduce two variations of this log attention: local attention and restart attention. See their diagram below for more information

From page 5. of the article. All these variations have the same overall time complexity O(L(log L)²), however the actual runtime of Local Attention + Logsparse would likely be longest due to it attending to the most cells.

The authors evaluate their approach on several different datasets including electricity consumption (recorded in 15-minute intervals), traffic in San Francisco (20-minute intervals), solar data production hourly (from 137 different power plants) and wind data (daily estimates of 28 counties wind potential as a percentage of overall power production). Their choice of ρ-quantile loss as an evaluation metric is a bit strange as normally I’d expect MAE, MAP, RMSE or something similar for a time series forecasting problem.

Equation for p quantile regression loss.

I’m still trying to grasp what exactly this metric represents, however at least from the results it appears that a lower score is better. Using this metric their convolutional self-attention transformer outperforms DeepAR, DeepState, ARIMA, and other models. They also conduct an ablation study where they look at the effect of kernel size when computing a seven-day forecast. They found that a kernel size of 5 or 6 generally produced the best result.

I think this is a good research article that addresses some of the short-comings of the transformer as applied to time-series data. I particularly think that the use of a convolutional kernel (of size greater than one) is really useful in time series problems where you want to capture surrounding context for the key and query vectors. There is currently no code, but NeurIPs is still more than a month away so hopefully, the authors release it between now and then.

Conclusion and future directions

In conclusion, self-attention and related architectures have led to improvements in several time series forecasting use cases, however, altogether they have not seen widespread adaptation. This likely revolves around several factors such as the memory bottleneck, difficulty encoding positional information, focus on pointwise values, and lack of research around handling multivariate sequences. Additionally, outside of NLP many researchers are not probably not familiar with self-attention and its potential. While simple models such as ARIMA in many cases make sense for industry problems I believe that transformers have a lot to offer as well.

Hopefully, the approaches summarized in this article shine some light on effectively applying transformers to time series problems. In a subsequent article, I plan on giving a practical step-by-step example of forecasting and classifying time-series data with a transformer in PyTorch. Any feedback and/or criticisms are welcome in the comments. Please let me know if I got something incorrect (which is quite possible given the complexity of the topic) and I will update the article.","['selfattention', 'forecasting', 'article', 'transformer', 'series', 'mechanism', 'attention', 'data', 'model', 'classification', 'authors']","Attention for time series forecasting and classificationHarnessing the most recent advances in NLP for time series forecasting and classification*Note this article was published on 10/18/19 I have no idea why Medium is saying that it is from 4/9/19.
With their recent success in NLP one would expect widespread adaptation to problems like time series forecasting and classification.
Attention for time series data: ReviewThe need to accurately forecast and classify time series data spans across just about every industry and long predates machine learning.
LSTNet is one of the first papers that proposes using an LSTM + attention mechanism for multivariate forecasting time series.
Simply speaking, this aims to select the useful information across the various feature time series data for predicting the target time series.",en,['Isaac Godfried'],2019-10-20 05:14:02.618000+00:00,"{'Time Series Forecasting', 'Time Series Analysis', 'Deep Learning', 'Transformers', 'Machine Learning'}","{'https://miro.medium.com/max/1200/1*SjKMs_iTOJaKqx45fpYEDQ.png', 'https://miro.medium.com/max/3316/1*4UBXHae1jVYvFUJeGnahoA.png', 'https://miro.medium.com/max/60/1*dsbD4fidz9Gj0oInt9bXLw.png?q=20', 'https://miro.medium.com/max/3664/1*RAFJxX_ZavSuwwcIHVJXIw.png', 'https://miro.medium.com/max/60/1*iUpjqSuiN7yYsvoNtUvbHg.png?q=20', 'https://miro.medium.com/max/60/1*RAFJxX_ZavSuwwcIHVJXIw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3664/1*jpjMOF14lGbbw5hdwnwiSg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3664/1*dsbD4fidz9Gj0oInt9bXLw.png', 'https://miro.medium.com/max/60/1*WgBqsy13291nK7SEs2LZ-Q.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/4176/1*eZQZel7w-Ukp7oOtXuocJg.png', 'https://miro.medium.com/max/60/1*SjKMs_iTOJaKqx45fpYEDQ.png?q=20', 'https://miro.medium.com/max/60/1*jpjMOF14lGbbw5hdwnwiSg.png?q=20', 'https://miro.medium.com/max/60/1*2YfLSGz7p2CFqR-ltTeCsQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*Sx7X1u5ElAJwgTxr.', 'https://miro.medium.com/max/3304/1*YBbULIOPnECwvW7fy9NK7A.png', 'https://miro.medium.com/fit/c/96/96/0*Sx7X1u5ElAJwgTxr.', 'https://miro.medium.com/max/60/1*eZQZel7w-Ukp7oOtXuocJg.png?q=20', 'https://miro.medium.com/max/3328/1*2YfLSGz7p2CFqR-ltTeCsQ.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*YBbULIOPnECwvW7fy9NK7A.png?q=20', 'https://miro.medium.com/max/3664/1*SjKMs_iTOJaKqx45fpYEDQ.png', 'https://miro.medium.com/max/3364/1*WgBqsy13291nK7SEs2LZ-Q.png', 'https://miro.medium.com/max/3328/1*iUpjqSuiN7yYsvoNtUvbHg.png', 'https://miro.medium.com/max/60/1*4UBXHae1jVYvFUJeGnahoA.png?q=20'}",2020-03-05 00:22:07.793768,6.586056232452393
https://towardsdatascience.com/stock-market-predictions-with-rnn-using-daily-market-variables-6f928c867fd2,Stock market predictions with RNN using daily market variables,"In this project I attempt to construct a RNN (the LSTM in particular) for predicting stock market returns. The prediction is made on S&P500 index, which is one of the most widely used indices for trading and benchmarking stock market returns. Then basic trading strategy execution is run on predictions and it’s performance relative to S&P500 index is our measure of success.

Stock markets in general are known very difficult to predict due to their high dimensionality space, and the S&P500 is probably the most market efficient stock index globally. For this reason the project task is more than challenging. Therefore it can be regarded more than anything as author’s learning of the techniques of machine learning and particularily neural networks in time-series analysis.

The project goes through following steps: Data generation, Data pre-processing and feature selection, Baseline prediction, LSTM training and demonstration of results. The project has been built in Python programming language with Keras library for neural networks.

Data generation for training and testing sets

More than 20 market variables (features) with daily frequency has been used in the project, with data history since the start of 2006 until April 2019. Data is sourced from Bloomberg Terminal. The features used are mostly various S&P500 technical indicators, but also market variables such as index forward consensus earnings estimates, US Treasury bond 2-year and 10-year yield, dollar index, commodity index and volatility index. Third category that is used is S&P500 index signal Close and the Open, High, Low known as OHLC set, and Trading Volume. The initial feature selection attempts to bring more variety and representation compared to most of the similar projects in this field, that are using mostly price signal or OHLC of the index. Different set of features have been experimented during project, but total 10 features are selected for the training at the end. The output for prediction is S&P500 index next day return (price change).

The data pre-processing follows the principles of time-series sequencing that is required in case of RNNs. It is more than 13 years of continuous daily data that is divided into 43 study periods with each of being length of 3 years (assuming 240 trading days in a year) for training and 60 days (approx. 3 months) for testing. It provides 10 years and 9 months of continuous prediction horizon. The Fig 1. below illustrates the data arrangement.

Fig 1. Dataset arrangement during the whole period

Further, each study period (training and testing) is sequenced as an example illustration in Fig 2. in terms of rolling features window and the corresponding output.

Fig 2. Input sequence illustration on selected features and output

The different timesteps lengths in range from 5 to 20 days have been experimented during the project, but the 10 days timestep length is selected to present here. The full dataset is arranged in following order of dimensions — studies, samples, timesteps, features.

Data preprocessing and feature selection

The raw input data for training is standardized to z-score for each study period, so that each feature in training data has mean 0 and standard deviation 1. Importantly, test data is standardized with mean and standard deviation obtained from the training data.

For determining the relative importance of features the regressor from the XGBoost library is implemented on the standardized features and the output. Fig 4. below shows the feature importances, out of which the 10 features with highest importance have been selected to be used going forward. However, when selecting features with highest importances we are risking to lose the relationships represented within full collection of features.

Fig 4. Feature importances from XGBoost

After data preprocessing, the datasets have following shapes ready to be used for training and testing.

X train (43, 711, 10, 10) y train (43, 711, 1) x test (43, 60, 10, 10) y test (43, 60, 1)

Baseline prediction — K-nearest neighbors

In order to evaluate the ability of RNN in stock market prediction, we implement baseline prediction with K-nearest neighbors (K-NN) algorithm using scikit-learn regression. The different K-s in range from 1 to 200 have been regressed during training periods and the K-s resulting with lowest mean squared error during validation period have been used in particular study period in testing. Fig 5. below illustrates in upper panel the true S&P500 and predicted returns. The lower panel illustrates trading performance (before any real life costs) during testing period. Here we are using the symmetrical rule of being 150% invested in the index if predicted return is positive and 50% invested if the prediction is negative. The result is plotted together with S&P500 index buy-and-hold performance, during the full length of all testing periods with more than 2500 trading days. Even if the accuracy is the traditional measure to evaluate classification models, we are using performance here and in the later stage to compare the K-NN and RNN prediction ability. The K-NN model has resulted with outperformance during full period, returning 3.5 times the initial investment compared to 3.0 times for S&P500 buy-and-hold strategy. The red marks in lower panel represent long (1) and short (-1) position signals (in alt y-axis) with respect to full index investment. Even if the result is positive for K-NN during the whole period, it is not consistent as there are long periods (in years) of underperformance visible at the beginning and the second half of the timespan.

Fig 5. K-NN performance

LSTM prediction

The problem has been set as binary classification and assigning value of 1 for positive and 0 for negative daily returns. The LSTM network settings and architecture that are used are presented in Fig 6.

Fig 6. LSTM settings and architecture

For fitting the parameters are batch size 32, validation split 20% and 200 epochs. The training and validation loss results for each study period are in Fig 7. and respective accuracies in Fig. 8. These are showing that network is having difficulties to improve on validation during epochs. Importantly there are few outlier study sets with lower loss and higher accuracy, that can make a difference during testing period with regards to trading performance. Nevertheless the relationships approximated during training period seem to be marginally holding also during validation as the average validation accuracy across study periods after the last epoch is 53.75%.

Fig 7. LSTM network training and validation loss

Fig 8. LSTM network training and validation accuracies

LSTM model based trading performance

When applying the same trading rule for LSTM output as for K-NN, the resulting performance is presented in Fig 9. We can observe that trading result is 4 times the initial investment compared to 3 times of S&P500 buy-hold and 3.5 times of K-NN (Fig 5.). Therefore LSTM based trading performance has outperformed the S&P500 index and K-NN algorithm in this project.

Fig 9. S&P500 buy&hold and LSTM based trading performance

The project concludes that LSTM neural network can be good alternative for stock market time series prediction among other machine learning methods. The overall success of the presented LSTM setting in solving the task in this project is however questionable, since over all test periods the test accuracy 51.6% might not be sufficient. Further testing of different parameter and architecture settings are needed to improve the accuracy.

Future work: Neural networks for feature extraction

For next, the plan is to generate extracted features using autoencoder on convolutional neural network (CNN). The idea is to use the extracted features as input for the LSTM network. Due to time constraints the resulting losses, accuracies and trading performance is not presented here, but description of building autoencoder and extraction of compressed features is presented below. The idea to use CNN for particular purpose is sourced from nice article here, that provides comparison and results of different networks for autoencoders and among which CNN seems to be promising alternative.

The autoencoder network architecture that is built for the project is shown in Fig. 10. As it can be seen the attempt is to encode 10 features into 5 features. After fitting the encoder output from first convolutional layer it has been used to generate compressed features, that will be then fed to LSTM.

Fig. 10 CNN autoencoder

Further ideas include input transformation with wavelets and experimenting with different data processing methods and hyperparameters.

Disclaimer: The material presented in the story should not be considered as advice or a recommendation in relation to holding, purchasing or selling securities or other financial products and is not the advice or recommendation to create or use any technique or strategy referred in the story.","['period', 'stock', 'daily', 'index', 'market', 'variables', 'project', 'prediction', 'lstm', 'trading', 'data', 'rnn', 'sp500', 'training', 'features', 'using', 'predictions']","The prediction is made on S&P500 index, which is one of the most widely used indices for trading and benchmarking stock market returns.
Then basic trading strategy execution is run on predictions and it’s performance relative to S&P500 index is our measure of success.
The project goes through following steps: Data generation, Data pre-processing and feature selection, Baseline prediction, LSTM training and demonstration of results.
Third category that is used is S&P500 index signal Close and the Open, High, Low known as OHLC set, and Trading Volume.
Importantly, test data is standardized with mean and standard deviation obtained from the training data.",en,['Vahur Madisson'],2019-06-19 21:59:17.569000+00:00,{'Machine Learning'},"{'https://miro.medium.com/max/916/1*vt8evEtxcoKeSXNqoOCW7A.jpeg', 'https://miro.medium.com/max/1534/1*5K03CkdVCklH1eBxuKZvtw.jpeg', 'https://miro.medium.com/max/60/1*5l3-optwcjdnATYlYva6EA.jpeg?q=20', 'https://miro.medium.com/max/930/1*PRY7-kaZAj6hd78UV4l3VA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/882/1*4AqlpZftGe9jY45ZWsEf5w.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*EvCXhRlz6rIN-C1HAOxLPg.jpeg?q=20', 'https://miro.medium.com/max/56/1*4AqlpZftGe9jY45ZWsEf5w.jpeg?q=20', 'https://miro.medium.com/max/60/1*vt8evEtxcoKeSXNqoOCW7A.jpeg?q=20', 'https://miro.medium.com/max/44/1*1YeUNrku9F4J3ViAEPMUdg.jpeg?q=20', 'https://miro.medium.com/max/60/1*veu_xDKNclcdzk0I0YswBg.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1036/1*5l3-optwcjdnATYlYva6EA.jpeg', 'https://miro.medium.com/max/754/1*1YeUNrku9F4J3ViAEPMUdg.jpeg', 'https://miro.medium.com/max/864/1*Wnld0kIBtxs3hHsaoSjf9w.jpeg', 'https://miro.medium.com/fit/c/96/96/1*oRpxgopEqxcjwiHXEOb2Ag@2x.jpeg', 'https://miro.medium.com/max/1714/1*EvCXhRlz6rIN-C1HAOxLPg.jpeg', 'https://miro.medium.com/max/926/1*veu_xDKNclcdzk0I0YswBg.jpeg', 'https://miro.medium.com/max/60/1*PRY7-kaZAj6hd78UV4l3VA.jpeg?q=20', 'https://miro.medium.com/max/60/1*Wnld0kIBtxs3hHsaoSjf9w.jpeg?q=20', 'https://miro.medium.com/max/1724/1*t8pOY0Zfd5xr0Ne80rtL1g.jpeg', 'https://miro.medium.com/max/1832/1*vt8evEtxcoKeSXNqoOCW7A.jpeg', 'https://miro.medium.com/max/60/1*t8pOY0Zfd5xr0Ne80rtL1g.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*5K03CkdVCklH1eBxuKZvtw.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*oRpxgopEqxcjwiHXEOb2Ag@2x.jpeg'}",2020-03-05 00:22:14.086982,6.2922163009643555
https://towardsdatascience.com/how-to-use-convolutional-neural-networks-for-time-series-classification-56b1b0a07a57,How to Use Convolutional Neural Networks for Time Series Classification,"How to Use Convolutional Neural Networks for Time Series Classification

A gentle introduction, state-of-the-art model overview, and a hands-on example.

Photo by Christin Hume on Unsplash.

Introduction

A large amount of data is stored in the form of time series: stock indices, climate measurements, medical tests, etc. Time series classification has a wide range of applications: from identification of stock market anomalies to automated detection of heart and brain diseases.

There are many methods for time series classification. Most of them consist of two major stages: on the first stage you either use some algorithm for measuring the difference between time series that you want to classify (dynamic time warping is a well-known one) or you use whatever tools are at your disposal (simple statistics, advanced mathematical methods etc.) to represent your time series as feature vectors. In the second stage you use some algorithm to classify your data. It can be anything from k-nearest neighbors and SVMs to deep neural network models. But one thing unites these methods: they all require some kind of feature engineering as a separate stage before classification is performed.

Fortunately, there are models that not only incorporate feature engineering in one framework, but also eliminate any need to do it manually: they are able to extract features and create informative representations of time series automatically. These models are recurrent and convolutional neural networks (CNNs).

Research has shown that using CNNs for time series classification has several important advantages over other methods. They are highly noise-resistant models, and they are able to extract very informative, deep features, which are independent from time. In this article we will examine in detail how exactly the 1-D convolution works on time series. Then, I will give an overview of a more sophisticated model proposed by the researchers from Washington University in St. Louis. Finally, we will look at a simplified multi-scale CNN code example.

1-D Convolution for Time Series

Imagine a time series of length n and width k. The length is the number of timesteps, and the width is the number of variables in a multivariate time series. For example, for electroencephalography it is the number of channels (nodes on the head of a person), and for a weather time series it can be such variables as temperature, pressure, humidity etc.

The convolution kernels always have the same width as the time series, while their length can be varied. This way, the kernel moves in one direction from the beginning of a time series towards its end, performing convolution. It does not move to the left or to the right as it does when the usual 2-D convolution is applied to images.

1-D Convolution for Time Series. Source: [2] (modified).

The elements of the kernel get multiplied by the corresponding elements of the time series that they cover at a given point. Then the results of the multiplication are added together and a nonlinear activation function is applied to the value. The resulting value becomes an element of a new “filtered” univariate time series, and then the kernel moves forward along the time series to produce the next value. The number of new “filtered” time series is the same as the number of convolution kernels. Depending on the length of the kernel, different aspects, properties, “features” of the initial time series get captured in each of the new filtered series.

The next step is to apply global max-pooling to each of the filtered time series vectors: the largest value is taken from each vector. A new vector is formed from these values, and this vector of maximums is the final feature vector that can be used as an input to a regular fully connected layer. This whole process is illustrated in the picture above.

Let’s take it to another level

With this simple example in mind, let’s examine the model of a multi-scale convolutional neural network for time series classification [1].

The multi-scalability of this model consists in its architecture: in the first convolutional layer the convolution is performed on 3 parallel independent branches. Each branch extracts features of different nature from the data, operating at different time and frequency scales.

The framework of this network consists of 3 consecutive stages: transformation, local convolution, and full convolution.

Multi-Scale Convolutional Neural Network Architecture [1].

Transformation

On this stage different transformations are applied to the original time series on 3 separate branches. The first branch transformation is identity mapping, meaning that the original time series remains intact.

The second branch transformation is smoothing the original time series with a moving average with various window sizes. This way, several new time series with different degrees of smoothness are created. The idea behind this is that each new time series consolidates information from different frequencies of the original data.

Finally, the third branch transformation is down-sampling the original time series with various down-sampling coefficients. The smaller the coefficient, the more detailed the new time series is, and, therefore, it consolidates information about the time series features on a smaller time scale. Down-sampling with larger coefficients results in less detailed new time series which capture and emphasize those features of the original data that exhibit themselves on larger time scales.

Local Convolution

On this stage the 1-D convolution with different filter sizes that we discussed earlier is applied to the time series. Each convolutional layer is followed by a max-pooling layer. In the previous, simpler example global max pooling was used. Here, max pooling is not global, but still the pooling kernel size is extremely large, much larger than the sizes you are used to when working with image data. More specifically, the pooling kernel size is determined by the formula n/p, where n is the length of the time series, and p is a pooling factor, typically chosen between the values {2, 3, 5}. This stage is called local convolution because each branch is processed independently.

Full Convolution

On this stage all the outputs of local convolution stage from all 3 branches are concatenated. Then several more convolutional and max-pooling layers are added. After all the transformations and convolutions, you are left with a flat vector of deep, complex features that capture information about the original time series in a wide range of frequency and time scale domains. This vector is then used as an input to fully connected layers with Softmax function on the last layer.

Keras Example

from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D

from keras.models import Model #this base model is one branch of the main model

#it takes a time series as an input, performs 1-D convolution, and returns it as an output ready for concatenation def get_base_model(input_len, fsize):

#the input is a time series of length n and width 19

input_seq = Input(shape=(input_len, 19))

#choose the number of convolution filters

nb_filters = 10

#1-D convolution and global max-pooling

convolved = Conv1D(nb_filters, fsize, padding=""same"", activation=""tanh"")(input_seq)

processed = GlobalMaxPooling1D()(convolved)

#dense layer with dropout regularization

compressed = Dense(50, activation=""tanh"")(processed)

compressed = Dropout(0.3)(compressed)

model = Model(inputs=input_seq, outputs=compressed)

return model #this is the main model

#it takes the original time series and its down-sampled versions as an input, and returns the result of classification as an output def main_model(inputs_lens = [512, 1024, 3480], fsizes = [8,16,24]):

#the inputs to the branches are the original time series, and its down-sampled versions

input_smallseq = Input(shape=(inputs_lens[0], 19))

input_medseq = Input(shape=(inputs_lens[1] , 19))

input_origseq = Input(shape=(inputs_lens[2], 19)) #the more down-sampled the time series, the shorter the corresponding filter

base_net_small = get_base_model(inputs_lens[0], fsizes[0])

base_net_med = get_base_model(inputs_lens[1], fsizes[1])

base_net_original = get_base_model(inputs_lens[2], fsizes[2]) embedding_small = base_net_small(input_smallseq)

embedding_med = base_net_med(input_medseq)

embedding_original = base_net_original(input_origseq) #concatenate all the outputs

merged = Concatenate()([embedding_small, embedding_med, embedding_original])

out = Dense(1, activation='sigmoid')(merged) model = Model(inputs=[input_smallseq, input_medseq, input_origseq], outputs=out)

return model

This model is a much simpler version of the multi-scale convolutional neural network.

It takes the original time series and 2 down-sampled versions of it (medium and small length) as an input. The first branch of the model processes the original time series of length 3480 and of width 19. The corresponding convolution filter length is 24. The second branch processes the medium-length (1024 timesteps) down-sampled version of the time series, and the filter length used here is 16. The third branch processes the shortest version (512 timesteps) of the time series, with the filter length of 8. This way every branch extracts features on different time scales.

After convolutional and global max-pooling layers, dropout regularization is added, and all the outputs are concatenated. The last fully connected layer returns the result of classification.

Conclusion

In this article I tried to explain how deep convolutional neural networks can be used to classify time series. It is worth mentioning that the proposed method is not the only one that exists. There are ways of presenting time series in the form of images (for example, using their spectrograms), to which a regular 2-D convolution can be applied.

Thank you very much for reading this article. I hope it was helpful to you, and I would really appreciate your feedback.

References:

[1] Z. Cui, W. Chen, Y. Chen, Multi-Scale Convolutional Neural Networks for Time Series Classification (2016), https://arxiv.org/abs/1603.06995.

[2] Y. Kim, Convolutional Neural Networks for Sentence Classification (2014), https://arxiv.org/abs/1408.5882.","['convolution', 'stage', 'convolutional', 'series', 'neural', 'length', 'features', 'networks', 'model', 'classification', 'original', 'branch']","How to Use Convolutional Neural Networks for Time Series ClassificationA gentle introduction, state-of-the-art model overview, and a hands-on example.
These models are recurrent and convolutional neural networks (CNNs).
ConclusionIn this article I tried to explain how deep convolutional neural networks can be used to classify time series.
References:[1] Z. Cui, W. Chen, Y. Chen, Multi-Scale Convolutional Neural Networks for Time Series Classification (2016), https://arxiv.org/abs/1603.06995.
[2] Y. Kim, Convolutional Neural Networks for Sentence Classification (2014), https://arxiv.org/abs/1408.5882.",en,['Margarita Granat'],2019-10-05 03:23:13.406000+00:00,"{'Neural Networks', 'Data Science', 'Deep Learning', 'Machine Learning', 'Programming'}","{'https://miro.medium.com/max/60/1*iJyzEak-RGfpcBC9v-8oAg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*QdOrwCG-6rVF7WfRSw-Bcg.jpeg', 'https://miro.medium.com/max/60/1*H3JzmYY38w-awkqA02rEMQ.png?q=20', 'https://miro.medium.com/max/1200/1*QTpqH0mr_1kMEXTiN8uCfQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3596/1*iJyzEak-RGfpcBC9v-8oAg.png', 'https://miro.medium.com/max/2352/1*H3JzmYY38w-awkqA02rEMQ.png', 'https://miro.medium.com/fit/c/96/96/1*QdOrwCG-6rVF7WfRSw-Bcg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*QTpqH0mr_1kMEXTiN8uCfQ.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/12000/1*QTpqH0mr_1kMEXTiN8uCfQ.jpeg'}",2020-03-05 00:22:17.109479,3.0224976539611816
https://towardsdatascience.com/survival-analysis-intuition-implementation-in-python-504fde4fcf8e,Survival Analysis: Intuition & Implementation in Python,"Table of Contents

Introduction Definitions Mathematical Intuition Kaplan-Meier Estimate Cox Proportional Hazard Model End Note Additional Resources

Introduction

Survival Analysis is a set of statistical tools, which addresses questions such as ‘how long would it be, before a particular event occurs’; in other words we can also call it as a ‘time to event’ analysis. This technique is called survival analysis because this method was primarily developed by medical researchers and they were more interested in finding expected lifetime of patients in different cohorts (ex: Cohort 1- treated with Drug A, & Cohort 2- treated with Drug B). This analysis can be further applied to not just traditional death events, but to many different types of events of interest in different business domains. We will discuss more on the definition of events and time to events in the next section.

Definitions

As mentioned above that the Survival Analysis is also known as Time to Event analysis. Thus, from the name itself, it is evident that the definition of Event of interest and the Time is vital for the Survival Analysis. In order to understand the definition of time and event, we will define the time and event for various use cases in industry.

Predictive Maintenance in Mechanical Operations: Survival Analysis applies to mechanical parts/ machines to answer about ‘how long will the machine last?’. Predictive Maintenance is one of its applications. Here, Event is defined as the time at which the machine breaks down. Time of origin is defined as the time of start of machine for the continuous operations. Along with the definition of time we should also define time scale (time scale could be weeks, days, hours..). The difference between the time of event and the time origin gives us the time to event. Customer Analytics (Customer Retention): With the help of Survival Analysis we can focus on churn prevention efforts of high-value customers with low survival time. This analysis also helps us to calculate Customer Life Time Value. In this use case, Event is defined as the time at which the customer churns / unsubscribe. Time of origin is defined as the time at which the customer starts the service/subscription with a company. Time scale could be months, or weeks. The difference between the time of event and the time origin gives us the time to event. Marketing Analytics (Cohort Analysis): Survival Analysis evaluates the retention rates of each marketing channel. In this use case, Event is defined as the time at which the customer unsubscribe a marketing channel. Time of origin is defined as the time at which the customer starts the service / subscription of a marketing channel. Time scale could be months, or weeks. Actuaries: Given the risks of a population, survival analysis evaluates the probability of the population to die in a particular time range. This analysis helps the insurance companies to evaluate the insurance premiums. Guess, the event and time definition for this use case!!!

I hope the definition of a event, time origin, and time to event is clear from the above discussion. Now its time to delve a bit deeper into the mathematical formulation of the analysis.

Mathematical Intuition

Lets assume a non-negative continuous random variable T, representing the time until some event of interest. For example, T might denote:

• the time from the customer’s subscription to the customer churn.

• the time from start of a machine to its breakdown.

• the time from diagnosis of a disease until death.

Since we have assumed a random variable T (a random variable is generally represented in capital letter), so we should also talk about some of its attributes.

T is a random variable, ‘what is random here ?’. To understand this we will again use our earlier examples as follows.

• T is the time from customer’s(a randomly selected customer) subscription to the customer churn.

• T is the time from start of a randomly selected machine to its breakdown.

• T is the time from diagnosis of a disease until death of a randomly selected patient.

T is continuous random variable, therefore it can take any real value. T is non-negative, therefore it can only take positive real values (0 included).

For such random variables, probability density function (pdf) and cumulative distribution function (cdf) are commonly used to characterize their distribution.

Thus, we will assume that this random variable has a probability density function f(t) , and cumulative distribution function F(t) .

pdf : f(t)

cdf : F(t) : As per the definition of cdf from a given pdf, we can define cdf as F(t) = P (T< t) ; here , F(t) gives us the probability that the event has occurred by duration t. In simple words, F(t) gives us the proportion of population with the time to event value less than t.

cdf as the integral form of pdf

Survival Function: S(t) = 1 - F(t)= P(T ≥t); S(t) gives us the probability that the event has not occurred by the time t . In simple words, S(t) gives us the proportion of population with the time to event value more than t.

Survival Function in integral form of pdf

Hazard Function : h(t) : Along with the survival function, we are also interested in the rate at which event is taking place, out of the surviving population at any given time t. In medical terms, we can define it as “out of the people who survived at time t, what is the rate of dying of those people”.

Lets make it even more simpler:

Lets write it in the form of its definition:

h(t) = [( S(t) -S(t + dt) )/dt] / S(t)

limit dt → 0

2. From its formulation above we can see that it has two parts. Lets understand each part

Instantaneous rate of event: ( S(t) -S(t + dt) )/dt ; this can also be seen as the slope at any point t of the Survival Curve, or the rate of dying at any time t.

Also lets assume the total population as P.

Here, S(t) -S(t + dt) , this difference gives proportion of people died in time dt, out of the people who survived at time t. Number of people surviving at t is S(t)*P and the number of people surviving at t+dt is S(t+dt)*P. Number of people died during dt is (S(t) -S(t + dt))*P. Instantaneous rate of people dying at time t is (S(t) -S(t + dt))*P/dt.

Proportion Surviving at time t: S(t); We also know the surviving population at time t, S(t)*P.

Thus dividing number of people died in time dt, by the number of people survived at any time t, gives us the hazard function as measure of RISK of the people dying, which survived at the time t.

The hazard function is not a density or a probability. However, we can think of it as the probability of failure in an inﬁnitesimally small time period between (t) and (t+ dt) given that the subject has survived up till time t. In this sense, the hazard is a measure of risk: the greater the hazard between times t1 and t2, the greater the risk of failure in this time interval.

We have : h(t) = f(t)/S(t) ; [Since we know that ( S(t) -S(t + dt) )/dt = f(t)] This is a very important derivation. The beauty of this function is that Survival function can be derived from Hazard function and vice versa. The utility of this will be more evident while deriving a survival function from a given hazard function in Cox Proportional Model (Last segment of the article).

These were the most important mathematical definitions and the formulations required to understand the survival analysis. We will end our mathematical formulation here and move forward towards estimation of survival curve.

Kaplan-Meier Estimate

In the Mathematical formulation above we assumed the pdf function and thereby derived Survival function from the assumed pdf function. Since we don’t have the true survival curve of the population, thus we will estimate the survival curve from the data.

There are two main methods to estimate the survival curve. The ﬁrst method is a parametric approach. This method assumes a parametric model, which is based on certain distribution such as exponential distribution, then we estimate the parameter, and then finally form the estimator of the survival function. A second approach is a powerful non-parametric method called the Kaplan-Meier estimator. We will discuss it in this section. In this section we will also try to create the Kaplan-Meier curve manually as well as by using the Python library (lifelines).

Here, ni is deﬁned as the population at risk at time just prior to time ti; and di is defined as number of events occurred at time ti. This, will become more clear with the example below.

We will discuss an arbitrary example from a very small self created data, to understand the creation of Kaplan Meier Estimate curve, manually as well as using a python package.

Event, Time and Time Scale Definition for the Example:

The example below(Refer Fig 1) shows the data of 6 users of a website. These users visit the website and leaves that website after few minutes. Thus, event of interest is the time in which a user leaves the website. Time of origin is defined as the time of opening the website by a user and the time scale is in minutes. The study starts at time t=0 and ends at time t=6 minutes.

Censorship:

Point worth noting here is that during the study period , event happened with 4 out of 6 users(shown in red), while two users (shown in green) continued and the event didn’t happened till the end of the study; such data is called the Censored data.

In case of censorship, as here in case of user 4 and user 5, we don’t know at what time the event will occur, but still we are using that data to estimate the probability of survival. If we choose not to include the censored data, then it is highly likely that our estimates would be highly biased and under-estimated. The inclusion of censored data to calculate the estimates, makes the Survival Analysis very powerful, and it stands out as compared to many other statistical techniques.

Calculations for KM Curve and the interpretation:

Now, lets talk about the calculations done to create the KM Curve below (Refer Fig 1). In figure 1, Kaplan Meier Estimate curve, x axis is the time of event and y axis is the estimated survival probability.

From t=0 till t<2.5 or t∈[0 , 2.5), number of users at risk(ni) at time t=0 is 6 and number of events occurred(di) at time t=0 is 0, therefore for all t in this interval, estimated S(t) = 1. From the definition of the event we can say that 100% is the probability that the time between a user opens the website and exit the website is greater than 2.499* minutes.

From t=2.5 till t<2.4 or t ∈ [2.5 , 4), number of users at risk(ni) at time just before time 2.5 minutes (2.4999* mins) is 6 and number of events occurred(di) at time t=2.5 minutes is 1, therefore therefore for all t in this interval, estimated S(t)= 0.83. From the definition of the event we can say that 83% is the probability that the time between a user opens the website and exit the website is greater than 3.999* minutes.

From t=4 till t<5 or t ∈[4 , 5), number of users at risk(ni) at time just before time 4 minutes (3.999* mins) is 5 and number of events occurred(di) at time t=4 minutes is 2, therefore for all t in this interval, estimated S(t) = 0.5. This result can also be verified by simple mathematics of relative frequency. For any t∈[4,5), lets say t=4.5, total number of users at the start were 6, total number remaining at t are 3. Therefore, the probability of the users spending more than 4.5 (or any time t ∈[4,5)) minutes on website is (3/6), which is 50%.

Similarly, we can estimate the probability for other time intervals (refer table calculations in fig 1)

Mathematically, for any time t ∈ [t1, t2), we have

S(t) = P(survive in [0, t1)) × P(survive in [t1, t] | survive in [0, t1))

fig 1: a. Shows the user level time data in color.; b. Shows Kaplan Meier (KM)Estimate Curve; c. Formula for estimation of KM curve; d. Table showing the calculations

# Python code to create the above Kaplan Meier curve from lifelines import KaplanMeierFitter



## Example Data

durations = [5,6,6,2.5,4,4]

event_observed = [1, 0, 0, 1, 1, 1]



## create a kmf object

kmf = KaplanMeierFitter()



## Fit the data into the model

kmf.fit(durations, event_observed,label='Kaplan Meier Estimate')



## Create an estimate

kmf.plot(ci_show=False) ## ci_show is meant for Confidence interval, since our data set is too tiny, thus i am not showing it.

Real World Example:

As mentioned earlier that Survival Analysis can be used for the cohort analysis, to gain insights. So, here we will be using the Telco-Customer-Churn data set, to gain insight about the lifelines of customers in different cohorts.

Github link for the code: Link

Lets create two cohorts of customers based on whether a customer has subscribed for Streaming TV or not. We want to know that which cohort has the better customer retention.

The required code for plotting the Survival Estimates is given below.

kmf1 = KaplanMeierFitter() ## instantiate the class to create an object



## Two Cohorts are compared. Cohort 1. Streaming TV Not Subscribed by users, and Cohort 2. Streaming TV subscribed by the users. groups = df['StreamingTV']

i1 = (groups == 'No') ## group i1 , having the pandas series for the 1st cohort

i2 = (groups == 'Yes') ## group i2 , having the pandas series for the 2nd cohort





## fit the model for 1st cohort

kmf1.fit(T[i1], E[i1], label='Not Subscribed StreamingTV')

a1 = kmf1.plot()



## fit the model for 2nd cohort

kmf1.fit(T[i2], E[i2], label='Subscribed StreamingTV')

kmf1.plot(ax=a1)

Fig 2: Kaplan Meier Curve of the two cohorts.

We have two survival curves , one for each cohort. From the curves, it is evident that the customers, who have subscribed for the Streaming TV, have better customer retention as compared to the customers, who have not subscribed for the Streaming TV. At any point t across the timeline, we can see that the survival probability of the cohort in blue is less than the cohort in red. For the cohort in blue, the survival probability is decreasing with high rate in first 10 months and it gets relatively better after that; however, for the red cohort, the rate of decrease in survival rate is fairly constant. Therefore, for the cohort , which has not subscribed for the Streaming TV, efforts should be made to retain the customers in first 10 volatile months.

We can do more such cohort analysis from the survival curves of the different cohorts. This cohort analysis represents the limited use case of the potential of the survival analysis because we are using it for the aggregated level of the data. We can create the Survival Curves for even the individual users based on the effects of covariates on the baseline Survival Curves. This will be our focal point of the next section of this article.

Cox Proportional Hazard Model

The time to event for an individual in the population is very important for the survival curves at the aggregate level; however, in real life situations along with the event data we also have the covariates (features) of that individual. In such cases, it is very important to know about the impact of covariates on the survival curve. This would help us in predicting the survival probability of an individual, if we know the associated covariates values.

For example, in the telco-churn example discussed above, we have each customer’s tenure when they churned (the event time T) and the customer’s Gender, MonthlyCharges, Dependants, Partner, PhoneService etc. The other variables are the covariates in this example. We are often interested in how these covariates impacts the survival probability function.

In such cases, it is the conditional survival function S(t|x) = P(T > t|x). Here x denotes the covariates. In our example, we are interested in S(tenure > t|(Gender, MonthlyCharges, Dependants, Partner, PhoneService etc)).

The Cox (proportional hazard) model is one of the most popular model combining the covariates and the survival function. It starts with modeling the hazard function.

Here, β is the vector of coeﬃcients of each covariate. The function ho(t) is called the baseline hazard function.

The Cox model assumes that the covariates have a linear multiplication eﬀect on the hazard function and the eﬀect stays the same across time.

The idea behind the model is that the log-hazard of an individual is a linear function of their static covariates, and a population-level baseline hazard that changes over time. [Source: lifelines documentation]

From the above equation we can also derive cumulative conditional hazard function as below:

As we are already aware that we can derive survival function from the hazard function with the help of expression derived in above section. Thus, we can get the survival function for each subject/individual/customer.

Basic implementation in python:

We will now discuss about its basic implementation in python with the help of lifelines package. We have used the same telco-customer-churn data-set, which we have been using in the above sections. We will run a python code for predicting the survival function at customer level.

from lifelines import CoxPHFitter ## My objective here is to introduce you to the implementation of the model.Thus taking subset of the columns to train the model.

## Only using the subset of the columns present in the original data df_r= df.loc[:['tenure', 'Churn', 'gender', 'Partner', 'Dependents', 'PhoneService','MonthlyCharges','SeniorCitizen','StreamingTV']] df_r.head() ## have a look at the data

## Create dummy variables

df_dummy = pd.get_dummies(df_r, drop_first=True)

df_dummy.head()

# Using Cox Proportional Hazards model

cph = CoxPHFitter() ## Instantiate the class to create a cph object cph.fit(df_dummy, 'tenure', event_col='Churn') ## Fit the data to train the model cph.print_summary() ## HAve a look at the significance of the features

The summary statistics above indicates the significance of the covariates in predicting the churn risk. Gender doesn’t play any significant role in predicting the churn, whereas all the other covariates are significant.

Interesting point to note here is that , the β (coef ) values in case of covariates MonthlyCharges and gender_Male is approximately zero (~-0.01), but still the MonthlyCharges plays a significant role in predicting churn , while the latter is insignificant. The reason is that the MonthlyCharges is continuous value and it can vary from the order of tens, hundreds to thousands, when multiplied by the small coef (β=-0.01), it becomes significant. On the other hand, the covariate gender can only take the value 0 or 1, and in both the cases [exp(-0.01 * 0), exp(-0.01*1)] it will be insignificant.

## We want to see the Survival curve at the customer level. Therefore, we have selected 6 customers (rows 5 till 9).



tr_rows = df_dummy.iloc[5:10, 2:]

tr_rows

## Lets predict the survival curve for the selected customers.

## Customers can be identified with the help of the number mentioned against each curve. cph.predict_survival_function(tr_rows).plot()

fig 2. It shows the Survival Curves at customer level of customer number 5,6,7,8, and 9

Fig 2 . shows the survival curves at customer level. It shows the survival curves for customer number 5,6,7,8, & 9.

Creating the survival curves at each customer level helps us in proactively creating a tailor made strategy for high-valued customers for different survival risk segments along the timeline.

End Note

Though, there are many other things which are still remaining to be covered in survival analysis such as ‘checking proportionality assumption’, & ‘model selection’ ; however, with a basic understanding of the mathematics behind the analysis, and the basic implementation of the survival analysis (using the lifelines package in python) will help us in implementing this model in any pertinent business use case.

Additional Resources

The following resources were extremely helpful not only in motivating me to study the survival analysis but also in making this article. Check them out for more on survival analysis.","['implementation', 'event', 'probability', 'function', 'intuition', 'st', 'python', 'number', 'cohort', 'customer', 'curve', 'survival', 'analysis']","DefinitionsAs mentioned above that the Survival Analysis is also known as Time to Event analysis.
Marketing Analytics (Cohort Analysis): Survival Analysis evaluates the retention rates of each marketing channel.
Since we don’t have the true survival curve of the population, thus we will estimate the survival curve from the data.
Real World Example:As mentioned earlier that Survival Analysis can be used for the cohort analysis, to gain insights.
End NoteThough, there are many other things which are still remaining to be covered in survival analysis such as ‘checking proportionality assumption’, & ‘model selection’ ; however, with a basic understanding of the mathematics behind the analysis, and the basic implementation of the survival analysis (using the lifelines package in python) will help us in implementing this model in any pertinent business use case.",en,['Anurag Pandey'],2019-10-08 06:07:47.820000+00:00,"{'Data Science', 'Python', 'Probability', 'Statistical Analysis', 'Analytics'}","{'https://miro.medium.com/max/1702/1*NBHIGMMu_Sv2mesOpuPX8Q.png', 'https://miro.medium.com/max/60/1*MtkjtPOT9F_z24yQhvMjcA.png?q=20', 'https://miro.medium.com/max/856/1*Ckhi9soE9Lx2lIf9tPVLMQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*jc7f9x_HdwbC7DAq2VL0nw.png?q=20', 'https://miro.medium.com/max/60/1*Ckhi9soE9Lx2lIf9tPVLMQ.png?q=20', 'https://miro.medium.com/max/356/1*7RiJgl6BcwVqZt4Wbh-Hgw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*A88XQ9VzGPgb4bJdMeZUxQ.png', 'https://miro.medium.com/max/1712/1*Ckhi9soE9Lx2lIf9tPVLMQ.png', 'https://miro.medium.com/proxy/1*eTTG3UqwDCPxLWA-bvpPCA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1694/1*UrXonL44aQoP3W0oeUavJA.png', 'https://miro.medium.com/max/60/1*7RiJgl6BcwVqZt4Wbh-Hgw.png?q=20', 'https://miro.medium.com/max/60/1*iFL6xXGzSrrCgeZze77Z1A.png?q=20', 'https://miro.medium.com/max/60/1*3fCwNBgQ0O0HJNte0ZrXSQ.png?q=20', 'https://miro.medium.com/max/860/1*3Pic9gIdkVTPkGaxshJ62A.png', 'https://miro.medium.com/max/1164/1*iFL6xXGzSrrCgeZze77Z1A.png', 'https://miro.medium.com/max/432/1*MtkjtPOT9F_z24yQhvMjcA.png', 'https://miro.medium.com/max/1700/1*jc7f9x_HdwbC7DAq2VL0nw.png', 'https://miro.medium.com/max/60/1*5qd0S5almsxKvIF-xzZF2A.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/1*A88XQ9VzGPgb4bJdMeZUxQ.png', 'https://miro.medium.com/max/60/1*NfYjSmifXEbLyb7XgZEaVA.png?q=20', 'https://miro.medium.com/max/60/1*NKUggOhdyJztcBBGq2vLMg.png?q=20', 'https://miro.medium.com/max/1690/1*5qd0S5almsxKvIF-xzZF2A.png', 'https://miro.medium.com/max/60/1*_HG7KpWGiGpT4gTVQWyxhg.png?q=20', 'https://miro.medium.com/max/60/1*NBHIGMMu_Sv2mesOpuPX8Q.png?q=20', 'https://miro.medium.com/max/2042/1*JbFKVoEfmOmY1kyWSRekUA.png', 'https://miro.medium.com/max/60/1*JbFKVoEfmOmY1kyWSRekUA.png?q=20', 'https://miro.medium.com/max/60/1*3Pic9gIdkVTPkGaxshJ62A.png?q=20', 'https://miro.medium.com/max/318/1*NKUggOhdyJztcBBGq2vLMg.png', 'https://miro.medium.com/max/1134/1*_HG7KpWGiGpT4gTVQWyxhg.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1572/1*NfYjSmifXEbLyb7XgZEaVA.png', 'https://miro.medium.com/max/60/1*EKfcNCQceebZRJelB5DBaA.png?q=20', 'https://miro.medium.com/max/1692/1*EKfcNCQceebZRJelB5DBaA.png', 'https://miro.medium.com/max/1704/1*3fCwNBgQ0O0HJNte0ZrXSQ.png', 'https://miro.medium.com/max/60/1*UrXonL44aQoP3W0oeUavJA.png?q=20'}",2020-03-05 00:22:23.845110,6.735631465911865
https://towardsdatascience.com/using-python-and-selenium-to-get-coordinates-from-street-addresses-62706b6ac250,Using Python and Selenium to get coordinates from street addresses,"Exploring Google Maps :

rawpixel.com

Before any web scraping job, it is essential to explore the website you want to extract data from. In our case, it’s Google Maps.

First, let’s study how searching for a full address using the search bar inside Google Maps affects the URL of the result page. For this, I’ll go with the fictitious address Grinch house mount crumpit whoville because I want Google Maps to return no results :

As you can see above, we get www.google.com/maps/search/ followed by the address we searched for. In other words, if we want to search for an address XYZ within Google Maps, all we have to do is use the URL www.google.com/maps/search/XYZ , without having to interact with the search bar itself.

The idea here is to generate a new column within mel where we combine www.google.com/maps/search/ with every Full_Address we have in our dataframe mel, and then have Selenium iterate over them visiting the URLs one after the other.

Let’s create that new Url column :

mel['Url'] = ['https://www.google.com/maps/search/' + i for i in mel['Full_Address'] ]

Now that we have a column containing all the URLs we want to crawl, let’s take a look at the address G Se 11 431 St Kilda Rd Melbourne for example :

www.google.com/maps/search/G Se 11 431 St Kilda Rd Melbourne

The above link gives us :

The above address corresponds to the charity Australian Nurses Memorial Centre. Let’s search for it on Google Maps by name :

We get the exact same spot, but not the same coordinates in the URL. That’s because the coordinates in the URL are linked to how the map is centered and not to the marker (they change if you zoom in or out). That’s why we’re going to extract the coordinates directly from the source code of the page itself.

To view the source code, right click on a blank space within the page (outside of the map) and choose View Page Source (CTRL+U or Command+U in Mac). Now search for -37.8 or 144.9 within the source page :

You’ll find the coordinates we are seeking in many places throughout the hot mess that is the source code. But they are mostly useful to us if they are comprised within an HTML tag we can target. Luckily, there is one meta tag we can make useful here :

For now, let’s note that it’s a meta tag with an attribute content containing the URL we want to extract, and an attribute itemprop with the value image which can be used to identify and target this particular meta tag.","['url', 'google', 'address', 'lets', 'python', 'page', 'tag', 'search', 'addresses', 'source', 'maps', 'street', 'selenium', 'coordinates', 'using']","Exploring Google Maps :rawpixel.comBefore any web scraping job, it is essential to explore the website you want to extract data from.
In our case, it’s Google Maps.
First, let’s study how searching for a full address using the search bar inside Google Maps affects the URL of the result page.
Let’s search for it on Google Maps by name :We get the exact same spot, but not the same coordinates in the URL.
To view the source code, right click on a blank space within the page (outside of the map) and choose View Page Source (CTRL+U or Command+U in Mac).",en,['Khalid El Mouloudi'],2020-01-29 13:35:31.704000+00:00,"{'Pandas', 'Mapping', 'Python', 'Web Scraping', 'Selenium'}","{'https://miro.medium.com/max/2962/1*cEswDkWqVj3Bpo3Zw9uBRA.png', 'https://miro.medium.com/max/2094/1*lImFiCK4_KuMJ8huPm_EPw.png', 'https://miro.medium.com/max/2962/1*IDcTpidlPaFBD473jux6tg.png', 'https://miro.medium.com/max/2962/1*0UuyvrW5gVVUIzW93VzxZQ.png', 'https://miro.medium.com/max/1642/1*yM60i6mfxVk8GlUiiMPTWg.png', 'https://miro.medium.com/max/60/1*0UuyvrW5gVVUIzW93VzxZQ.png?q=20', 'https://miro.medium.com/max/3400/1*oNUuJJUC_J7hCzbZ0zds2Q.jpeg', 'https://miro.medium.com/max/60/1*8b70fHIS09h-b_KJ0CV_UA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*mvA82yYsraetbvZd9KpCSw.png?q=20', 'https://miro.medium.com/max/1400/1*ri1lJL-PTdrahE17D29A8g.gif', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2432/1*e0mPQ0M3FGf6dhxBSvILXQ.png', 'https://miro.medium.com/max/2962/1*1dSdBkGK0pXu58n2dLKYJQ.png', 'https://miro.medium.com/max/2400/1*26vmDnfp60vQ49GUXgslfg.jpeg', 'https://miro.medium.com/max/60/1*cEswDkWqVj3Bpo3Zw9uBRA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*oNUuJJUC_J7hCzbZ0zds2Q.jpeg', 'https://miro.medium.com/max/1328/1*7UsaC4LjuxdXl41TQHtc_Q.png', 'https://miro.medium.com/max/2840/1*W2nEY89kbEpf-f1OOJV8oQ.png', 'https://miro.medium.com/max/60/1*oNUuJJUC_J7hCzbZ0zds2Q.jpeg?q=20', 'https://miro.medium.com/max/3082/1*8b70fHIS09h-b_KJ0CV_UA.png', 'https://miro.medium.com/max/60/1*7UsaC4LjuxdXl41TQHtc_Q.png?q=20', 'https://miro.medium.com/max/1500/1*qTBGchl7ZA7-EQEcT4JEyg.gif', 'https://miro.medium.com/freeze/max/60/1*qTBGchl7ZA7-EQEcT4JEyg.gif?q=20', 'https://miro.medium.com/max/60/1*26vmDnfp60vQ49GUXgslfg.jpeg?q=20', 'https://miro.medium.com/max/1206/1*CMzGOjASfF8x_DJAF1qBuQ.png', 'https://miro.medium.com/max/1400/1*pba2Nz0ufVbUjblyiOm89g.png', 'https://miro.medium.com/max/60/1*yM60i6mfxVk8GlUiiMPTWg.png?q=20', 'https://miro.medium.com/max/1328/1*17IjuHiRMK4vVhgwmKy5MA.png', 'https://miro.medium.com/max/2200/1*V-ZnQnITEHddAXu6fcVQLQ.png', 'https://miro.medium.com/max/60/1*W2nEY89kbEpf-f1OOJV8oQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/38/1*Imomhbh8NGz5dynFKOj5fg.png?q=20', 'https://miro.medium.com/max/60/1*17IjuHiRMK4vVhgwmKy5MA.png?q=20', 'https://miro.medium.com/max/60/1*jctw5sCqClI_Sw6Upp1Qwg.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*fk9hQEYn9OnSTgKgNFpsXQ.jpeg', 'https://miro.medium.com/max/60/1*bS5q2yPhnzbqpN9nR3odYQ.png?q=20', 'https://miro.medium.com/max/2212/1*jctw5sCqClI_Sw6Upp1Qwg.png', 'https://miro.medium.com/fit/c/96/96/2*fk9hQEYn9OnSTgKgNFpsXQ.jpeg', 'https://miro.medium.com/max/60/1*e0mPQ0M3FGf6dhxBSvILXQ.png?q=20', 'https://miro.medium.com/max/1772/1*bS5q2yPhnzbqpN9nR3odYQ.png', 'https://miro.medium.com/max/60/1*lImFiCK4_KuMJ8huPm_EPw.png?q=20', 'https://miro.medium.com/max/60/1*pba2Nz0ufVbUjblyiOm89g.png?q=20', 'https://miro.medium.com/freeze/max/60/1*ri1lJL-PTdrahE17D29A8g.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/60/1*4f05tFnsoSMs01R4PQfR4Q.gif?q=20', 'https://miro.medium.com/max/2624/1*4f05tFnsoSMs01R4PQfR4Q.gif', 'https://miro.medium.com/max/60/1*V-ZnQnITEHddAXu6fcVQLQ.png?q=20', 'https://miro.medium.com/max/60/1*IDcTpidlPaFBD473jux6tg.png?q=20', 'https://miro.medium.com/max/60/1*1dSdBkGK0pXu58n2dLKYJQ.png?q=20', 'https://miro.medium.com/max/672/1*Imomhbh8NGz5dynFKOj5fg.png', 'https://miro.medium.com/max/60/1*1jdsCFtKJDfp9afXRtB4hg.png?q=20', 'https://miro.medium.com/max/3564/1*1jdsCFtKJDfp9afXRtB4hg.png', 'https://miro.medium.com/max/60/1*CMzGOjASfF8x_DJAF1qBuQ.png?q=20', 'https://miro.medium.com/max/1802/1*mvA82yYsraetbvZd9KpCSw.png'}",2020-03-05 00:22:30.949319,7.10420823097229
https://medium.com/statuscode/the-many-faces-of-distinct-in-postgresql-c52490de5954,The many faces of DISTINCT in PostgreSQL,"I started my programming career as an Oracle DBA. It took a few years but eventually I got fed up with the corporate world and I went about doing my own thing.

When I no longer had the comfy cushion of Oracle enterprise edition I discovered PostgreSQL. After I gotten over not having proper partitions and MERGE statement (aka UPSERT) I found some nice unique features in PostgreSQL. Oddly enough, a lot of them contained the word DISTINCT.

Duplication is scary (The shining, 1980)

DISTINCT

I created a simple Employee table with name, department and salary using mock data from this site:

haki=# \d employee

Table ""public.employee""

Column | Type | Modifiers

------------+-----------------------+-----------

id | integer | not null

name | character varying(30) |

department | character varying(30) |

salary | integer |

Indexes:

""employee_pkey"" PRIMARY KEY, btree (id)

haki=# select * from employee limit 5;

id | name | department | salary

----+----------------+----------------------+--------

1 | Carl Frazier | Engineering | 3052

2 | Richard Fox | Product Management | 13449

3 | Carolyn Carter | Engineering | 8366

4 | Benjamin Brown | Business Development | 7386

5 | Diana Fisher | Services | 10419

(5 rows)

What is DISTINCT?

SELECT DISTINCT eliminates duplicate rows from the result.

The simplest use of distinct is, for example, to get a unique list of departments:

haki=# SELECT DISTINCT department FROM employee;

department

--------------------------

Services

Support

Training

Accounting

Business Development

Marketing

Product Management

Human Resources

Engineering

Sales

Research and Development

Legal

(12 rows)

(easy CS students, I know it’s not normalized…)

We can do the same thing with group by

SELECT department FROM employee GROUP BY department;

but we are talking about DISTINCT.

DISTINCT ON

A classic job interview question is finding the employee with the highest salary in each department.

This is what they teach in the university:

SELECT

*

FROM

employee

WHERE

(department, salary) IN (

SELECT

department,

MAX(salary)

FROM

employee

GROUP BY

department

)

ORDER BY

department; id | name | department | salary

----+------------------+--------------------------+--------

30 | Sara Roberts | Accounting | 13845

4 | Benjamin Brown | Business Development | 7386

3 | Carolyn Carter | Engineering | 8366

20 | Janet Hall | Human Resources | 2826

14 | Chris Phillips | Legal | 3706

10 | James Cunningham | Legal | 3706

11 | Richard Bradley | Marketing | 11272

2 | Richard Fox | Product Management | 13449

25 | Evelyn Rodriguez | Research and Development | 10628

17 | Benjamin Carter | Sales | 6197

24 | Jessica Elliott | Services | 14542

7 | Bonnie Robertson | Support | 12674

8 | Jean Bailey | Training | 13230

Legal has two employees with the same high salary. Depending on the use case, this query can get pretty nasty.

If you graduated a while back, you already know a few things about databases and you heard about analytic and window functions, you might do this:

WITH ranked_employees AS (

SELECT

ROW_NUMBER() OVER (

PARTITION BY department ORDER BY salary DESC

) AS rn,

*

FROM

employee

)

SELECT

*

FROM

ranked_employees

WHERE

rn = 1

ORDER BY

department;

The result is the same without the duplicates:

rn | id | name | department | salary

----+----+------------------+--------------------------+--------

1 | 30 | Sara Roberts | Accounting | 13845

1 | 4 | Benjamin Brown | Business Development | 7386

1 | 3 | Carolyn Carter | Engineering | 8366

1 | 20 | Janet Hall | Human Resources | 2826

1 | 14 | Chris Phillips | Legal | 3706

1 | 11 | Richard Bradley | Marketing | 11272

…

Up until now, this is what I would have done.

Now for the real treat, PostgreSQL has a special nonstandard clause to find the first row in a group:

SELECT DISTINCT ON (department)

*

FROM

employee

ORDER BY

department,

salary DESC;

This is wild! Why nobody told me this is possible?

The docs explain DISTINCT ON:

SELECT DISTINCT ON ( expression [, …] ) keeps only the first row of each set of rows where the given expressions evaluate to equal

And the reason I haven’t heard about it is:

Nonstandard Clauses

DISTINCT ON ( … ) is an extension of the SQL standard.

PostgreSQL does all the heavy lifting for us. The only requirement is that we ORDER BY the field we group by ( department in this case). It also allows for “grouping” by more than one field which only makes this clause even more powerful.

IS DISTINCT FROM

Comparing values in SQL can result in three outcomes — true , false or unknown :

WITH t AS (

SELECT 1 AS a, 1 AS b UNION ALL

SELECT 1, 2 UNION ALL

SELECT NULL, 1 UNION ALL

SELECT NULL, NULL

)

SELECT

a,

b,

a = b as equal

FROM

t; a | b | equal

------+------+-------

1 | 1 | t

1 | 2 | f

NULL | 1 | NULL

NULL | NULL | NULL

The result of comparing NULL with NULL using equality (=) is UNKNOWN (marked as NULL in the table).

In SQL 1 = 1 and NULL IS NULL but NULL != NULL.

It’s important to be aware of this subtlety because comparing nullable fields might yield unexpected results.

The full condition to get either true or false when comparing nullable fields is:

(a is null and b is null)

or

(a is not null and b is not null and a = b)

And the result:

a | b | equal | full_condition

------+------+-------+----------

1 | 1 | t | t

1 | 2 | f | f

NULL | 1 | NULL | f

NULL | NULL | NULL | t



This is the result we want but it is very long. Is there a better way?

PostgreSQL implements the SQL standard for safely comparing nullable fields:

SELECT

a,

b,

a = b as equal,

a IS DISTINCT FROM b AS is_distinct_from

FROM

t; a | b | equal | is_distinct_from

------+------+-------+------------------

1 | 1 | t | f

1 | 2 | f | t

NULL | 1 | NULL | t

NULL | NULL | NULL | f

PostgreSQL wiki explain IS DISTINCT FROM :

IS DISTINCT FROM and IS NOT DISTINCT FROM … treat NULL as if it was a known value, rather than a special case for unknown.

Much better — short and verbose.

How other databases handle this?

MySQL — a special operator <=> with similar functionality.

a special operator with similar functionality. Oracle — Provide a function called LNNVL to compare nullable fields (good luck with that…).

Provide a function called LNNVL to compare nullable fields (good luck with that…). MSSQL — Couldn’t find a similar function.

ARRAY_AGG (DISTINCT)

ARRAY_AGG was one of the major selling points of PostgreSQL when I was transitioning from Oracle.

ARRAY_AGG aggregates values into an array:

SELECT

department,

ARRAY_AGG(name) AS employees

FROM

employee

GROUP BY

department; department | employees

--------------------------+-------------------------------------

Services | {""Diana Fisher"",""Jessica Elliott""}

Support | {""Bonnie Robertson""}

Training | {""Jean Bailey""}

Accounting | {""Phillip Reynolds"",""Sean Franklin""}

Business Development | {""Benjamin Brown"",""Brian Hayes""}

Marketing | {""Richard Bradley"",""Arthur Moreno""}

Product Management | {""Richard Fox"",""Randy Wells""}

Human Resources | {""Janet Hall""}

Engineering | {""Carl Frazier"",""Carolyn Carter""}

Sales | {""Benjamin Carter""}

Research and Development | {""Donna Reynolds"",""Ann Boyd""}

Legal | {""James Cunningham"",""George Hanson""}

I find ARRAY_AGG useful mostly in the CLI for getting a quick view of the data, or when used with an ORM.

PostgreSQL took it the extra mile and implemented the DISTINCT option for this aggregate function as well. Using DISTINCT we can, for example, quickly view the unique salaries in each department:

SELECT

department,

ARRAY_AGG(DISTINCT salary) AS salaries

FROM

employee

GROUP BY

department; department | salaries

--------------------------+---------------

Accounting | {11203}

Business Development | {2196,7386}

Engineering | {1542,3052}

Human Resources | {2826}

Legal | {1079,3706}

Marketing | {5740}

Product Management | {9101,13449}

Research and Development | {6451,10628}

Sales | {6197}

Services | {2119}

Support | {12674}

Training | {13230}

We can immediately see that everyone in the support department are making the same salary.

How other databases handle this?

MySQL — Has a similar function called GROUP_CONCAT.

— Has a similar function called GROUP_CONCAT. Oracle — Has an aggregate function called ListAgg. It has no support for DISTINCT. Oracle introduced the function in version 11.2 and up until then the world wide web was filled with custom implementations.

— Has an aggregate function called ListAgg. It has no support for DISTINCT. Oracle introduced the function in version 11.2 and up until then the world wide web was filled with custom implementations. MsSQL — The closest I found was a function called STUFF that accepts an expression.

Take away

The only take away from this article is that you should always go back to the basics!","['called', 'function', 'faces', 'null', 'distinct', 'richard', 'development', 'b', 'department', 'postgresql', 'oracle', 'salary']","The simplest use of distinct is, for example, to get a unique list of departments:haki=# SELECT DISTINCT department FROM employee;department--------------------------ServicesSupportTrainingAccountingBusiness DevelopmentMarketingProduct ManagementHuman ResourcesEngineeringSalesResearch and DevelopmentLegal(12 rows)(easy CS students, I know it’s not normalized…)We can do the same thing with group bySELECT department FROM employee GROUP BY department;but we are talking about DISTINCT.
Now for the real treat, PostgreSQL has a special nonstandard clause to find the first row in a group:SELECT DISTINCT ON (department)*FROMemployeeORDER BYdepartment,salary DESC;This is wild!
In SQL 1 = 1 and NULL IS NULL but NULL != NULL.
PostgreSQL implements the SQL standard for safely comparing nullable fields:SELECTa,b,a = b as equal,a IS DISTINCT FROM b AS is_distinct_fromFROMt; a | b | equal | is_distinct_from------+------+-------+------------------1 | 1 | t | f1 | 2 | f | tNULL | 1 | NULL | tNULL | NULL | NULL | fPostgreSQL wiki explain IS DISTINCT FROM :IS DISTINCT FROM and IS NOT DISTINCT FROM … treat NULL as if it was a known value, rather than a special case for unknown.
PostgreSQL took it the extra mile and implemented the DISTINCT option for this aggregate function as well.",en,['Haki Benita'],2018-12-23 18:25:01.362000+00:00,"{'Coding', 'Postgres', 'Sql', 'Database', 'Programming'}","{'https://miro.medium.com/fit/c/80/80/1*Pcw-PTvfNRJzUOjFepPtfw.jpeg', 'https://miro.medium.com/max/768/1*XiSPpTrmKhhCMRhTDcfNtQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*PH4B9mGLVkKo1RG0yLZyYQ.jpeg', 'https://miro.medium.com/freeze/max/60/1*1WDpOBhEsve_e0hJi8PXbA.gif?q=20', 'https://miro.medium.com/fit/c/160/160/1*UFV5PqoT3G0zf-O76HW0dw.png', 'https://miro.medium.com/max/342/1*V618LlaFl0MRyGeoAcBZlw.png', 'https://miro.medium.com/max/1600/1*nKT5PWQ9uCG-wuQgVQNozQ.jpeg', 'https://miro.medium.com/max/2048/1*AnDyrLgCPKfHcaSBYJaXyA.png', 'https://miro.medium.com/max/1536/1*XiSPpTrmKhhCMRhTDcfNtQ.jpeg', 'https://miro.medium.com/max/60/1*XiSPpTrmKhhCMRhTDcfNtQ.jpeg?q=20', 'https://miro.medium.com/max/344/1*V618LlaFl0MRyGeoAcBZlw.png', 'https://miro.medium.com/fit/c/96/96/1*UFV5PqoT3G0zf-O76HW0dw.png', 'https://miro.medium.com/max/60/1*AnDyrLgCPKfHcaSBYJaXyA.png?q=20', 'https://miro.medium.com/max/1000/1*1WDpOBhEsve_e0hJi8PXbA.gif', 'https://miro.medium.com/fit/c/160/160/1*mvZEUGpXfDd9TouEFN8eRg.png', 'https://miro.medium.com/max/60/1*nKT5PWQ9uCG-wuQgVQNozQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*UFV5PqoT3G0zf-O76HW0dw.png'}",2020-03-05 00:22:31.823305,0.8729877471923828
https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72,Manage your Machine Learning Lifecycle with MLflow — Part 1.,"The Machine Learning Lifecycle Conundrum

Machine Learning (ML) is not easy, but creating a good workflow which you can reproduce, revisit and deploy to production is even harder. There has been many advances towards creating a good platform or managing solution for ML. Note that this is not the Data Science (DS) Lifecycle, which is more complex and has many parts.

The ML lifecycle exists inside the DS lifecycle.

You can check some of the projects for creating ML workflows here:

These packages are great, but not so easy to follow. Maybe the solution is a mix of these three, or something like that. But here I’ll present you the latest solution created by Databricks called MLflow.

Getting started with MLflow

MLflow is an open source platform for the complete machine learning lifecycle.

MLflow is designed to work with any ML library, algorithm, deployment tool or language. It is very easy to add MLflow to your existing ML code so you can benefit from it immediately, and to share code using any ML library that others in your organization can run. MLflow is also an open source project that users and library developers can extend.

Installing MLflow

Installing MLflow is very easy, you just have to run:

pip install mlflow

And this is according to the creators. But I faced several issues while installing it. So here are my recommendations (if you can run mlflow in your terminal after installing ignore ):

From Databricks: MLflow cannot be installed on the MacOS system installation of Python. We recommend installing Python 3 through the Homebrew package manager using brew install python . (In this case, installing mlflow is now pip3 install mlflow ).

That did not work for me and I got this error:

~ ❯ mlflow

Traceback (most recent call last):

File ""/usr/bin/mlflow"", line 7, in <module>

from mlflow.cli import cli

File ""/usr/lib/python3.6/site-packages/mlflow/__init__.py"", line 8, in <module>

import mlflow.projects as projects # noqa

File ""/usr/lib/python3.6/site-packages/mlflow/projects.py"", line 18, in <module>

from mlflow.entities.param import Param

File ""/usr/lib/python3.6/site-packages/mlflow/entities/param.py"", line 2, in <module>

from mlflow.protos.service_pb2 import Param as ProtoParam

File ""/usr/lib/python3.6/site-packages/mlflow/protos/service_pb2.py"", line 127, in <module>

options=None, file=DESCRIPTOR),

TypeError: __init__() got an unexpected keyword argument 'file'

And the way of solving that was not very easy. I’m using MacOS btw. To solve that I needed to update the protobuf library. To do that I installed the Google’s protobuf library from source:

Download the 3.5.1 version. I had the 3.3.1 before. Follow these steps:

Or try using Homebrew.

If your installation works, run

mlflow

and you should see this:

Usage: mlflow [OPTIONS] COMMAND [ARGS]... Options:

--version Show the version and exit.

--help Show this message and exit. Commands:

azureml Serve models on Azure ML.

download Downloads the artifact at the specified DBFS...

experiments Tracking APIs.

pyfunc Serve Python models locally.

run Run an MLflow project from the given URI.

sagemaker Serve models on SageMaker.

sklearn Serve SciKit-Learn models.

ui Run the MLflow tracking UI.

Quickstart with MLflow

Now that you have MLflow installed let’s run a simple example.

import os

from mlflow import log_metric, log_param, log_artifact



if __name__ == ""__main__"":

# Log a parameter (key-value pair)

log_param(""param1"", 5)



# Log a metric; metrics can be updated throughout the run

log_metric(""foo"", 1)

log_metric(""foo"", 2)

log_metric(""foo"", 3)



# Log an artifact (output file)

with open(""output.txt"", ""w"") as f:

f.write(""Hello world!"")

log_artifact(""output.txt"")

Save that to train.py and then run with

python train.py

You will see the following:

Running test.py

And that’s it? Nope. With MLflow you have a UI that you can access easily writing:

mlflow ui

And you will see (localhost:5000 by default):","['machine', 'easy', 'serve', 'lifecycle', 'manage', 'python', 'learning', 'ml', 'installing', 'run', 'mlflow', 'line', 'using', 'library']","The Machine Learning Lifecycle ConundrumMachine Learning (ML) is not easy, but creating a good workflow which you can reproduce, revisit and deploy to production is even harder.
MLflow is designed to work with any ML library, algorithm, deployment tool or language.
So here are my recommendations (if you can run mlflow in your terminal after installing ignore ):From Databricks: MLflow cannot be installed on the MacOS system installation of Python.
(In this case, installing mlflow is now pip3 install mlflow ).
run Run an MLflow project from the given URI.",en,['Favio Vázquez'],2018-06-13 16:40:15.882000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Mlflow', 'Data'}","{'https://miro.medium.com/max/60/0*KyuCY_JVO5sMwOxk.png?q=20', 'https://miro.medium.com/max/60/1*K8osy-1ju5fqMaxxGrsj6w.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/4856/1*eGCq8CWVkCmF_a3KARMluA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/2*zcPDtaZcELwMoXEFOeBKiA.png', 'https://miro.medium.com/max/5012/1*n-fYHrsu1ipBE_z0j9iapw.png', 'https://miro.medium.com/max/60/1*eGCq8CWVkCmF_a3KARMluA.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*zcPDtaZcELwMoXEFOeBKiA.png', 'https://miro.medium.com/max/4852/1*D1LN3R9byRXl01Aw3Ra4dw.png', 'https://miro.medium.com/max/60/0*sEopGE9U81tpIkTv?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*dslIrTa9vtUvAUv5oixsEQ.jpeg', 'https://miro.medium.com/max/5756/1*akUJE7w95naWMV29NBmu6A.png', 'https://miro.medium.com/max/3000/0*sEopGE9U81tpIkTv', 'https://miro.medium.com/max/1500/0*KyuCY_JVO5sMwOxk.png', 'https://miro.medium.com/max/60/1*gLVq3Rt0nigt_1NLF2p45Q.png?q=20', 'https://miro.medium.com/max/60/1*CbTXZapAqi-EO1VTZdcq4A.png?q=20', 'https://miro.medium.com/max/1228/1*CbTXZapAqi-EO1VTZdcq4A.png', 'https://miro.medium.com/max/60/1*D1LN3R9byRXl01Aw3Ra4dw.png?q=20', 'https://miro.medium.com/max/60/1*dslIrTa9vtUvAUv5oixsEQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*n-fYHrsu1ipBE_z0j9iapw.png?q=20', 'https://miro.medium.com/max/3840/1*dslIrTa9vtUvAUv5oixsEQ.jpeg', 'https://miro.medium.com/max/4608/1*K8osy-1ju5fqMaxxGrsj6w.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*akUJE7w95naWMV29NBmu6A.png?q=20', 'https://miro.medium.com/max/4880/1*gLVq3Rt0nigt_1NLF2p45Q.png'}",2020-03-05 00:22:40.101902,8.27760124206543
https://towardsdatascience.com/getting-started-with-mlflow-52eff8c09c61,Getting started with mlFlow,"What is mlFlow?

mlFlow is a framework that supports the machine learning lifecycle. This means that it has components to monitor your model during training and running, ability to store models, load the model in production code and create a pipeline.

The framework introduces 3 distinct features each with it’s own capabilities.

MlFlow Tracking

Tracking is maybe the most interesting feature of the framework. It allows you to create an extensive logging framework around your model. You can define custom metrics so that after a run you can compare the output to previous runs.

We will mainly focus on this part but also give you a peek into the other features.

MlFlow Projects

This feature allows you to create a pipeline if you so desire. This feature uses its own template to define how you want to run the model on a cloud environment. As most companies have a way to run code in production this feature might be of less interest to you.

MlFlow Models

Finally we have the Models feature. An mlFlow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools — for example, real-time serving through a REST API or batch inference on Apache Spark.

Theory done: Time to get going

Well the theory is always nice but now it’s time for a more hand-ons approach. First we will need to spin up a mlFlow server before we can actually start. To do this properly I created a docker container for ease of deployment.

Before we show the code it is also important to configure the storage backend. As we want our model to be stored somewhere I have chosen Azure blob storage (please note that AWS S3 is also supported).

So create a blob storage account and inside create a container.

Blob storage account Azure

Once it has been created you will need to write the wasb link down as you will need this value for starting up docker. The url is usually defined as follows: “wasbs://<container>@<storage_account_name>.blob.core.windows.net”

Next we can start building the docker. As mlFlow requires python I made my life a bit easier by starting from a python image. Basically you can start from any image as long as you make sure Python is available in the container. The only 2 packages you need to install for this example are the following:

mlflow version 0.8.0

azure-storage 0.36.0

For the full docker solution just have a look at my Github account: https://github.com/Ycallaer/mlflowdocker

Next you will need to build the container, if you don’t know how just have a look in the readme.

Once you start the docker image on your local machine it should be available through the following url: http://localhost:5000/

If all is well your homepage might look something like this.

Homepage mlFlow

Adapting model to work with mlFlow

Once the server part is ready it is time to adapt our code. I have created a small ARIMA model implementation to showcase the framework.

Before you can start we will need to define the URL on which the server is running. You do this by calling the method “set_tracking_uri”. This has been hardcoded for this demo but ideally this would point to a public endpoint.

Next we will need to create an experiment. This should be a unique identifier. One way of ensuring it is unique is by generating a uuid each time the experiment method is called. In my example I hardcoded it to speed up the demo.

By calling the “start_run()” method we tell mlFlow that this is the start point of the run and this will set the start date for your run. Don’t forget to pass the experiment id to the method, so that all logging is kept within the experiment.

Omitting the experiment id will result in all logs being written to the default experiment.

Start point mlFlow Tracking

Next we need to specify which values we want to monitor during the run. On the one hand you have “log_param()” which logs a key-value pair for string values. On the other hand we have “log_metric()” which only allows you to define the value as an integer.

Example of log_param and log_metric

The visual result

Now let’s go back to the UI and see what the visual result of our run is. On the homepage you will find your experiment listed on the left side.

Homepage with experiment

If you click on the date you will get an overview of that specific run.

Detailed info of an experiment

Next to the metrics you will see a graph symbol, if you click on this you can see how this individual metric changed during the run.

Detailed graph of a metric

Saving the model

We also want to be able to save the model using the framework. The framework allows you to load and save the model in a format compatible with most popular frameworks (eg: Scikit, Pytorch, Tensorflow,…). For a complete list have a look here.

The code to save the model as an artifact is rather easy:

Example of log_model call in mlFlow

The result of the fitting will be passed as the first parameter to the function, the second part is the directory. If you navigate to the UI and click on the run you will find the artifact information on the bottom of the page.

Model saved on Azure from mlFlow

If you are the paranoid type you can now have a look at the blob storage account to verify that the model was actually saved.

Model on blob store

Nice right? If you want you can have a look at the load feature and start building applications around the model.

The full repo that was used for this demo can be found on Github.

Issue

The bigger issue that I found with the framework was that all the logging was only stored in the docker container even if you have defined a storage backend. This means that if you restart the container all your logging will be lost. I logged an issue (https://github.com/mlflow/mlflow/issues/613), which got the response that the current team is redesigning the logging features. So fingers crossed.","['look', 'create', 'feature', 'framework', 'docker', 'need', 'run', 'model', 'storage', 'started', 'mlflow', 'getting', 'start']","MlFlow TrackingTracking is maybe the most interesting feature of the framework.
MlFlow ProjectsThis feature allows you to create a pipeline if you so desire.
First we will need to spin up a mlFlow server before we can actually start.
By calling the “start_run()” method we tell mlFlow that this is the start point of the run and this will set the start date for your run.
Start point mlFlow TrackingNext we need to specify which values we want to monitor during the run.",en,['Yves Callaert'],2019-08-26 19:05:27.573000+00:00,"{'Machine Learning', 'Docker', 'Logging'}","{'https://miro.medium.com/max/980/1*kFyW5qtNayFJQF0QblhWTA.png', 'https://miro.medium.com/max/4576/1*yj5Y9AHxDYmqSvJAigrpGQ.png', 'https://miro.medium.com/max/3232/1*PgBHgcx_vcmgxLEZqLoe1w.png', 'https://miro.medium.com/max/490/1*kFyW5qtNayFJQF0QblhWTA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/4844/1*SljIu8IzESWXmjY_-Ul4XQ.png', 'https://miro.medium.com/max/60/1*djXeKgjhj63dwbSEwy8Dbw.png?q=20', 'https://miro.medium.com/max/60/1*SljIu8IzESWXmjY_-Ul4XQ.png?q=20', 'https://miro.medium.com/max/5912/1*w7EOcww2-9XxJeICZM3hOQ.png', 'https://miro.medium.com/max/60/1*bD28MewQKPjaMGmfqewIow.png?q=20', 'https://miro.medium.com/max/60/1*PgBHgcx_vcmgxLEZqLoe1w.png?q=20', 'https://miro.medium.com/max/2172/1*djXeKgjhj63dwbSEwy8Dbw.png', 'https://miro.medium.com/max/60/1*nfcJbyWxi8RA-rQoirsXKA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*WzjNjfR-aXWKLxHEuj60kQ.png?q=20', 'https://miro.medium.com/max/60/1*kFyW5qtNayFJQF0QblhWTA.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*K5NA92Qqe3JSWqSU.', 'https://miro.medium.com/max/60/1*yj5Y9AHxDYmqSvJAigrpGQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/5872/1*bD28MewQKPjaMGmfqewIow.png', 'https://miro.medium.com/max/60/1*w7EOcww2-9XxJeICZM3hOQ.png?q=20', 'https://miro.medium.com/max/2700/1*8je68SglkT0W5EQU2BtloQ.png', 'https://miro.medium.com/max/4432/1*wwg5GK3tUPR8WWSfh0KO_A.png', 'https://miro.medium.com/fit/c/96/96/0*K5NA92Qqe3JSWqSU.', 'https://miro.medium.com/max/60/1*wwg5GK3tUPR8WWSfh0KO_A.png?q=20', 'https://miro.medium.com/max/2228/1*WzjNjfR-aXWKLxHEuj60kQ.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/5636/1*nfcJbyWxi8RA-rQoirsXKA.png', 'https://miro.medium.com/max/60/1*8je68SglkT0W5EQU2BtloQ.png?q=20'}",2020-03-05 00:22:46.746775,6.644872426986694
https://medium.com/analytics-vidhya/accuracy-precision-recall-and-many-more-what-the-heck-9ea99fef7016,Accuracy-Precision-Recall and much more,"Accuracy-Precision-Recall and much more

Understanding which is the best metric for your classification problem.

Why accuracy is not always preferred while working with imbalanced datasets?

Photo by Aswin Anand on Unsplash

General misconception

While building any classification model, many people take accuracy as the metric by default. But that is a mistake. To make it more clear, let us try to understand what is accuracy

Accuracy: It is the measure of correctly classified results, irrespective of classes. It is also called as Overall Success Rate. Let us take an example of binary classification- detecting Credit Card Fraud. Suppose we have built a model (Let’s call it M1) with following results, where YES=Fraud and NO=Good transactions

Confusion Matrix of Model M1

Accuracy of above model (M1) is 67.8%. Following formula is used to calculate.

Let us now try to build a naive model, without any complex algorithms. Our model (M2) will classify everything as Good Transactions. The confusion matrix will look something like below:

Confusion Matrix of Model M2

Accuracy of Model M2 is: 85.8%. Without any complex algorithms we could improve the accuracy of our model. Isn’t it great!!

Of course it is wrong, not the calculation, but the selected Metric is wrong. We should not use accuracy as metric for this dataset. But why? This dataset is imbalanced. In that dataset Actual Fraud cases are only 14% and rest are all genuine transactions (86%).

We need to select the right metric to solve this problem. Let us now see what are available metrics for such imbalance dataset and for binary classification

Precision

This is also called as Positive Predicted Value (PPV). This can be thought as out of Positively predicted values, how many are relevant (How many are actually positive)? It is calculated with following formula

Recall

Also called as Sensitivity, Hit rate or True Positive rate. We can think this as out of actual positive cases how many were detected by our model. It is calculated with below formula.","['rate', 'metric', 'let', 'dataset', 'accuracyprecisionrecall', 'model', 'positive', 'classification', 'following', 'matrix', 'accuracy']","Accuracy-Precision-Recall and much moreUnderstanding which is the best metric for your classification problem.
Photo by Aswin Anand on UnsplashGeneral misconceptionWhile building any classification model, many people take accuracy as the metric by default.
The confusion matrix will look something like below:Confusion Matrix of Model M2Accuracy of Model M2 is: 85.8%.
It is calculated with following formulaRecallAlso called as Sensitivity, Hit rate or True Positive rate.
We can think this as out of actual positive cases how many were detected by our model.",en,['Karthik C Sunil'],2019-09-24 04:54:32.589000+00:00,"{'Evaluation', 'Metrics', 'Recall', 'Precision', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*ropWEYFkl-cxpNuH2uGBRQ.png?q=20', 'https://miro.medium.com/max/1260/1*ropWEYFkl-cxpNuH2uGBRQ.png', 'https://miro.medium.com/max/1016/1*U7TZWz4diOXtAbxrKzGs3A.png', 'https://miro.medium.com/fit/c/160/160/2*jx-bg0CSpe7ijdFCgWGIxg.jpeg', 'https://miro.medium.com/max/1200/0*QI6sOMl6m5_pQDaW', 'https://miro.medium.com/max/882/1*4eIu4IxDP2NY4tzPhe5Vnw.png', 'https://miro.medium.com/max/60/1*k2DSRDhTULv1s584wXwPoQ.png?q=20', 'https://miro.medium.com/max/60/0*QI6sOMl6m5_pQDaW?q=20', 'https://miro.medium.com/max/290/1*cK8jYS5H7rDYhb0vZkW4NA.png', 'https://miro.medium.com/fit/c/80/80/2*f0xB_2JvkHaF1zlsF-hsHA.jpeg', 'https://miro.medium.com/max/9312/0*QI6sOMl6m5_pQDaW', 'https://miro.medium.com/max/1012/1*xiSiKzjA10RG15oRRCJYdA.png', 'https://miro.medium.com/max/808/1*k2DSRDhTULv1s584wXwPoQ.png', 'https://miro.medium.com/fit/c/80/80/1*PFdJBI5MLv6iMemP3QtVlA.jpeg', 'https://miro.medium.com/max/60/1*U7TZWz4diOXtAbxrKzGs3A.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*miCA9MEw8TjpXyR0xY1w-A.png', 'https://miro.medium.com/fit/c/96/96/2*jx-bg0CSpe7ijdFCgWGIxg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*-22Bm0tuymQyxNXw1WBxyQ.jpeg', 'https://miro.medium.com/max/60/1*4eIu4IxDP2NY4tzPhe5Vnw.png?q=20', 'https://miro.medium.com/max/60/1*xiSiKzjA10RG15oRRCJYdA.png?q=20'}",2020-03-05 00:22:47.829968,1.083193063735962
https://towardsdatascience.com/how-to-calibrate-undersampled-model-scores-8f3319c1ea5b,How to Calibrate Undersampled Model Scores,"How to Calibrate Undersampled Model Scores

Imbalanced data problems in binary prediction models and a simple but effective way to take care of them with Python and R.

Random Undersampling Flower

Imbalanced Data, the Root of all Evil

The imbalanced data is simply referred to as a situation that one of the classes forms a high majority and dominates other classes. For machine learning, a skewed distribution of target values might cause an accuracy bias in algorithms and affect the performance of models negatively. At this point, the aim of the model and performance metrics are substantial.

Let’s clarify with an example. If your Target columns have True classes with a %1, a model that always predict False will be %99 of times successful in terms of basic accuracy. However, this is impractical in most cases because the cost of false positive (or Type I Error) and false negative (or Type II Error) predictions are usually not equal.

There are a couple of solutions to imbalanced data problem but in this article, I will mention the undersampling method and the calibration process which adjust the final scores afterward.

What is Undersampling?

Assume that your data has a binary target variable with a highly skewed ratio. In order to balance the ratio of target and increase the focus of the machine learning algorithm on the minority class, the rows of the majority class are reduced. This process is called as undersampling and applied in the data preparation phase before the model training.

The data before and after undersampling

After the undersampling process, some side effects are seen on the distribution of the model scores. For instance, if the ratio of the True classes in the training data is %5, we expect that the average of the probability predictions to be %5 as well. But, in case we manipulate the target class ratio, we also change the distribution of prediction scores. Random undersampling of non-target classes improves the prior probability of the target class in the train data and it ends up with increased probability predictions.

Is This a Problem?

If you aim to select a certain number of instances according to their prediction scores, undersampling is not a problem, by the reason of the fact that it does not change the probability score order of the instances. For example, if you need to specify a population with the highest propensity score for your marketing campaign, you do not need to worry about the side effects of undersampling.

However, in some cases, a realistic probability prediction matters:

Customer lifetime value or similar calculations need calibrated probability predictions. Suppose that you have product A with a value of 100$ and user B with a propensity score of 0.1 to Product A . So, user B has a value of 100$*0.1=10$ in terms of marketing.

with a value of 100$ and with a propensity score of 0.1 to . So, has a value of 100$*0.1=10$ in terms of marketing. If the cost of false positive or false negative prediction is high, it is required to have realistic probability predictions. In a situation that you want to specify a criminal, false positives are mostly intolerable. Or if you try to predict a person whether has cancer or not, the order is usually meaningless, on the other hand, the probability is vital.

“if you give all events that happen a probability of .6 and all the events that don’t happen a probability of .4, your discrimination is perfect but your calibration is miserable”. Daniel Kahneman

What does Calibration Change?

If you use AUC as a model evaluation metric, you cannot see any difference before and after calibration, because AUC cares about distinguishing the classes, not the probability of them. However, if you use a metric such as Log Loss that works with the likelihood function, it differs.

In the chart below, you can see the probability predictions distributions of undersampled data before and after calibration. The vertical purple line shows the prior probability of the target class in the original data. It can be seen on the chart, that the red area of undersampled model predictions becomes highly coherent with the prior probability after the calibration process.

The comparison of the score distribution before and after calibration

Let’s Calibrate

To adjust the probabilities in the model output, we calibrate them. There are two well-known calibration algorithms: Platt’s Scaling and Isotonic Regression.

Besides these, I want to talk about another uncomplicated calibration formula and its functions in python and R.

Here are the explanations of function parameters:

data: Probability predictions array of the model output

Probability predictions array of the model output train_pop: Total row count in the training dataset

Total row count in the training dataset target_pop: Total row count of the target class in the training dataset

Total row count of the target class in the training dataset sampled_train_pop: Total row count in the training dataset after undersampling

Total row count in the training dataset after undersampling sampled_target_pop: Total row count of the target class in the training dataset after undersampling

Calibration Functions:

Calibration function in R

Calibration function in Python

How to use the function?

Let’s say your goal is to generate a model that shows the credit default probabilities and your original training data has 50,000 rows with only 500 of them labeled as target class. When you sample your non-target instances randomly and reduce the total row count to 10,000, while conserving 500 target rows, our calibration function becomes:

calibration(model_results, 50000, 500, 10000, 500)

Here model_results is your model probability output array. After you train your model and put the results in it, your function is ready to use. Enjoy it!

References","['probability', 'row', 'calibration', 'scores', 'target', 'model', 'data', 'calibrate', 'class', 'training', 'count', 'undersampled', 'predictions']","This process is called as undersampling and applied in the data preparation phase before the model training.
But, in case we manipulate the target class ratio, we also change the distribution of prediction scores.
However, in some cases, a realistic probability prediction matters:Customer lifetime value or similar calculations need calibrated probability predictions.
If the cost of false positive or false negative prediction is high, it is required to have realistic probability predictions.
It can be seen on the chart, that the red area of undersampled model predictions becomes highly coherent with the prior probability after the calibration process.",en,['Emre Rençberoğlu'],2019-02-12 20:42:19.721000+00:00,"{'Data Science', 'Python', 'R', 'Machine Learning', 'Predictive Analytics'}","{'https://miro.medium.com/max/1000/1*CTAQAWecOUH33P_xamo2qQ.png', 'https://miro.medium.com/max/60/1*CTAQAWecOUH33P_xamo2qQ.png?q=20', 'https://miro.medium.com/max/60/1*DaPFxe-3cMZ38fNFWCcq8A.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*XFo7hUXnAGjDzDmV-YdNuw.png?q=20', 'https://miro.medium.com/max/960/1*p0eojaIRlmEduq3yXohDnQ.gif', 'https://miro.medium.com/fit/c/96/96/1*PFdJBI5MLv6iMemP3QtVlA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/60/1*p0eojaIRlmEduq3yXohDnQ.gif?q=20', 'https://miro.medium.com/max/1080/1*XFo7hUXnAGjDzDmV-YdNuw.png', 'https://miro.medium.com/max/60/1*qKaETvbiR9SsYMMgx1eFNw.png?q=20', 'https://miro.medium.com/max/1442/1*qKaETvbiR9SsYMMgx1eFNw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/freeze/max/480/1*p0eojaIRlmEduq3yXohDnQ.gif', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1432/1*DaPFxe-3cMZ38fNFWCcq8A.png', 'https://miro.medium.com/fit/c/160/160/1*PFdJBI5MLv6iMemP3QtVlA.jpeg'}",2020-03-05 00:22:55.587306,7.756335735321045
https://towardsdatascience.com/probability-calibration-for-imbalanced-dataset-64af3730eaab,Probability Calibration for Imbalanced Dataset,"1. How does Probability Calibration work?

Photo by John Schnobrich on Unsplash

As briefly mentioned above, undersampling causes a bias in the posterior probabilities. This is due to the characteristic of random undersampling, which downsizes the majority class by removing them randomly until both classes have the same number of observations. This makes the class distribution of the training set different from the one in the test set. So how exactly probability calibration using Bayes Minimum Risk theory works on this problem? — the basic idea of this method is trying to reduce/remove the bias caused by random undersampling by taking into the undersampling ratio β account. Let’s have a look into some definitions:

Let ps be the probability of the prediction being a positive class after random undersampling;

, and p be the probability of the prediction given features (unbalanced). We can write ps as a function of p;

, where β is a probability of selecting negative class with undersampling, which can be expressed below.

, which can be written

The equation above can be solved for p and expressed as below.

So after applying undersampling ratio β, we can calculate for p, which is the unbiased probability.

The threshold for this is can be;

, which is a probability of a positive class in a dataset.","['ratio', 'probability', 'set', 'prediction', 'dataset', 'calibration', 'p', 'imbalanced', 'class', 'β', 'random', 'undersampling', 'ps']","So how exactly probability calibration using Bayes Minimum Risk theory works on this problem?
— the basic idea of this method is trying to reduce/remove the bias caused by random undersampling by taking into the undersampling ratio β account.
Let’s have a look into some definitions:Let ps be the probability of the prediction being a positive class after random undersampling;, and p be the probability of the prediction given features (unbalanced).
So after applying undersampling ratio β, we can calculate for p, which is the unbiased probability.
The threshold for this is can be;, which is a probability of a positive class in a dataset.",en,['Kyosuke Morita'],2020-02-19 12:27:52.371000+00:00,"{'Imbalanced Data', 'Data Science', 'Machine Learning', 'Bayes Theorem', 'Calibration'}","{'https://miro.medium.com/max/60/1*dd58BiKT3muw3LX3e6zEbA.png?q=20', 'https://miro.medium.com/max/200/1*dd58BiKT3muw3LX3e6zEbA.png', 'https://miro.medium.com/max/2560/1*IkIZuAoJ-XwKO36iFw_uCA.png', 'https://miro.medium.com/max/60/1*0N0FtZJztIqUodH0fmvUzw.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*FP0B06YDZczjZZ-hMPr9KQ.jpeg', 'https://miro.medium.com/max/464/1*AnkhDVTLcIMA8MBGczv3Kw.png', 'https://miro.medium.com/max/60/1*w-VK4WWmFxE5Gb25BhEY3g.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/10032/0*aVlGKl6J1hEyekOQ', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/10368/0*iXXeDAwF_qfYcdQg', 'https://miro.medium.com/max/60/0*aVlGKl6J1hEyekOQ?q=20', 'https://miro.medium.com/max/60/1*AnkhDVTLcIMA8MBGczv3Kw.png?q=20', 'https://miro.medium.com/max/8960/0*FHofPNpqNpjkLPvl', 'https://miro.medium.com/max/392/1*q7BNiwCiySHLeh6_wpTsUg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/0*aVlGKl6J1hEyekOQ', 'https://miro.medium.com/max/220/1*2neW8brsdxrUx5lExaVA0g.png', 'https://miro.medium.com/max/1790/1*HOOBory5LmWUG1hZiehh9w.png', 'https://miro.medium.com/max/1150/1*c9hb_61dZ3ynqRXs_fWcOQ.png', 'https://miro.medium.com/fit/c/96/96/2*FP0B06YDZczjZZ-hMPr9KQ.jpeg', 'https://miro.medium.com/max/60/1*Tf_Pq0yJPchFW70_LhEOfw.png?q=20', 'https://miro.medium.com/max/60/1*Qp7pxPIDLM4740DHTUFEDw.png?q=20', 'https://miro.medium.com/max/2560/1*mdUuG2CruAuF02OefMFazg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*2neW8brsdxrUx5lExaVA0g.png?q=20', 'https://miro.medium.com/max/60/1*HOOBory5LmWUG1hZiehh9w.png?q=20', 'https://miro.medium.com/max/234/1*0N0FtZJztIqUodH0fmvUzw.png', 'https://miro.medium.com/max/3840/1*Tf_Pq0yJPchFW70_LhEOfw.png', 'https://miro.medium.com/max/40/0*FHofPNpqNpjkLPvl?q=20', 'https://miro.medium.com/max/60/1*IkIZuAoJ-XwKO36iFw_uCA.png?q=20', 'https://miro.medium.com/max/60/1*aBEeSnCO_9exVoVLIU3MDg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*c9hb_61dZ3ynqRXs_fWcOQ.png?q=20', 'https://miro.medium.com/max/450/1*w-VK4WWmFxE5Gb25BhEY3g.png', 'https://miro.medium.com/max/60/1*mdUuG2CruAuF02OefMFazg.png?q=20', 'https://miro.medium.com/max/2560/1*CoHOdqEYtTNV2K0oECP-Ig.png', 'https://miro.medium.com/max/60/1*CoHOdqEYtTNV2K0oECP-Ig.png?q=20', 'https://miro.medium.com/max/60/1*q7BNiwCiySHLeh6_wpTsUg.png?q=20', 'https://miro.medium.com/max/568/1*Qp7pxPIDLM4740DHTUFEDw.png', 'https://miro.medium.com/max/60/0*iXXeDAwF_qfYcdQg?q=20', 'https://miro.medium.com/max/1792/1*aBEeSnCO_9exVoVLIU3MDg.png'}",2020-03-05 00:23:05.122583,9.534275531768799
https://towardsdatascience.com/the-proper-way-to-use-machine-learning-metrics-4803247a2578,The proper way to use Machine Learning metrics,"However, for the data scientist, they’re not good enough to identify the best model among several models. This is because these measures only focus on binary outcomes and not at the confidence (i.e probabilities) at which the model made predictions. This limitation is addressed in different ways by the following measures.

4 intermediate measures

Precision Recall curve: if you change the threshold probability of your model (generally by default at 0.5), the Precision and the Recall will vary in the opposite direction. The higher the threshold, the higher the Precision and the lower the Recall. Similarly, the lower the threshold, the lower the Precision and the higher the Recall. The goal is to identify the threshold with the best balance between Precision and Recall.

Looking at the Precision-Recall curve above, some data are easily classified by the model (high Precision), but some are not (low Precision)

F-1 score (and F-beta score): if selecting the threshold with the Precision-Recall curve is the empirical way, using the F-score is the mathematical way. If for a given problem, Precision is 5 times more important than Recall, then the model with the highest F-5 score (beta=5) should be the best model for this problem.

F-beta score ranges from 0 to 1, 1 being a perfect model.

ROC curve: ROC is the acronym of Receiver Operating Characteristic which was used by radar engineers in World War-II. This definition is obsolete in Machine Learning. ROC curve is simply a way to visualise a model’s performance. If a ROC curve is highly skewed to the top left, it means the model is very accurate, while a straight diagonal means the model is no better than tossing a coin.

On the left side: ROC curve. On the right side: predicted probabilities of the ML model. The further away the red and blue curves (positives and negatives) the better the model and the more skewed the ROC curve

AUC: short for Area Under Curve, it’s basically the information contained from the ROC curve in one positive number. AUC is great because it makes it simple to compare multiple models: you select the one with the highest AUC. However it’s very hard to interpret the value of AUC. An AUC of 75% is in no way the same as an Accuracy of 75% (I hear that sometimes…).

Usually, good values of AUC start from .75, but again, this depends on the problem and looking at absolute values is generally not helpful. You’d rather use it to compare models. If your model has an AUC of 0.57 that means there’s likely no signal in your data.

AUC ranges from 0.5 and 1, 1 being a perfect model.

1 advanced measure

LogLoss: short for logarithmic loss, a more mathematical and abstract notion here. It assigns a weight to each predicted probability. The further the probability from the actual value, the larger the weight. The goal is to minimize the overall sum of all the error weights. Note that the weight drastically increases if the probability is close to 0 and the actual is 1 (same with the opposite values 1 and 0). LogLoss discriminates models that are too confident on wrong predictions and is largely used in Machine Learning because it has useful mathematical properties. The problem is that the value of the LogLoss is impossible to interpret.

How do you explain a LogLoss value of 0.34 to a business team? You simply cannot.

Using these measures the right way

These measures of accuracy will underpin a framework to select and validate the right ML model

You could compute these measures of accuracy on all the models you try, find optimal thresholds for each of the models, compare their Confusion Matrices with their optimal threshold and finally take your decision on which model x threshold fits the best to your problem. This would take multiple iterations, thousands of lines of Python code and a solid number of headaches to complete.

Or you could follow a simpler approach that’s as efficient. An approach in 3 steps:

Try diverse models and rank them using LogLoss/AUC Once the best model identified, select the optimal probability threshold using the Precision-Recall curve/F-score/ROC curve Interpret your results and communicate them with the Confusion Matrix, Precision & Recall

In details,

Try diverse models and rank them using LogLoss/AUC

First thing, there’s no free lunch in Machine Learning. You’ll never know in advance which algorithm will work the best on a specific dataset. Try diverse models, this way you’ll have a higher chance to find the best model for your problem.

Then use LogLoss and/or AUC to rank them and identify the best candidates. Why? LogLoss and AUC are positive numbers that are great to compare models. Indeed they’re simple but at the same time they embed a lot of abstract mathematical concepts that guarantee that the models with the best LogLoss or best AUC are ‘good’. That’s the reason why they’re widely used on Kaggle.

Of course don’t look at LogLoss/AUC on the training set, look at them on the test set (or even better, on a cross-validation; and always keep a holdout sample please). Note that selecting the best model with LogLoss can lead to more calibrated models than AUC (paper).

_

Example: you try 5 different models: XGBoost, RandomForest, SVM, Decision Tree, Regularised Logistic Regression. After fine-tuning, XGBoost has the best AUC on a 5-fold cross-validation. You select XGBoost and go to the 2nd step. Select the optimal probability threshold using Precision-Recall curve/F-score/ROC curve

Once the best model (or 2–3 candidate models) identified, use the Precision-Recall curve (or F-score or ROC curve) to identify the optimal probability threshold to keep for your model. This requires some good understanding on how to interpret the Precision-Recall curve.

_

Example: Looking at Precision-Recall curve, you note that for a Recall of 20% your XGBoost model reaches a Precision of 90%. However, by increasing the Recall to 50%, Precision drops at 50%. On this problem, let’s say you focus on high Precision. You therefore select the threshold that gives you 90% Precision knowing that you catch only 20% of positives in the data. Interpret your results and communicate them using the Confusion Matrix, Precision and Recall

Finally what matters to the business. For 100 predictions that the model makes, how many of them are expected to be good? With the selected threshold, how many cases will the model never catch?… The business can put numbers and $ figures next to them and even get an expected ROI.

_

Example: 90% Precision means that for 10 predicted positive cases that the model makes, 9 of them are correct. Let’s say for this problem a correct positive prediction means 1000$ saved and one wrong prediction means 0$ saved. For the probability threshold chosen at the 2nd step (getting 20% Recall and 90% Precision), and based on historical data, you estimate that the model will predict on average 50 positive cases a month. This translates into

50 x 90% x 12mths x 1000$ = 540k$ a year.

Now that’s one way to translate ML into business value 💸💸💸

Additional remarks

If possible, get a non-ML benchmark. Measure the accuracy of an existing rule-based engine, an expert’s judgement, a naive strategy, or a simple statistical model (logistic regression…). That way, you can quantify the value added of ML (which sometimes is not necessarily better) I lied when I said this is just about accuracy. Selecting the best model should also take into consideration whether the model learnt the right way and understanding why it predicts this specific way (using tools like feature importance, partial dependance, prediction explanation…). Sometimes it’s better to select a model that is not the best, but which has fewer variables, takes less time to score new data, and is easier to productionize

Bibliography","['machine', 'way', 'models', 'probability', 'proper', 'recall', 'threshold', 'learning', 'curve', 'precision', 'model', 'auc', 'metrics', 'best']","However, for the data scientist, they’re not good enough to identify the best model among several models.
Try diverse models, this way you’ll have a higher chance to find the best model for your problem.
Note that selecting the best model with LogLoss can lead to more calibrated models than AUC (paper).
After fine-tuning, XGBoost has the best AUC on a 5-fold cross-validation.
Selecting the best model should also take into consideration whether the model learnt the right way and understanding why it predicts this specific way (using tools like feature importance, partial dependance, prediction explanation…).",en,['Félix Revert'],2019-09-11 06:18:52.082000+00:00,"{'AI', 'Scikit Learn', 'Python', 'Explainable Ai', 'Machine Learning'}","{'https://miro.medium.com/freeze/max/60/1*-0uAw0_6tpPda6fGN8hU6g.gif?q=20', 'https://miro.medium.com/freeze/max/60/0*opAcfzI-PszZqzpM.gif?q=20', 'https://miro.medium.com/max/60/0*yyR3WCSjut0sM3Yq.png?q=20', 'https://miro.medium.com/max/4140/0*911SVdn4awN2IQEn.jpg', 'https://miro.medium.com/max/1200/0*fWDO1A1jiGK58H4d.jpg', 'https://miro.medium.com/max/2800/0*fWDO1A1jiGK58H4d.jpg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*fWDO1A1jiGK58H4d.jpg?q=20', 'https://miro.medium.com/max/60/0*b6gJWPeYe_zQBu99.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2196/0*b6gJWPeYe_zQBu99.png', 'https://miro.medium.com/max/1280/0*yyR3WCSjut0sM3Yq.png', 'https://miro.medium.com/max/3196/1*ImnNsPi4iVJ52gnERoSR6A.png', 'https://miro.medium.com/max/60/0*dAvITxG3V6_EInLl.jpg?q=20', 'https://miro.medium.com/max/60/0*911SVdn4awN2IQEn.jpg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1000/0*dAvITxG3V6_EInLl.jpg', 'https://miro.medium.com/max/3832/1*-0uAw0_6tpPda6fGN8hU6g.gif', 'https://miro.medium.com/max/1672/0*opAcfzI-PszZqzpM.gif', 'https://miro.medium.com/fit/c/160/160/1*y_mPGPA3yxnl6725B6HK8w.jpeg', 'https://miro.medium.com/fit/c/96/96/1*y_mPGPA3yxnl6725B6HK8w.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*ImnNsPi4iVJ52gnERoSR6A.png?q=20'}",2020-03-05 00:23:07.153760,2.0301756858825684
https://towardsdatascience.com/how-to-make-your-model-awesome-with-optuna-b56d490368af,How to make your model awesome with Optuna,"Example walk-through

Jason and the Argonauts source

Data

I used the 20 newsgroups dataset from Scikit-Learn to prepare the experiment. You can find the data import below:

Model

It’s a Natural Language Processing problem, and the model’s pipeline contains a feature extraction step and a classifier. The code for the pipeline looks as follows:

Optimization study

The School of Athens by Raphael source

The created study optimizes both the vectorization step and the model hyperparameters. It’s possible to choose out of 5 distributions:

uniform — float values

log-uniform — float values

discrete uniform — float values with intervals

integer — integer values

categorical — categorical values from a list

The syntax looks like this:

The values are then passed to the parameters dictionary and later on set to the optimized model. The values could be suggested inside the dictionary, but it would make the code lines very long and hard to read.

The last step of defining the function is actually to define the objective. It should return one value only. It’s highly recommended to score the model based on cross-validation (stratified if possible) with a high number of folds (8 is an absolute minimum).

Please keep in mind that as for Feb 24th 2019 it’s only possible to minimize the function’s value. Maximization objective is not implemented yet, so if you want to find the highest value just put a minus before it.

I’ve also added a line dumping the study into a pickle file. It allows you to keep the progress even if the process is somehow interrupted (by your decision or your machine’s). You can find the objectives code below:

To create your study’s instance, you can either create a new one or load it from the pickle file to continue previous experiments. The second step is running the study. You can specify how long the study lasts in the number of trials (n_trials) or in time in seconds (timeout). The latter’s last trial starts before the timeout, and the whole study lasts a little bit longer than specified. You can find the code and the default output below.

Note that the best hyperparameters so far are displayed.

You can access the best metric’s value and best parameters dictionary with best_value and best_params attributes respectively. You can access the trials with trials attribute, but Optuna creators prepared something better. Use trials_dataframe() method to create a Pandas DataFrame with trials’ details.

After the study ends, you can set the best parameters to the model and train it on the full dataset.

To visualize the ongoing process, you can access the pickle file from another Python’s thread (i.e., Jupyter Notebook).

Ongoing study’s progress

You can find the Example notebook and the Visualization Notebook in this GitHub repository.

The pruning technique

A man pruning olives source

Optuna’s creators state that the package outperforms Hyperopt in terms of speed and quality. Below is what they write about it on the project’s webpage:

Pruning feature automatically stops unpromising trials at the early stages of the training (a.k.a., automated early-stopping). Optuna provides interfaces to concisely implement the pruning mechanism in iterative training algorithms. […] For instance, our benchmark experiment demonstrates the advantage of the pruning feature in comparison with an existing optimization framework.

I have to say that I have some mixed feelings in terms of the pruning mechanism. I firmly believe that the cross-validation technique with many folds is essential in terms of hyperparameters’ optimization. In the pruning examples provided by Optuna’s developers at each trial, the validation set is sampled. In my opinion, it increases the metric’s variance and therefore makes optimization less reliable. If the validation set was constant, it would cause surrogate’s model overfitting to the set. Of course, it may not be such a problem if the dataset is big to prevent the variance/overfitting problems to occur.

In my opinion, the best option would be to marry cross-validation and pruning somehow. Maybe validating the trial after k folds (with k smaller than the total number of folds) would be a good idea?","['best', 'set', 'awesome', 'value', 'study', 'pruning', 'model', 'step', 'values', 'trials', 'optuna', 'folds']","The code for the pipeline looks as follows:Optimization studyThe School of Athens by Raphael sourceThe created study optimizes both the vectorization step and the model hyperparameters.
It’s highly recommended to score the model based on cross-validation (stratified if possible) with a high number of folds (8 is an absolute minimum).
You can access the trials with trials attribute, but Optuna creators prepared something better.
After the study ends, you can set the best parameters to the model and train it on the full dataset.
If the validation set was constant, it would cause surrogate’s model overfitting to the set.",en,['Piotr Gabrys'],2019-02-28 11:38:37.192000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Hyperparameter Tuning', 'Optuna'}","{'https://miro.medium.com/max/7640/1*hMtEV040oAti2lShjKDTcA.jpeg', 'https://miro.medium.com/max/60/1*s-F_vHuy6oEWTbvv1o9OTg.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*GPd8S93a_5PhmwWF39-ctg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*BWaG7Wi1Hd7AcawhTcI36Q.png?q=20', 'https://miro.medium.com/max/60/1*QtiA4BzsH5O26c8d-__hyA.jpeg?q=20', 'https://miro.medium.com/max/640/1*QtiA4BzsH5O26c8d-__hyA.jpeg', 'https://miro.medium.com/max/60/1*cBsjAXjkxf66eDmoyyQNUA.jpeg?q=20', 'https://miro.medium.com/max/60/1*phcusXB6P8mSXF6qfJuHgw.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*8Neot1-fmECqthpKYqdhDg.jpeg', 'https://miro.medium.com/max/778/1*GPd8S93a_5PhmwWF39-ctg.png', 'https://miro.medium.com/max/4614/1*BWaG7Wi1Hd7AcawhTcI36Q.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/4112/1*phcusXB6P8mSXF6qfJuHgw.jpeg', 'https://miro.medium.com/max/948/1*cBsjAXjkxf66eDmoyyQNUA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*8Neot1-fmECqthpKYqdhDg.jpeg', 'https://miro.medium.com/max/60/1*hMtEV040oAti2lShjKDTcA.jpeg?q=20', 'https://miro.medium.com/max/4538/1*s-F_vHuy6oEWTbvv1o9OTg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*phcusXB6P8mSXF6qfJuHgw.jpeg'}",2020-03-05 00:23:15.456498,8.302738666534424
https://towardsdatascience.com/anomaly-detection-with-isolation-forest-visualization-23cd75c281e2,Anomaly Detection with Isolation Forest & Visualization,"A sudden spike or dip in a metric is an anomalous behavior and both the cases needs attention. Detection of anomaly can be solved by supervised learning algorithms if we have information on anomalous behavior before modeling, but initially without feedback its difficult to identify that points. So we model this as an unsupervised problem using algorithms like Isolation Forest,One class SVM and LSTM. Here we are identifying anomalies using isolation forest.

The data here is for a use case(eg revenue, traffic etc ) is at a day level with 12 metrics. We have to identify first if there is an anomaly at a use case level. Then for better actionability, we drill down to individual metrics and identify anomalies in them.

import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) warnings.filterwarnings('ignore')

import os

print(os.listdir(""../input"")) df=pd.read_csv(""../input/metric_data.csv"")

df.head()

Now do a pivot on the dataframe to create a dataframe with all metrics at a date level. Level the multi-index pivot dataframe and treat na with 0.

metrics_df=pd.pivot_table(df,values='actuals',index='load_date',columns='metric_name')

metrics_df.reset_index(inplace=True)

metrics_df.fillna(0,inplace=True)

metrics_df

Isolation forest tries to separate each point in the data.In case of 2D it randomly creates a line and tries to single out a point. Here an anomalous point could be separated in a few steps while normal points which are closer could take significantly more steps to be segregated.

I am not going deep into each parameter. Contamination is an important parameter here and I have arrived at its value based on trial and error on validating its results with outliers in 2D plot. It stands for percentage of outlier points in the data.

I am using sklearn’s Isolation Forest here as it is a small dataset with few months of data, while recently h2o’s isolation forest is also available which is more scalable on high volume datasets would be worth exploring.

More details of the algorithm can be found here : https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf

More details on H2O Isolation forest : https://github.com/h2oai/h2o-tutorials/tree/master/tutorials/isolation-forest

metrics_df.columns

#specify the 12 metrics column names to be modelled

to_model_columns=metrics_df.columns[1:13]

from sklearn.ensemble import IsolationForest

clf=IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.12), \

max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)

clf.fit(metrics_df[to_model_columns]) pred = clf.predict(metrics_df[to_model_columns])

metrics_df['anomaly']=pred

outliers=metrics_df.loc[metrics_df['anomaly']==-1]

outlier_index=list(outliers.index)

#print(outlier_index)

#Find the number of anomalies and normal points here points classified -1 are anomalous

print(metrics_df['anomaly'].value_counts())

Number of outliers are 15 indicated by -1

Now here we have 12 metrics on which we have classified anomalies based on isolation forest.We will try to visualize the results and check if the classification makes sense.

Normalize and fit the metrics to a PCA to reduce the number of dimensions and then plot them in 3D highlighting the anomalies.

import matplotlib.pyplot as plt

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from mpl_toolkits.mplot3d import Axes3D

pca = PCA(n_components=3) # Reduce to k=3 dimensions

scaler = StandardScaler()

#normalize the metrics

X = scaler.fit_transform(metrics_df[to_model_columns])

X_reduce = pca.fit_transform(X) fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')

ax.set_zlabel(""x_composite_3"") # Plot the compressed data points

ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=""inliers"",c=""green"") # Plot x's for the ground truth outliers

ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],

lw=2, s=60, marker=""x"", c=""red"", label=""outliers"")

ax.legend()

plt.show()

3D plot of outliers highlighted

Now as we see the 3D point the anomaly points are mostly wide from the cluster of normal points,but a 2D point will help us to even judge better.

Lets try plotting the same fed to a PCA reduced to 2 dimensions.

from sklearn.decomposition import PCA

pca = PCA(2)

pca.fit(metrics_df[to_model_columns]) res=pd.DataFrame(pca.transform(metrics_df[to_model_columns])) Z = np.array(res) plt.title(""IsolationForest"")

plt.contourf( Z, cmap=plt.cm.Blues_r) b1 = plt.scatter(res[0], res[1], c='green',

s=20,label=""normal points"") b1 =plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='green',s=20, edgecolor=""red"",label=""predicted outliers"")

plt.legend(loc=""upper right"")

plt.show()

So a 2D plot gives us a clear picture that the algorithm classifies anomalies points in the use case rightly.

Anomalies are highlighted as red edges and normal points are indicated with green points in the plot.

Here the contamination parameter plays a great factor.

Our idea here is to capture all the anomalous point in the system.

So its better to identify few points which might be normal as anomalous(false positives) ,but not to miss out catching an anomaly(true negative).(So i have specified 12% as contamination which varies based on use case)

Now we have figured the anomalous behavior at a use case level.But to be actionable on the anomaly its important to identify and provide information on which metrics are anomalous in it individually.

The anomalies identified by the algorithm should make sense when viewed visually(sudden dip/peaks) by the business user to act upon it. So creating a good visualization is equally important in this process.

This function creates actuals plot on a time series with anomaly points highlighted on it. Also a table which provides actual data, the change and conditional formatting based on anomalies.

from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

import plotly.plotly as py

import matplotlib.pyplot as plt

from matplotlib import pyplot

import plotly.graph_objs as go

init_notebook_mode(connected=True)

def plot_anomaly(df,metric_name):

df.load_date = pd.to_datetime(df['load_date'].astype(str), format=""%Y%m%d"")

dates = df.load_date

#identify the anomaly points and create a array of its values for plot

bool_array = (abs(df['anomaly']) > 0)

actuals = df[""actuals""][-len(bool_array):]

anomaly_points = bool_array * actuals

anomaly_points[anomaly_points == 0] = np.nan

#A dictionary for conditional format table based on anomaly

color_map = {0: ""'rgba(228, 222, 249, 0.65)'"", 1: ""yellow"", 2: ""red""}



#Table which includes Date,Actuals,Change occured from previous point

table = go.Table(

domain=dict(x=[0, 1],

y=[0, 0.3]),

columnwidth=[1, 2],

# columnorder=[0, 1, 2,],

header=dict(height=20,

values=[['<b>Date</b>'], ['<b>Actual Values </b>'], ['<b>% Change </b>'],

],

font=dict(color=['rgb(45, 45, 45)'] * 5, size=14),

fill=dict(color='#d562be')),

cells=dict(values=[df.round(3)[k].tolist() for k in ['load_date', 'actuals', 'percentage_change']],

line=dict(color='#506784'),

align=['center'] * 5,

font=dict(color=['rgb(40, 40, 40)'] * 5, size=12),

# format = [None] + ["",.4f""] + [',.4f'],

# suffix=[None] * 4,

suffix=[None] + [''] + [''] + ['%'] + [''],

height=27,

fill=dict(color=[test_df['anomaly_class'].map(color_map)],#map based on anomaly level from dictionary

)

))

#Plot the actuals points

Actuals = go.Scatter(name='Actuals',

x=dates,

y=df['actuals'],

xaxis='x1', yaxis='y1',

mode='line',

marker=dict(size=12,

line=dict(width=1),

color=""blue"")) #Highlight the anomaly points

anomalies_map = go.Scatter(name=""Anomaly"",

showlegend=True,

x=dates,

y=anomaly_points,

mode='markers',

xaxis='x1',

yaxis='y1',

marker=dict(color=""red"",

size=11,

line=dict(

color=""red"",

width=2))) axis = dict(

showline=True,

zeroline=False,

showgrid=True,

mirror=True,

ticklen=4,

gridcolor='#ffffff',

tickfont=dict(size=10)) layout = dict(

width=1000,

height=865,

autosize=False,

title=metric_name,

margin=dict(t=75),

showlegend=True,

xaxis1=dict(axis, **dict(domain=[0, 1], anchor='y1', showticklabels=True)),

yaxis1=dict(axis, **dict(domain=[2 * 0.21 + 0.20, 1], anchor='x1', hoverformat='.2f'))) fig = go.Figure(data=[table, anomalies_map, Actuals], layout=layout) iplot(fig)

pyplot.show()

A helper function to find percentage change,classify anomaly based on severity.

The predict function classifies the data as anomalies based on the results from decision function on crossing a threshold.

Say if the business needs to find the next level of anomalies which might have an impact, this could be used to identify those points.

The top 12 quantiles are identified anomalies(high severity), based on decision function here we identify the 12–24 quantile points and classify them as low severity anomalies.

def classify_anomalies(df,metric_name):

df['metric_name']=metric_name

df = df.sort_values(by='load_date', ascending=False)

#Shift actuals by one timestamp to find the percentage chage between current and previous data point

df['shift'] = df['actuals'].shift(-1)

df['percentage_change'] = ((df['actuals'] - df['shift']) / df['actuals']) * 100

#Categorise anomalies as 0-no anomaly, 1- low anomaly , 2 - high anomaly

df['anomaly'].loc[df['anomaly'] == 1] = 0

df['anomaly'].loc[df['anomaly'] == -1] = 2

df['anomaly_class'] = df['anomaly']

max_anomaly_score = df['score'].loc[df['anomaly_class'] == 2].max()

medium_percentile = df['score'].quantile(0.24)

df['anomaly_class'].loc[(df['score'] > max_anomaly_score) & (df['score'] <= medium_percentile)] = 1

return df

Identify anomalies for individual metrics and plot the results.

X axis — date

Y axis — Actual values and anomaly points.

Actual values of metrics are indicated in the blue line and anomaly points are highlighted as red points.

In the table, the background red indicates high anomalies and yellow indicates low anomalies.

import warnings

warnings.filterwarnings('ignore')

for i in range(1,len(metrics_df.columns)-1):

clf.fit(metrics_df.iloc[:,i:i+1])

pred = clf.predict(metrics_df.iloc[:,i:i+1])

test_df=pd.DataFrame()

test_df['load_date']=metrics_df['load_date']

#Find decision function to find the score and classify anomalies

test_df['score']=clf.decision_function(metrics_df.iloc[:,i:i+1])

test_df['actuals']=metrics_df.iloc[:,i:i+1]

test_df['anomaly']=pred

#Get the indexes of outliers in order to compare the metrics with use case anomalies if required

outliers=test_df.loc[test_df['anomaly']==-1]

outlier_index=list(outliers.index)

test_df=classify_anomalies(test_df,metrics_df.columns[i])

plot_anomaly(test_df,metrics_df.columns[i])

Yes, from the plots we are able to capture the sudden spikes, dips in the metrics and project them.

Also, the conditional formatted table gives us insights on cases like data, not present(value is zero) captured as high anomaly which could be a potential result of broken pipeline in data processing which needs fixing along with highlighting high and low-level anomalies.

How to use this?

If the current timestamp is anomalous for a use case drill down to metrics figure out the set of metrics which have high anomalies in the timestamp to perform RCA on it.

Also, feedback from the business user can be updated back in the data which would help in turning this to a supervised/semi-supervised learning problem and compare their results.

An enhancement here would be to combine anomalous behavior which occurs continuously. For eg., big sale days which would result in a spike in metrics for a few days could be shown as a single behavior.","['anomalous', 'forest', 'import', 'isolation', 'identify', 'points', 'anomaly', 'based', 'visualization', 'data', 'plot', 'anomalies', 'detection', 'metrics']","Then for better actionability, we drill down to individual metrics and identify anomalies in them.
This function creates actuals plot on a time series with anomaly points highlighted on it.
The predict function classifies the data as anomalies based on the results from decision function on crossing a threshold.
X axis — dateY axis — Actual values and anomaly points.
Actual values of metrics are indicated in the blue line and anomaly points are highlighted as red points.",en,['Adithya Krishnan'],2019-03-04 13:59:10.534000+00:00,"{'Data Science', 'Timeseries', 'Isolation Forests', 'Anomaly Detection', 'Visualization'}","{'https://miro.medium.com/max/2592/1*6V6nnv_mgDpk7KGdA5QJfQ.png', 'https://miro.medium.com/max/2692/1*4P2vi2YVj4nHbU5SZ9i7Ig.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*5-_a67tjd4oP51bmJt8lxA.png?q=20', 'https://miro.medium.com/max/1200/1*4P2vi2YVj4nHbU5SZ9i7Ig.png', 'https://miro.medium.com/max/48/1*aFJR22pl40PMOkP84u3Hdw.png?q=20', 'https://miro.medium.com/max/60/1*Q5JHUtqz_Fc5VTbdCIyFwg.png?q=20', 'https://miro.medium.com/max/1244/1*ye2ZfSbdtbZMacYwPPz2ww.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*ye2ZfSbdtbZMacYwPPz2ww.png?q=20', 'https://miro.medium.com/max/1440/1*Q5JHUtqz_Fc5VTbdCIyFwg.png', 'https://miro.medium.com/max/1580/1*yuOHjmcGGCxj2hh-oVcCKw.png', 'https://miro.medium.com/max/60/1*6V6nnv_mgDpk7KGdA5QJfQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*dBleFTaHF4NVkOHVa4fUzA.png?q=20', 'https://miro.medium.com/max/1028/1*dBleFTaHF4NVkOHVa4fUzA.png', 'https://miro.medium.com/fit/c/160/160/1*j_1e4ZZkAt8TKZvrgb4EWw.jpeg', 'https://miro.medium.com/max/60/1*4P2vi2YVj4nHbU5SZ9i7Ig.png?q=20', 'https://miro.medium.com/max/3872/1*5-_a67tjd4oP51bmJt8lxA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2076/1*aFJR22pl40PMOkP84u3Hdw.png', 'https://miro.medium.com/fit/c/96/96/1*j_1e4ZZkAt8TKZvrgb4EWw.jpeg', 'https://miro.medium.com/max/60/1*yuOHjmcGGCxj2hh-oVcCKw.png?q=20'}",2020-03-05 00:23:21.808542,6.352044105529785
https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8,Benchmarking Categorical Encoders,"Helmert coding is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. It compares each level of a categorical variable to the mean of the subsequent levels. Hence, the first contrast compares the mean of the dependent variable for “A” with the mean of all of the subsequent levels of categorical column (“B”, “C”, “D”), the second contrast compares the mean of the dependent variable for “B” with the mean of all of the subsequent levels (“C”, “D”), and the third contrast compares the mean of the dependent variable for “C” with the mean of all of the subsequent levels (in our case only one level — “D”).

This type of encoding can be useful in certain situations where levels of the categorical variable are ordered, say, from lowest to highest, or from smallest to largest.

Frequency Encoder

Category representation — Frequency Encoding

Frequency Encoding counts the number of a category’s occurrences in the dataset. New categories in test dataset encoded with either “1” or counts of category in a test dataset, which makes this encoder a little bit tricky: encoding for different sizes of test batch might be different. You should think about it beforehand and make preprocessing of the train as close to the test as possible.

To avoid such problem, you might also consider using a Frequency Encoder variation — Rolling Frequency Encoder (RFE). RFE counts the number a category’s occurrences for the last dt timesteps from a given observation (for example, for dt= 24 hours).

Nevertheless, Frequency Encoding and RFE are especially efficient when your categorical column has “long tails”, i.e. several frequent values and the remaining ones have only a few examples in the dataset. In such a case, Frequency Encoding would catch the similarity between rare columns.

Target Encoder (TE)

Category representation — Target Encoding

Target Encoding has probably become the most popular encoding type because of Kaggle competitions. It takes information about the target to encode categories, which makes it extremely powerful. The encoded category values are calculated according to the following formulas:

Here, mdl — min data (samples) in leaf, a — smoothing parameter, representing the power of regularization. Recommended values for mdl and a are in the range of 1 to 100. New values of category and values with just single appearance in train dataset are replaced with the prior ones.

Target Encoder is a powerful tool, yet it has a huge disadvantage — target leakage: it uses information about the target. Because of the target leakage, model overfits the training data which results in unreliable validation and lower test scores. To reduce the effect of target leakage, we may increase regularization (it’s hard to tune those hyperparameters without unreliable validation), add random noise to the representation of the category in train dataset (some sort of augmentation), or use Double Validation.

M-Estimate Encoder

Category representation — M-Estimate Encoder

M-Estimate Encoder is a simplified version of Target Encoder. It has only one hyperparameter — m, which represents the power of regularization. The higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.

In different sources, you may find another formula of M-Estimator. Instead of y+ there is n in the denominator. I found that such representation has similar scores.

UPD (17.07.2019): The formula for M-Estimate Encoder in Categorical Encoders library contained a bug. The right one should have n in the denominator. However, both approaches show pretty good scores. The following benchmark is done via “wrong” formula.

Weight Of Evidence Encoder (WOE)

Category representation — Weight Of Evidence Encoder

Weight Of Evidence is a commonly used target-based encoder in credit scoring. It is a measure of the “strength” of a grouping for separating good and bad risk (default). It is calculated from the basic odds ratio:

a = Distribution of Good Credit Outcomes

b = Distribution of Bad Credit Outcomes

WoE = ln(a / b)

However, if we use formulas as is, it might lead to target leakage and overfit. To avoid that, regularization parameter a is induced and WoE is calculated in the following way:

James-Stein Encoder

Category representation — James-Stein Encoder Encoder

James-Stein Encoder is a target-based encoder. This encoder is inspired by James–Stein estimator — the technique named after Charles Stein and Willard James, who simplified Stein’s original Gaussian random vectors mean estimation method of 1956. Stein and James proved that a better estimator than the “perfect” (i.e. mean) estimator exists, which seems to be somewhat of a paradox. However, the James-Stein estimator outperforms the sample mean when there are several unknown populations means — not just one.

The idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:

Encoding is aimed to improve the estimation of the category’s mean target (first member of the amount) by shrinking them towards a more central average (second member of the amount). The only hyperparameter in the formula is B — the power of shrinking. It could be understood as the power of regularization, i.e. the bigger values of B will result in the bigger weight of global mean (underfit), while the lower values of B are, the bigger weight of condition mean (overfit).

One way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:

Intuitively, the formula can be seen in the following sense: if we could not rely on the estimation of category mean to target (it has high variance), it means we should assign a bigger weight to the global mean.

Wait, but how we could trust the estimation of variance if we could not rely on the estimation of the mean? Well, we may either say that the variance among all categories is the same and equal to the global variance of y (which might be a good estimation, if we don’t have too many unique categorical values; it is called pooled variance or pooled model) or replace the variances with squared standard errors, which penalize small observation counts (independent model).

Seems quite fair, but James-Stein Estimator has a big disadvantage — it is defined only for normal distribution (which is not the case for any classification task). To avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.

Leave-one-out Encoder (LOO)

Category representation — Leave-one-out Encoding

Leave-one-out Encoding (LOO or LOOE) is another example of target-based encoders. The name of the method clearly speaks for itself: we calculate mean target of category k for observation j if observation j is removed from the dataset:

While encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:

One of the problems with LOO, just like with all other target-based encoders, is target leakage. But when it comes to LOO, this problem gets really dramatic, as far as we may perfectly classify the training dataset by making a single split: the optimal threshold for category k could be calculated with the following formula:

Another problem with LOO is a shift between values in the train and the test samples. You could observe it from the picture above. Possible values for category “A” in the train sample are 0.67 and 0.33, while in the test one — 0.5. It is a result of the different number of counts in train and test datasets: for category “A” denominator is equal to n for test and n-1 for train dataset. Such a shift may gradually reduce the performance of tree-based models.

Catboost Encoder

Category representation — CatBoost Encoder

Catboost is a recently created target-based categorical encoder. It is intended to overcome target leakage problems inherent in LOO. In order to do that, the authors of Catboost introduced the idea of “time”: the order of observations in the dataset. Clearly, the values of the target statistic for each example rely only on the observed history. To calculate the statistic for observation j in train dataset, we may use only observations, which are collected before observation j, i.e. i≤j:

To prevent overfitting, the process of target encoding for train dataset is repeated several times on shuffled versions of the dataset and results are averaged. Encoded values of the test data are calculated the same way as in LOO Encoder:

Catboost “on the fly” Encoding is one of the core advantages of CatBoost — library for gradient boosting, which showed state of the art results on several tabular datasets when it was presented by Yandex.

Validation Approaches

Model validation is probably the most important aspect of Machine Learning. While working with data that contains categorical variables, we may want to use one of the three types of validation. None Validation is the simplest one, yet least accurate. Double Validation could show great scores, but it is as slow as a turtle. And Single Validation is kind of a cross between the first two methods.

This section is devoted to the discussion of each validation type in details. For better understanding, for each type of validation, I added block diagrams of the pipeline.

None Validation","['encoders', 'category', 'train', 'benchmarking', 'dataset', 'encoding', 'mean', 'target', 'encoder', 'categorical', 'validation', 'test', 'values']","Target Encoder (TE)Category representation — Target EncodingTarget Encoding has probably become the most popular encoding type because of Kaggle competitions.
New values of category and values with just single appearance in train dataset are replaced with the prior ones.
Target Encoder is a powerful tool, yet it has a huge disadvantage — target leakage: it uses information about the target.
M-Estimate EncoderCategory representation — M-Estimate EncoderM-Estimate Encoder is a simplified version of Target Encoder.
To calculate the statistic for observation j in train dataset, we may use only observations, which are collected before observation j, i.e.",en,['Denis Vorotyntsev'],2019-08-08 19:31:20.713000+00:00,"{'Data Science', 'Software Development', 'Machine Learning', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/2794/1*FPz875Pa14_5wwedQ9irPA.png', 'https://miro.medium.com/max/2944/1*aFLhGR4GKfJJxZJlTR4ZLQ.png', 'https://miro.medium.com/max/1520/1*iXbydvTevqCQRpM5OCuRVg.png', 'https://miro.medium.com/max/60/1*I2C4uJP8lKmMVzmsREFv3g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*Cf-SONgtmAiXbriWT5-prQ.png', 'https://miro.medium.com/max/1516/1*8F5KJnekzeWYo_x6jYOBTQ.png', 'https://miro.medium.com/max/60/1*CQ0CSJY8yBq0P0L4i2ztwQ.png?q=20', 'https://miro.medium.com/max/60/1*pRl2Vk_ZR72VhuHi9S3ZuA.png?q=20', 'https://miro.medium.com/max/60/1*WfnICPX4S2BrQ4WtZ3GqDQ.png?q=20', 'https://miro.medium.com/max/60/0*355gkQdrJO4a3jsY.png?q=20', 'https://miro.medium.com/max/60/0*Z8BiGIP6WXnfVwfb.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*Cf-SONgtmAiXbriWT5-prQ.png', 'https://miro.medium.com/max/1534/1*7pm_k0Vq47JrWxFBwl2Tiw.png', 'https://miro.medium.com/max/60/1*4pd9zNTsRJuTnVUJm-TUgg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1800/1*tA7wtgwXPnHkiZNf7ugPVg.jpeg', 'https://miro.medium.com/max/902/1*B2A6dKhKrMW7kqfZmm7HjQ.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*3JiE2wfGv8U29epjnih73w.png?q=20', 'https://miro.medium.com/max/60/1*Be0SlDIsHeP27EFmeyWdRg.png?q=20', 'https://miro.medium.com/max/1542/1*3JiE2wfGv8U29epjnih73w.png', 'https://miro.medium.com/max/60/1*wEFbHb-4a-2by1bVC_GTVQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1526/1*wEFbHb-4a-2by1bVC_GTVQ.png', 'https://miro.medium.com/max/60/1*4zvPe5mBKMBkZyMurq12Cw.png?q=20', 'https://miro.medium.com/max/60/1*iXbydvTevqCQRpM5OCuRVg.png?q=20', 'https://miro.medium.com/max/2104/1*4zvPe5mBKMBkZyMurq12Cw.png', 'https://miro.medium.com/max/1530/1*Be0SlDIsHeP27EFmeyWdRg.png', 'https://miro.medium.com/max/628/1*wl-W3sDYRWn3bjsymeUFhA.png', 'https://miro.medium.com/max/776/0*aAN3a_9YgpGWhGuq.png', 'https://miro.medium.com/max/60/1*JZ_42L2eO5YZ7rf7oNeCOQ.png?q=20', 'https://miro.medium.com/max/60/1*FPz875Pa14_5wwedQ9irPA.png?q=20', 'https://miro.medium.com/max/1502/1*pRl2Vk_ZR72VhuHi9S3ZuA.png', 'https://miro.medium.com/max/60/1*tA7wtgwXPnHkiZNf7ugPVg.jpeg?q=20', 'https://miro.medium.com/max/60/0*-9XWX-8w2XKBTQfM.png?q=20', 'https://miro.medium.com/max/60/1*U__Hc5o3Ed6kzyULpgbCbw.png?q=20', 'https://miro.medium.com/max/2116/1*U__Hc5o3Ed6kzyULpgbCbw.png', 'https://miro.medium.com/max/3224/1*JZ_42L2eO5YZ7rf7oNeCOQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1644/1*6jGLBSyu93bpKs7xnnDD2g.png', 'https://miro.medium.com/max/976/0*2tjuRxApVW_amjAi.png', 'https://miro.medium.com/max/718/0*8WvCLuxURnwTGPnr.png', 'https://miro.medium.com/max/60/1*wl-W3sDYRWn3bjsymeUFhA.png?q=20', 'https://miro.medium.com/max/688/0*355gkQdrJO4a3jsY.png', 'https://miro.medium.com/max/60/0*CSGNWfI8arLasHAE.png?q=20', 'https://miro.medium.com/max/60/1*6jGLBSyu93bpKs7xnnDD2g.png?q=20', 'https://miro.medium.com/max/3224/1*KKe4Y0wampl2KswsDd6MWQ.png', 'https://miro.medium.com/max/60/1*8F5KJnekzeWYo_x6jYOBTQ.png?q=20', 'https://miro.medium.com/max/784/0*-9XWX-8w2XKBTQfM.png', 'https://miro.medium.com/max/900/1*tA7wtgwXPnHkiZNf7ugPVg.jpeg', 'https://miro.medium.com/max/60/1*B2A6dKhKrMW7kqfZmm7HjQ.png?q=20', 'https://miro.medium.com/max/60/0*aAN3a_9YgpGWhGuq.png?q=20', 'https://miro.medium.com/max/60/1*aFLhGR4GKfJJxZJlTR4ZLQ.png?q=20', 'https://miro.medium.com/max/60/1*7pm_k0Vq47JrWxFBwl2Tiw.png?q=20', 'https://miro.medium.com/max/590/0*Z8BiGIP6WXnfVwfb.png', 'https://miro.medium.com/max/418/0*CSGNWfI8arLasHAE.png', 'https://miro.medium.com/max/60/0*zZNdDd_6wpQq_-k5.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*KKe4Y0wampl2KswsDd6MWQ.png?q=20', 'https://miro.medium.com/max/60/0*8WvCLuxURnwTGPnr.png?q=20', 'https://miro.medium.com/max/772/1*CQ0CSJY8yBq0P0L4i2ztwQ.png', 'https://miro.medium.com/max/60/0*2tjuRxApVW_amjAi.png?q=20', 'https://miro.medium.com/max/2140/1*4pd9zNTsRJuTnVUJm-TUgg.png', 'https://miro.medium.com/max/2494/1*I2C4uJP8lKmMVzmsREFv3g.png', 'https://miro.medium.com/max/504/0*zZNdDd_6wpQq_-k5.png', 'https://miro.medium.com/max/1514/1*WfnICPX4S2BrQ4WtZ3GqDQ.png'}",2020-03-05 00:23:24.102494,2.293951988220215
https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900,An Easier Way to Encode Categorical Features,"Photo by Ash Edmonds on Unsplash

An Easier Way to Encode Categorical Features

Using the python category encoder library to handle high cardinality variables in machine learning

I have recently been working on a machine learning project which had several categorical features. Many of these features were high cardinality, or in other words, had a high number of unique values. The simplest method of handling categorical variables is usually to perform one-hot encoding, where each unique value is converted into a new column with 1 or a 0 denoting the presence or absence of this value. However, when the cardinality of a feature is high this method will often produce so many new features that the model performance decreases.

I started to write my own encoders to try alternative methods to encode some of the features starting with something called weight of evidence. In a binary classification problem weight of evidence uses the distribution of unique values in the feature in both the positive and negative class and creates a new feature relating to these values. Naturally, this took a while to encode and then get it to work in my existing scikit-learn pipeline.

Then I stumbled across this library called category_encoders which has, not only weight of evidence but pretty much every possible way to encode categorical features already written and ready to use. This means that I no longer need to write this custom encoder and I can now quickly evaluate a whole host of different encoders and select the best one. In this post, I want to share this library and give an example of how it can be used in a scikit-learn pipeline.

Category encoders

This library comprises a set of transformers which follow the scikit-learn style which means that as well as being used alone, they can also be used inside a scikit-learn pipeline. The transformers provide a wide variety of methods to transform categorical data including the very popular one-hot encoding. This library is particularly useful for handling high cardinality features where the one-hot encoding method is likely to result in poorer model performance.

Let’s look at an example of this in use. In the following example I am using the adults dataset which I have downloaded from the UCI machine learning repository. This data comprises a set of characteristics about each person and the target variable which denotes whether they earn under or over $50k per year.

The library can be installed via pip.

pip install category_encoders

Or Conda.

conda install -c conda-forge category_encoders

To start with here are the imports I am using.

import pandas as pd

import numpy as np from sklearn import preprocessing

from sklearn.model_selection import train_test_split

from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier

from sklearn.impute import SimpleImputer

from sklearn.preprocessing import StandardScaler

from sklearn.compose import ColumnTransformer

from sklearn.metrics import f1_score

import category_encoders as ce

Next, I am downloading the data and transforming it into a pandas data frame.

I then create a variable for each feature type, categorical and numerical, for later use in the pipeline, and splitting the values into test and train data sets. One important thing to note is that although scikit-learn can handle non-numerical target variables, the category_encoders library cannot. So one additional step here is to use the LabelEncoder to transform the y labels.

numeric_features = adults_data.select_dtypes(include=['int64', 'float64']).columns

categorical_features = adults_data.select_dtypes(include=['object']).drop(['income'], axis=1).columns X = adults_data.drop('income', axis=1)

y = adults_data['income'] le = preprocessing.LabelEncoder()

label_encoder = le.fit(y)

y = label_encoder.transform(y) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

Next, I am running the below code which constructs a pipeline and loops through a list of category_encoders printing the score for each model. I have used a Random Forest model as a simple example.

encoder_list = [ce.backward_difference.BackwardDifferenceEncoder,

ce.basen.BaseNEncoder,

ce.binary.BinaryEncoder,

ce.cat_boost.CatBoostEncoder,

ce.hashing.HashingEncoder,

ce.helmert.HelmertEncoder,

ce.james_stein.JamesSteinEncoder,

ce.one_hot.OneHotEncoder,

ce.leave_one_out.LeaveOneOutEncoder,

ce.m_estimate.MEstimateEncoder,

ce.ordinal.OrdinalEncoder,

ce.polynomial.PolynomialEncoder,

ce.sum_coding.SumEncoder,

ce.target_encoder.TargetEncoder,

ce.woe.WOEEncoder

] for encoder in encoder_list:



numeric_transformer = Pipeline(steps=[

('imputer', SimpleImputer(strategy='median')),

('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[

('imputer', SimpleImputer(strategy='constant', fill_value='missing')),

('woe', encoder())])



preprocessor = ColumnTransformer(

transformers=[

('num', numeric_transformer, numeric_features),

('cat', categorical_transformer, categorical_features)])



pipe = Pipeline(steps=[('preprocessor', preprocessor),

('classifier', RandomForestClassifier(n_estimators=500))])



model = pipe.fit(X_train, y_train)



y_pred = model.predict(X_test)

print(encoder)

print(f1_score(y_test, y_pred, average='macro'))

The output is as follows. You can see from the below that for this model the ordinal encoder gives the best score and the leave one out encoder gives the lowest.

Handling categorical variables is just one aspect of tuning a machine learning model there are of course many other steps involved. The category_encoder library provides a way to very quickly evaluate a number of different methods for handling these features. It is definitely something I will be using often in future projects.

Thanks for reading!","['way', 'encode', 'import', 'variables', 'high', 'scikitlearn', 'model', 'categorical', 'encoder', 'data', 'features', 'library', 'easier']","Photo by Ash Edmonds on UnsplashAn Easier Way to Encode Categorical FeaturesUsing the python category encoder library to handle high cardinality variables in machine learningI have recently been working on a machine learning project which had several categorical features.
Then I stumbled across this library called category_encoders which has, not only weight of evidence but pretty much every possible way to encode categorical features already written and ready to use.
The transformers provide a wide variety of methods to transform categorical data including the very popular one-hot encoding.
Handling categorical variables is just one aspect of tuning a machine learning model there are of course many other steps involved.
The category_encoder library provides a way to very quickly evaluate a number of different methods for handling these features.",en,['Rebecca Vickery'],2019-10-12 18:03:17.858000+00:00,"{'Data Science', 'Artificial Intelligence', 'Python', 'Machine Learning', 'Programming'}","{'https://miro.medium.com/fit/c/96/96/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/max/2312/1*3LTsC4NqUE_erCwXD8gBOw.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/11520/1*Rcws6K8FP4aQhTpgGz9HXQ.jpeg', 'https://miro.medium.com/fit/c/160/160/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*Rcws6K8FP4aQhTpgGz9HXQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1200/1*Rcws6K8FP4aQhTpgGz9HXQ.jpeg', 'https://miro.medium.com/max/60/1*3LTsC4NqUE_erCwXD8gBOw.png?q=20'}",2020-03-05 00:23:31.080662,6.977171897888184
https://towardsdatascience.com/collaborate-on-mlflow-experiments-in-neptune-fb4f8f84a995,How to make your MLflow projects easy to share and collaborate on.,"TL;DR (for the mlflow users)

In your workflow change this:

mlflow ui

to this:

neptune mlflow

and have your mlflow experiments:

hosted,

backed-up,

organized,

easy to share and discuss with others.

Sharing your work can be just as easy as me sharing this MLflow experiment run with you.

mlflow and neptune.ai integartion

MLflow: The great, the good and the so so

MLFlow is an awesome open-source project that has been gaining a lot of popularity lately. In a nutshell, it lets you:

track your machine learning projects

makes it easy to reproduce the results

lets you deploy your models to production

If you haven’t heard of MLFlow yet, you should definitely check it out. This blog post, written by the authors of mlflow, should give you a pretty good picture. Long story short, mflow has the potential to become an industry standard when it comes to tracking, reproducibility, and deployment of machine learning models. It does a good job at it already, and with such an amazing team behind it, I believe mlflow is here to stay.

However, when you want to add organization and collaboration, things aren’t peachy just yet. You need to host your mlflow server, make sure that the right people have access, have backups and so on. In addition, the experiment comparison interface offered by mlflow is a bit rough around the edges, especially if you want to use it as a team.

MLflow focuses on tracking, reproducibility, and deployment, not on organization and collaboration. As simple as that.

Here is where I do my song and dance and tell you that you can take what you like from MLflow and enjoy organization and collaboration in a beautiful, hosted UI that Neptune gives you.

My song and dance.

We have recently created and open-sourced neptune-mlflow to let you integrate Neptune and MLflow with literally one command.

neptune mlflow

hosted MLflow experiments in Neptune

Comparison of experiment runs.

Share your experiments and models with others.

If you want to learn more, go ahead and read about experiment management in Neptune.

Got you interested?

Let’s dive into details.

Tracking in MLflow

Setup

We need to start by setting-up mlflow tracking for our experiments. As an example, I will train some neural networks on the mnist dataset.

Let’s look at this simple training script and then add mlflow to it.

Training script

Ok, so we define parameters, load the data, create a model and optimizer, train and evaluate our network and save it. Pretty standard.

Now, in order to start tracking with mlflow, we need to create what mlflow calls a run :

Here we go, we can now log information to mlflow.

I will start with hyperparameters:

When the training is done, I would like to know what was the loss and accuracy on the test set. Let’s log those metrics:

Finally, let’s log model weights to make sure that all the important experiment information is saved.

So the full training script with MLflow tracking looks like this:

Training script with MLflow tracking.

You can experiment with new ideas quickly and have the metadata like hyperparameters, metrics and model weights saved by MLflow. With that, you gain control over your experimentation process. Feels good, doesn’t it?

It is probably a good idea to run a few experiments with different parameters and try to improve test_accuracy of the model.

Explore experiments

Now, that we have generated some experiment metadata, we can start exploring it. MLflow comes with an experiment comparison interface that you can fire-up by running:

mlflow ui

and get something like this:

MLflow experiment comparison interface.

You can search through experiments, see the hyperparameters, sort them by metrics and add notes to experiment runs. However, in my opinion, there are quite a few things that could be improved. To list a few:

adjusting the experiment dashboard to your needs is not possible,

comparing learning curves for different metrics is not great. One needs to do it on a one-by-one basis,

comparing learning curves for different runs(TensorBoard style) is not there yet,

space for project-level knowledge sharing (notes, global changelog)is missing.

Those are precisely some of the things that Neptune does really well and why we figured integrating Neptune with MLflow could bring so much to the community.

Organization and collaboration in Neptune

Setup Neptune (skip if you already have it)

Register. Go to neptune.ai and sign up. It is completely free for non-organizations, and you can invite others to join your team! Get your Neptune API token. To do that, click on the Get API Token button on the top left.

Get your API token.

3. Set NEPTUNE_API_TOKEN environment variable. Go to your console and run:

export NEPTUNE_API_TOKEN=’your_long_api_token’

4. Create your first project. Click on Projects and the New project . Choose a name for it and whether you want it public or private.

Create a new project.

Install neptune-mlflow

pip install neptune-mlflow

Sync MLflow with Neptune.

Go to your project directory and run:

neptune mlflow --project USER_NAME/PROJECT_NAME

You can simplify the command, and drop the --project argument. Set NEPTUNE_PROJECT variable to USER_NAME/PROJECT_NAME by running:

export NEPTUNE_PROJECT=USER_NAME/PROJECT_NAME

and you can synchronize your mlruns directory with Neptune by executing:

neptune mlflow

Easy peasy huh?

Organize experiments

Now your experiment metadata is safely stored and backed-up in Neptune. You can customize your dashboard however you like. You can add tags and group experiments with custom filters.

Project dashboard.

You can add project wiki and share knowledge with others.

Project Wiki.

You can even compare the experiment runs TensorBoard-style with one click.

Collaborate on experiments

One huge advantage of having your ml experiments managed in Neptune is that you can share your work with others. By simply sending the link to an experiment charts, selected group of experiments or model diagnostic plots you can explain your question or problem quickly and effectively.

In simple words, I can track my experiment runs with MLflow and share it with you by sending you an experiment link. How cool is that?

Just click on one of the links above and see how easy knowledge sharing can be.

Moreover, anyone that has access to the project can download every part of it from the UI. Sharing your best model with a colleague is as simple as clicking the download button.

Experiment outputs.

“Okey, but who has access to my project?” — You

You decide. Keep your work public or make it private. Just invite others and choose a role(admin/contributor/viewer) for every member of your project.

Invite contributors.

Final thoughts

Neptune + MLflow is an interesting combination if you want to have a common framework for tracking, reproducibility, and deployment but at the same time, you care about knowledge organization and collaboration.

If you want to see how Neptune + MLflow duo is working for your project, go to neptune.ai and sign up.

Thanks for staying till the end. Let me share this quote with you in return.","['good', 'collaborate', 'easy', 'project', 'share', 'experiment', 'neptune', 'experiments', 'projects', 'tracking', 'model', 'mlflow', 'add']","TL;DR (for the mlflow users)In your workflow change this:mlflow uito this:neptune mlflowand have your mlflow experiments:hosted,backed-up,organized,easy to share and discuss with others.
Sharing your work can be just as easy as me sharing this MLflow experiment run with you.
So the full training script with MLflow tracking looks like this:Training script with MLflow tracking.
Go to your project directory and run:neptune mlflow --project USER_NAME/PROJECT_NAMEYou can simplify the command, and drop the --project argument.
If you want to see how Neptune + MLflow duo is working for your project, go to neptune.ai and sign up.",en,['Jakub Czakon'],2020-01-16 15:30:24.818000+00:00,"{'Data Science', 'Knowledge Sharing', 'Machine Learning', 'Mlflow', 'Version Control'}","{'https://miro.medium.com/max/3748/1*5ytGcfjiS64u-W2HCaFGsg.png', 'https://miro.medium.com/max/60/1*FhKrMpcUPeZxNsZpKNv4BA.png?q=20', 'https://miro.medium.com/max/3748/1*O4mPXKWsndAMVEF8GscQwQ.png', 'https://miro.medium.com/max/2650/1*FhKrMpcUPeZxNsZpKNv4BA.png', 'https://miro.medium.com/max/1808/1*0SyM2FXvjzwt28psw9TBEA.png', 'https://miro.medium.com/max/3282/1*f8X1le2rfdr2wSXfWShqNw.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3758/1*LWKuIeKBrQ_OA96k3-EYzw.png', 'https://miro.medium.com/fit/c/160/160/1*76yVgFSE4AoNMqmT7VD9rw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*oYx6L__CznZFEiGSGSWK9g.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*76yVgFSE4AoNMqmT7VD9rw.png', 'https://miro.medium.com/max/601/1*h4DZ8HtcHHMBX35SG0MmDg.png', 'https://miro.medium.com/max/1280/1*02qf5mcOfa5GlAljVz4Jbw.jpeg', 'https://miro.medium.com/max/60/1*5ytGcfjiS64u-W2HCaFGsg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*h4DZ8HtcHHMBX35SG0MmDg.png?q=20', 'https://miro.medium.com/max/2560/1*TyvPQDo8E4aUh-YMAcCcAg.jpeg', 'https://miro.medium.com/max/3750/1*oYx6L__CznZFEiGSGSWK9g.png', 'https://miro.medium.com/max/3702/1*eNXCw8yREOBPpItWGZSGVQ.png', 'https://miro.medium.com/max/60/1*0SyM2FXvjzwt28psw9TBEA.png?q=20', 'https://miro.medium.com/max/60/1*WwWJcMM4UaKst0pTnB3ERg.png?q=20', 'https://miro.medium.com/max/60/1*O4mPXKWsndAMVEF8GscQwQ.png?q=20', 'https://miro.medium.com/max/60/1*EbiC-HJOBu4ySW0L9YSdvA.jpeg?q=20', 'https://miro.medium.com/max/60/1*f8X1le2rfdr2wSXfWShqNw.png?q=20', 'https://miro.medium.com/max/1202/1*h4DZ8HtcHHMBX35SG0MmDg.png', 'https://miro.medium.com/max/2560/1*EbiC-HJOBu4ySW0L9YSdvA.jpeg', 'https://miro.medium.com/max/60/1*eNXCw8yREOBPpItWGZSGVQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3374/1*WwWJcMM4UaKst0pTnB3ERg.png', 'https://miro.medium.com/max/60/1*TyvPQDo8E4aUh-YMAcCcAg.jpeg?q=20', 'https://miro.medium.com/max/60/1*02qf5mcOfa5GlAljVz4Jbw.jpeg?q=20', 'https://miro.medium.com/max/60/1*LWKuIeKBrQ_OA96k3-EYzw.png?q=20'}",2020-03-05 00:23:33.050330,1.9696683883666992
https://medium.com/@harrynicholls/7-popular-technical-indicators-and-how-to-use-them-to-increase-your-trading-profits-7f13ffeb8d05,7 Popular Technical Indicators and How to Use Them to Increase Your Trading Profits,"Photo by Chris Liverani on Unsplash

Ever wondered how to use technical indicators in trading? Well wonder no more, this article introduces 7 popular indicators, and the strategies you can use to profit from their signals.

Technical trading involves reviewing charts and making decisions based on patterns and indicators.

These patterns are particular shapes that candlesticks form on a chart, and can give you information about where the price is likely to go next.

Indicators are additions or overlays on the chart that provide extra information through mathematical calculations on price and volume. They also tell you where the price is likely to go next.

There are 4 major types of indicator:

Trend

Momentum

Volume

Volatility

Trend indicators tell you which direction the market is moving in, if there is a trend at all. They’re sometimes called oscillators, because they tend to move between high and low values like a wave. Trend indicators we’ll discuss include Parabolic SAR, parts of the Ichimoku Kinko Hyo, and Moving Average Convergence Divergence (MACD).

Momentum indicators tell you how strong the trend is and can also tell you if a reversal is going to occur. They can be useful for picking out price tops and bottoms. Momentum indicators include Relative Strength Index (RSI), Stochastic, Average Directional Index (ADX), and Ichimoku Kinko Hyo.

Volume indicators tell you how volume is changing over time, how many units of bitcoin are being bought and sold over time. This is useful because when the price changes, the volume gives an indication of how strong the move is. Bullish moves on high volume are more likely to be maintained than those on low volume.

We won’t cover volume indicators here, but this class includes On-Balance Volume, Chaikin Money Flow, and Klinger Volume Oscillator.

Volatility indicators tell you how much the price is changing in a given period. Volatility is a very important part of the market, and without it there’s no way to make money! The price has to move for you to make a profit, right?

The higher the volatility is, the faster a price is changing. It tells you nothing about direction, just the range of prices.

Low volatility indicates small price moves, high volatility indicates big price moves. High volatility also suggests that there are price inefficiencies in the market, and traders spell “inefficiency”, P-R-O-F-I-T. We’ll cover 1 volatility indicator today, Bollinger Bands.

So why are indicators so important? Well, they give you an idea of where the price might go next in a given market. At the end of the day, this is what we want to know as traders. Where is the price going to go? So we can position ourselves to take advantage of the move and make money!

As a trader, it’s your job to understand where the market might go, and be prepared for any eventuality. You don’t need to know exactly where the market is going to go, but understand the different possibilities, and be positioned for whichever one materializes.

Remember, traders make money in bull AND bear markets. We take advantage of long AND short positions. Don’t get too attached to the direction of the market, as long as the price is moving you can profit. Indicators will help you to do this.

Without further ado, here are the stars of the show.

1) Bollinger Bands

Bollinger bands are a volatility indicator. They consist of a simple moving average, and 2 lines plotted at 2 standard deviations on either side of the central moving average line. The outer lines make up the band.

Simply, when the band is narrow the market is quiet. When the band is wide the market is loud.

You can use Bollinger Bands to trade in both ranging and trending markets.

In a ranging market, look out for the Bollinger Bounce. The price tends to bounce from one side of the band to the other, always returning to the moving average. You can think of this like regression to the mean. The price naturally returns to the average as time passes.

In this situation, the bands act as dynamic support and resistance levels. If the price hits the top of the band, then place a sell order with a stop loss just above the band to protect against a break out. The price should revert back down towards the average, and maybe even to the bottom band, where you could take profits. Check out the screenshot below.

Bollinger Bounce, chart via TradingView

When the market is trending, you can use the Bollinger Squeeze to time your trade entry and catch breakouts early on. When the bands get closer together (i.e. they squeeze), it indicates that a breakout is about to happen. It doesn’t tell you anything about direction so be prepared for the price to go either way.

If the candles breakout below the bottom band, the move will generally continue in a downtrend.

If the candles breakout above the top band, the move will generally continue in an uptrend. Take a look at the screenshot below.

Bollinger Squeeze and subsequent upwards breakout, chart via TradingView

In summary, look out for the Bollinger Bounce in ranging markets, the price will tend to return to the mean. In trending markets, use the Bollinger Squeeze. It doesn’t tell you which way the price is going to go, just that it’s going to go.

2) Ichimoku Kinko Hyo (AKA Ichimoku Cloud)

Ichimoku Kinko Hyo (AKA Ichimoku Cloud) is a collection of lines plotted on the chart. It’s an indicator that measures future price momentum, and determines areas of future support and resistance. At first glance this looks like a very complex indicator, so here’s a breakdown of what the different lines mean:

Kijun Sen (blue line): Also called standard line or base line, this is calculated by averaging the highest high and the lowest low for the past 26 periods

(blue line): Also called standard line or base line, this is calculated by averaging the highest high and the lowest low for the past 26 periods Tenkan Sen (red line): The turning line. It’s derived by averaging the highest high and the lowest low for the past nine periods

(red line): The turning line. It’s derived by averaging the highest high and the lowest low for the past nine periods Chikou Span (green line): Also called the lagging line. It’s today’s closing price plotted 26 periods behind

(green line): Also called the lagging line. It’s today’s closing price plotted 26 periods behind Senkou Span (red/green band): The first Senkou line is calculated by averaging the Tenkan Sen and the Kijun Sen and plotted 26 periods ahead. The second Senkou line is calculated by averaging the highest high and the lowest low over the past 52 periods, and plotting it 26 periods ahead

Ichimoku Kinko Hyo, chart via TradingView

So how can you translate these lines into trading profits? I’m glad you asked.

The Senkou span acts as dynamic support and resistance levels. If the price is above the Senkou span, the top line acts as first support, and the bottom line as second support.

If the prices below the Senkou span, the bottom line acts as the first resistance, and the top line as the second resistance. Simple as that!

Senkou span as dynamic support and resistance, chart via TradingView

The Kijun Sen (blue line) can be used to confirm trends. If the price breakouts above the Kijun Sen, it’s likely to rise further. Conversely, if the price drops below this line, then it’s likely it’ll go lower.

Kijun Sen breakout into an uptrend, chart via TradingView

The Tenkan Sen (red line) can also be used to confirm trends. If the line is moving up or down, it indicates the market is trending. And if it’s moving sideways, then the market is ranging.

Flat Tenkan Sen = ranging market, chart via TradingView

Downwards Tenkan Sen = downtrend, chart via TradingView

Remember, the red line is a trend indicator.

The Chikou span (green line) is plotted 26 periods BEHIND the current period. This is a key fact to remember. It can be used as a trend indicator of sorts. When the line crosses the price in a bottom-up direction, the price is likely to go up. When the line crosses the price in a top-down direction, the price is likely to go down.

Chikou Span crosses price from top-down = start of downtrend, chart via TradingView

That’s a whole lot going on in one indicator! And a whole lot of information it can give you. You just have to remember what each line means. If you get your Kijun Sen and your Chikou Span mixed up, then you could mistake a downtrend for an uptrend. And that’d be disastrous for your trading account!

3) Relative Strength Index (RSI)

Next up, the Relative Strength Index (RSI). This is a momentum indicator plotted on a separate scale. There’s a single line scaled from 0 to 100 that identifies overbought and oversold conditions in the market. Readings over 70 indicate an overbought market, and readings below 30 indicate an oversold market. Got it? Alright, let’s see how you can make money from this guy.

The whole idea behind RSI is to pick the tops and bottoms, to get into a market as the trend is reversing. This can help you to take advantage of the whole move. Take a look at the chart below.

Uptrend after RSI indicates oversold conditions, chart via TradingView

Around Feb 6, the market was deep into oversold territory. This is a strong buy signal. If you had bought the market here and held on until RSI moved above 70 (around Feb 17), then you would’ve grabbed a whopping 490,000 pips! That’s almost $5,000 per BTC!

RSI can also be used to confirm trend formations. If RSI is above the 50 level, the market is probably in an uptrend. Conversely, if the line is below 50, the market is probably in a downtrend.

In the example below, RSI indicated oversold conditions on Feb 1–2. This looked like a good buy at the time. But it turned out the be a fakeout. See how it turned out…

RSI didn’t breakthrough 50 = fakeout, chart via TradingView

Initially, the price started to rise, but RSI didn’t breakthrough the 50 level on Feb 4. And you can see what happened after that. The market dropped like a stone, all the way down to below $6,000.

In the previous example of the uptrend, you can see that RSI did manage to breakthrough 50, even though it hovered around that area for about a week.

If you’re more risk-averse, then waiting for trend confirmation may be the way to go. It’s a trade-off between 2 things. On one hand you stand to make more profit by getting into a trend early, but you’ll also be wrong more often and potentially lose a lots of pips to your stops.

On the other hand you can wait for the trend to be confirmed and be right more often, but you’ll also miss a portion of the move so stand to make less profit.

It all depends on your risk disposition. Are you willing to accept many small losses and a few big winners? Or do you want lots of smaller winners?

Only you can make that decision. It’s my job to provide you with the methods and tools to play the game, but you have to decide how you’re going to use them.

4) Moving Average Convergence Divergence (MACD)

Next on the roster we have Moving Average Convergence Divergence (AKA MACD). This is a trend indicator and it consists of a fast line, slow line, and a histogram. Have you had a coffee yet? This is going to be a little confusing, so pay attention!

The inputs for this indicator are a faster-moving average (MA-fast), a slower-moving average (MA-slow), and a number defining the period for yet another moving average (MA-period).

The MACD fast line is a moving of the moving average of the difference between MA-fast and MA-slow. Let that sink in.

The MACD slow line is a moving average of the MACD fast line. The number of periods is defined by MA-period.

Finally, the histogram shows the difference between the MACD fast and slow lines.

Don’t worry if you don’t get it first time, we’ll go through an example.

Say you have MACD “12, 26, 9” (a common default setting). This means that the fast line is the moving average of the difference between the 12-period and 26-period moving averages. The slow line is a 9-period moving average of the MACD fast line. And the histogram is the difference between the MACD lines.

You might have to re-read that a few times to get it. And make sure you do get it because MACD is a very useful indicator.

So what’s this “convergence divergence” thing all about? Well, the moving averages and the histogram are plotted on a separate chart and you’ll see that the lines crossover from time to time.

MACD convergence and divergence, chart via TradingView

As the difference between the 2 lines gets smaller, they get closer together, i.e. converge. When the difference gets bigger, they get further apart, i.e. diverge.

It’s this characteristic of the indicator that you can use in trading.

When a new trend is forming, the MACD lines will converge, eventually they’ll crossover (indicating that the trend has reversed), and the lines then start to diverge. At the point of crossover, the histogram will disappear because the difference between the lines is 0.

Take a look at the chart below.

MACD crossover, chart via TradingView

The fast line (blue) crossed over the slow line (orange) around Feb 19. This indicates that the previous downtrend has ended and an uptrend is starting. And would you look at that… There’s an uptrend for the next few days! Almost a 200,000 pip move and a good $2K profit per BTC.

One thing to note about MACD is that it’s made up of moving averages of other moving averages. This means that it lags behind price quite a lot, so might not be the best indicator to use if you want to get into trends early. But it’s great for confirming trends.

5) Parabolic Stop and Reverse (SAR)

Time to move on to something a little simpler, Parabolic SAR. This is a trend indicator. Dots are placed on the chart above or below the price, and they indicate the potential direction of the price movement. Does it get much simpler?!

Parabolic SAR, chart via TradingView

How can such a simple indicator be used in trading? Well, I’ll tell you. When the dots are above the price, the market is in downtrend, indicating that you should be short.

When the dots are below the price, the market is in an uptrend, indicating that you should be long. Easy as pie.

Parabolic SAR points, chart via TradingView

One thing to be aware of. Do not use Parabolic SAR in a ranging market, when the price is moving sideways. There’ll be a lot of noise and the dots will flip from side-to-side giving you no clear signal.

Add Parabolic SAR to your trading arsenal and use it to get a handle on strong trends.

6) Stochastic

Next in line, the stochastic indicator. This is a momentum indicator, and can be used to find where a trend might be ending. In a similar fashion to RSI, it’s used to determine when an asset is overbought or oversold.

It’s made up of 2 lines plotted on a separate chart.

As you might’ve already guessed, stochastic can help you to pick an entry point and get into a trend at the very beginning. When the stochastic lines are above 80, the market is overbought, and a DOWNTREND is likely to follow.

Stochastic > 80 indicates overbought market, chart via TradingView

Now when the stochastic lines are below 20, it indicates that the market is oversold, and an UPTREND is likely to follow.

Stochastic < 20 indicates oversold market, chart via TradingView

The same caveats as RSI apply here. When trying to get into trends early, there will be many fakeouts, so you should be prepared with stop losses in case the market doesn’t go your way.

As always, use the indicator to give you an idea of where the market is likely to go. Don’t bet your house on it though. Good risk management prevails.

7) Average Directional Index (ADX)

Here’s another oscillator, but this time it’s a trend indicator. Average Directional Index (ADX) values range from 0 to 100, and is intended to give you a signal of trend strength.

If ADX is below 20, the trend is weak. If it’s above 50, the trend is strong. Bear in mind though, that ADX doesn’t tell you the direction of the trend, just the strength.

ADX below 20 (red line), price is ranging, chart via TradingView

When trading, you can use ADX to avoid fakeouts. It’s really best used in combination with other indicators, as (despite the name) it doesn’t give you any information about trend direction.

Combined with a directional trend indicator, such as Parabolic SAR, ADX can confirm that a trend is strong and is going to continue. This should give you more confidence when entering into a position.

ADX above 50 (green line), strong trend, chart via TradingView

ADX can also help you to exit the trade when the trend weakens, to avoid getting caught by price retracements.

As with many trend indicators, ADX lags behind the price, so is not useful if you want to get in on trends early. But it is useful if you only want to trade strong trends.

Those are 7 popular indicators that you’ll see around. Create your own charts, play around with the indicators and get a feel for how they work.

Be aware, the default parameters for the indicators might be the best for cryptocurrencies, or for your trading style, so change them. See how the parameters affect the signals you get from the indicators, and whether this gives you better entries, or helps you to catch better trends.

I highly recommend experimenting early on and finding your own style.

BONUS: Trading with multiple indicators

One big caveat to all of the above. Trading from a single indicator will not make you rich. Each indicator has it’s own limitations and will not be correct 100% of the time.

However, you can use these indicators in concert to get better signals and overall make more profitable trades. Here are a couple of examples to get you started.

Parabolic SAR & Ichimoku Cloud

Trading with the Ichimoku Cloud and Parabolic SAR indicators, chart via TradingView

Here I’ve added the Parabolic SAR indicator and Ichimoku Cloud to a BTCUSD 1D chart. On day 1, the price closed below the Kijun Sen (blue line), indicating that the price might go lower. If you were using the Ichimoku by itself, you might’ve entered the trade here. But you’re not, so hold on.

On day 2, there was a slight recovery, but the price is still below the Kijun Sen, so still bearish. Parabolic SAR is still below the candlestick, a bullish signal. There’s still conflict between you’re 2 indicators, so do nothing. Wait it out. Be patient.

On day 3, there’s a big drop in price. We’re now way under the Kijun Sen, and look, Parabolic SAR has flipped sides, it’s now above the candlestick! Both indicators are showing bearish signals, now’s the time to enter the market and go short!

You enter the market and for the next 26 days, the market goes the way you want. DOWN! If you’d sold around $14K on day 3, you could’ve picked up over 800,000 pips. That’s $8K per BTC. A big win.

You can see that there were a couple of fakeouts where the price closed above the Kijun Sen (Jan 20 and 28). If you’d only been looking at the Ichimoku Cloud, maybe you would’ve closed out your position. Parabolic SAR was still bearish, so could’ve given you much needed information about where the price was headed.

RSI & Bollinger Bands

Trading with Bollinger Bands and RSI, chart via TradingView

Here’s another example where 2 indicators are better than one. Using RSI and Bollinger Bands, you can confirm when the market is turning and pick a great entry point.

At the end of October, 2017, RSI started to show the market was overbought. This might’ve prompted you to sell! But the Bollinger Bands were still expanding, so no dice.

RSI continued to show overbought conditions through the beginning of November, and the market continued to rise through this time. If you’d bought back in October, you would be having serious second thoughts by now. The Bollinger bands have started to contract though, showing that the market is getting quiet.

On November 5, the bands really start to squeeze together AND RSI is showing that the market is overbought. Is it a good time to sell? Yessir! Both indicators agree! The market is ready to make a move, and that move is down.

If you’d sold on November 5, you could’ve picked up 200,000 pips over the next week, a nice $2,000 per BTC. That would’ve set you up for a great Christmas 2017!","['indicators', 'chart', 'average', 'market', 'indicator', 'increase', 'trend', 'price', 'trading', 'technical', 'profits', 'moving', 'line', 'popular', 'sen']","Trend indicators we’ll discuss include Parabolic SAR, parts of the Ichimoku Kinko Hyo, and Moving Average Convergence Divergence (MACD).
They consist of a simple moving average, and 2 lines plotted at 2 standard deviations on either side of the central moving average line.
Flat Tenkan Sen = ranging market, chart via TradingViewDownwards Tenkan Sen = downtrend, chart via TradingViewRemember, the red line is a trend indicator.
4) Moving Average Convergence Divergence (MACD)Next on the roster we have Moving Average Convergence Divergence (AKA MACD).
ADX below 20 (red line), price is ranging, chart via TradingViewWhen trading, you can use ADX to avoid fakeouts.",en,['Harry Nicholls'],2019-07-27 19:41:31.079000+00:00,"{'Beginner', 'Technical Analysis', 'Bitcoin', 'Learning', 'Trading'}","{'https://miro.medium.com/max/60/1*rXju4A6yjaiTqCZs-qKjwg.png?q=20', 'https://miro.medium.com/max/8064/0*31rLljL6asqrdWZo.', 'https://miro.medium.com/max/2364/1*nd7007ecV0f2bDiVqlkTFQ.png', 'https://miro.medium.com/max/2364/1*sIYRFscCyYPyFS2Z3ce18g.png', 'https://miro.medium.com/max/2364/1*t5RdR9DCoey67vYw_TW1iw.png', 'https://miro.medium.com/max/60/1*sIYRFscCyYPyFS2Z3ce18g.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*ppjYWcleKPRheCjmrk4ZKQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*ppjYWcleKPRheCjmrk4ZKQ.jpeg', 'https://miro.medium.com/max/60/1*jYQlLS9jP3dAPWdI6iJQ-Q.png?q=20', 'https://miro.medium.com/max/60/1*BqLA2EdKBtPv622KaKYUxw.png?q=20', 'https://miro.medium.com/max/60/1*lhfBludNdoAoIthLjMgX8Q.png?q=20', 'https://miro.medium.com/max/1200/0*31rLljL6asqrdWZo.', 'https://miro.medium.com/max/2364/1*rXju4A6yjaiTqCZs-qKjwg.png', 'https://miro.medium.com/max/60/1*w-qzI2Q2i7emGK3qMmVWjA.png?q=20', 'https://miro.medium.com/max/2364/1*YctfL4-ipnYUInHZAKfMvQ.png', 'https://miro.medium.com/max/60/1*5L2Q3CQ7npq7-SOEYUFqjQ.png?q=20', 'https://miro.medium.com/max/60/1*YctfL4-ipnYUInHZAKfMvQ.png?q=20', 'https://miro.medium.com/max/2364/1*YecqT2VfwqcrBCpJqhEOog.png', 'https://miro.medium.com/max/60/1*oYSMq_G2-4NQ24lYjBO5BQ.png?q=20', 'https://miro.medium.com/max/2364/1*cnIKfP4F28ejAJFd91TnbQ.png', 'https://miro.medium.com/max/2364/1*w-qzI2Q2i7emGK3qMmVWjA.png', 'https://miro.medium.com/max/2364/1*oYSMq_G2-4NQ24lYjBO5BQ.png', 'https://miro.medium.com/fit/c/80/80/1*5VsuE0dIXNeg9qhrYfkXWw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*ol46A-oGj8NaJtbbQIzY6Q.jpeg', 'https://miro.medium.com/max/60/1*t5RdR9DCoey67vYw_TW1iw.png?q=20', 'https://miro.medium.com/max/60/1*DRb8jBlIk2T__M9VXWQrAg.png?q=20', 'https://miro.medium.com/max/2364/1*lhfBludNdoAoIthLjMgX8Q.png', 'https://miro.medium.com/max/60/1*aNoGhZzh-9DdM49VEkgIzw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ppjYWcleKPRheCjmrk4ZKQ.jpeg', 'https://miro.medium.com/max/2364/1*aNoGhZzh-9DdM49VEkgIzw.png', 'https://miro.medium.com/max/60/0*31rLljL6asqrdWZo.?q=20', 'https://miro.medium.com/max/60/1*cnIKfP4F28ejAJFd91TnbQ.png?q=20', 'https://miro.medium.com/max/2364/1*5L2Q3CQ7npq7-SOEYUFqjQ.png', 'https://miro.medium.com/max/60/1*lpeS2kyHaPCN-Y_Mr0tjfA.png?q=20', 'https://miro.medium.com/max/60/1*nd7007ecV0f2bDiVqlkTFQ.png?q=20', 'https://miro.medium.com/max/2364/1*X5RhyknQrgPJreoyPkM43A.png', 'https://miro.medium.com/max/2364/1*lpeS2kyHaPCN-Y_Mr0tjfA.png', 'https://miro.medium.com/max/60/1*YecqT2VfwqcrBCpJqhEOog.png?q=20', 'https://miro.medium.com/max/60/1*C86KE0puZhs85PYBEqFWSA.png?q=20', 'https://miro.medium.com/max/60/1*X5RhyknQrgPJreoyPkM43A.png?q=20', 'https://miro.medium.com/max/2364/1*JLuBPAiXjq1hCZYtqQ2dhA.png', 'https://miro.medium.com/max/2364/1*DRb8jBlIk2T__M9VXWQrAg.png', 'https://miro.medium.com/max/2364/1*C86KE0puZhs85PYBEqFWSA.png', 'https://miro.medium.com/max/60/1*JLuBPAiXjq1hCZYtqQ2dhA.png?q=20', 'https://miro.medium.com/max/60/1*bABfvHe01ObCJv0qP5bGmQ.png?q=20', 'https://miro.medium.com/max/2364/1*BqLA2EdKBtPv622KaKYUxw.png', 'https://miro.medium.com/max/2364/1*jYQlLS9jP3dAPWdI6iJQ-Q.png', 'https://miro.medium.com/max/2364/1*bABfvHe01ObCJv0qP5bGmQ.png'}",2020-03-05 00:23:34.422235,1.3709044456481934
https://medium.com/python-data/quandl-getting-end-of-day-stock-data-with-python-8652671d6661,Quandl: Getting End of Day Stock Data with Python,"An analysis as good as the data allows it to be. It is imperative that Quants get access to highly organized and quality databases. Over the course of this journey, we have only relied on Yahoo! Finance for all stock data needs. This post offers an alternative to Yahoo! Finance; Quandl

Image Credit: https://www.quandl.com/

Before we proceed, I just want to point out that this post is not some “bash Yahoo! Finance as much as you can” type of post. Yahoo! Finance is an amazing resource which has served so many analysts so well over the years. After AOL acquired Yahoo, pandas lost its ability to remotely pull data from Yahoo’s servers. This incident left so many quants scrambling for alternatives as their scripts relied heavily on Yahoo’s data.

Pandas DataReader module has regained access to Yahoo’s data but that ordeal taught a great lesson about not “keeping all my eggs in basket”. When preparing the code for the next post after this, an interesting question popped up in my head “What if Yahoo decommissions Pandas’ access to its API again?”

In Quandl, I found a good alternative. This post covers how to pull the end of day stock data from Quandl with our BFF Python. Before proceeding, the quandl python package must be installed. If you don’t have it installed already, simply use this code:

conda install quandl OR pip install quandl

Great! Head over to Quandl, make a free account and let’s start pulling some data with the help of Quandl’s Python API. An account with Quandl is necessary for access to we need plus one needs API key to make more than 50 calls to Quandl.

Quandl organizes its data mostly in tables which can be filtered for exact information. Let’s look at parameters and filter operators users of the API need to extract exact information required:

PARAMETER: qopts.columns

Request data from specific columns by passing the qopts.columns parameter. If you want to query for multiple columns, include the column names separated by a comma. FILTERS: .gte=

Modifies the parameter to return values greater than or equal to the requested value .lte=

Modifies the parameter to return values less than or equal to the requested value

For tables, the process is basically:

getting the table needed with the exact Quandl table code (“WIKI/PRICES” in this case),

use qopts, gte & lte to filter columns and time range required.

For detailed overview of how data is organized by Quandl, check out the official documentation","['stock', 'exact', 'access', 'post', 'python', 'yahoos', 'end', 'day', 'data', 'finance', 'api', 'quandl', 'getting', 'parameter']","This incident left so many quants scrambling for alternatives as their scripts relied heavily on Yahoo’s data.
Pandas DataReader module has regained access to Yahoo’s data but that ordeal taught a great lesson about not “keeping all my eggs in basket”.
This post covers how to pull the end of day stock data from Quandl with our BFF Python.
Before proceeding, the quandl python package must be installed.
Head over to Quandl, make a free account and let’s start pulling some data with the help of Quandl’s Python API.",en,['Bernard Brenyah'],2017-10-24 11:39:00.114000+00:00,"{'Python', 'Tools', 'Finance', 'Quant', 'Investing'}","{'https://miro.medium.com/fit/c/96/96/1*8pYuiMoGSX2MS0Irh-LvGA.jpeg', 'https://miro.medium.com/max/1024/1*iqhp3evDfgC4NefNbQUZvg.png', 'https://miro.medium.com/max/2048/1*iqhp3evDfgC4NefNbQUZvg.png', 'https://miro.medium.com/max/60/1*iqhp3evDfgC4NefNbQUZvg.png?q=20', 'https://miro.medium.com/max/134/1*ZnX64xzUfPtHTV6KFAUhWg.png', 'https://miro.medium.com/fit/c/160/160/1*8pYuiMoGSX2MS0Irh-LvGA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*8pYuiMoGSX2MS0Irh-LvGA.jpeg'}",2020-03-05 00:23:35.321380,0.8971445560455322
https://towardsdatascience.com/technical-analysis-library-to-financial-datasets-with-pandas-python-4b2b390d3543,Technical Analysis library to financial datasets with Python Pandas,"During the last months, I have been studying some financial time series such as predict bitcoin price or different challenges proposed by Numer.ai, Two Sigma Investment or G-Research. Giving that said, we have decided to develop a technical analysis library in python based on the Pandas library. You can find the library at:

This new library is oriented to do “Feature Engineering” from typical financial datasets that typically include columns such as “Timestamp”, “Open”, “High”, “Low”, “Close” and “Volume”. This library will be used by data scientifics that want to resolve Machine Learning problems using Python technology stack for data science (Pandas, Scikit-Learn, XGBoost, LightGBM, Keras, TensorFlow, etc).

At this moment, these tools are getting good results to predict almost anything, but they are not working correctly when are used to face financial problems. They are not working correctly because the rows in the dataset only contains information about a specific period of time (e.g. 6 hours or one day) which is not sufficient to generate good predictions with the current models. To improve the predictions, we need to provide more information (features) to the dataset as the current models get better results when more information is provided.

Technical Analysis is focused on providing new information from the past to forecast the direction of price. By adding the information generated by different indicators for the different variables (“Volume”, “Volatility”, “Trend”, “Momentum”, etc), we can improve the quality of the original dataset.

Now, we will explain two examples in detail:

Bollinger Bands

The Bollinger Bands are used to analyze the volatility of the price for an asset in a specific period of time. There are 3 bands, the Middle Band (MB) is the average of the price in the last n periods, the Upper (UB) and Lower Bands (LB) are equal to the middle band, but adding and subtracting x times the standard deviation. The normal parameters that are being used are n = 20 periods and x = 2. So:

MB = SUM(n last close values) / n UB = MB + (X * StdDev) LB = MB — (X * StdDev)

Bollinger Bands example [Image[2] (Own image generated with Matplotlib)]

In the library, the closing price variable is converted to 5 new features. Apart from the 3 Bollinger Bands, we generate another 2 indicators that will indicate when the closing value has a value higher than the Upper Bollinger Band or lower than the Lower Bollinger Band. Therefore, these two characteristics will be 0 except when the closing value get out of these bands, which will be 1.

If we take a look at the image 2, when the closing wave (blue) surpasses the upper or lower bands, there are sudden changes in the price, and it is usually a good idea to sell when it is higher than the Upper Band and to buy when it is lower than the Lower Band.

MACD

Moving Average Convergence Divergence is a trading indicator that focuses on exponential moving average (EMA). To calculate it we use:

MACD = EMA(n1, close) — EMA(n2, close) MACD_Signal = EMA(n3, MACD) MACD_Difference = MACD — MACD_Signal The typical values for the variables are n1=12, n2=26 and n3=9, however other values can be substituted in the library depending on your trading style and goals.

MACD example [Image[3] (Own image generated with Matplotlib)]

The theory tells us that when the MACD curve (blue) is smaller than the MACD_Signal (orange) or when the MACD difference (green curve which represents the difference between MACD_Signal and MACD curve) has a value lower than 0, the price trend will be bearish. On the contrary, it indicates a price increase.

At this moment, the library has implemented 32 indicators:

Volume

Accumulation/Distribution Index (ADI)

On-Balance Volume (OBV)

On-Balance Volume mean (OBV mean)

Chaikin Money Flow (CMF)

Force Index (FI)

Ease of Movement (EoM, EMV)

Volume-price Trend (VPT)

Negative Volume Index (NVI)

Volatility

Average True Range (ATR)

Bollinger Bands (BB)

Keltner Channel (KC)

Donchian Channel (DC)

Trend

Moving Average Convergence Divergence (MACD)

Average Directional Movement Index (ADX)

Vortex Indicator (VI)

Trix (TRIX)

Mass Index (MI)

Commodity Channel Index (CCI)

Detrended Price Oscillator (DPO)

KST Oscillator (KST)

Ichimoku Kinkō Hyō (Ichimoku)

Momentum

Money Flow Index (MFI)

Relative Strength Index (RSI)

True strength index (TSI)

Ultimate Oscillator (UO)

Stochastic Oscillator (SR)

Williams %R (WR)

Awesome Oscillator (AO)

Others

Daily Return (DR)

Cumulative Return (CR)

These indicators result in 58 features. The developers can set a lot of input parameters such as the size of windows, different constants or smart automatic fill NaN values generated in the methods.

We have uploaded a first stable version of the library to GitHub and it can be installed by using “pip”. The library is in continue development so we will be including more indicators, features, documentation, etc. Please, let us know about any comment, contribution or feedback.

Also, I am a software freelance focused on Data Science using Python tools such as Pandas, Scikit-Learn, Zipline or Catalyst. Don’t hesitate to contact me if you need something related to this library, Technical Analysis, Algo Trading, Machine Learning, etc.","['lower', 'information', 'index', 'pandas', 'oscillator', 'macd', 'band', 'datasets', 'bands', 'python', 'price', 'volume', 'technical', 'library', 'financial', 'analysis']","Giving that said, we have decided to develop a technical analysis library in python based on the Pandas library.
You can find the library at:This new library is oriented to do “Feature Engineering” from typical financial datasets that typically include columns such as “Timestamp”, “Open”, “High”, “Low”, “Close” and “Volume”.
Technical Analysis is focused on providing new information from the past to forecast the direction of price.
Also, I am a software freelance focused on Data Science using Python tools such as Pandas, Scikit-Learn, Zipline or Catalyst.
Don’t hesitate to contact me if you need something related to this library, Technical Analysis, Algo Trading, Machine Learning, etc.",en,['Dario Lopez Padial'],2018-12-31 10:37:48.717000+00:00,"{'Feature Engineering', 'Data Science', 'Timeseries', 'Technical Analysis', 'Machine Learning'}","{'https://miro.medium.com/max/1200/1*FxtgxKvjwbP7qchdcshY4w.jpeg', 'https://miro.medium.com/max/1280/0*gdAeMotAOKXsYzrN.', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/96/96/1*m-6yWIkXWE6E4OzffyelQQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*FxtgxKvjwbP7qchdcshY4w.jpeg?q=20', 'https://miro.medium.com/max/1280/0*XaH9aDLjuZr_oSmG.', 'https://miro.medium.com/max/60/0*XaH9aDLjuZr_oSmG.?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/1*m-6yWIkXWE6E4OzffyelQQ.png', 'https://miro.medium.com/max/3734/1*FxtgxKvjwbP7qchdcshY4w.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*gdAeMotAOKXsYzrN.?q=20'}",2020-03-05 00:23:42.120727,6.798344612121582
https://towardsdatascience.com/automate-stacking-in-python-fc3e7834772e,Automate Stacking In Python,"Automate Stacking In Python

How to Boost Your Performance While Saving Time

Introduction

Utilizing stacking (stacked generalizations) is a very hot topic when it comes to pushing your machine learning algorithm to new heights. For instance, most if not all winning Kaggle submissions nowadays make use of some form of stacking or a variation of it. First introduced in the 1992 paper Stacked Generalization by David Wolpert, their main purpose is to reduce the generalization error. According to Wolpert, they can be understood “as a more sophisticated version of cross-validation”. While Wolpert himself noted at the time that large parts of stacked generalizations are “black art”, it seems that building larger and larger stacked generalizations win over smaller stacked generalizations. However, as these models keep increasing in size, they also increase in complexity. Automating the process of building different architectures would significantly simplify this process. The remainder of this article will deal with the package vecstack I recently came across that is attempting just this.

What Does a Stacked Generalization Look like?

The main idea behind the structure of a stacked generalization is to use one or more first level models, make predictions using these models and then use these predictions as features to fit one or more second level models on top. To avoid overfitting, cross-validation is usually used to predict the OOF (out-of-fold) part of the training set. There are two different variants available in this package but I’m going to describe ‘Variant A’ in this paragraph. To get the final predictions in this variant, we take the mean or mode of all of our predictions. The whole process can be visualized using this GIF from vecstacks’ documentation:

Use Case: Building a Stacked Generalization for Classification

After having taken a look at the documentation, it was time to try using the package myself and see how it works. To do so, I decided to use the wine data set available on the UCI Machine Learning Repository. The problem statement for this data set is to use the 13 features, which all represent different aspects of the wine, to predict from which of three cultivars in Italy the wine was derived.

To get started, let’s first import the packages we are going to need for our project:

import pandas as pd

from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import RandomForestClassifier

from xgboost import XGBClassifier

from vecstack import stacking

Now we are ready to import our data and take a look at it to gain a better understanding of what it looks like:

link = '

names = ['Class', 'Alcohol', 'Malic acid', 'Ash',

'Alcalinity of ash' ,'Magnesium', 'Total phenols',

'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',

'Proline'] = ' https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data' = ['Class', 'Alcohol', 'Malic acid', 'Ash','Alcalinity of ash' ,'Magnesium', 'Total phenols','Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines','Proline'] df = pd.read_csv(link, header=None, names=names)

df.sample(5)

Running the code chunk above gives us:

Note that I used .sample() instead if .head() to avoid being potentially misled by assuming the entire data set has the structure of the first five rows. Luckily, this data set does not have any missing values, so we can easily use it to test our package right away without any of the usually required data cleaning and preparation.

Following this, we are going to separate the response from the input variables and perform an 80:20 train-test-split following the example on vecstacks’ documentation.

y = df[['Class']]

X = df.iloc[:,1:] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

We are getting closer to the interesting part. Remember the GIF from earlier? It is time now to define a few first level models for our stacked generalization. This step definitely deserves its own article but for purposes of simplicity, we are going to use three models: A KNN-Classifier, a Random Forest Classifier and an XGBoost Classifier.

models = [

KNeighborsClassifier(n_neighbors=5,

n_jobs=-1),



RandomForestClassifier(random_state=0, n_jobs=-1,

n_estimators=100, max_depth=3),



XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1,

n_estimators=100, max_depth=3)

]

These parameters were not tuned prior to setting them as the purpose of this article is testing the package. If you were to optimize performance, you should not just copy and paste these.

Taking the next part of code from the documentation, we are essentially performing the GIF’s first part using first level models to make predictions:

S_train, S_test = stacking(models,

X_train, y_train, X_test,

regression=False,



mode='oof_pred_bag',



needs_proba=False,



save_dir=None,



metric=accuracy_score,



n_folds=4,



stratified=True,



shuffle=True,



random_state=0,



verbose=2)

The stacking function takes several inputs:

models : the first level models we defined earlier

: the first level models we defined earlier X_train, y_train, X_test : our data

: our data regression : Boolean indicating whether we want to use the function for regression. In our case set to False since this is a classification

: Boolean indicating whether we want to use the function for regression. In our case set to False since this is a classification mode: using the earlier describe out-of-fold during cross-validation

using the earlier describe out-of-fold during cross-validation needs_proba : Boolean indicating whether you need the probabilities of class labels

: Boolean indicating whether you need the probabilities of class labels save_dir : save the result to directory Boolean

: save the result to directory Boolean metric : what evaluation metric to use (we imported the accuracy_score in the beginning)

: what evaluation metric to use (we imported the accuracy_score in the beginning) n_folds : how many folds to use for cross-validation

: how many folds to use for cross-validation stratified : whether to use stratified cross-validation

: whether to use stratified cross-validation shuffle : whether to shuffle the data

: whether to shuffle the data random_state : setting a random state for reproducibility

: setting a random state for reproducibility verbose: 2 here refers to printing all info

Doing so, we get the following output:

task: [classification]

n_classes: [3]

metric: [accuracy_score]

mode: [oof_pred_bag]

n_models: [4] model 0: [KNeighborsClassifier]

fold 0: [0.72972973]

fold 1: [0.61111111]

fold 2: [0.62857143]

fold 3: [0.76470588]

----

MEAN: [0.68352954] + [0.06517070]

FULL: [0.68309859] model 1: [ExtraTreesClassifier]

fold 0: [0.97297297]

fold 1: [1.00000000]

fold 2: [0.94285714]

fold 3: [1.00000000]

----

MEAN: [0.97895753] + [0.02358296]

FULL: [0.97887324] model 2: [RandomForestClassifier]

fold 0: [1.00000000]

fold 1: [1.00000000]

fold 2: [0.94285714]

fold 3: [1.00000000]

----

MEAN: [0.98571429] + [0.02474358]

FULL: [0.98591549] model 3: [XGBClassifier]

fold 0: [1.00000000]

fold 1: [0.97222222]

fold 2: [0.91428571]

fold 3: [0.97058824]

----

MEAN: [0.96427404] + [0.03113768]

FULL: [0.96478873]

Again, referring to the GIF, all that’s left to do now is fit the second level model(s) of our choice on our predictions to make our final predictions. In our case, we are going to use an XGBoost Classifier. This step is not significantly different from a regular fit-and-predict in sklearn except for the fact that instead of using X_train to train our model, we are using our predictions S_train.

model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1,

n_estimators=100, max_depth=3)



model = model.fit(S_train, y_train) y_pred = model.predict(S_test) print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred)) Output: Final prediction score: [0.97222222]

Conclusion

Using vecstacks’ stacking automation, we’ve managed to predict the correct wine cultivar with an accuracy of approximately 97.2%! As you can see, the API does not collide with the sklearn API and could, therefore, provide a helpful tool when trying to speed up your stacking workflow.

As always, if you have any feedback or found mistakes, please don’t hesitate to reach out to me.","['models', 'set', 'import', 'crossvalidation', 'python', 'stacked', 'data', 'level', 'stacking', 'automate', 'using', 'predictions']","Automate Stacking In PythonHow to Boost Your Performance While Saving TimeIntroductionUtilizing stacking (stacked generalizations) is a very hot topic when it comes to pushing your machine learning algorithm to new heights.
For instance, most if not all winning Kaggle submissions nowadays make use of some form of stacking or a variation of it.
To do so, I decided to use the wine data set available on the UCI Machine Learning Repository.
Luckily, this data set does not have any missing values, so we can easily use it to test our package right away without any of the usually required data cleaning and preparation.
It is time now to define a few first level models for our stacked generalization.",en,['Lukas Frei'],2019-01-10 13:56:23.797000+00:00,"{'Deep Learning', 'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Kaggle'}","{'https://miro.medium.com/max/4000/1*GRhVv2cujurPkNPpXFYLXg.png', 'https://miro.medium.com/freeze/max/46/1*8bY4deuFLgE5NXOuIFVUNA.gif?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*GRhVv2cujurPkNPpXFYLXg.png?q=20', 'https://miro.medium.com/freeze/max/400/1*8bY4deuFLgE5NXOuIFVUNA.gif', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*JySEHXhHzsvI1pOljFeyfw.jpeg', 'https://miro.medium.com/max/1318/1*9uCwjY5uRkRrX2VNST7R0w.gif', 'https://miro.medium.com/max/800/1*8bY4deuFLgE5NXOuIFVUNA.gif', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/freeze/max/60/1*9uCwjY5uRkRrX2VNST7R0w.gif?q=20', 'https://miro.medium.com/fit/c/160/160/1*JySEHXhHzsvI1pOljFeyfw.jpeg'}",2020-03-05 00:23:44.079486,1.9577910900115967
https://towardsdatascience.com/columntransformer-meets-natural-language-processing-da1f116dd69f,ColumnTransformer Meets Natural Language Processing,"ColumnTransformer Meets Natural Language Processing

How to combine several feature extraction mechanisms or transformations into a single transformer in a scikit-learn pipeline

Since published several articles on text classification, I have received inquiries on how to deal with mixed input feature types, for example, how to combine numeric, categorical and text features in a classification or regression model.

Therefore, I decided to write a post using an example to answer this question.

There are several different methods to append or combine different feature types. One method is using scipy.sparse.hstack that stack sparse matrices horizontally.

However, I will introduce one method, the new hot baby on the block: ColumnTransformer function from sklearn. If you would like to try it out, you will need to upgrade your sklearn to 0.20.

The Data

An excellent data set for this demonstration is Mercari Price Suggestion Challenge that builds a machine learning model to automatically suggest the right product prices. The data can be found here.","['natural', 'different', 'feature', 'text', 'sklearn', 'example', 'types', 'data', 'combine', 'method', 'columntransformer', 'language', 'using', 'processing', 'meets']","ColumnTransformer Meets Natural Language ProcessingHow to combine several feature extraction mechanisms or transformations into a single transformer in a scikit-learn pipelineSince published several articles on text classification, I have received inquiries on how to deal with mixed input feature types, for example, how to combine numeric, categorical and text features in a classification or regression model.
Therefore, I decided to write a post using an example to answer this question.
There are several different methods to append or combine different feature types.
One method is using scipy.sparse.hstack that stack sparse matrices horizontally.
However, I will introduce one method, the new hot baby on the block: ColumnTransformer function from sklearn.",en,['Susan Li'],2019-04-24 23:27:53.277000+00:00,"{'Scikit Learn', 'Python', 'Naturallanguageprocessing', 'Machine Learning', 'NLP'}","{'https://miro.medium.com/max/2560/1*DB15m43YbGcpbPjNsU0bFg.jpeg', 'https://miro.medium.com/max/902/1*rgNxa4Vu0mSlqtmwEIraEQ.png', 'https://miro.medium.com/max/1200/1*DB15m43YbGcpbPjNsU0bFg.jpeg', 'https://miro.medium.com/max/1368/1*FD-RcTvOGig_DhOyB7Cy1Q.png', 'https://miro.medium.com/max/586/1*ZoAe7_objleslanceOD-Wg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*rgNxa4Vu0mSlqtmwEIraEQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3000/1*NVejwdFIcKxrg_PIddPY5Q.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*DB15m43YbGcpbPjNsU0bFg.jpeg?q=20', 'https://miro.medium.com/max/56/1*tvnXfaObG2YlLkEyLuSiNg.png?q=20', 'https://miro.medium.com/max/58/1*FD-RcTvOGig_DhOyB7Cy1Q.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*NVejwdFIcKxrg_PIddPY5Q.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/1326/1*tvnXfaObG2YlLkEyLuSiNg.png', 'https://miro.medium.com/max/60/1*ZoAe7_objleslanceOD-Wg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg'}",2020-03-05 00:23:50.399821,6.319334983825684
https://towardsdatascience.com/custom-transformers-in-python-part-ii-6fe111fc82e4,Custom Transformers and Pipelines in Python,"Data Cleaning is the most important part of any Machine Learning project. The fact that your data may be in multiple formats and spread across different systems makes it imperative that the data is properly massaged before it’s fed to an ML Model. Data preparation is one of the most tedious and time-consuming steps in the ML process (some surveys show data scientists spend 80% of their time on data prep and management!). However, this is also the most crucial step since your input data is the primary ingredient to the model. After all, why would you make your cake with sour flour? Python offers certain packages which provide different tools to ease the data preparation process and one such solution is the use of Custom Transformers along with Pipelines. In this article, let’s take a look at what custom transformers are and then delve into coding custom transformers in a pipeline for mean encoding and shirt-sizing.

Transformers

If you’ve worked on machine learning problems, you probably know that transformers in Python can be used to clean, reduce, expand or generate features. The fit method learns parameters from a training set and the transform method applies transformations to unseen data. Examples of predefined transformers include StandardScaler, LabelEncoder, Normalize, etc. In this article, let’s explore custom transformers.

Why use Custom Transformers

There are several predefined transformers available in different Python packages, that allow us to easily apply different transformations on our data sets. Why, then, do we need custom transformers? Depending on the problem being addressed, custom transformers can help overcome some of the incapability or challenges posed by predefined transformers. For example, let us consider label encoding. Say, you are trying to label encode a column ‘Region’ with possible values {East, Central, West}. If the Train data set does not contain any data with value ‘Central’ for the Region column, then LabelEncoder cannot handle the transformation on occurrences of ‘Central’

in the Test or Validation data sets.

Applications of Custom Transformers with Pipelines

Custom transformers can be used in pipelines where multiple transformers are applied in sequence. The output from one transformer is fed as input into the next transformer and you may face the challenge of incompatible types i.e. the first transformer may output a NumPy array and the subsequent transformer may expect a data frame as input. Custom transformers can save the day in such scenarios. Before we go into the coding details, let’s quickly refresh the concepts of pipelines, shirt-sizing and encoding.

A “pipeline” denotes a series of concatenated data transformations. Every step in a pipeline feeds from the previous step and data flows through the pipeline from beginning to end. This helps optimize the process of model building, thus allowing more time to understand the underlying data. Custom transformers can be used in a pipeline for mean encoding and shirt-sizing.

Shirt sizing, in its simplest form, involves creating bins and assigning a label to each bin such as, Small, Medium, Large, X-Large, hence the reference to shirt sizing. Generally, when we think of transformers for numerical variables, we default to either standard scalar or applying a log transformation on the columns. These are in contrast to shirt sizing which lets you create categorical variables from numerical variables and is usually followed by other methods such as mean encoding that then generate more meaningful features.

Similarly, for transforming categorical variables, we commonly consider Label Encoding or One Hot. Label Encoding assigns random values to data which may confuse our model into thinking that a column has data with an explicit hierarchy when that may not really be the case. To avoid this, we tend to ‘one hot encode’ columns. This, then, could leave us with a resultant data set that has too many columns. For example, if we create one hot encoding for a ‘State’ column, we could end up with 50 columns which is not only tedious to handle but may also not be meaningful. One way to overcome the above challenges is to use Mean Encoding. Mean Encoding is also a transformer for categorical variables wherein we take the mean of a column with the target column. Mean Encoding works well if our target variable is a binary variable (classification problems) but also has its own drawbacks, which is a topic for another discussion. As is true with most Data Science problems, there is no one best solution for all problems.

Coding Custom Transformers in a Pipeline

The data set we are using is “Home Credit Default Risk” from Kaggle which can be downloaded from here. For simplicity, our code examines only the application data — “application_{train|test}.csv”. It focuses on the column “AMT_INCOME_TOTAL” for shirt sizing and columns short-sized “AMT_INCOME_TOTAL” and “CODE_GENDER” for mean encoding.

First off, import the required packages and read the data from files application_train.csv (into variable df_app_train) and application_test.csv (into variable df_app_test). Let’s then define a custom transformer.

Defining a Custom Transformer

The “ColumnSelector” shown below is a custom transformer which creates a subset data frame from the provided columns of an input data frame. A custom transformer is inherited from the BaseEstimator and TransformerMixin classes as shown below. “__init__” is a constructor.

class <classname> (BaseEstimator, TranformerMixin):

Column Selector — Custom Transformer that extracts required columns

fit in this case does nothing. But generally, fit is used to get and store parameters to perform transformations (more when we talk about Shirt Sizing). transform is used to perform the transformation on the input data set using the parameters from the fit function. Here, it’s creating a subset data frame based on the input columns.

Define Shirt Sizing Custom Transformer

DFShirtSize (in the above code) enables creation and labeling of bins.

Fit applies the log transformation on the given columns and gets the cuts (bins) for the given values and stores them in a dictionary, we call “settings” in the above code. Settings is a global variable, which allows reuse in the transform function.

Before applying fit, make sure the Train data is separated from validation and test data. fit function is applied on Train data and then transform function on Train, Test and Validation data sets. The ability to store parameters ensures that there’s no data leakage from Test or Validation data sets. In order to address the new values in Test and Validation data sets, -inf and inf are added as the first and last elements of cuts (global variable).

Transform reads the parameters from the settings dictionary, accordingly creates bins and labels the input column values. Here, input column bins are created and labeled based on the cuts in the settings dictionary.

Define Mean Encoding Custom Transformer

Mean encoding uses the fraction of times a feature is present out of all the times the feature is in the data set. When there’s an occurrence of an unseen label in test or validation data, then the mean of the target variable replaces the unseen label to avoid creation of Nan’s in the data.

Creating Pipelines

Pipeline steps may be defined as a separate list or directly in the pipeline. Here, the “Column Selector” transformer enables us to pass only the required subset to further steps. Let us create a list of columns that shall go through different transformations and then lay out the list of steps for the pipeline .

SSEncoding is a list of transformers. Pipeline object is created by the Pipeline(SSEncoding) command.Once the Pipeline object is created, fit and transform can be called to perform data transformation. Upon applying fit, all the transformer functions are applied on the invoking data set and parameters are obtained and stored. The above fit command applies both ColumnSelector and DFShirtsize on df_app_train. Let us see an example with data

Data before transformation:

Train data upon transformation:

Train Data upon Transformation

Test data upon transformation:

Test Data upon Transformation

We use train data to fit and transform both train and test data. This helps avoid data leakage. Repeat for mean encoding as shown below.

Calling fit and transform:

Fit

Transform

Calling transform on test data:

Thus, we applied shirt sizing on numerical values to get categorical values. The resulting categorical values were then mean encoded to prepare data that can be consumed by ML algorithms. The use of custom transformers and pipelines can aid us in the process of data preparation and allow more time for analysis and modeling — which is where we should be spending most of our time.

With inputs from Dennis Ignatenko and guidance of Brian Monteiro, both awesome mentors in my Data Science journey.","['fit', 'custom', 'transformer', 'column', 'transformers', 'python', 'encoding', 'mean', 'data', 'pipelines', 'values', 'pipeline']","Python offers certain packages which provide different tools to ease the data preparation process and one such solution is the use of Custom Transformers along with Pipelines.
In this article, let’s take a look at what custom transformers are and then delve into coding custom transformers in a pipeline for mean encoding and shirt-sizing.
In this article, let’s explore custom transformers.
Depending on the problem being addressed, custom transformers can help overcome some of the incapability or challenges posed by predefined transformers.
Applications of Custom Transformers with PipelinesCustom transformers can be used in pipelines where multiple transformers are applied in sequence.",en,['Kishan Kumar'],2019-11-08 04:54:25.663000+00:00,"{'Data Science', 'Transformers', 'Python', 'Machine Learning', 'Pipeline'}","{'https://miro.medium.com/fit/c/160/160/0*kaus22ddTmmpIOgO.jpg', 'https://miro.medium.com/max/1248/1*EIi6r2Wbj-lMRd6Q1s9LCg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*5sVigZYByNAesV-I10sUTQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/96/96/0*kaus22ddTmmpIOgO.jpg', 'https://miro.medium.com/max/624/1*EIi6r2Wbj-lMRd6Q1s9LCg.png', 'https://miro.medium.com/max/1248/1*PdKARUREBxbLXcOljWFLVQ.png', 'https://miro.medium.com/max/1248/1*8WTdilhKv0OqHw8Y74uKow.png', 'https://miro.medium.com/max/1248/1*aKQbPnUjR27TtpZs7Hmx1A.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*8WTdilhKv0OqHw8Y74uKow.png?q=20', 'https://miro.medium.com/max/1248/1*XckUhsVozWGdAaoyeqmY0w.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*XckUhsVozWGdAaoyeqmY0w.png?q=20', 'https://miro.medium.com/max/60/1*PdKARUREBxbLXcOljWFLVQ.png?q=20', 'https://miro.medium.com/max/1248/1*5sVigZYByNAesV-I10sUTQ.png', 'https://miro.medium.com/max/60/1*EIi6r2Wbj-lMRd6Q1s9LCg.png?q=20', 'https://miro.medium.com/max/60/1*aKQbPnUjR27TtpZs7Hmx1A.png?q=20'}",2020-03-05 00:23:52.642384,2.241546869277954
https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65,ML Data Pipelines with Custom Transformers in Python,"How you can use inheritance and sklearn to write your own custom transformers and pipelines for machine learning preprocessing

80% of the total time spent on most data science projects is spent on cleaning and preprocessing the data. We’ve all heard that right? So it only makes sense we find ways to automate the pre-processing and cleaning as much as we can.

Scikit-Learn pipelines are composed of steps , each of which has to be some kind of transformer except the last step which can be a transformer or an estimator such as a machine learning model. When I say transformer , I mean transformers such as the Normalizer, StandardScaler or the One Hot Encoder to name a few. But say, what if before I use any of those, I wanted to write my own custom transformer not provided by Scikit-Learn that would take the weighted average of the 3rd, 7th and 11th columns in my dataset with a weight vector I provide as an argument ,create a new column with the result and drop the original columns? In addition to doing that and most importantly what if I also wanted my custom transformer to seamlessly integrate with my existing Scikit-Learn pipeline and its other transformers? Sounds great and lucky for us Scikit-Learn allows us to do that.

Inheritance in Python

To understand how we can write our own custom transformers with scikit-learn, we first have to get a little familiar with the concept of inheritance in Python. All transformers and estimators in scikit-learn are implemented as Python classes , each with their own attributes and methods. So every time you write Python statements like these -

You are essentially creating an instance called ‘one_hot_enc’ of the class ‘OneHotEncoder’ using its class constructor and passing it the argument ‘False’ for its parameter ‘sparse’. The OneHotEncoder class has methods such as ‘fit’, ‘transform’ and fit_transform’ and others which can now be called on our instance with the appropriate arguments as seen here.

In order for our custom transformer to be compatible with a scikit-learn pipeline it must be implemented as a class with methods such as fit, transform, fit_transform, get_params , set_params so we’re going to write all of those…… or we can simply just code the kind of transformation we want our transformer to apply and inherit everything else from some other class!

To understand the concept of inheritance in Python, take a look at this lego evolution of Boba Fett below.

Say I want to write a class that looks like the lego on the right end. I could very well start from the very left, build my way up to it writing all of my own methods and such. However, what if I could start from the one just behind the one I am trying to make. I would not have to start from scratch, I would already have most of the methods that I need without writing them myself .I could just add or make changes to it till I get to the finished class that does what I need it to do. Wouldn’t that be great? Well that’s exactly what inheritance allows us to do.

This concept will become clearer as we write our own transformers below. If you want to get a little more familiar with classes and inheritance in Python before moving on, check out these links below.

So by now you might be wondering, well that’s great! But where will I find these base classes that come with most of the methods I need to write my transformer class on top of? Fret not. Scikit-Learn provides us with two great base classes, TransformerMixin and BaseEstimator. Inheriting from TransformerMixin ensures that all we need to do is write our fit and transform methods and we get fit_transform for free. Inheriting from BaseEstimator ensures we get get_params and set_params for free. Since the fit method doesn’t need to do anything but return the object itself, all we really need to do after inheriting from these classes, is define the transform method for our custom transformer and we get a fully functional custom transformer that can be seamlessly integrated with a scikit-learn pipeline! Easy.

Illustration

The dataset I’m going to use for this illustration can be found on Kaggle via this link.

The goal of this illustration is to go through the steps involved in writing our own custom transformers and pipelines to pre-process the data leading up to the point it is fed into a machine learning algorithm to either train the model or make predictions. There may very well be better ways to engineer features for this particular problem than depicted in this illustration since I am not focused on the effectiveness of these particular features. The goal of this illustration to familiarize the reader with the tools they can use to create transformers and pipelines that would allow them to engineer and pre-process features anyway they want and for any dataset , as efficiently as possible.

Let’s get started then.

This dataset contains a mix of categorical and numerical independent variables which as we know will need to pre-processed in different ways and separately. This means that initially they’ll have to go through separate pipelines to be pre-processed appropriately and then we’ll combine them together. So the first step in both pipelines would have to be to extract the appropriate columns that need to be pushed down for pre-processing.

The syntax for writing a class and letting Python know that it inherits from one or more classes is pictured below since for any class we write, we get to inherit most of it from the TransformerMixin and BaseEstimator base classes.

Below is the code for our first custom transformer called FeatureSelector. The transform method for this constructor simply extracts and returns the pandas dataset with only those columns whose names were passed to it as an argument during its initialization.

As you can see, we put BaseEstimator and TransformerMixin in parenthesis while declaring the class to let Python know our class is going to inherit from them. Like all the constructors we’re going to write , the fit method only needs to return self. The transform method is what we’re really writing to make the transformer do what we need it to do. In this case it simply means returning a pandas data frame with only the selected columns.

Now that the constructor that will handle the first step in both pipelines has been written, we can write the transformers that will handle other steps in their appropriate pipelines, starting with the pipeline that will handle the categorical features.

Categorical Pipeline

Below is a list of features our custom transformer will deal with and how, in our categorical pipeline.

date : The dates in this column are of the format ‘YYYYMMDDT000000’ and must be cleaned and processed to be used in any meaningful way. The constructor for this transformer will allow us to specify a list of values for the parameter ‘use_dates’ depending on if we want to create a separate column for the year, month and day or some combination of these values or simply disregard the column entirely by passing in an empty list. By not hard coding the specifications for this feature, we give ourselves the ability to try out different combinations of values whenever we want without having to rewrite code.

: The dates in this column are of the format ‘YYYYMMDDT000000’ and must be cleaned and processed to be used in any meaningful way. The constructor for this transformer will allow us to specify a list of values for the parameter ‘use_dates’ depending on if we want to create a separate column for the year, month and day or some combination of these values or simply disregard the column entirely by passing in an empty list. By not hard coding the specifications for this feature, we give ourselves the ability to try out different combinations of values whenever we want without having to rewrite code. waterfront : Wether the house is waterfront property or not. Convert to binary — Yes or No

: Wether the house is waterfront property or not. Convert to binary — Yes or No view : How many times the house has been viewed. Most of the values are 0. The rest are very thinly spread between 1 and 4. Convert to Binary — Yes or No

: How many times the house has been viewed. Most of the values are 0. The rest are very thinly spread between 1 and 4. Convert to Binary — Yes or No yr_renovated : The year the house was renovated in. Most of the values are 0, presumably for never while the rest are very thinly spread between some years. Convert to Binary — Yes or No

Once all these features are handled by our custom transformer in the aforementioned way, they will be converted to a Numpy array and pushed to the next and final transformer in the categorical pipeline. A simple scikit-learn one hot encoder which returns a dense representation of our pre-processed data. Below is the code for our custom transformer.

Numerical Pipeline

Below is a list of features our custom numerical transformer will deal with and how, in our numerical pipeline.

bedrooms : Number of bedrooms in the house. Pass as it is.

: Number of bedrooms in the house. Pass as it is. bathrooms : Number of bathrooms in the house. The constructor for this transformer will have a parameter ‘bath_per_bead’ that takes in a Boolean value. If True, then the constructor will create a new column by computing bathrooms/bedrooms to calculate the number of bathrooms per bedroom and drop the original bathroom column. If False, then it will just pass the bathroom column as it is.

: Number of bathrooms in the house. The constructor for this transformer will have a parameter ‘bath_per_bead’ that takes in a Boolean value. If True, then the constructor will create a new column by computing bathrooms/bedrooms to calculate the number of bathrooms per bedroom and drop the original bathroom column. If False, then it will just pass the bathroom column as it is. sqft_living : Size of the living area of the house in square feet. Pass as it is.

: Size of the living area of the house in square feet. Pass as it is. sqft_lot : Total size of the lot in square feet. Pass as it is.

: Total size of the lot in square feet. Pass as it is. floors : Number of floors in the house. Pass as it is.

: Number of floors in the house. Pass as it is. condition : Discrete variable describing the condition of the house with values from 1–5. Pass as it is.

: Discrete variable describing the condition of the house with values from 1–5. Pass as it is. grade : Overall grade given to the housing unit, based on King County grading system with values from 1–13. Pass as it is.

: Overall grade given to the housing unit, based on King County grading system with values from 1–13. Pass as it is. sqft_basement : Size of the basement in the house in square feet if any. 0 for houses that don’t have basements. Pass as it is.

: Size of the basement in the house in square feet if any. 0 for houses that don’t have basements. Pass as it is. yr_built : The year the house was built in. The constructor for this transformer will have another parameter ‘years_old’ that also takes in a Boolean value. If True, then the constructor will create a new column by computing the age of the house in 2019 by the subtracting the year it was built in from 2019 and it will drop the original yr_built column. If False, then it will just pass the yr_built column as it is.

Once all these features are handled by our custom numerical transformer in the numerical pipeline as mentioned above, the data will be converted to a Numpy array and passed to the next step in the numerical pipeline, an Imputer which is another kind of scikit-learn transformer. The Imputer will compute the column-wise median and fill in any Nan values with the appropriate median values. From there the data would be pushed to the final transformer in the numerical pipeline, a simple scikit-learn Standard Scaler. Below is the code for the custom numerical transformer.

Combining the pipelines together

Now that we’ve written our numerical and categorical transformers and defined what our pipelines are going to be, we need a way to combine them, horizontally. We can do that using the FeatureUnion class in scikit-learn. We can create a feature union class object in Python by giving it two or more pipeline objects consisting of transformers. Calling the fit_transform method for the feature union object pushes the data down the pipelines separately and then results are combined and returned. In our case since the first step for both of our pipelines is to extract the appropriate columns for each pipeline, combining them using feature union and fitting the feature union object on the entire dataset means that the appropriate set of columns will be pushed down the appropriate set of pipelines and combined together after they are transformed! Isn’t that awesome?

I didn’t even tell you the best part yet. It will parallelize the computation for us! That’s right, it’ll transform the data in parallel and put it back together! So it will be most likely be faster than any script that deals with this kind of preprocessing linearly where it’s most likely a little more work to parallelize it. We don’t have to worry about doing that manually anymore. Our FeatureUnion object will take care of that as many times as we want. All we have to do is call fit_transform on our full feature union object. Below is the code that creates both pipelines using our custom transformers and others and then combines them together.

Now you might have noticed that I didn’t include any machine learning models in the full pipeline. The reason for that is that I simply can’t. The FeatureUnion object takes in pipeline objects containing only transformers. A machine learning model is an estimator. The workaround for that is I can make another Pipeline object , and pass my full pipeline object as the first step and add a machine learning model as the final step. The full preprocessed dataset which will be the output of the first step will simply be passed down to my model allowing it to function like any other scikit-learn pipeline you might have written! Here’s the code for that.

We simply fit the pipeline on an unprocessed dataset and it automates all of the preprocessing and fitting with the tools we built. The appropriate columns are split , then they’re pushed down the appropriate pipelines where they go through 3 or 4 different transformers each (7 in total!) with arguments we decide on and the the pre-processed data is put back together and pushed down the model for training! Calling predict does the same thing for the unprocessed test data frame and returns the predictions! Here’s a simple diagram I made that shows the flow for our machine learning pipeline.

Simple flow diagram for our pipeline

In addition to fit_transform which we got for free because our transformer classes inherited from the TransformerMixin class, we also have get_params and set_params methods for our transformers without ever writing them because our transformer classes also inherit from class BaseEstimator.

These methods will come in handy because we wrote our transformers in a way that allows us to manipulate how the data will get preprocessed by providing different arguments for parameters such as use_dates, bath_per_bed and years_old. Just using simple product rule, that’s about 108 parameter combinations I can try for my data just for the preprocessing part! Which I can set using set_params without ever re-writing a single line of code. Since this pipeline functions like any other pipeline, I can also use GridSearch to tune the hyper-parameters of whatever model I intend to use with it!

There you have it. Now you know how to write your own fully functional custom transformers and pipelines on your own machine to automate handling any kind of data , the way you want it using a little bit of Python magic and Scikit-Learn. There is obviously room for improvement , such as validating that the data is in the form you expect it to be , coming from the source before it ever gets to the pipeline and giving the transformers the ability to handle and report unexpected errors. However , just using the tools in this article should make your next data science project a little more efficient and allow you to automate and parallelize some tedious computations.

If there is anything that I missed or something was inaccurate or if you have absolutely any feedback , please let me know in the comments. I would greatly appreciate it. Thank you.","['custom', 'transformer', 'pass', 'column', 'transformers', 'python', 'ml', 'house', 'data', 'pipelines', 'values', 'pipeline']","How you can use inheritance and sklearn to write your own custom transformers and pipelines for machine learning preprocessing80% of the total time spent on most data science projects is spent on cleaning and preprocessing the data.
Inheritance in PythonTo understand how we can write our own custom transformers with scikit-learn, we first have to get a little familiar with the concept of inheritance in Python.
Below is the code for our first custom transformer called FeatureSelector.
Below is the code that creates both pipelines using our custom transformers and others and then combines them together.
Now you know how to write your own fully functional custom transformers and pipelines on your own machine to automate handling any kind of data , the way you want it using a little bit of Python magic and Scikit-Learn.",en,['Sam T'],2019-03-03 02:25:14.738000+00:00,"{'Data Pipeline', 'Machine Learning', 'Data Processing', 'Data Preparation'}","{'https://miro.medium.com/max/2400/1*Lw2NOHs4Zx62vlEwBSVmZg.jpeg', 'https://miro.medium.com/max/60/1*UMhBkEJQQZs6Ur3fv0Dplg.png?q=20', 'https://miro.medium.com/max/610/1*eroPf-aNxt7fJ_oOpOC-AQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*eroPf-aNxt7fJ_oOpOC-AQ.png?q=20', 'https://miro.medium.com/max/3840/1*UMhBkEJQQZs6Ur3fv0Dplg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/160/160/1*abvNEVzV5Sy2gmkDoN4gXg.jpeg', 'https://miro.medium.com/max/60/1*Lw2NOHs4Zx62vlEwBSVmZg.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/992/1*b0rUb-3fH6bpvwVpHrcFUQ.png', 'https://miro.medium.com/max/1200/1*Lw2NOHs4Zx62vlEwBSVmZg.jpeg', 'https://miro.medium.com/max/56/1*b0rUb-3fH6bpvwVpHrcFUQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*abvNEVzV5Sy2gmkDoN4gXg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:23:58.976003,6.332610607147217
https://towardsdatascience.com/automl-for-data-enthusiasts-30582b660cda,AutoML for Data Enthusiasts,"Could AutoML (automated machine learning) be the answer to allow anyone with the desire (a Data Enthusiast) to create machine learning models?

Who is a Data Enthusiast?

the software developer who wants to try machine learning

the college student with aspirations to become a Data Scientist

the mid-level corporate Manager looking to up their game

the Analyst looking to differentiate their skillset

many others…

AutoML saves a Data Scientist loads of time by automating data preparation, feature extraction, model selection, and model tuning. But does it make it machine learning easily available to the Data Enthusiasts?

The answer is it really depends on the AutoML tool, in which there is a wide array of user-friendly and not so user-friendly tools.

The Challenge

If you are not a Data Scientist and just want to try out machine learning (and hey, you may not even write code) I’m calling you a Data Enthusiast. Looking at the chart below, there are a lot more Data Enthusiasts than actual Data Scientists (20 to 1 ratio).

The reality is, of the 4 million Data Enthusiasts, some also hold awesome domain knowledge, and if equipped with the right tools can accelerate machine learning efforts for an organization.

Potential Solution

AutoML tools are typically geared for one of the following (this article focuses on #2):

help current Data Scientists become more productive by automating part of the model building process give Data Enthusiasts the ability to creating machine learning models

Not every AutoML tool is the answer as most tools focus on #1 or #2. Looking at the comparison of 8 AutoML tools below, you see half have rating of “red” in terms of Ease of Use, which wouldn’t be good candidates for a Data Enthusiast.

Comparison of AutoML Tools for a Data Enthusiast

There are many ways to carve up the plethora of tools of AutoML packages (including MLaaS services), but I wanted to focus on three important considerations for Data Enthusiasts.

Ease of Use (can you create without writing code)

Budget (have free & premium versions). Trial <> Free.

Customization (e.g. interactive data manipulation tools, model delivery options)

This comparison of eight AutoML tools is really a mix of a growing list of AutoML & MLaaS solutions available today. Included are some of the large enterprise players such as AWS ML, H2O, and DataRobot. Also included are your open source powerhouses such as Auto-Weka, Auto-Sklearn, and TPOT. The group was rounded out by promising tools built for Data Enthusiasts with PredictiveData and MLJar.

When comparing the performance of 4 of 8 of the AutoML tools evaluated, here is how they stack up on the titanic dataset.

Results sourced from https://hackernoon.com/a-brief-overview-of-automatic-machine-learning-solutions-automl-2826c7807a2a

Summary

AutoML is showing tremendous advancements in making machine learning easier, and the tools are rapidly improving. These should bring us closer to tapping into the power of 4 million Data Enthusiasts, as long as you can find the right balance of ease of use, budget, and customization.","['machine', 'answer', 'automl', 'tools', 'learning', 'userfriendly', 'data', 'model', 'enthusiasts', 'looking']","Could AutoML (automated machine learning) be the answer to allow anyone with the desire (a Data Enthusiast) to create machine learning models?
But does it make it machine learning easily available to the Data Enthusiasts?
Looking at the chart below, there are a lot more Data Enthusiasts than actual Data Scientists (20 to 1 ratio).
Comparison of AutoML Tools for a Data EnthusiastThere are many ways to carve up the plethora of tools of AutoML packages (including MLaaS services), but I wanted to focus on three important considerations for Data Enthusiasts.
interactive data manipulation tools, model delivery options)This comparison of eight AutoML tools is really a mix of a growing list of AutoML & MLaaS solutions available today.",en,['Josh Janzen'],2019-04-16 01:17:50.095000+00:00,"{'Machine Learning', 'Analytics', 'Data Science'}","{'https://miro.medium.com/max/60/1*1uVDPl-qLni8bfyt3O5EPg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*iJt6PKKpvbnBFDYo6tDsfw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1000/1*1uVDPl-qLni8bfyt3O5EPg.png', 'https://miro.medium.com/fit/c/96/96/2*H5PwXPyIqPfSrg0blE-iMg.png', 'https://miro.medium.com/max/964/1*iJt6PKKpvbnBFDYo6tDsfw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/2*H5PwXPyIqPfSrg0blE-iMg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2000/1*1uVDPl-qLni8bfyt3O5EPg.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2400/1*K6xCZbCuEu34O6TCvgOryA.png', 'https://miro.medium.com/max/60/1*K6xCZbCuEu34O6TCvgOryA.png?q=20'}",2020-03-05 00:24:05.624860,6.64885687828064
https://towardsdatascience.com/practical-tips-for-class-imbalance-in-binary-classification-6ee29bcdb8a7,Practical tips for class imbalance in binary classification,"0. Introduction and motivation

Binary classification problem is arguably one of the simplest and most straightforward problems in Machine Learning. Usually we want to learn a model trying to predict whether some instance belongs to a class or not. It has many practical applications ranging from email spam detection to medical testing (determine if a patient has a certain disease or not).

Slightly more formally, the goal of binary classification is to learn a function f(x) that map x (a vector of features for an instance/example) to a predicted binary outcome ŷ (0 or 1). Most classification algorithms, such as logistic regression, Naive Bayes and decision trees, output a probability for an instance belonging to the positive class: Pr(y=1|x).

Class imbalance is the fact that the classes are not represented equally in a classification problem, which is quite common in practice. For instance, fraud detection, prediction of rare adverse drug reactions and prediction gene families (e.g. Kinase, GPCR). Failure to account for the class imbalance often causes inaccurate and decreased predictive performance of many classification algorithms. In this post, I will introduce a couple of practical tips on how to combat class imbalance in binary classification, most of which can be easily adapted to multi-class scenarios.","['tips', 'prediction', 'practical', 'problem', 'class', 'imbalance', 'learn', 'classification', 'detection', 'instance', 'binary']","Introduction and motivationBinary classification problem is arguably one of the simplest and most straightforward problems in Machine Learning.
Slightly more formally, the goal of binary classification is to learn a function f(x) that map x (a vector of features for an instance/example) to a predicted binary outcome ŷ (0 or 1).
Class imbalance is the fact that the classes are not represented equally in a classification problem, which is quite common in practice.
Failure to account for the class imbalance often causes inaccurate and decreased predictive performance of many classification algorithms.
In this post, I will introduce a couple of practical tips on how to combat class imbalance in binary classification, most of which can be easily adapted to multi-class scenarios.",en,['Zichen Wang'],2019-01-24 01:51:56.816000+00:00,"{'TensorFlow', 'Scikit Learn', 'Machine Learning', 'Classification', 'Tutorial'}","{'https://miro.medium.com/fit/c/96/96/0*EQVueUMtlxPIK3Lr.', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*14XgN-u__K8ZA95pmingyw.jpeg?q=20', 'https://miro.medium.com/max/1422/1*SR5XqPqYsr5XzGkKP_b6DQ.png', 'https://miro.medium.com/max/60/1*SR5XqPqYsr5XzGkKP_b6DQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/974/0*cUNL0PuZTSnONE9H', 'https://miro.medium.com/max/60/1*hrM1rZpDfLCJShfzm8yL-Q.png?q=20', 'https://miro.medium.com/max/1176/1*q-UQsXD7Emu_a5KmnE-Pkw.png', 'https://miro.medium.com/max/314/0*223gmnaDe5iEOU0g', 'https://miro.medium.com/max/373/1*hmwwssqDTmFJvI3eY_E-5w.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/782/1*hrM1rZpDfLCJShfzm8yL-Q.png', 'https://miro.medium.com/max/60/1*q-UQsXD7Emu_a5KmnE-Pkw.png?q=20', 'https://miro.medium.com/max/746/1*hmwwssqDTmFJvI3eY_E-5w.png', 'https://miro.medium.com/max/60/0*cUNL0PuZTSnONE9H?q=20', 'https://miro.medium.com/max/1852/1*14XgN-u__K8ZA95pmingyw.jpeg', 'https://miro.medium.com/fit/c/160/160/0*EQVueUMtlxPIK3Lr.', 'https://miro.medium.com/max/60/1*MAzxZx0EDTLEHLTPn9USjQ.png?q=20', 'https://miro.medium.com/max/60/1*L58AdsfXZViiFFBLVjTZMw.png?q=20', 'https://miro.medium.com/max/4444/1*L58AdsfXZViiFFBLVjTZMw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/788/1*MAzxZx0EDTLEHLTPn9USjQ.png', 'https://miro.medium.com/max/60/1*hmwwssqDTmFJvI3eY_E-5w.png?q=20', 'https://miro.medium.com/max/60/0*223gmnaDe5iEOU0g?q=20'}",2020-03-05 00:24:07.934415,2.3095550537109375
https://medium.com/neo4j/neo4j-graph-visualization-like-a-pro-18651963ebd4,Visualizing Neo4j Database Contents Like a Pro!,"And this is what the app will look like when we use npm start or yarn start to compile, serve, and run the code in our browser:

Boring — but it works!

You will agree with me that this is not really an outstanding visualization, yet, and what we really want to do is inspect our Neo4j database contents!

Introducing Cypher and Bolt

In order to connect Neo4j with yFiles for HTML, we first add the JavaScript Bolt driver to our application.

On the console we do:

> yarn add neo4j-driver

Then in the code we simply load the Neo4j driver by adding the following import statement at the top of the file:

import neo4j from 'neo4j-driver/lib/browser/neo4j-web'

Make sure you have the credentials and IP address available for the Neo4j database that you would like to explore, because with the next line of code, we can already connect to the database:

const neo4jDriver = neo4j.driver(""bolt://1.2.3.4"",

neo4j.auth.basic(""theusername"", ""theS3cr3t""))

(Of course you will need to adjust the IP address, the user name and the password.)

We are now going to simplify the scaffolded code first that yeoman created for us. Strip it down so that it looks like this:

This will result in a boring empty graph view, because we only queried the database for the nodes, but we are not doing anything with the results, yet. At this point it is important to understand that the core yFiles for HTML library does not know how to interpret the results of a query returned by the Neo4j Bolt connector. However there are utility convenience classes that will help us with the import of any kind of structured data that we have access to in our JavaScript environment. The data could be fetched from a file, from the network, via REST, WebSockets, or created dynamically on the client. Of course this works with the Neo4j data, too: All we need to have is some understanding of the way the Neo4j response data is structured.

For the simple query above the resulting JavaScript object looks like this:

{

""records"": [{

""keys"": [""node""],

""length"": 1,

""_fields"": [{

""identity"": {

""low"": 0,

""high"": 0

},

""labels"": [""Movie""],

""properties"": {

""title"": ""The Matrix"",

""tagline"": ""Welcome to the Real World"",

""released"": {

""low"": 1999,

""high"": 0

}

}

}

],

""_fieldLookup"": {

""node"": 0

}

}

We can use the following Bolt JavaScript API to obtain an array of all the nodes queried:

const nodes = nodeResult.records.map(record => record.get(""node""))

Now that we have the nodes, we can use the convenience class GraphBuilder from yFiles to get the graph onto the screen:

graphBuilder.nodesSource = nodes

graphBuilder.buildGraph()

That was easy! But the result still leaves a lot to be desired from a visual and interactive perspective:

That still doesn’t look exactly overwhelming…

Let’s improve that: we assign a nicer style for the node visualization and a simple label mapping: If the node has a property named ""title"" or ""name"" we use this for the label. And instead of leaving all nodes at the default location, we apply an automatic layout algorithm to arrange the nodes nicely on the screen.

The graph looks nicer, now, but we still don’t see any relations.

No relations, yet. So still not better than a list.

Let’s change that with another query to the database! We query all the edges between the nodes that we have queried before using this code:

This is what a result looks like in JSON form:

{

""identity"": {

""low"": 7,

""high"": 0

},

""start"": {

""low"": 8,

""high"": 0

},

""end"": {

""low"": 0,

""high"": 0

},

""type"": ""ACTED_IN"",

""properties"": {

""roles"": [""Emil""]

}

}

As you can see, the relationships reference their source and target nodes via the identity property that was attached to the nodes. We can use the toString function provided by Bolt, to obtain a representation of the identities that can be consumed easily by GraphBuilder . We tell the helper class to use this information to resolve the connections end points and build the in-memory graph for the display:

Putting it all together this is what we have now in our loading function:

And this is what the visualization looks like in the browser:

Finally, we begin to see some structure

We still need to work a little on the interactivity, the layout, and the visualization. Right now, we are hardly any better than most other tools. Let’s say we want to show some details in a tool-tip, let the user select elements and react to clicks in the graph. For this we add the so-called GraphViewerInputMode to our component:

const inputMode = new GraphViewerInputMode()

graphComponent.inputMode = inputMode

This enables the user to select elements, pan around conveniently, and use the keyboard to navigate the graph. We can now also easily specify rich tool-tips for our elements: For this we use the information that the database provided us with and we list all the properties in a very simple HTML list:

And in order to better understand the structure of the graph, we add the option for the user to center the layout around a given item that she double-clicks:

That’s it. Check out the final result, again:

Much better! Neo4j database visualized using trivial yFiles for HTML code

I guess you can see that with only very little customizations you are now able to adjust the Cypher queries to your specific databases and create rich, interactive, visualizations of your data!

That’s it already for this very short introduction on how to use yFiles for HTML to visualize your Neo4j databases. You will find a commented version of the sources for this demo on GitHub.

What next?

Of course there is a lot more functionality that has not yet been covered in this post. Take a look at the many online demos for yFiles for HTML to get an idea what else you could do with the yFiles libraries and Neo4j!

I leave it as an exercise for you to improve the visualization to look like this 😉:

After polishing the UI, this is what a diagram can look like.

And to give you more ideas about the possibilities, with some coding, yFiles for HTML, and Neo4j you can:

So what are you waiting for? Go and visualize your Neo4j database like a pro! Happy diagramming!","['graph', 'looks', 'neo4j', 'yfiles', 'nodes', 'pro', 'visualizing', 'contents', 'visualization', 'database', 'look', 'html', 'low']","You will agree with me that this is not really an outstanding visualization, yet, and what we really want to do is inspect our Neo4j database contents!
Introducing Cypher and BoltIn order to connect Neo4j with yFiles for HTML, we first add the JavaScript Bolt driver to our application.
The graph looks nicer, now, but we still don’t see any relations.
That’s it already for this very short introduction on how to use yFiles for HTML to visualize your Neo4j databases.
Go and visualize your Neo4j database like a pro!",en,['Sebastian Müller'],2018-05-29 12:11:38.957000+00:00,"{'JavaScript', 'Data Visualization', 'Diagramming', 'Database', 'Neo4j'}","{'https://miro.medium.com/max/1998/1*IKQqfAqFv6zzrR2QACjcLA.png', 'https://miro.medium.com/max/56/1*7bbqmVTkliOEysWdLZbxUw.png?q=20', 'https://miro.medium.com/max/60/1*QPv6OKOSYSyWZnXH0eQSrA.png?q=20', 'https://miro.medium.com/max/650/1*7bbqmVTkliOEysWdLZbxUw.png', 'https://miro.medium.com/max/60/1*IKQqfAqFv6zzrR2QACjcLA.png?q=20', 'https://miro.medium.com/max/60/1*a7MIJSWkvJOkPhHQ_JBtvA.png?q=20', 'https://miro.medium.com/max/60/1*SYoq5U9eK8ucNa2CPXPU6A.png?q=20', 'https://miro.medium.com/max/60/1*hwQeieQ49NH7Vnd8BFpdCQ.png?q=20', 'https://miro.medium.com/max/180/1*VcXxX-LCX9H7Qpt8uL6Kfw.png', 'https://miro.medium.com/fit/c/80/80/0*topUTNdjQnA5--iL.jpeg', 'https://miro.medium.com/fit/c/160/160/1*kDtqhvHY-ANL4yq4XetGOw.png', 'https://miro.medium.com/fit/c/80/80/0*5IlZxeGNaJHoLkFj', 'https://miro.medium.com/max/60/1*SS9z5apeVJK5WdWvicy1EA.png?q=20', 'https://miro.medium.com/max/1970/1*hwQeieQ49NH7Vnd8BFpdCQ.png', 'https://miro.medium.com/fit/c/96/96/1*DnRk76vA8GSREsNKnXgwDQ.png', 'https://miro.medium.com/max/1166/1*UaAuGL-Qbaby7VGSYGvjvA.png', 'https://miro.medium.com/max/985/1*hwQeieQ49NH7Vnd8BFpdCQ.png', 'https://miro.medium.com/max/60/1*UaAuGL-Qbaby7VGSYGvjvA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*DnRk76vA8GSREsNKnXgwDQ.png', 'https://miro.medium.com/max/960/1*SS9z5apeVJK5WdWvicy1EA.png', 'https://miro.medium.com/max/2518/1*SYoq5U9eK8ucNa2CPXPU6A.png', 'https://miro.medium.com/max/2328/1*QPv6OKOSYSyWZnXH0eQSrA.png', 'https://miro.medium.com/fit/c/80/80/0*hmhjbwkprwEOimDT.jpg', 'https://miro.medium.com/max/1982/1*a7MIJSWkvJOkPhHQ_JBtvA.png'}",2020-03-05 00:24:08.976462,1.0420472621917725
https://towardsdatascience.com/catching-that-flight-visualizing-social-network-with-networkx-and-basemap-ce4a0d2eaea6,Catching that flight: Visualizing social network with Networkx and Basemap,"Process airports and routes datasets:

Networkx cannot read the data in its raw form, so our first job is to process the data to acquire a clean dataframe of routes that could be read by Networkx. Here, we use pandas to parse Excel files into dataframes ,extract and process the information. Notice how the two dataset are connected by the code of the airport (the three letter IATA code). You can find the full code to process the data in my source code here.

The aim of our data-processing step is to acquire the following two Panda dataframes:

A condensed routes_us data frame where each row represents a unique air route and the total number of airlines operate on that route (for example, there are 3 airlines that operate the flight from Lehigh Valley Airport (ABE) to Atlanta International Airport (ATL).

Table 3: a condensed and cleaned routes dataframe. This dataframe is used by Networkx to construct the graph network with nodes and edges

A condensed position dataframe where each row represent each USA airports with IATA code, and more importantly, longitudinal and latitudinal details

Table 4: a condensed and cleaned position of airport dataset. This dataframe is used by Basemap to plot the nodes (airports) correctly on a USA map

With the former dataframe, we are ready to draw our very first sketch of the flight networks.

At first, we translated our dataframe into a graph. Notice that our graph is a directed graph, that is, a graph with a set of vertices connected by edges having directions associated with them. This means that in our graph, the two routes JFK-ATL and ATL-JFK are distinct since even though they are connecting the same 2 nodes, the two routes have different (opposite) directions.

We use Networkx’s from_panda_dataframe() function to quickly import our graph. Here we create a graph from our dataframe routes_us, where the source is ‘Source Airport’ column, the target is ‘Dest Airport’ column using a Directed Graph model. edge_attr means that we can add information to the edges of the graph. I have added the number of airlines operated on a route as the edge attribute

graph = nx.from_pandas_dataframe(routes_us, source = 'Source Airport', target = 'Dest Airport', edge_attr = 'number of flights',create_using = nx.DiGraph())

Networkx does have a graphical tool that we can use to draw our network.

plt.figure(figsize = (10,9)) nx.draw_networkx(graph) plt.savefig(""./images/map_0.png"", format = ""png"", dpi = 300) plt.show()

Graph drawn by Networkx’s default draw network function

The problem with this rough network is that we really cannot tell which airport is which and how routes are related to one another. Maybe it is a better idea to plot the airport in the exact geographical position in a American map. How do we do that? Aha, Basemap !!!","['graph', 'network', 'code', 'condensed', 'airport', 'source', 'routes', 'process', 'catching', 'networkx', 'route', 'basemap', 'data', 'visualizing', 'social', 'dataframe', 'flight']","Notice how the two dataset are connected by the code of the airport (the three letter IATA code).
You can find the full code to process the data in my source code here.
Table 3: a condensed and cleaned routes dataframe.
Here we create a graph from our dataframe routes_us, where the source is ‘Source Airport’ column, the target is ‘Dest Airport’ column using a Directed Graph model.
I have added the number of airlines operated on a route as the edge attributegraph = nx.from_pandas_dataframe(routes_us, source = 'Source Airport', target = 'Dest Airport', edge_attr = 'number of flights',create_using = nx.DiGraph())Networkx does have a graphical tool that we can use to draw our network.",en,['Tuan Doan Nguyen'],2018-08-17 17:17:34.117000+00:00,"{'Data Science', 'Machine Learning', 'Data Analysis', 'Towards Data Science', 'Data Visualisation'}","{'https://miro.medium.com/max/1778/1*YlXltm97gCECX_MKdyX8Tg.png', 'https://miro.medium.com/max/60/0*mzc5Yy6U5u8Laz6d.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*HarGUpN_2PMmcdbk.PNG?q=20', 'https://miro.medium.com/max/60/0*Mjwz1BOsEup6U-8t.png?q=20', 'https://miro.medium.com/max/778/0*HarGUpN_2PMmcdbk.PNG', 'https://miro.medium.com/max/2214/1*sBdrHBYJrLsW3UhE3kbQRA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*TBSuZUdUCm3E1R1QYcTXMQ.jpeg', 'https://miro.medium.com/max/7090/1*dNiI9O7McPsLx9Gw2mr39w.png', 'https://miro.medium.com/max/4000/0*uCGOw6ih4XqzJCUP.png', 'https://miro.medium.com/max/60/0*mPNMA9CJuIIL2Ao2.PNG?q=20', 'https://miro.medium.com/max/60/1*YlXltm97gCECX_MKdyX8Tg.png?q=20', 'https://miro.medium.com/max/60/1*sBdrHBYJrLsW3UhE3kbQRA.png?q=20', 'https://miro.medium.com/max/60/0*x97fZ_xLPQXMa-Tq.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2560/1*TBSuZUdUCm3E1R1QYcTXMQ.jpeg', 'https://miro.medium.com/max/4000/0*Mjwz1BOsEup6U-8t.png', 'https://miro.medium.com/max/60/1*TBSuZUdUCm3E1R1QYcTXMQ.jpeg?q=20', 'https://miro.medium.com/max/60/0*uCGOw6ih4XqzJCUP.png?q=20', 'https://miro.medium.com/max/1532/0*mPNMA9CJuIIL2Ao2.PNG', 'https://miro.medium.com/max/4000/0*x97fZ_xLPQXMa-Tq.png', 'https://miro.medium.com/max/60/1*dNiI9O7McPsLx9Gw2mr39w.png?q=20', 'https://miro.medium.com/max/60/1*54CQLeTmdsW2LX4aShOfQg.jpeg?q=20', 'https://miro.medium.com/max/4000/0*mzc5Yy6U5u8Laz6d.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/1*L1EL0fXxRsX-XDiKirqkbg.jpeg', 'https://miro.medium.com/fit/c/96/96/1*L1EL0fXxRsX-XDiKirqkbg.jpeg', 'https://miro.medium.com/max/4116/1*54CQLeTmdsW2LX4aShOfQg.jpeg'}",2020-03-05 00:24:16.473540,7.496042013168335
https://medium.com/neo4j/py2neo-v4-2bedc8afef2,Py2neo v4: The Next Generation,"As both a busy dad and the team lead of the Neo4j Drivers Team, the amount of spare time I’ve had to work on personal projects over the past couple of years has been somewhat reduced. But with all the interest by Python users in keeping Py2neo going, I couldn’t resist starting work on it again. So I’m delighted to finally be able to deliver a brand new major release with a number of shiny new features and improvements. Please welcome Py2neo v4!

As usual you can find the library at the Python Package Index and can install it with pip (pip install py2neo). Take a look at the comprehensive documentation and if you encounter any problems, shout out in the #neo4j-python channel in the neo4j-users Slack.

Py2neo now wraps the 1.6 release of the official Python driver, which takes care of all the low-level, nitty-gritty, binary-winery(?) things a database driver needs to handle. This allows Py2neo to focus on higher-level features and proper pythonic API and integrations.

Grabbing a Graph

As with every previous version of Py2neo, the main way into the library is through the Graph. The constructor can accept a range of settings and the code behind this has now been completely overhauled to fix several former issues, such as a failure to recognise custom port numbers. The default protocol is also now Bolt, rather than HTTP, and the hard requirement on HTTP has been completely removed.

So, to get connected, simply create a Graph object:

>>> from py2neo import Graph

>>> graph = Graph(""bolt://myserver:7687"", auth=(""neo4j"", ""psswrd""))

Along with a regular connection URI, the full set of settings accepted by the Graph constructor is as follows:

auth - a 2-tuple of (user, password)

user

password

secure (boolean flag)

scheme

host

port

user_agent

API Overview

Py2neo exposes several logical layers of API on top of the official Python driver. The lowest level Cypher API provides Cypher execution facilities very similar to those in the driver, but with a few extras such as coercion to a Table object:

>>> graph.run(""MATCH (a:Person)

RETURN a.name, a.born LIMIT 3"").to_table() a.name | a.born

--------------------|--------

Laurence Fishburne | 1961

Hugo Weaving | 1960

Lilly Wachowski | 1967 >>> graph.evaluate(""MATCH (a:Person) RETURN count(a)"") 142

The next level up, the Entity API, wraps Cypher in convenience functions that provide a full set of CRUD operations on Node and Relationship objects. This can make for clearer application code at the expense of fine-grained control. The NodeMatcher, for example, constructs and executes a Cypher MATCH statement and returns Node objects:

>>> [(a[""name""], a[""born""])

for a in graph.nodes.match(""Person"").limit(3)] [('Laurence Fishburne', 1961),

('Hugo Weaving', 1960),

('Lilly Wachowski', 1967)]

Other Entity API methods include Graph.create, Graph.delete and Graph.merge (as well as similar transactional variants). Note that Graph.merge has now been completely rewritten to use Cypher’s UNWIND clause internally. This addresses some previous performance issues for the method when used at scale.

The topmost level of API is Py2neo’s OGM API. This allows creation of GraphObjects that wrap nodes in native classes and provide attributes to model their relationships and properties.

>>> from py2neo.ogm import GraphObject, Property >>> class Person(GraphObject):

... name = Property()

... born = Property()

...

>>> [(a.name, a.born) for a in Person.match(graph).limit(3)] [('Laurence Fishburne', 1961),

('Hugo Weaving', 1960),

('Lilly Wachowski', 1967)]

More about Matchers

The old py2neo.selection module has been renamed to py2neo.matching and the NodeSelector is now called NodeMatcher. There’s also a new RelationshipMatcher, which is an evolution of the old Graph.match method implementation.

A NodeMatcher offers a DSL that can be used to locate nodes which fulfil a specific set of criteria. Typically, a single node can be identified passing a specific label and property key-value pair. However, any number of labels and any condition supported by the Cypher WHERE clause is allowed.

For a simple match by label and property use the match method:

>>> graph.nodes.match(""Person"", name=""Keanu Reeves"").first() (_224:Person {born:1964,name:""Keanu Reeves""})

For a more comprehensive match using Cypher expressions, the where method can be used to further refine the selection. Here, the underscore character can be used to refer to the nodes being filtered:

>>> list(matcher.match(""Person"").where(""_.name =~ 'K.*'"")) [(_57:Person {born: 1957, name: 'Kelly McGillis'}),

(_80:Person {born: 1958, name: 'Kevin Bacon'}),

(_83:Person {born: 1962, name: 'Kelly Preston'}),

(_224:Person {born: 1964, name: 'Keanu Reeves'}),

(_226:Person {born: 1966, name: 'Kiefer Sutherland'}),

(_243:Person {born: 1957, name: 'Kevin Pollak'})]

Orders and limits can also be applied:

>>> list(matcher.match(""Person"").where(""_.name =~ 'K.*'"")

.order_by(""_.name"").limit(3))

[(_224:Person {born: 1964, name: 'Keanu Reeves'}),

(_57:Person {born: 1957, name: 'Kelly McGillis'}),

(_83:Person {born: 1962, name: 'Kelly Preston'})]

And if only a count of matched entities is required, the length of a match can be evaluated:

>>> len(matcher.match(""Person"").where(""_.name =~ 'K.*'"")) 6

The underlying query is only evaluated when the selection undergoes iteration or when a specific evaluation method is called (such as with first). This means that a NodeMatch instance may be reused before and after data changes for different results.

Relationship matching is similar:

>>> keanu = graph.nodes.match(""Person"", name=""Keanu Reeves"").first() >>> list(graph.relationships.match((keanu, None), ""ACTED_IN"")

.limit(3)) [(Keanu Reeves)-[:ACTED_IN {roles: ['Neo']}]->(_6),

(Keanu Reeves)-[:ACTED_IN {roles: ['Neo']}]->(_158),

(Keanu Reeves)-[:ACTED_IN {roles: ['Julian Mercer']}]->(_151)]

And lastly, the Node and Relationship objects received can be reused, along with new instances, for further operations. Note that Relationship objects are now always dynamic subclasses of the Relationship base class and can be created via those subclasses:

>>> mary_poppins = Node(""Movie"", title=""Mary Poppins"") >>> ACTED_IN = Relationship.type(""ACTED_IN"") >>> graph.create(ACTED_IN(keanu, mary_poppins)) >>> graph.match((keanu, mary_poppins)).first() (Keanu Reeves)-[:ACTED_IN {}]->(_189)

Reporting, Analytics and Data Science Integrations

Version 4 brings some new opportunities for reporting and data analysis as well as integration with several popular data science libraries.

The new Table class provides methods for multiple styles of output, including Github-flavored markdown, HTML, CSV and TSV. It also has a _repr_html_ method attached, which allows results to be rendered elegantly in Jupyter.","['graph', 'cypher', 'generation', 'used', 'py2neo', 'born', 'api', 'method', 'v4', 'property', 'match', 'keanu']","But with all the interest by Python users in keeping Py2neo going, I couldn’t resist starting work on it again.
As usual you can find the library at the Python Package Index and can install it with pip (pip install py2neo).
Py2neo now wraps the 1.6 release of the official Python driver, which takes care of all the low-level, nitty-gritty, binary-winery(?)
Grabbing a GraphAs with every previous version of Py2neo, the main way into the library is through the Graph.
>>> from py2neo.ogm import GraphObject, Property >>> class Person(GraphObject):... name = Property()... born = Property()...>>> [(a.name, a.born) for a in Person.match(graph).limit(3)] [('Laurence Fishburne', 1961),('Hugo Weaving', 1960),('Lilly Wachowski', 1967)]More about MatchersThe old py2neo.selection module has been renamed to py2neo.matching and the NodeSelector is now called NodeMatcher.",en,['Nigel Small'],2018-06-08 12:50:35.087000+00:00,"{'Cypher', 'Python', 'Jupyter', 'Announcements', 'Neo4j'}","{'https://miro.medium.com/max/1600/1*GpqU7LPXBSovdvSuSgQYzg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*aouX4skvmW7qS0TTe1RcbQ.jpeg', 'https://miro.medium.com/max/180/1*VcXxX-LCX9H7Qpt8uL6Kfw.png', 'https://miro.medium.com/fit/c/80/80/0*topUTNdjQnA5--iL.jpeg', 'https://miro.medium.com/fit/c/96/96/1*aouX4skvmW7qS0TTe1RcbQ.jpeg', 'https://miro.medium.com/max/60/0*hpGYPPlG8QqYMRK0?q=20', 'https://miro.medium.com/fit/c/80/80/0*hmhjbwkprwEOimDT.jpg', 'https://miro.medium.com/max/2280/0*hpGYPPlG8QqYMRK0', 'https://miro.medium.com/fit/c/160/160/1*kDtqhvHY-ANL4yq4XetGOw.png', 'https://miro.medium.com/max/800/1*GpqU7LPXBSovdvSuSgQYzg.jpeg', 'https://miro.medium.com/max/52/1*GpqU7LPXBSovdvSuSgQYzg.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*5IlZxeGNaJHoLkFj'}",2020-03-05 00:24:17.459904,0.9863646030426025
https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998,An introduction to Grid Search,"This article will explain in simple terms what grid search is and how to implement grid search using sklearn in python.

What is grid search?

Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.

Why should I use it?

If you work with ML, you know what a nightmare it is to stipulate values for hyper parameters. There are libraries that have been implemented, such as GridSearchCV of the sklearn library, in order to automate this process and make life a little bit easier for ML enthusiasts.

How does it work?

Here’s a python implementation of grid search using GridSearchCV of the sklearn library.

from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVR gsc = GridSearchCV(

estimator=SVR(kernel='rbf'),

param_grid={

'C': [0.1, 1, 100, 1000],

'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],

'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]

},

cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)

First, we need to import GridSearchCV from the sklearn library, a machine learning library for python. The estimator parameter of GridSearchCV requires the model we are using for the hyper parameter tuning process. For this example, we are using the rbf kernel of the Support Vector Regression model(SVR). The param_grid parameter requires a list of parameters and the range of values for each parameter of the specified estimator. The most significant parameters required when working with the rbf kernel of the SVR model are c, gamma and epsilon . A list of values to choose from should be given to each hyper parameter of the model. You can change these values and experiment more to see which value ranges give better performance. A cross validation process is performed in order to determine the hyper parameter value set which provides the best accuracy levels.

grid_result = gsc.fit(X, y)

best_params = grid_result.best_params_ best_svr = SVR(kernel='rbf', C=best_params[""C""], epsilon=best_params[""epsilon""], gamma=best_params[""gamma""],

coef0=0.1, shrinking=True,

tol=0.001, cache_size=200, verbose=False, max_iter=-1)

We then use the best set of hyper parameter values chosen in the grid search, in the actual model as shown above.

If you want to know how to use grid search and cross-validation and train and test a model check out this article.

Hope this article helped you. Until next time…Adios!

References","['introduction', 'grid', 'hyper', 'sklearn', 'process', 'search', 'model', 'parameter', 'values', 'gridsearchcv', 'using']","This article will explain in simple terms what grid search is and how to implement grid search using sklearn in python.
What is grid search?
Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model.
Here’s a python implementation of grid search using GridSearchCV of the sklearn library.
If you want to know how to use grid search and cross-validation and train and test a model check out this article.",en,[],2019-01-05 14:57:49.334000+00:00,"{'Beginner', 'Machine Learning', 'Sklearn', 'Tutorial'}","{'https://miro.medium.com/fit/c/80/80/1*-pV8MMqetli5oQhovupPrg.jpeg', 'https://miro.medium.com/max/60/0*HWBspyXRFFJRXRG4?q=20', 'https://miro.medium.com/max/60/1*iUkbA8Dlj-5B0S8u0oRNbQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*ewcxmwIHC3RQwRqB.', 'https://miro.medium.com/fit/c/96/96/2*7e18TMZ5PyqmFatGl5Mnog.png', 'https://miro.medium.com/max/1224/1*iUkbA8Dlj-5B0S8u0oRNbQ.png', 'https://miro.medium.com/max/612/1*iUkbA8Dlj-5B0S8u0oRNbQ.png', 'https://miro.medium.com/max/432/1*fr3BFYQ7ZiodLBbY1emz9g.png', 'https://miro.medium.com/fit/c/160/160/1*2mBCfRUpdSYRuf9EKnhTDQ.png', 'https://miro.medium.com/max/1400/0*HWBspyXRFFJRXRG4', 'https://miro.medium.com/fit/c/160/160/2*7e18TMZ5PyqmFatGl5Mnog.png', 'https://miro.medium.com/fit/c/80/80/0*030HOaJEWhVa-6K_.'}",2020-03-05 00:24:18.839000,1.3790955543518066
https://medium.com/janis/how-to-make-your-manychat-bots-smarter-with-dialogflow-part-2-e0cf1d881bd4,How to use Dialogflow to understand your users and create engaging responses,"How to use Dialogflow to understand your users and create engaging responses

Learn how to take advantage of Google’s artificial intelligence to understand user input and delight your users with responses.

For the complete guide on how to build AI-enhanced Manychat bots click here

Dialogflow intents have an area to create a response. You can respond directly in Dialogflow with a text response and your message will be delivered to Messenger, or you can redirect your response back to Manychat and respond with a Flow.

Responding from Dialogflow","['create', 'manychat', 'engaging', 'understand', 'respond', 'user', 'users', 'responses', 'dialogflow', 'response', 'responseslearn', 'text']","How to use Dialogflow to understand your users and create engaging responsesLearn how to take advantage of Google’s artificial intelligence to understand user input and delight your users with responses.
For the complete guide on how to build AI-enhanced Manychat bots click hereDialogflow intents have an area to create a response.
You can respond directly in Dialogflow with a text response and your message will be delivered to Messenger, or you can redirect your response back to Manychat and respond with a Flow.
Responding from Dialogflow",en,['Josh Barkin'],2019-04-06 19:35:21.313000+00:00,"{'AI', 'Manychat', 'Artificial Intelligence', 'Bots', 'Chatbots'}","{'https://miro.medium.com/max/60/0*kw-XilbCQKMjHXJK.png?q=20', 'https://miro.medium.com/max/2464/0*cvXXYcWK2tXoi7EI.png', 'https://miro.medium.com/max/60/0*_odzbGvpNX7z5utZ.png?q=20', 'https://miro.medium.com/max/2296/0*S0UUYp6QLR935qJt.png', 'https://miro.medium.com/max/2312/0*dySkGW-0lmyh3pnp.png', 'https://miro.medium.com/max/60/0*zohuutS6i4zVYwQQ.png?q=20', 'https://miro.medium.com/max/414/1*tisk1aw8khKr-LOaiVTQYA.png', 'https://miro.medium.com/max/2142/0*KVa1VO6sd6tj9pwt.png', 'https://miro.medium.com/max/2462/0*Abq0G304RB5orKSL.png', 'https://miro.medium.com/max/60/0*dySkGW-0lmyh3pnp.png?q=20', 'https://miro.medium.com/max/2352/0*_odzbGvpNX7z5utZ.png', 'https://miro.medium.com/max/60/1*6YPgZJVT1orJUN_oRsnjfw.png?q=20', 'https://miro.medium.com/max/2452/0*kw-XilbCQKMjHXJK.png', 'https://miro.medium.com/max/60/0*U5iZgy8WpfZjVzXu.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*8wpBo44Q9cdu2pBCk7tExQ.jpeg', 'https://miro.medium.com/max/60/0*cvXXYcWK2tXoi7EI.png?q=20', 'https://miro.medium.com/max/60/0*GLdMRheU0Jtu9OAN.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*E16EL-Atz9zUVWd52ngwAg.png', 'https://miro.medium.com/max/60/0*KVa1VO6sd6tj9pwt.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*8wpBo44Q9cdu2pBCk7tExQ.jpeg', 'https://miro.medium.com/max/60/0*yGo6P2n3aqZKtaA4.png?q=20', 'https://miro.medium.com/max/2880/1*6YPgZJVT1orJUN_oRsnjfw.png', 'https://miro.medium.com/fit/c/80/80/1*8wpBo44Q9cdu2pBCk7tExQ.jpeg', 'https://miro.medium.com/max/2386/0*GLdMRheU0Jtu9OAN.png', 'https://miro.medium.com/max/1200/1*6YPgZJVT1orJUN_oRsnjfw.png', 'https://miro.medium.com/max/2466/0*zohuutS6i4zVYwQQ.png', 'https://miro.medium.com/max/60/0*Abq0G304RB5orKSL.png?q=20', 'https://miro.medium.com/max/60/0*S0UUYp6QLR935qJt.png?q=20', 'https://miro.medium.com/max/3096/0*yGo6P2n3aqZKtaA4.png', 'https://miro.medium.com/max/2408/0*U5iZgy8WpfZjVzXu.png'}",2020-03-05 00:24:20.680206,1.8401806354522705
https://medium.com/@cigen_rpa/overview-of-robotic-process-automation-in-the-healthcare-industry-b5c91069e6e0,Overview of Robotic Process Automation in the Healthcare Industry,"According to a report published by the United States Census Bureau in 2016, a new demographic trend has emerged. By 2020, people over the age of 65 will outnumber children under 5. And by 2050, the percentage of people 65 and over will be more than twice the percentage of young children. This so-called “demographic earthquake” is expected to start taking effect on the global labour market.

This means, among others, that the number of people who need medical assistance is also on the rise. The healthcare industry must find the resources to adapt to this new demographic trend. Robotic process automation in healthcare should thus be taken full advantage of, in the attempt to provide adequate medical care to a larger number of people.

The positive effects of automation in the healthcare industry extend beyond this aspect. Additionally, it can make a significant contribution to cutting down costs and to providing better customer care. It is important to note that these are precisely the healthcare priorities mentioned by three quarters of hospital CEOs, according to a HealthLeaders report from 2013.

The benefits of using robotic process automation to streamline processes is not anything new, while that between streamlined processes, efficiency and reduced costs is commonsensical. If you think about the expected growth of 6.5% for health care costs, according to UiPath, and about the fact that aKPMG study shows that robotic process automation can increase savings up to 50%, then you should definitely consider the capacity of automation to bolster cost reduction.

One other way that RPA might contribute to savings is by more accurately identifying the health care claims that do not meet the requirements (around 30–40%, according to the abovementioned study), and thus recovering a lot of money that might have otherwise gone unpaid.

Moreover, if back-office, routine tasks are passed on to software robots, this would allow doctors to use their human skills in order to better address their patients’ needs instead of filling in consistently names, dates and addresses. Robotic process automation might facilitate customer service in health care.

According to the statistics available in McKinsey Quarterly, 36% of the healthcare tasks — mostly, managerial and back-office — are amenable to automation. So let us now lay out these growth and efficiency opportunities that robotic process automation could bring to healthcare.

Robotic Process Automation in healthcare means increased operational efficiency

The benefits of robotic process automation in healthcare mean increased operational efficiency, productivity and cost-savviness. In fact, there is more than this quantitative approach to the issue; we should not forget the qualitative dimension of RPA in healthcare. For instance, Peter B. Nichol also makes the point that “the conversation has expanded beyond cost reduction to quality, engagement, and innovation”. But let us first spell out the basics, with our eyes on data from the HfS Blueprint Report.

1. The KPMG report we mentioned earlier advocates a large scope of RPA use for the whole of a hospital’s revenue cycle because of the suitability of automation for structured data, with specified parameter sets. The revenue cycle includes administrative and clinical functions necessary for the management of patients’ accounts, from preregistration to bill payment.

Functions of the revenue cycle such as new patient appointment requests, patient pre-arrival and arrival, or claim denials are particularly well suited for automation. Just think about how costly errors like faulty data entries may be! So then why not go for an error-free partner, like RPA promises to be? More, RPA can facilitate compliance with the large number of healthcare regulations, which is vital when dealing with claims denials.

[Click to read more…]","['overview', 'industry', 'report', 'according', 'process', 'revenue', 'number', 'rpa', 'healthcare', 'robotic', 'automation', 'care']","Robotic process automation in healthcare should thus be taken full advantage of, in the attempt to provide adequate medical care to a larger number of people.
The benefits of using robotic process automation to streamline processes is not anything new, while that between streamlined processes, efficiency and reduced costs is commonsensical.
Robotic process automation might facilitate customer service in health care.
So let us now lay out these growth and efficiency opportunities that robotic process automation could bring to healthcare.
Robotic Process Automation in healthcare means increased operational efficiencyThe benefits of robotic process automation in healthcare mean increased operational efficiency, productivity and cost-savviness.",en,['Cigen Rpa'],2018-04-13 09:35:38.535000+00:00,"{'Enterprise Technology', 'Automation', 'Rpa', 'Robotics', 'Healthcare'}","{'https://miro.medium.com/fit/c/80/80/0*qQgmseBN1U5wZ7UL.jpg', 'https://miro.medium.com/max/1920/1*22X5ZZcmURn7tc9QSsEejA.jpeg', 'https://miro.medium.com/max/960/1*bA3r8iCS-29QVpvjxEgCYw.jpeg', 'https://miro.medium.com/max/60/1*bA3r8iCS-29QVpvjxEgCYw.jpeg?q=20', 'https://miro.medium.com/max/60/1*22X5ZZcmURn7tc9QSsEejA.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/2*xEqAMfze3rhtQKBYi5dc4w.jpeg', 'https://miro.medium.com/fit/c/80/80/1*YySvv8hL2kCQmYqFKM3vxw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*Lg0-IpOa_Vy9coUhvvRN8Q.jpeg', 'https://miro.medium.com/max/1920/1*bA3r8iCS-29QVpvjxEgCYw.jpeg', 'https://miro.medium.com/fit/c/96/96/2*xEqAMfze3rhtQKBYi5dc4w.jpeg'}",2020-03-05 00:24:21.768801,1.0885956287384033
https://medium.com/google-cloud/use-cases-and-a-few-different-ways-to-get-files-into-google-cloud-storage-c8dce8f4f25a,Use Cases and Different Ways to get Files Into Google Cloud Storage,"Including AppEngine with Firebase Resumable Uploads

For this article I will break down down a few different ways to interact with Google Cloud Storage (GCS). The GCP docs state the following ways to upload your data: via the UI, via the gsutil CLI tool, or via JSON API in various languages. I’ll go over a few specific use cases and approaches for the ingress options to GCS below.

1. upload form on Google App Engine (GAE) using the JSON api

Use case: public upload portal (small files)

2. upload form with firebase on GAE using the JSON api

Use case: public upload portal (large files), uploads within mobile app

3. gsutil command line integrated with scripts or schedulers like cron

Use case: backups/archive, integration with scripts, migrations

4. S3 / GCP compatible file management programs such as Cyberduck

Use case: cloud storage management via desktop, migrations

5. Cloud function (GCF)

Use case: Integration, changes in buckets, HTTP requests

6. Cloud console

Use case: cloud storage management via desktop, migrations

1. App Engine nodejs with JSON API for smaller files

You can launch a small nodejs app on GAE for accepting smaller files directly to GCS ~20MB pretty easily. I started with the nodejs GCS sample for GAE on the GCP github account here.

This is a nice solution for integrating uploads around 20MB. Just remember the nginx servers behind GAE have a file upload limit. So if you try and upload something say around 50MB, you’ll receive an nginx error: ☹️

Nginx file upload error

You can try and upload the file size limit in the js file but still the web servers behind GAE will have a limit for file uploads. So, if you plan to create an upload form on App Engine, be sure to have a file size limitation in your UI.

Nodejs upload form for small files — I’ll likely take this app down at some point.

2. App Engine nodejs Firebase with JSON API and Resumable uploads for large files

Since the previous example only works for smaller files, I wondered how can we solve for uploading larger files say 100MB or 1GB? I started with the nodejs app engine storage example here.

After attempting to use resumable uploads in GCS API with TUS and failing I enlisted help from my friend Nathan @ www.incline.digital to help with another approach.

With the help of Nathan we integrated resumable uploads with firebase SDK. Code can be found here

https://github.com/mkahn5/gcloud-resumable-uploads.

Reference: https://firebase.google.com/docs/storage/web/upload-files

User interaction with the Firebase powered GAE Upload Form

While not very elegant with no status bar or anything fancy this solution does work for uploading large files from the web. 🙌🏻

Resumable File upload form on GAE — I’ll likely take this app down at some point.

GCS in the Firebase UI

3. gsutil from local or remote

gsutil makes it easy to copy files to and from cloud storage buckets

Just make sure you have the google cloud sdk on your workstation or remote server (https://cloud.google.com/sdk/downloads), set project and authenticate and thats it.

mkahnucf@meanstack-3-vm:~$ gsutil ls

gs://artifacts.testing-31337.appspot.com/

gs://staging.testing-31337.appspot.com/

gs://testing-31337-public/

gs://testing-31337.appspot.com/

gs://us.artifacts.testing-31337.appspot.com/

gs://vm-config.testing-31337.appspot.com/

gs://vm-containers.testing-31337.appspot.com/ mkahnucf@meanstack-3-vm:~/nodejs-docs-samples/appengine/storage$ gsutil cp app.js gs://testing-31337-public Copying file://app.js [Content-Type=application/javascript]... / [1 files][ 2.7 KiB/ 2.7 KiB] Operation completed over 1 objects/2.7 KiB.

More details here.

gsutil makes it just easy to automate backup of directories, sync changes in directories, backup database dumps, and easily integrate with apps or schedulers for scripted file uploads to GCS.

Below is the rsync cron I have for my cloud storage bucket and the html files on my blog. This way I have consistency between my GCS bucket and my GCE instances if I decide to upload a file via www or via GCS UI.

Using gsutil on GCP to backup and sync GCS files

root@mkahncom-instance-group-multizone-kr5q:~# crontab -l */2 * * * * gsutil rsync -r /var/www/html gs://mkahnarchive/mkahncombackup */2 * * * * gsutil rsync -r gs://mkahnarchive/mkahncombackup /var/www/html

4. Cyberduck (MacOS) or any application with an s3 interface

Enjoy an client ftp type experience with Cyberduck on MacOS for GCS.

Cyberduck has very nice oauth integration for connecting to the GCS API built into the interface.

After authenticating with oauth you can browse all of your buckets and upload to them via the cyberduck app. Nice option to have for moving many directories or folders into multiple buckets.

More info on CyberDuck here.

5. Cloud Function

You can also configure a Google Cloud Function (GCF) to upload files to GCS from a remote or local location. This tutorial below is just for uploading files in a directory to GCS. Run the cloud function and it zips a local directory files and puts the zip into the GCS stage bucket.

Try the tutorial:

https://cloud.google.com/functions/docs/tutorials/storage

Michaels-iMac:gcf_gcs mkahnimac$ gcloud beta functions deploy helloGCS -stage-bucket mike-kahn-functions -trigger-bucket mikekahn-public-upload

Copying file:///var/folders/kq/5kq2pt090nx3ghp667nwygz80000gn/T/tmp6PXJmJ/fun.zip [Content-Type=application/zip]…

- [1 files][ 634.0 B/ 634.0 B]

Operation completed over 1 objects/634.0 B.

Deploying function (may take a while — up to 2 minutes)…done.

availableMemoryMb: 256

entryPoint: helloGCS

eventTrigger:

eventType: providers/cloud.storage/eventTypes/object.change

resource: projects/_/buckets/mikekahn-public-upload

latestOperation: operations/bWlrZS1rYWhuLXBlcnNvbmFsL3VzLWNlbnRyYWwxL2hlbGxvR0NTL1VFNmhlY1RZQV9j

name: projects/mike-kahn-personal/locations/us-central1/functions/helloGCS

serviceAccount: mike-kahn-personal@appspot.gserviceaccount.com

sourceArchiveUrl: gs://mike-kahn-functions/us-central1-helloGCS-wghzlmkeemix.zip

status: READY

timeout: 60s

updateTime: ‘2017–05–31T03:08:05Z’

You can also use cloud functions created to display bucket logs. Below shows a file uploaded via my public upload form and deleted via the console ui. This could be handy for pub/sub notifications or for reporting.

Michaels-iMac:gcf_gcs mkahnimac$ gcloud beta functions logs read helloGCS LEVEL NAME EXECUTION_ID TIME_UTC LOG D helloGCS 127516914299587 2017-05-31 03:46:19.412 Function execution started

I helloGCS 127516914299587 2017-05-31 03:46:19.502 File FLIGHTS BANGKOK.xlsx metadata updated.

D helloGCS 127516914299587 2017-05-31 03:46:19.523 Function execution took 113 ms, finished with status: 'ok'

D helloGCS 127581619801475 2017-05-31 18:31:00.156 Function execution started

I helloGCS 127581619801475 2017-05-31 18:31:00.379 File FLIGHTS BANGKOK.xlsx deleted.

D helloGCS 127581619801475 2017-05-31 18:31:00.478 Function execution took 323 ms, finished with status: 'ok'

Cloud Functions can come in handy for background tasks like regular maintenance from events on your GCP infrastructure or from activity on HTTP applications. Check out the how-to guides for writing and deploying cloud functions here.

6. Cloud Console UI

The UI works well for GCS administration. GCP even has a transfer service for files on S3 buckets on AWS or other s3 buckets elsewhere. One thing that is lacking in the portal currently would be object lifecycle management. This is nice for automated archiving to coldline cheaper object storage for infrequently accessed files or files over a certain age in buckets. For now you can only modify object lifecycle via gsutil or via API. Like most GCP features they start at the function/API level then make their way into that portal (the way it should be IMO) and I’m fine with that. I expect object lifecycle rules to be implemented into the GCP portal at some point in the future. 😃

GCS UI

In summary I’ve used a few GCP samples and tutorials that are available to display to different ways to get files onto GCS. GCS is flexible with many ingress options that can be integrated into systems or applications quite easily! In 2017 the use cases for object storage are abundant and GCP makes it easy to send and receive files in GCS.

Leave a comment for any interesting use cases for GCS that I may have missed or that we should explore. Thanks!

Check my blog for more updates.","['file', 'different', 'files', 'gcs', 'cloud', 'gcp', 'gsutil', 'upload', 'ways', 'storage', 'cases', 'hellogcs', 'google', 'app']","Including AppEngine with Firebase Resumable UploadsFor this article I will break down down a few different ways to interact with Google Cloud Storage (GCS).
S3 / GCP compatible file management programs such as CyberduckUse case: cloud storage management via desktop, migrations5.
Cloud consoleUse case: cloud storage management via desktop, migrations1.
Below is the rsync cron I have for my cloud storage bucket and the html files on my blog.
Cloud FunctionYou can also configure a Google Cloud Function (GCF) to upload files to GCS from a remote or local location.",en,['Mike Kahn'],2017-06-07 21:36:43.945000+00:00,"{'Google Cloud Platform', 'Google App Engine', 'Google Compute Engine', 'Google Cloud Storage', 'Cloud Computing'}","{'https://miro.medium.com/max/1208/1*LPgri-Sm3j-wo6L38OoUBw.png', 'https://miro.medium.com/max/60/1*b0Hm052Ci4fS4NOHsKEw5Q.png?q=20', 'https://miro.medium.com/max/2846/1*vanrzndxksuBxaJ5J0o5Mw.png', 'https://miro.medium.com/max/1230/1*Par6cTJvRYPhBSp5hyDgOQ.png', 'https://miro.medium.com/max/60/1*HpR3ju7gpEmlP2dRS5j1sg.png?q=20', 'https://miro.medium.com/max/392/1*HpR3ju7gpEmlP2dRS5j1sg.png', 'https://miro.medium.com/max/1076/1*6BVujZ7nbXnOsS4-MIQZ5Q.png', 'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/60/1*6BVujZ7nbXnOsS4-MIQZ5Q.png?q=20', 'https://miro.medium.com/max/1202/1*wK62p4CR8ZXFF8mhleCdeg.png', 'https://miro.medium.com/max/60/1*2n8gJvl98L9cjItVEJm0DQ.png?q=20', 'https://miro.medium.com/max/60/1*Par6cTJvRYPhBSp5hyDgOQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/60/1*sj039ndwBTelX8xntq6EWw.png?q=20', 'https://miro.medium.com/max/784/1*HpR3ju7gpEmlP2dRS5j1sg.png', 'https://miro.medium.com/fit/c/96/96/1*U4WHeMwopIvCAxtW8jPlZw.png', 'https://miro.medium.com/max/60/1*vanrzndxksuBxaJ5J0o5Mw.png?q=20', 'https://miro.medium.com/max/2202/1*sj039ndwBTelX8xntq6EWw.png', 'https://miro.medium.com/max/2688/1*b0Hm052Ci4fS4NOHsKEw5Q.png', 'https://miro.medium.com/max/60/1*wK62p4CR8ZXFF8mhleCdeg.png?q=20', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/fit/c/160/160/1*U4WHeMwopIvCAxtW8jPlZw.png', 'https://miro.medium.com/max/1536/1*2n8gJvl98L9cjItVEJm0DQ.png', 'https://miro.medium.com/max/60/1*LPgri-Sm3j-wo6L38OoUBw.png?q=20'}",2020-03-05 00:24:23.316735,1.547933578491211
https://medium.com/google-cloud/no-localhost-no-problem-using-google-cloud-shell-as-my-full-time-development-environment-22d5a1942439,No localhost? No problem! Using Google Cloud Shell as my full time development environment.,"I recently switched to a Chromebook full time (the awesome Pixelbook), and the first question I get asked is “How do you do any development work on it?”

While Chromebooks are getting more powerful local development (a la Crostini), I’ve actually been using Google Cloud Shell as my primary development environment. In fact, I haven’t opened a local shell for months now.

So far, the experience has been awesome, and I’ve learned a bunch along the way. I wanted to share some tips, here they are!

What is Google Cloud Shell

Cloud Shell is a free terminal that you can use for whatever you want, and it runs 100% in a web browser.

Click this link to open Cloud Shell: https://console.cloud.google.com/cloudshell

Seeing how most of my work requires the internet, I’m completely fine with being tied to a browser.

What does it come with?

Cloud Shell comes with most of the tools I use on a daily basis right out of the box. These include gcloud, node, kubectl, docker, go, python, git, vim, and more.

There is 5GB of persistent storage that is tied to your $HOME directory, and this is also completely free.

The other really nice thing is that you are automatically authenticated to the current Google Cloud project you’re working in. This makes setup super easy, everything just works!

Tips and Tricks

1. Running a web server (with auto-HTTPS for FREE!)

Most people developing cloud apps usually run a web server of some sort. Usually, you would run it locally and just type in “localhost” into your web browser to access it.

This is not possible with Cloud Shell, so the team created a neat “web preview” function that creates a URL on the fly to point to your local server.

You can open up any port from 2000 to 65000, and your web traffic will come through!

Warning: There is some transparent auth done behind the scenes, so the URL the Cloud Shell opens might not work if you give it to someone else. I’d recommend a tool like ngrok if you want to share your “localhost” connection remotely.

2. Get extra power with “boost mode”

By default, Cloud Shell runs on a g1-small VM, which can be under-powered for some tasks. You can easily upgrade to a n1-standard-1 by enabling “boost mode.” It’s like the TURBO button on an old PC, but this time it actually works :)

3. Edit your files with a GUI

Yes yes, vim and emacs and nano are great and all. But sometimes you just want a nice, comfortable GUI to work with.

Cloud Shell ships with a customized version of the Orion editor.

While its not as good as VS Code or Eclipse, its actually a fully featured editor and I feel quite productive with it!

4. Upload/Download files

If you have files locally you want to upload to cloud shell, just click the “Upload” button in the menu and choose your file.

To download files, run this inside Cloud Shell:

$ cloudshell dl <FILENAME>

And it will download the file!

5. Persist binary/program installations

Because Cloud Shell only persists your $HOME directory, if you install things that don’t come out of the box with Cloud Shell, chances are it will be gone the next time you use it.

If you install things with apt-get, there really is no good solution. However, if you are downloading the binary directly or are compiling from source, you can create a path in your $HOME directory (for example, /home/sandeepdinesh/bin) and add that to your PATH. Any binaries in this folder will run like normal, and they will be persisted between reboots.

Bonus: Open in Cloud Shell

If you have a git repository in a public place (like a public GitHub repo), you can actually create a link that will open the end user’s Cloud Shell and automatically clone the repo to their $HOME directory.

Because it is all free, it is a great way to get someone up and running with your project without having to worry about their local machine!

Conclusion

I’ve had a ton of success using Cloud Shell as my primary dev environment. If you are using a Chromebook or if you just want a free Linux shell that you can access from any browser, I’d definitely check out Cloud Shell.

There are a ton of new features coming down the line as well, I’ll be sure talk about them when they come out!","['come', 'web', 'shell', 'environment', 'cloud', 'local', 'run', 'development', 'actually', 'work', 'problem', 'localhost', 'google', 'using', 'open']","Click this link to open Cloud Shell: https://console.cloud.google.com/cloudshellSeeing how most of my work requires the internet, I’m completely fine with being tied to a browser.
Cloud Shell comes with most of the tools I use on a daily basis right out of the box.
Get extra power with “boost mode”By default, Cloud Shell runs on a g1-small VM, which can be under-powered for some tasks.
Persist binary/program installationsBecause Cloud Shell only persists your $HOME directory, if you install things that don’t come out of the box with Cloud Shell, chances are it will be gone the next time you use it.
ConclusionI’ve had a ton of success using Cloud Shell as my primary dev environment.",en,['Sandeep Dinesh'],2018-06-26 00:19:46.498000+00:00,"{'Coding', 'Google Cloud Platform', 'Development', 'Tools', 'Linux'}","{'https://miro.medium.com/max/60/1*unQ6jcZ3E0KFuAHm78ywDg.png?q=20', 'https://miro.medium.com/max/2832/1*unQ6jcZ3E0KFuAHm78ywDg.png', 'https://miro.medium.com/fit/c/96/96/1*Duqualbt7up5yFp39tlUgQ.jpeg', 'https://miro.medium.com/freeze/max/60/1*vG9zeb2ZI6XcWyxJPsrVuQ.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/1200/1*iTarHCEVG_vD3pWkQs4JHA.png', 'https://miro.medium.com/freeze/max/60/1*1WiC7L2mV_x16geFpwoYkQ.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/max/3596/1*luFiLc62ue2Unob4jVbwaA.gif', 'https://miro.medium.com/fit/c/160/160/1*Duqualbt7up5yFp39tlUgQ.jpeg', 'https://miro.medium.com/max/3596/1*6gUIHGIccf4BCdFS3Kx-UA.gif', 'https://miro.medium.com/max/3224/1*vG9zeb2ZI6XcWyxJPsrVuQ.gif', 'https://miro.medium.com/freeze/max/60/1*luFiLc62ue2Unob4jVbwaA.gif?q=20', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/freeze/max/60/1*6gUIHGIccf4BCdFS3Kx-UA.gif?q=20', 'https://miro.medium.com/max/2828/1*iTarHCEVG_vD3pWkQs4JHA.png', 'https://miro.medium.com/max/3584/1*1WiC7L2mV_x16geFpwoYkQ.gif', 'https://miro.medium.com/max/60/1*iTarHCEVG_vD3pWkQs4JHA.png?q=20'}",2020-03-05 00:24:24.962380,1.6456446647644043
https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c,Using Python and Auto ARIMA to Forecast Seasonal Time Series,"Hi! I’m Jose Portilla and I teach Python, Data Science and Machine Learning online to over 500,000 students! If you’re interested in learning more about how to do types of analysis and visualization in this blog posts, you can check out discount coupon links to my courses on Python for Data Science and Machine Learning and using Python for Financial Analysis and Algorithmic Trading.

Alright, on to the discussion of time series!

Time Series Basics

Let’s first discuss what a time series is and what it’s not. We’ll also talk about what kinds of time series are suitable for ARIMA based forecasting models.

Time Series data is experimental data that has been observed at different points in time (usually evenly spaced, like once a day). For example, the data of airline ticket sales per day is a time series. However, just because a series of events has a time element does not automatically make it a time series, such as the dates of major airline disasters, which are randomly spaced and are not time series. These types of random processes are known as point process.

Time Series have several key features such as trend, seasonality, and noise.

What we’ll be doing in this article is analyzing these features of a time series data set, and then seeing if we can use mathematical models to forecast into the future. We’ll also see how we can split our original time series data set to evaluate how well our model predicts the future.

Not every time series is suitable for forecasting, so don’t expect any get rich quick schemes on forecasting stock prices :)

Forecasting with ARIMA

“Prediction is very difficult, especially about the future”.

Forecasting is the process of making predictions of the future, based on past and present data. One of the most common methods for this is the ARIMA model, which stands for AutoRegressive Integrated Moving Average.

In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are labeled p,d,and q.

p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values. For example, forecasting that if it rained a lot over the past few days, you state its likely that it will rain tomorrow as well.

d is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series. You can imagine an example of this as forecasting that the amount of rain tomorrow will be similar to the amount of rain today, if the daily amounts of rain have been similar over the past few days.

q is the parameter associated with the moving average part of the model.

If our model has a seasonal component (we’ll show this in more detail later), we use a seasonal ARIMA model (SARIMA). In that case we have another set of parameters: P,D, and Q which describe the same associations as p,d, and q, but correspond with the seasonal components of the model.

The methods we will employ in this blog example will only take in data from a uni-variate time series. That means we really are only considering the relationship between the y-axis value the x-axis time points. We’re not considering outside factors that may be effecting the time series.

A common mistake beginners make is they immediately start to apply ARIMA forecasting models to data that has many outside factors, such as stock prices or a sports team’s performance. While ARIMA can be a powerful and relevant tool for times series related to those topics, if you only use it by itself and don’t account for outside factors, such as a CEO getting fired or an injury on the team, you won’t have good results. Keep this in mind as you begin to apply these concepts to your own data sets.","['forecast', 'set', 'forecasting', 'seasonal', 'arima', 'series', 'python', 'example', 'past', 'data', 'model', 'rain', 'auto', 'using']","Time Series data is experimental data that has been observed at different points in time (usually evenly spaced, like once a day).
What we’ll be doing in this article is analyzing these features of a time series data set, and then seeing if we can use mathematical models to forecast into the future.
We’ll also see how we can split our original time series data set to evaluate how well our model predicts the future.
One of the most common methods for this is the ARIMA model, which stands for AutoRegressive Integrated Moving Average.
If our model has a seasonal component (we’ll show this in more detail later), we use a seasonal ARIMA model (SARIMA).",en,['Jose Marcial Portilla'],2018-06-10 00:30:16.026000+00:00,"{'Data Science', 'Timeseries', 'Python', 'Science', 'Machine Learning'}","{'https://miro.medium.com/fit/c/80/80/1*0aQ8g-rQptZrhSI10etK3g.jpeg', 'https://miro.medium.com/max/54/1*dIrSiGBQ3HUZLIw54qqDxQ.jpeg?q=20', 'https://miro.medium.com/fit/c/96/96/1*Hh1e2iNuYLrqEpM015aImA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*A0bSgE6Sr2EjM8InArLY5g.jpeg', 'https://miro.medium.com/max/2730/1*SX5fwlvc79gxcOSfcUQz5Q.jpeg', 'https://miro.medium.com/max/780/1*HnCK6pg5sUFMjkTHRbECWg.png', 'https://miro.medium.com/fit/c/160/160/1*Hh1e2iNuYLrqEpM015aImA.jpeg', 'https://miro.medium.com/max/2700/1*dIrSiGBQ3HUZLIw54qqDxQ.jpeg', 'https://miro.medium.com/max/52/1*HnCK6pg5sUFMjkTHRbECWg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*suzWyqbb7WlgyZut', 'https://miro.medium.com/max/60/1*SX5fwlvc79gxcOSfcUQz5Q.jpeg?q=20', 'https://miro.medium.com/max/1200/1*SX5fwlvc79gxcOSfcUQz5Q.jpeg'}",2020-03-05 00:24:26.221466,1.2580859661102295
https://medium.com/fintechexplained/part-3-regression-analysis-bcfe15a12866,How Good Is My Predictive Model — Regression Analysis,"Forecasting is an important concept in econometric and data science. It is also widely used in artificial intelligence and risk management by quantitative analysts, statistical modellers and technologists. Additionally, a large number of studies are carried out to understand and predict variables and their future movements.

Please read disclaimer before proceeding.

Predictive analysis is a black art. There are many right answers.

What will we cover in this article?

This article focuses on the concept of regression analysis. In particular, I have divided the article into 6 parts.

Explanation Of Regression Analysis 3 Steps Of Regression Analysis Understanding Of Residuals Ordinary Least Square Explanation Overview Of Model Criterion

Let’s Start

One of the most famous methodologies to forecast a variable’s behaviour is to use regression analysis. This technique requires formulating a mathematical equation/model that can be used to give us an estimated value which is as close as possible to the actual observed value.

Regression Analysis

Regression analysis process is primarily used to explain relationships between variables and help us build a predictive model. Moreover, it can explain how changes in one variable can be used to explain changes in other variables. Regression analysis could be linear or non-linear.

Regression analysis is all about projecting a dependent variable on a set of one or more predetermined independent variables.

Understanding regression analysis is important when we want to model relationships between variables. It can help us understand how close our calculations are to reality. Regression analysis is a simple yet powerful technique.

Regression Analysis Applications

A large number of applications exist that utilise regression analysis:

Algorithms used in supervised machine learning for fitting a smoothing function to target data points and during fitting a decision boundary between target data points. In finance, regression analysis is used by analysts to understand markets, in particular what traders need to buy/sell to replace high exposure stocks. Regression analysis process helps analysts perform better sensitivity analysis where different values of independent variable are chosen and applied into some form of calculation to see how the values impact dependent variable. For instance, hedge funds can use regression analysis to see changes in interest rates and how bond price will charge. Regression analysis can be used in businesses whereby managers can send employees on training courses and use the regression analysis to see how training impacts productivity.

3 Steps Of Regression Analysis

To help us understand regression analysis, I will explain the concept of regression analysis in 3 easy to understand steps. Furthermore, within each step, I will attempt to explain key terms too:

Step 1: Set a goal

Let’s assume our goal is to understand the relationship between interest rates and EUR/USD exchange rate and how strongly correlated these two variables are.

Our dependent variable is: Interest Rates as it is dependent on EUR/USD exchange rates. Dependent variable is known as regressand in statistics.

Our independent variable is: EUR/USD exchange rates. Independent variable is known as regressor in statistics.

Step 2: Plot a scatter plot

Aim is to understand how interest rates move by changes in EUR To USD exchange rates.

Start by collecting data over a period of time, let’s say daily rate changes for 1 year. This data is known as training data.

Plot a scatter plot chart, where Y axis represents interest rates and X axis represents EUR to USD exchange rates. The scatter plot indicates the relationship between regressand and the regressor(s).

From the data, calculate mean of interest rates and draw a line, denoted as Ỹ (Y hat)

Draw a best fit line between interest rates and EUR to USD exchange rates data points. This could be an iterative task as the best fit line needs to minimise the difference between the line and the actual point.

Step 3: Formulate An Equation To Predict Dependent Variable

From the line, do you see a relationship? May be linear positive or negative relationship or may be non at all?

The best fit line is known as conditional expectation line. From the best fit line, we get three key components:

Intercept — where line crosses y axis — Known as ɑ (ALPHA) Slope — Tell us expected change in y over unit change in x — β (BETA) Distance between actual data point and the best fit data point — ε (RESIDUAL)

SLOPE is also known as Beta

Mathematically this relationship is represented as linear formula:

y at time t = Alpha + Beta at time t * x at time t + residual at time t.

The formula above states that forecast of y is conditional on x and an error term; whereby an error term is the difference between our best fit line and the actual value. It is the random component which our formula cannot predict and it is also known as residual.

The formula above is building foundation for risk management. The concepts are indicating that when the error terms are small then we have a very good regression fit. As a result, variable x can be used to explain variable y.

Example — Interest Rates Vs EUR To USD Exchange Rates

The chart below illustrates best fit line for Interest Rates vs EUR To USD exchange rates:

Y axis = Interest Rates

X axis = EUR To USD Exchange Rates

Y mean = 4.5

Each blue dot = Actual Value of Y

Red Line = Best fit line

Distance between Point On Red Line and Actual Value Of Y = Residual (Also the unexplained difference)

The line above states:

Intercept = 3.7 (When EUR/USD = 0)

Slope = 1.1 (Change in interest rates over change in EUR/USD rates)

Therefore:

Forecasted Value of Interest Rate = 3.7 + 1.1 x (EUR To USD Exchange Rate) + residual

We can now use any EUR to USD exchange rate to find value of interest rate.

Understanding Of Residuals

Residuals are noise terms. They are also known as error terms. Residuals indicate deviation of Y values from each expected value.



Residual tells us how good our model is against the actual value. No one knows what each residual value represents in true population but all statistician know that it exists.

Calculating the real values of intercept, slope and residual terms is complex in nature. Ordinary Least Square can help us formulate a better quality model.

Ordinary Least Square Explained

Finding the best fit line is iterative and complex in nature. We can employ Ordinary Least Square (OLS) methodology and rules to find the best fit line. OLS estimates the population beta and alpha by minimizing the sum of square residuals.

What Is OLS?

OLS is a method for estimating the unknown parameters in a linear regression model. The most suitable regression models are based on Ordinary Least Squares (OLS) Estimation.

There are 6 common OLS assumptions:

Errors are independent of x, have a constant variance and their mean is 0. Errors are uncorrelated with each other. Errors have a normal distribution. Large outliers are not observed in the data Relationship is linear between y and x(s) Model has not omitted any appropriate independent variable.

The best fit line is the one that minimises sum of squared differences between actual and estimated results. Taking average of minimum sum of squared difference is known as Mean Squared Error (MSE).

Smaller the value, better the regression model.

Pitfalls of Breaking OLS Assumptions:

Regression analysis model is based on strict restrictions of OLS. If we break these restrictions (which can happen in real stochastic nature of variables), we can then create downward biased coefficients.



In this part of the article, I will explain pitfalls of linear regression:

Specification error: This happens if we omit a variable that the true model is dependent on and if the omitted variable is correlated with the included variables. It can lead to biased estimated coefficients, which leads to estimation errors. Omitted variable bias is when omitted variable is correlated with the included regressor and is a determinant of the dependent variable. Autocorrelation: OLS assumes that there is no systematic correlation in the residuals. This can lead to biases in the standard error which can cause problems with inference. Efficiency Error: When our estimation ignores all available information. Heteroskedasticity: When residuals have changing variance across observations. In that case, it is best to check residuals, perform diagnostics and construct standard errors or may be provide an alternative specification. Multicollinearity: When regression is subject to multicollinearity then it means that the x variables are highly correlated e.g. using two currencies that are fixed to each other and it can lead to unreliable and unstable Beta. The solution is to remove the variables that are highly correlated. Calculated covariance of the regressors (x), if it is high than the regressors are highly correlated.

The pitfalls can lead to inconsistent, biased and inaccurate results.

How Do We Calculate Mean Squared Error

As the name implies, Mean Squared Error (MSE) is the mean of the residuals (error) squared. To calculate MSE, we need to calculate:

Explained Sum Of Squares (ESS) Sum Of Squared Residuals (SSR) Total Sum of Squares, (TSS)

Explained Sum Of Squares (ESS)

ESS is Explained or Estimated Sum Of Squares

Squaring(Y Estimated — Mean value of Y) and then sum all of the values.

Sum Of Squared Residuals (SSR)

SSR is sum of squared residuals. It is calculated by:

Squaring(Y Estimated — Actual Y Value) And then sum all values.

Total Sum of Squares (TSS)

TSS is simply total sum of all squares. It is calculated as ESS+SSR. This means total sum of squares is sum all squares of (y estimated — residual) + sum of squares of (y estimated — mean y)

Let’s understand the points above in greater detail:

First we need to subtract fitted values of y (y hat) from actual values of y. The difference is known as residual. Some of the values will be positive and the other values will be negative. Therefore square the residuals and then sum them. This is known as sum of squared residuals: SSR From the points of y, we can calculate the mean of y. Calculate the difference between value of best fit line and mean of Y. Square the differences and then sum the values. This final value is known as sum of squared deviations of y from its mean: ESS Add them both and you have explained and unexplained added to give you total sum of squares (TSS).

TSS = ESS + SSR

This is how we can forecast population y from estimated values of alpha, beta and residual.

Let’s measure quality of our regression model: R Squared

Once we know the size of residuals, we can start assessing how good our regression fit is. Regression fitness can be measured by R squared and adjusted R squared.



R-Squared: goodness of fit



Measures explained variation over total variation. Additionally, R squared is also known as coefficient of determination and it measures quality of fit.



Formula to calculate R squared is:

R squared = 1 — (SSR/TSS)

This means R squared = 1 — Variance (residual)/Variance (y)

Higher R squared, the better.

Let’s understand R squared with an example

Let’s say you want to hedge a future investment. You could choose a number of assets to find the asset that is correlated with your portfolio. Choose the asset with highest R squared because the asset that is most closely correlated with the asset you wish to hedge will have the least basis risk.



R squared measures the degree to which size of the errors is smaller than that of the original dependent variable of y.

Values Of R Squared

R squared of 0 means our best fit line is poor.

R squared value of 1 means our regression analysis is excellent and we could explain the relationship accurately.

Let’s understand that a bit:

If our best fit line to explain relationship between interest rates and EUR/USD exchange rate is poor then R square will be very small (close to 0). It will be small because SSR will be very large. As a result, R squared will be 0.

If the model is good then SSR will be 0 and R squared will be 1.

Correlation Coefficient: Square root of R square

Correlation between two variables can be calculated by taking square root of R.

Standard Error Of Regression (SER): SER measures variability of actual and estimated values of Y. It is the standard deviation of the residuals. It is calculated as

[Standard deviation of sample/Sqt(N)]

Now from R square, check if OLS conditions are met:

Is variance of residual constant (plot R squared).

Are residuals normally distributed?

R Squared Cons

R squared Has Its Own Problems:

Problem 1: R squared increases as you add more predictors to your model. For example, if you start adding more variables to predict a model then it will likely improve your model’s predictability and will start producing values close to actual values.

R squared increases as you add more predictors to your model. For example, if you start adding more variables to predict a model then it will likely improve your model’s predictability and will start producing values close to actual values. Problem 2: Too many predictors mean more random noise and can produce misleading R squared.

As, adding more independent variables, k, can increase the value of R, let’s introduce a better measure: Adjusted R Squared

Let’s measure quality of our regression model: Adjusted R Squared

R squared by itself is not good enough as it doesn’t take into account the number of variables that gave us the degree of determination. As a result, adjusted R squared is calculated. Let’s go over adjusted R squared metric.

Adjusted R Squared =

[ SSR / (n — K) ] / [ SST (n — 1) ]

Or:

Adjusted R Squared:

1 — [ [(n-1)/(n-k-1] x [1 — R squared]]

n = number of observations,

k = number of independent variables

Degree of freedom in our model is n— 1, where n is the size of the sample.

Adjusted R square is always lower than the R-squared.

Tip: You must assess residual plots, as R-squared alone cannot determine whether the coefficient estimates and predictions are biased.

Adjusted R-squared only improves if the new predictor improves the model. It compares the explanatory power of regression models that contain different numbers of predictors. It is adjusted for the number of predictors in the model.

Overview Of Model Criterion

Models require parameters. Parameters for a model are selected so that they can maximize the likelihood of a value occurring in a sample.

To accomplish higher probability of a value occurring in a sample, first distribution of the variable is calculated and then parameters are searched that maximize likelihood until the likelihood function fails to increase.

We can get higher R squared by adding variables in the forecasting model. This adds the risk of over-fitting the model. For example, assume we want to calculate MSE for a predictive model. Adding more factors (independent variables) can end up giving us better fit.

A number of model selection criterion are implemented to penalise the models that include higher number of independent variables.

These includes Unbiased Mean squared error (MSE) known as s-squared, Akaike (AIC) and Schwarz (SIC) Criterion.

SIC has the highest penalty factor and it increases at exponential rate with increasing more variables. Therefore SIC is the most consistent selection criterion. These criterion are not covered in this article but it’s worth mentioning that these criterion adjust R squared by taking number of independent variables into account. As a result, addition of more variables does not always increase R squared value.

Note: Higher R squared value implies model is fitted better and our forecasting is accurate.

Once residuals (actual value — predicted value) are computed, they can be plotted against time to determine if the model is serially correlated.

Finally

This article highlighted the concept of regression analysis that is required to build a forecasting model. It also explained how to test goodness of fit of a model once it is implemented.

Hope it helps. Please let me know of your feedback.","['good', 'r', 'regression', 'y', 'variables', 'value', 'squared', 'model', 'sum', 'line', 'analysis', 'predictive']","Explanation Of Regression Analysis 3 Steps Of Regression Analysis Understanding Of Residuals Ordinary Least Square Explanation Overview Of Model CriterionLet’s StartOne of the most famous methodologies to forecast a variable’s behaviour is to use regression analysis.
Regression analysis can be used in businesses whereby managers can send employees on training courses and use the regression analysis to see how training impacts productivity.
3 Steps Of Regression AnalysisTo help us understand regression analysis, I will explain the concept of regression analysis in 3 easy to understand steps.
Let’s measure quality of our regression model: R SquaredOnce we know the size of residuals, we can start assessing how good our regression fit is.
Formula to calculate R squared is:R squared = 1 — (SSR/TSS)This means R squared = 1 — Variance (residual)/Variance (y)Higher R squared, the better.",en,['Farhad Malik'],2019-06-19 14:23:26.120000+00:00,"{'Data Science', 'Regression Analysis', 'Machine Learning', 'Fintech', 'Mathematics'}","{'https://miro.medium.com/max/60/0*yaR425hQbx0krYGt?q=20', 'https://miro.medium.com/max/60/0*Rxw24Ju9GUroBsm-?q=20', 'https://miro.medium.com/max/60/0*lsAghL3MIghToyaA.jpg?q=20', 'https://miro.medium.com/max/1200/0*Rxw24Ju9GUroBsm-', 'https://miro.medium.com/fit/c/96/96/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/max/72/1*sTwya1_faQWOvmCthaKzsQ.png', 'https://miro.medium.com/max/640/0*GGneH030uxalA78F.jpg', 'https://miro.medium.com/max/8000/0*HSfQlR1t0FRSeKBb', 'https://miro.medium.com/fit/c/160/160/1*sTwya1_faQWOvmCthaKzsQ.png', 'https://miro.medium.com/max/40/0*HSfQlR1t0FRSeKBb?q=20', 'https://miro.medium.com/max/60/0*rgjhWbfg2XtevmOo?q=20', 'https://miro.medium.com/max/5000/0*yaR425hQbx0krYGt', 'https://miro.medium.com/max/13378/0*nxa-Mu84yZCxLS-d', 'https://miro.medium.com/max/60/0*GGneH030uxalA78F.jpg?q=20', 'https://miro.medium.com/fit/c/160/160/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/max/11674/0*Rxw24Ju9GUroBsm-', 'https://miro.medium.com/max/5000/0*rgjhWbfg2XtevmOo', 'https://miro.medium.com/max/60/0*ay1WJBkuULTIm3zE?q=20', 'https://miro.medium.com/max/60/0*nxa-Mu84yZCxLS-d?q=20', 'https://miro.medium.com/max/640/0*lsAghL3MIghToyaA.jpg', 'https://miro.medium.com/max/10368/0*ay1WJBkuULTIm3zE'}",2020-03-05 00:24:27.254394,1.0329275131225586
https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8,"How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine","The Estimators API in tf.contrib.learn is a very convenient way to get started using TensorFlow. The really cool thing from my perspective about the Estimators API is that using it is a very easy way to create distributed TensorFlow models. Many of the TensorFlow samples that you see floating around on the internets are not distributed — they assume that you will be running the code on a single machine. People start with such code and then are immeasurably saddened to learn that the low-level TensorFlow code doesn’t actually work on their complete dataset. They then have to do lots of work to add distributed training code around the original sample, and who wants to edit somebody else’s code?

N ote: Estimators have now moved into core Tensorflow. Updated code that uses tf.estimator instead of tf.contrib.learn.estimator is now on GitHub — use the updated code as a starting point.

So, please, please, please, if you see a TensorFlow sample that doesn’t use the Estimators API, ignore it. It will be a lot of work to make it work on your production (read: large) datasets — there will be monitors, coordinators, parameter servers, and all kinds of systems programming craziness that you don’t want to have to dive into. Start with the Estimator API and use the Experiment class. (Disclaimer: my views, not that of my employer).

Time series prediction needs a custom estimator

The Estimators API comes with a Deep Neural Network classifier and regressor. If you have typical structured data, follow the tutorial linked above or take this training course from Google Cloud (soon to be available on Coursera) and you’ll be on your way to creating machine learning models that work on real-world, large datasets in your relational data warehouse. But what if you don’t have a typical structured data problem? In that case, you will often need to create a custom estimator. In this blog post, I will show you how.

A common type of data that you will want to do machine learning on is time-series data. Essentially, your inputs are a set of numbers and you want to predict the next number in that sequence. In this article, I will make it a bit more general and assume that you want to predict the last two numbers of the sequence. As the Computer Science proverb goes, if you can do two, you can do N.

The traditional neural network architecture that is used for sequence-to-sequence prediction is called a Recurrent Neural Network (RNN). See this article and this one for a very accessible introduction to RNNs. But you don’t need to know how to implement a RNN to use one, so once those articles go deeper than you want, quit.

To follow along with this article, have my Jupyter notebook open in another browser window. I am only showing key snippets of code here. The notebook (and the GitHub folder) contains all of the code.

Simulate some time-series data

It’s usually easier to learn with a small toy dataset that you can generate as much as you want of. Real data will come with its own quirks! So, let’s generate a bunch of time-series data. Each sequence will consist of 10 numbers. We will use the first eight as inputs and the last two as the labels (i.e., what is to be predicted):

The code to generate these time-series sequences using numpy (np):

SEQ_LEN = 10

def create_time_series():

freq = (np.random.random()*0.5) + 0.1 # 0.1 to 0.6

ampl = np.random.random() + 0.5 # 0.5 to 1.5

x = np.sin(np.arange(0,SEQ_LEN) * freq) * ampl

return x

Write a bunch of these time-series sequences to CSV files (train.csv and valid.csv) and we are in business. We’ve got data.

Input Function

The way the Estimators API in TensorFlow works is that you need to provide an input_fn to read your data. You don’t provide x and y values. Instead, you provide a function that returns inputs and labels. The inputs is a dictionary of all your inputs (name-of-input to tensor) and the labels is a tensor.

In our case, our CSV file simply consists of 10 floating point numbers. The DEFAULTS serves to specify the data type for the tensors. We want to read the data 20 lines at a time; that’s the BATCH_SIZE. A batch is the number of samples over which gradient descent is performed. You will need to experiment with this number — if it is too large, your training will be slow and if it is too small, your training will bounce around won’t converge. Since we have only input, the name you give that input doesn’t really matter. We’ll call it rawdata.

DEFAULTS = [[0.0] for x in xrange(0, SEQ_LEN)]

BATCH_SIZE = 20

TIMESERIES_COL = 'rawdata'

N_OUTPUTS = 2 # in each sequence, 1-8 are features, and 9-10 is label

N_INPUTS = SEQ_LEN - N_OUTPUTS

The input_fn that the Estimators API wants should take no parameters. However, we do want to be able to provide the filename(s) to read on the command line. So, Let’s write a read_dataset() function that returns an input_fn.

# read data and convert to needed format

def read_dataset(filename, mode=tf.contrib.learn.ModeKeys.TRAIN):

def _input_fn():

num_epochs = 100 if mode == tf.contrib.learn.ModeKeys.TRAIN else 1

The first thing that we do is to decide the number of epochs. This is how many times we need to go through the dataset. We’ll go through the dataset 100 times if we are training, but only once if we are evaluating.

Next, we’ll do wild-card expansion. Lots of times, Big Data programs produce sharded files such as train.csv-0001-of-0036 and so, we’d like to simply provide train.csv* as the input. We use this to populate a filename queue and then use a TextLineReader to read the data:

# could be a path to one file or a file pattern.

input_file_names = tf.train.match_filenames_once(filename)

filename_queue = tf.train.string_input_producer(

input_file_names, num_epochs=num_epochs, shuffle=True) reader = tf.TextLineReader()

_, value = reader.read_up_to(filename_queue, num_records=BATCH_SIZE) value_column = tf.expand_dims(value, -1)

After this, we decode the data, treating the first 8 numbers as inputs and the last two as the label. The inputs, when we read it, is a list of 8 tensors each of which is batchsize x 1. Using tf.concat makes it a single 8xbatchsize tensor. This is important because the Estimators API wants tensors not lists.

# all_data is a list of tensors

all_data = tf.decode_csv(value_column, record_defaults=DEFAULTS)

inputs = all_data[:len(all_data)-N_OUTPUTS] # first few values

label = all_data[len(all_data)-N_OUTPUTS : ] # last few values



# from list of tensors to tensor with one more dimension

inputs = tf.concat(inputs, axis=1)

label = tf.concat(label, axis=1)

print 'inputs={}'.format(inputs)



return {TIMESERIES_COL: inputs}, label # dict of features, label

Define a RNN

If we we were using a LinearRegressor, DNNRegressor, DNNLinearCombinedRegressor, etc., we could have simply used the existing class. But because we are doing sequence-to-sequence prediction, we have to write our own model function. At least right now, the Estimators API doesn’t come with an out-of-the-box RNNRegressor. So, let’s roll out our own RNN model using low-level TensorFlow functions.

LSTM_SIZE = 3 # number of hidden layers in each of the LSTM cells



# create the inference model

def simple_rnn(features, targets, mode):

# 0. Reformat input shape to become a sequence

x = tf.split(features[TIMESERIES_COL], N_INPUTS, 1)

#print 'x={}'.format(x)



# 1. configure the RNN

lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)

outputs, _ = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)



# slice to keep only the last cell of the RNN

outputs = outputs[-1]

#print 'last outputs={}'.format(outputs)



# output is result of linear activation of last layer of RNN

weight = tf.Variable(tf.random_normal([LSTM_SIZE, N_OUTPUTS]))

bias = tf.Variable(tf.random_normal([N_OUTPUTS]))

predictions = tf.matmul(outputs, weight) + bias



# 2. Define the loss function for training/evaluation

#print 'targets={}'.format(targets)

#print 'preds={}'.format(predictions)

loss = tf.losses.mean_squared_error(targets, predictions)

eval_metric_ops = {

""rmse"": tf.metrics.root_mean_squared_error(targets, predictions)

}



# 3. Define the training operation/optimizer

train_op = tf.contrib.layers.optimize_loss(

loss=loss,

global_step=tf.contrib.framework.get_global_step(),

learning_rate=0.01,

optimizer=""SGD"")



# 4. Create predictions

predictions_dict = {""predicted"": predictions}



# 5. return ModelFnOps

return tflearn.ModelFnOps(

mode=mode,

predictions=predictions_dict,

loss=loss,

train_op=train_op,

eval_metric_ops=eval_metric_ops)

Recall that we had to package up the inputs into a single tensor to pass it as the features out of the input_fn. Step 0 simply reverses that process and gets back the list of tensors.

A Recurrent Neural Network consists of a BasicLSTMLCell to which you pass in the input. You get back outputs and states. Slice it to keep only the last cell of the RNN — we are not using any of the previous states. Other architectures are possible. For example, I could have trained the network to have only one output always and used rolling windows. I’ll talk about how to modify my example to do that at the end of this article.

The comments in the code above are pretty self-explanatory regarding the other steps. We are not doing anything surprising there. This is a regression problem, so I’m using RMSE.

Create an Experiment

The Experiment class is the smart one in the Estimators API. It knows how to take the model function, input functions for training and validation and do reasonable things regarding distribution, early stopping, etc. So, let’s hand off our pieces to it:

def get_train():

return read_dataset('train.csv', mode=tf.contrib.learn.ModeKeys.TRAIN)



def get_valid():

return read_dataset('valid.csv', mode=tf.contrib.learn.ModeKeys.EVAL)



def experiment_fn(output_dir):

# run experiment

return tflearn.Experiment(

tflearn.Estimator(model_fn=simple_rnn, model_dir=output_dir),

train_input_fn=get_train(),

eval_input_fn=get_valid(),

eval_metrics={

'rmse': tflearn.MetricSpec(

metric_fn=metrics.streaming_root_mean_squared_error

)

}

)



shutil.rmtree('outputdir', ignore_errors=True) # start fresh each time

learn_runner.run(experiment_fn, 'outputdir')

Training on the Cloud

The code above works on a single machine, and if you package it up into a Python module, you can also submit it to Cloud ML Engine to have it trained in a serverless way:

OUTDIR=gs://${BUCKET}/simplernn/model_trained

JOBNAME=simplernn_$(date -u +%y%m%d_%H%M%S)

REGION=us-central1

gsutil -m rm -rf $OUTDIR

gcloud ml-engine jobs submit training $JOBNAME \

--region=$REGION \

--module-name=trainer.task \

--package-path=${REPO}/simplernn/trainer \

--job-dir=$OUTDIR \

--staging-bucket=gs://$BUCKET \

--scale-tier=BASIC \

--runtime-version=1.0 \

-- \

--train_data_paths=""gs://${BUCKET}/train.csv*"" \

--eval_data_paths=""gs://${BUCKET}/valid.csv*"" \

--output_dir=$OUTDIR \

--num_epochs=100

A common variant: very long time-series

In this article, I assumed that you have thousands of short (10-element) sequences. What if you have a very long sequence? For example, you might have the price of a stock or the temperature reading from a sensor. In such cases, what you could do is to break up your long sequence into rolling sequences of fixed length. This length is obviously arbitrary, but think of it as the “look-back” interval of the RNN. Here is TensorFlow code that will take a long sequence and break into smaller, overlapping sequences of a fixed length:

import tensorflow as tf

import numpy as np def breakup(sess, x, lookback_len):

N = sess.run(tf.size(x))

windows = [tf.slice(x, [b], [lookback_len]) for b in xrange(0, N-lookback_len)]

windows = tf.stack(windows)

return windows

For example:

x = tf.constant(np.arange(1,11, dtype=np.float32))

with tf.Session() as sess:

print 'input=', x.eval()

seqx = breakup(sess, x, 5)

print 'output=', seqx.eval()

will result in:

input= [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.]

output= [[ 1. 2. 3. 4. 5.]

[ 2. 3. 4. 5. 6.]

[ 3. 4. 5. 6. 7.]

[ 4. 5. 6. 7. 8.]

[ 5. 6. 7. 8. 9.]]

Once you have these fixed length sequences, everything is the same as before.

Happy coding!","['tensorflow', 'read', 'prediction', 'sequence', 'series', 'cloud', 'ml', 'engine', 'data', 'rnns', 'estimators', 'api', 'training', 'inputs', 'using', 'code']","The Estimators API in tf.contrib.learn is a very convenient way to get started using TensorFlow.
People start with such code and then are immeasurably saddened to learn that the low-level TensorFlow code doesn’t actually work on their complete dataset.
They then have to do lots of work to add distributed training code around the original sample, and who wants to edit somebody else’s code?
So, please, please, please, if you see a TensorFlow sample that doesn’t use the Estimators API, ignore it.
Time series prediction needs a custom estimatorThe Estimators API comes with a Deep Neural Network classifier and regressor.",en,['Lak Lakshmanan'],2018-01-12 18:44:44.690000+00:00,"{'Google Cloud Platform', 'Timeseries', 'Machine Learning', 'Rnn', 'TensorFlow'}","{'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://miro.medium.com/max/60/1*heIQAa4sV1M-YaMvjXSc7g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*opbfQNyOl9EgY6qWxwj_Xw.jpeg', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/fit/c/96/96/1*opbfQNyOl9EgY6qWxwj_Xw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/max/550/1*heIQAa4sV1M-YaMvjXSc7g.png', 'https://miro.medium.com/max/1100/1*heIQAa4sV1M-YaMvjXSc7g.png', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png'}",2020-03-05 00:24:28.396320,1.1419267654418945
https://medium.com/spikelab/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245,Forecasting multiple time-series using Prophet in parallel,"Forecasting multiple time-series using Prophet in parallel

A short story about multiprocessing

For a few weeks I have been using Facebook Prophet library, its a great tool for forecasting time-series, because is pretty simple to use and the forecasted results are pretty good!, but doesn’t run all the process in parallel, so basically if you want to forecast multiple timeseries all the process could take a lot of time, this is how we reduce the forecasting process time significantly using multiprocessing package from Python.

Generating time-series

We are going to generate 500 random time-series, the purpose of this post is not to evaluate the effectiveness of Prophet prediction, but the time required to do accomplish this.

So I wrote a function that generates random time-series between a time period:

import pandas as pd

import numpy as np def rnd_timeserie(min_date, max_date):

time_index = pd.date_range(min_date, max_date)

dates = (pd.DataFrame({'ds': pd.to_datetime(time_index.values)},

index=range(len(time_index))))

y = np.random.random_sample(len(dates))*10

dates['y'] = y

return dates

So one of our random time-series looks like this:

A random time-serie.

Lets generate 500 series

series = [rnd_timeserie('2018-01-01','2018-12-30') for x in range(0,500)]

We have generated our time-series, now its time to run Prophet.

Forecasting using Prophet

Let’s create a simple Prophet model, for this we define a function called run_prophet that takes a time-series and fits a model with the data, then we can use that model to predict the next 90 days.

from fbprophet import Prophet def run_prophet(timeserie):

model = Prophet(yearly_seasonality=False,daily_seasonality=False)

model.fit(timeserie)

forecast = model.make_future_dataframe(periods=90, include_history=False)

forecast = model.predict(forecast)

return forecast

For example, we can run this function with the first generated time-serie:

f = run_prophet(series[0])

f.head()

We can see our forecasted results for that serie:

Prophet output

Running 500 time-series

Now let’s add a timer and run prophet for the 500 time-series without using any kind of multiprocessing tool, i’m using tqdm so I can check the progress

start_time = time.time()

result = list(map(lambda timeserie: run_prophet(timeserie), tqdm(series)))

print(""--- %s seconds ---"" % (time.time() - start_time))

The previous code took: 12.53 minutes to run, the processors usage looked like this the whole time:

Processors usage running Prophet

Now, let’s add multiprocessing to our code, the idea here is to launch a process for each time-serie forecast, so we can run our run_prophet function in parallel while we do the map of the list.

For this we are going to use a Pool of process and quoting the documentation:

The Pool object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism).

from multiprocessing import Pool, cpu_count p = Pool(cpu_count())

predictions = list(tqdm(p.imap(run_prophet, series), total=len(series)))

p.close()

p.join()

print(""--- %s seconds ---"" % (time.time() - start_time))

With the previous code, we launch N processes depending of how many CPUs our machine has, then we run the run_prophet function for each time serie among the cpus.

The code took 3.04 minutes to run, the usage of CPUs in the whole run time looked like this:

CPU usage in multiprocess

So we got a speedup of 4.12!, which is pretty good!!!, my machine only has 8 CPU, if we want to run this faster, we could use a machine with more CPUs.

Conclusions

We could see that using multiprocessing is a great way to forecasting multiple time-series faster, in many problems multiprocessing could help to reduce the execution time of our code.","['forecasting', 'function', 'usage', 'process', 'multiple', 'timeseries', 'random', 'multiprocessing', 'run', 'prophet', 'parallel', 'using', 'code']","Forecasting multiple time-series using Prophet in parallelA short story about multiprocessingFor a few weeks I have been using Facebook Prophet library, its a great tool for forecasting time-series, because is pretty simple to use and the forecasted results are pretty good!, but doesn’t run all the process in parallel, so basically if you want to forecast multiple timeseries all the process could take a lot of time, this is how we reduce the forecasting process time significantly using multiprocessing package from Python.
Generating time-seriesWe are going to generate 500 random time-series, the purpose of this post is not to evaluate the effectiveness of Prophet prediction, but the time required to do accomplish this.
So I wrote a function that generates random time-series between a time period:import pandas as pdimport numpy as np def rnd_timeserie(min_date, max_date):time_index = pd.date_range(min_date, max_date)dates = (pd.DataFrame({'ds': pd.to_datetime(time_index.values)},index=range(len(time_index))))y = np.random.random_sample(len(dates))*10dates['y'] = yreturn datesSo one of our random time-series looks like this:A random time-serie.
Lets generate 500 seriesseries = [rnd_timeserie('2018-01-01','2018-12-30') for x in range(0,500)]We have generated our time-series, now its time to run Prophet.
ConclusionsWe could see that using multiprocessing is a great way to forecasting multiple time-series faster, in many problems multiprocessing could help to reduce the execution time of our code.",en,['Matias Aravena Gamboa'],2019-04-01 13:38:50.557000+00:00,"{'Ia', 'Facebook Prophet', 'Multiprocessing', 'Python', 'Machine Learning'}","{'https://miro.medium.com/max/60/1*_4D20Kscn86R0TlnkIb9PA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*7kVD0XTdaAtX6flQuByjBw.gif?q=20', 'https://miro.medium.com/max/5108/1*I8d3rQPRDulMEoKKNNjCKA.png', 'https://miro.medium.com/fit/c/96/96/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*9IsUSUbsBvqzcLoenqVyNA.jpeg', 'https://miro.medium.com/max/72/1*jmI1clbffK5X9gbWJt0EJw.png', 'https://miro.medium.com/fit/c/160/160/1*jJQAwYdaaVuSNMPhX0GfzA.png', 'https://miro.medium.com/max/60/1*cYrnMk4lDstZKx5bSlwmdA.png?q=20', 'https://miro.medium.com/max/3272/1*cYrnMk4lDstZKx5bSlwmdA.png', 'https://miro.medium.com/max/60/1*I8d3rQPRDulMEoKKNNjCKA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*SgD8rEsPq13r6O54irE4yg.jpeg', 'https://miro.medium.com/max/5560/1*3Rim9NnhD6uurIKTT2H0Lg.gif', 'https://miro.medium.com/freeze/max/60/1*3Rim9NnhD6uurIKTT2H0Lg.gif?q=20', 'https://miro.medium.com/max/1200/1*cYrnMk4lDstZKx5bSlwmdA.png', 'https://miro.medium.com/fit/c/80/80/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/max/5596/1*7kVD0XTdaAtX6flQuByjBw.gif', 'https://miro.medium.com/fit/c/160/160/1*Pg7xHHigQrgtvT_d97R7qA.jpeg', 'https://miro.medium.com/max/1780/1*_4D20Kscn86R0TlnkIb9PA.png'}",2020-03-05 00:24:29.266419,0.870098352432251
https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b,An End-to-End Project on Time Series Analysis and Forecasting with Python,"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.

Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post. We will demonstrate different approaches for forecasting retail sales time series. Let’s get started!

The Data

We are using Superstore sales data that can be downloaded from here.

import warnings

import itertools

import numpy as np

import matplotlib.pyplot as plt

warnings.filterwarnings(""ignore"")

plt.style.use('fivethirtyeight')

import pandas as pd

import statsmodels.api as sm

import matplotlib matplotlib.rcParams['axes.labelsize'] = 14

matplotlib.rcParams['xtick.labelsize'] = 12

matplotlib.rcParams['ytick.labelsize'] = 12

matplotlib.rcParams['text.color'] = 'k'

There are several categories in the Superstore sales data, we start from time series analysis and forecasting for furniture sales.

df = pd.read_excel(""Superstore.xls"")

furniture = df.loc[df['Category'] == 'Furniture']

We have a good 4-year furniture sales data.

furniture['Order Date'].min(), furniture['Order Date'].max()

Timestamp(‘2014–01–06 00:00:00’), Timestamp(‘2017–12–30 00:00:00’)

Data Preprocessing

This step includes removing columns we do not need, check missing values, aggregate sales by date and so on.

cols = ['Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Quantity', 'Discount', 'Profit']

furniture.drop(cols, axis=1, inplace=True)

furniture = furniture.sort_values('Order Date') furniture.isnull().sum()

Figure 1

furniture = furniture.groupby('Order Date')['Sales'].sum().reset_index()

Indexing with Time Series Data","['sales', 'forecasting', 'id', 'series', 'project', 'python', 'endtoend', 'ship', 'retail', 'date', 'data', 'superstore', 'values', 'analysis']","Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data.
Time series forecasting is the use of a model to predict future values based on previously observed values.
Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post.
We will demonstrate different approaches for forecasting retail sales time series.
The DataWe are using Superstore sales data that can be downloaded from here.",en,['Susan Li'],2018-09-05 12:18:21.429000+00:00,"{'Data Science', 'Timeseries', 'Python', 'Machine Learning', 'Statistical Analysis'}","{'https://miro.medium.com/max/2936/1*3nRjq-BbTJkf0EchAUih8w.png', 'https://miro.medium.com/max/60/1*AIri_6z7hjmBhHa_Vv8saA.png?q=20', 'https://miro.medium.com/max/2560/1*QGJbFHcjXhgUOrWCvho8jg.jpeg', 'https://miro.medium.com/max/3048/1*pmdxT4So6iA4S983UYgTvQ.png', 'https://miro.medium.com/max/1420/1*tmZVNbBjrxPY5VJOqFxKEw.png', 'https://miro.medium.com/max/1996/1*dXw-tN-WvTheIzRi7i_QNg.png', 'https://miro.medium.com/max/60/1*qZs_ojNijThKzBPNk9m-Mw.png?q=20', 'https://miro.medium.com/max/3016/1*ABh-_dlzzEdAQeKjwD6egA.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1632/1*Ob3xAxvdjdYNRoSxGJUN0g.png', 'https://miro.medium.com/max/60/1*ABh-_dlzzEdAQeKjwD6egA.png?q=20', 'https://miro.medium.com/max/60/1*3nRjq-BbTJkf0EchAUih8w.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*XZdcRvYiFe_AKZs43TPrAg.png?q=20', 'https://miro.medium.com/max/1048/1*XZdcRvYiFe_AKZs43TPrAg.png', 'https://miro.medium.com/max/3006/1*wZNvQLTTtIm3YTYCauCaOQ.png', 'https://miro.medium.com/max/60/1*FYJ3n87lMXpr0v_4r9j4Tg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*dXw-tN-WvTheIzRi7i_QNg.png?q=20', 'https://miro.medium.com/max/60/1*M6hu6OC2tztF7mMgfCk8oA.png?q=20', 'https://miro.medium.com/max/56/1*tmZVNbBjrxPY5VJOqFxKEw.png?q=20', 'https://miro.medium.com/max/1200/1*QGJbFHcjXhgUOrWCvho8jg.jpeg', 'https://miro.medium.com/max/60/1*bIXmoCO8fNr0VrVPEXryNQ.png?q=20', 'https://miro.medium.com/max/592/1*M6hu6OC2tztF7mMgfCk8oA.png', 'https://miro.medium.com/max/2954/1*KkKMhgmjYnYIDG-whuywFA.png', 'https://miro.medium.com/max/60/1*wZNvQLTTtIm3YTYCauCaOQ.png?q=20', 'https://miro.medium.com/max/60/1*HSfgtd2iIp7CQgVlPMRdUw.png?q=20', 'https://miro.medium.com/max/2986/1*xDCVJvqJkRymEv2dSIRIPg.png', 'https://miro.medium.com/max/60/1*QGJbFHcjXhgUOrWCvho8jg.jpeg?q=20', 'https://miro.medium.com/max/60/1*Ob3xAxvdjdYNRoSxGJUN0g.png?q=20', 'https://miro.medium.com/max/2524/1*HSfgtd2iIp7CQgVlPMRdUw.png', 'https://miro.medium.com/max/2168/1*AIri_6z7hjmBhHa_Vv8saA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*sThmSkOvLOGvyYPula8TDA.png?q=20', 'https://miro.medium.com/max/2240/1*qZs_ojNijThKzBPNk9m-Mw.png', 'https://miro.medium.com/max/1980/1*FYJ3n87lMXpr0v_4r9j4Tg.png', 'https://miro.medium.com/max/60/1*pmdxT4So6iA4S983UYgTvQ.png?q=20', 'https://miro.medium.com/max/60/1*KkKMhgmjYnYIDG-whuywFA.png?q=20', 'https://miro.medium.com/max/2248/1*bIXmoCO8fNr0VrVPEXryNQ.png', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/60/1*xDCVJvqJkRymEv2dSIRIPg.png?q=20', 'https://miro.medium.com/max/2352/1*B_XmeNOk8CshoEfyb6vDvQ.png', 'https://miro.medium.com/max/60/1*edG7312_ETG3FgTiXaFk1g.png?q=20', 'https://miro.medium.com/max/60/1*B_XmeNOk8CshoEfyb6vDvQ.png?q=20', 'https://miro.medium.com/max/2224/1*DzW7KYLD_M6KLtdKBV38TA.png', 'https://miro.medium.com/max/2184/1*edG7312_ETG3FgTiXaFk1g.png', 'https://miro.medium.com/max/1474/1*sThmSkOvLOGvyYPula8TDA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*DzW7KYLD_M6KLtdKBV38TA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg'}",2020-03-05 00:24:36.033780,6.766437530517578
https://towardsdatascience.com/time-series-in-python-exponential-smoothing-and-arima-processes-2c67f2a52788,Time Series in Python — Exponential Smoothing and ARIMA processes,"TL;DR: In this article you’ll learn the basics steps to performing time-series analysis and concepts like trend, stationarity, moving averages, etc. You’ll also explore exponential smoothing methods, and learn how to fit an ARIMA model on non-stationary data.

Time series are everywhere

Situation 1: You are responsible for a pizza delivery center and you want to know if your sales follow a particular pattern because you feel that every Saturday evening there is a increase in the number of your orders…

Situation 2: Your company is selling a product and you are in charge of predicting, or forecasting, the supplies needed for this product at a particular moment in the future…

Situation 3: You are monitoring a datacenter and you want to detect any anomaly such as an abnormal CPU usage which might cause a downtime on your servers. You follow the curve of the CPU usage and want to know when an anomaly occurs…

In each of these situations, you are dealing with time series. Analyzing series is a fascinating job because despite all mathematical models (including neural networks), we humans still fail to predict the future and have to deal with uncertainty. Let’s have a closer look at what time series are and which methods can be used to analyze them. In this article, we will extensively rely on the statsmodels library written in Python.

A time series is a data sequence ordered (or indexed) by time. It is discrete, and the the interval between each point is constant.

Properties and types of series

Trend : A long-term increase or decrease in the data. This can be seen as a slope (is doesn’t have to be linear) roughly going through the data.

Seasonality : A time series is said to be seasonal when it is affected by seasonal factors (hour of day, week, month, year, etc.). Seasonality can be observed with nice cyclical patterns of fixed frequency.

Cyclicity : A cycle occurs when the data exhibits rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.

Residuals : Each time series can be decomposed in two parts:

- A forecast, made up of one or several forecasted values

- Residuals. They are the difference between an observation and its predicted value at each time step. Remember that","['exponential', 'particular', 'seasonal', 'arima', 'usage', 'series', 'python', 'usually', 'methods', 'data', 'youll', 'learn', 'processes', 'product', 'smoothing']","TL;DR: In this article you’ll learn the basics steps to performing time-series analysis and concepts like trend, stationarity, moving averages, etc.
You’ll also explore exponential smoothing methods, and learn how to fit an ARIMA model on non-stationary data.
Let’s have a closer look at what time series are and which methods can be used to analyze them.
Seasonality : A time series is said to be seasonal when it is affected by seasonal factors (hour of day, week, month, year, etc.).
Residuals : Each time series can be decomposed in two parts:- A forecast, made up of one or several forecasted values- Residuals.",en,['Benjamin Etienne'],2019-09-30 10:17:08.165000+00:00,"{'Forecasting', 'Time Series Analysis', 'Data Science', 'Arima', 'Statistics'}","{'https://miro.medium.com/max/60/1*OdM1gyoTZKc6koTKsELQQw.png?q=20', 'https://miro.medium.com/max/60/1*LOpA2ty_2Exfkd2W0LYANA.png?q=20', 'https://miro.medium.com/max/60/1*QGUWw1aDdchmEfsI4Bveew.png?q=20', 'https://miro.medium.com/max/60/1*DiSfU5vrhO7aS3b7QkcJCQ.png?q=20', 'https://miro.medium.com/max/60/1*gPA8J1TFgNh9N1ikhdXxuA.png?q=20', 'https://miro.medium.com/max/60/1*tHXsJflbXnV26NwiX2G0DA.png?q=20', 'https://miro.medium.com/max/60/1*fiSwWGHZkpazvilnxGs9UA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*zgmYQ0PJ0MHXIc5mEYVSMA.jpeg', 'https://miro.medium.com/max/60/1*aG1P_lYvzV0iiReaR_WzFA.png?q=20', 'https://miro.medium.com/max/1222/1*QGUWw1aDdchmEfsI4Bveew.png', 'https://miro.medium.com/max/1828/1*luC4HUObtXk0dr7A9sKYVg.png', 'https://miro.medium.com/max/60/1*PdFWtB00k_iUfIHJWmo9Jw.png?q=20', 'https://miro.medium.com/max/60/1*MLIjPTMtWjuoI8kFclSs4w.png?q=20', 'https://miro.medium.com/max/1762/1*sp7_NBZNj_6ZpojQ4I_yCg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*luC4HUObtXk0dr7A9sKYVg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*oVKkecdhS3XGCsCVDhS2RQ.png?q=20', 'https://miro.medium.com/max/2162/1*Y9N6_GjWYI9eIfrpqZ24GA.png', 'https://miro.medium.com/max/1394/1*DiSfU5vrhO7aS3b7QkcJCQ.png', 'https://miro.medium.com/max/956/1*KwTyFVU3G3Y22ptUQRmgzQ.png', 'https://miro.medium.com/max/60/1*4iv5gtYGalArxDWgHOZvmQ.png?q=20', 'https://miro.medium.com/max/60/1*V-YCwPUpuEUIWKs3PVV7Og.png?q=20', 'https://miro.medium.com/max/60/1*w2YYQ53koQD0V88SBtgF-g.png?q=20', 'https://miro.medium.com/max/1786/1*MLIjPTMtWjuoI8kFclSs4w.png', 'https://miro.medium.com/max/1412/1*PdFWtB00k_iUfIHJWmo9Jw.png', 'https://miro.medium.com/max/60/1*5YxhuY9nGqiXdRIvMfXUEw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*9qJcmvji3Y2pCktS39GVwg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*zgmYQ0PJ0MHXIc5mEYVSMA.jpeg', 'https://miro.medium.com/max/1794/1*8aCU2mlnP88_bxwCjGas1g.png', 'https://miro.medium.com/max/1682/1*9qJcmvji3Y2pCktS39GVwg.png', 'https://miro.medium.com/max/1778/1*LOpA2ty_2Exfkd2W0LYANA.png', 'https://miro.medium.com/max/1436/1*gPA8J1TFgNh9N1ikhdXxuA.png', 'https://miro.medium.com/max/1440/1*w2YYQ53koQD0V88SBtgF-g.png', 'https://miro.medium.com/max/60/1*Y9N6_GjWYI9eIfrpqZ24GA.png?q=20', 'https://miro.medium.com/max/1766/1*tHXsJflbXnV26NwiX2G0DA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*KwTyFVU3G3Y22ptUQRmgzQ.png?q=20', 'https://miro.medium.com/max/1780/1*okLQmsUscXVNQKk1S1NJMQ.png', 'https://miro.medium.com/max/1474/1*fiSwWGHZkpazvilnxGs9UA.png', 'https://miro.medium.com/max/1790/1*5YxhuY9nGqiXdRIvMfXUEw.png', 'https://miro.medium.com/max/1752/1*OdM1gyoTZKc6koTKsELQQw.png', 'https://miro.medium.com/max/60/1*sp7_NBZNj_6ZpojQ4I_yCg.png?q=20', 'https://miro.medium.com/max/60/1*O8pxgq3EKWjnarL3g87BjA.png?q=20', 'https://miro.medium.com/max/1464/1*JNl9a5clla8c-rnnkW-_dw.png', 'https://miro.medium.com/max/1758/1*aG1P_lYvzV0iiReaR_WzFA.png', 'https://miro.medium.com/max/60/1*okLQmsUscXVNQKk1S1NJMQ.png?q=20', 'https://miro.medium.com/max/1081/1*Y9N6_GjWYI9eIfrpqZ24GA.png', 'https://miro.medium.com/max/1100/1*V-YCwPUpuEUIWKs3PVV7Og.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1816/1*O8pxgq3EKWjnarL3g87BjA.png', 'https://miro.medium.com/max/502/1*oVKkecdhS3XGCsCVDhS2RQ.png', 'https://miro.medium.com/max/1448/1*4iv5gtYGalArxDWgHOZvmQ.png', 'https://miro.medium.com/max/60/1*8aCU2mlnP88_bxwCjGas1g.png?q=20', 'https://miro.medium.com/max/58/1*JNl9a5clla8c-rnnkW-_dw.png?q=20'}",2020-03-05 00:24:38.334789,2.3000130653381348
https://towardsdatascience.com/time-series-forecasting-with-prophet-54f2ac5e722e,Time Series Forecasting with Prophet,"Time Series Forecasting with Prophet

Learn how to use Facebook’s Prophet to predict air quality

Photo by Frédéric Paulussen on Unsplash

Producing high quality forecasts is hard for many machine learning engineers. It requires a substantial amount of experience and and very specific skills. Also, other forecasting tools were too inflexible to incorporate useful assumptions.

For those reasons, Facebook open sourced Prophet, a forecasting tool available in both Python and R. This tool allows both experts and non-experts to produce high quality forecasts with minimal efforts.

Here, we will use Prophet to help us predict air quality!

The full notebook and dataset can be found here.

Let’s make some predictions!

We won’t go back to the future, unfortunately

Import and clean the data

As always, we start by importing some useful libraries:

Then import the dataset and preview it:

And you should see the following:

First five entries of the dataset

As you can see, the dataset contains information about the concentrations of different gases. They were recorded at every hour for each day. You can find a description of all features here.

If you explore the dataset a bit more, you will notice that there are many instances of the value -200. Of course, it does not make sense to have a negative concentration, so we will need to clean the data before modelling.

First, we get rid of all instances where there is an empty value:

After, we need to parse the date column as a date, and turn all measurements into floats:

Then, we aggregate the data by day, by taking the average of each measurement:

At this point, the data should look like this:

We still have some NaN that we need to get rid of. We can see how many NaN are present in each column with:

Let’s get rid of the columns that have more than 8 NaN:

Perfect! Now, we should aggregate the data by week, because it will give a smoother trend to analyze.

Awesome! Now, we are ready to explore the data a bit more.

Exploratory data analysis (EDA)

Let’s plot each column of the dataset:

Take the time to look at each plot and identify interesting trends. For the sake of length, we will only take the concentration of NOx.

Oxides of nitrogen are very harmful, as they react to form smog and acid rain, as well as being responsible for the formation of fine particles and ground level ozone. These have adverse health effects, so the concentration of NOx is a key feature of air quality.

Therefore, let’s remove all irrelevant columns before moving on to modelling:

Modelling

We start by importing Prophet:

Then, Prophet requires the date column to be named ds and the feature column to be named y:

Now, our data should be like this:

Then, we define a training set. For that we will hold out the last 30 entries for prediction and validation.

Then, we simply initialize Prophet, fit the model to the data, and make predictions!

And you should see the following:

Great! Here, yhat represents the prediction, while yhat_lower and yhat_upper represent the lower and upper bound of the prediction respectively.

Prophet allows you to easily plot the forecast:

And we get:

NOx concentration forecast

As you can see, Prophet simply used a straight downward line to predict the concentration of NOx in the future.

You can also use a command to see if the time series has any interesting features, such as seasonality:

And you get:

Here, Prophet only identified a downward trend with no seasonality.

Now, let’s evaluate performance of the model by calculating its mean absolute percentage error (MAPE) and mean absolute error (MAE):

And you should see that the MAPE is 13.86% and the MAE is 109.32, which is not that bad! Remember that we did not fine tune the model at all.

Finally, let’s just plot the forecast with its upper and lower bounds:

And you get:","['forecasting', 'column', 'prediction', 'predict', 'series', 'dataset', 'need', 'rid', 'concentration', 'data', 'plot', 'prophet']","Time Series Forecasting with ProphetLearn how to use Facebook’s Prophet to predict air qualityPhoto by Frédéric Paulussen on UnsplashProducing high quality forecasts is hard for many machine learning engineers.
For those reasons, Facebook open sourced Prophet, a forecasting tool available in both Python and R. This tool allows both experts and non-experts to produce high quality forecasts with minimal efforts.
Here, we will use Prophet to help us predict air quality!
Prophet allows you to easily plot the forecast:And we get:NOx concentration forecastAs you can see, Prophet simply used a straight downward line to predict the concentration of NOx in the future.
You can also use a command to see if the time series has any interesting features, such as seasonality:And you get:Here, Prophet only identified a downward trend with no seasonality.",en,['Marco Peixeiro'],2019-02-18 22:00:13.496000+00:00,"{'Data Science', 'Artificial Intelligence', 'Python', 'Machine Learning', 'Programming'}","{'https://miro.medium.com/max/4104/1*9eZmWnx6W29_LQsOmi6D8g.png', 'https://miro.medium.com/max/60/1*9eZmWnx6W29_LQsOmi6D8g.png?q=20', 'https://miro.medium.com/max/3106/1*z6JsOl_ciAo-CgKcGQFNZA.png', 'https://miro.medium.com/max/3100/1*i3viG6Eni1kAh1tCNACLPg.png', 'https://miro.medium.com/max/3224/1*t897EYoHD5BYPmUpP9N_2A.png', 'https://miro.medium.com/max/60/1*gQ6RqSWb28_xZ-sArG6mAA.png?q=20', 'https://miro.medium.com/max/60/1*ukfCbYNAuVVkBxCDtF2bvA.png?q=20', 'https://miro.medium.com/max/60/1*uOUngSjithAkR0vVP7V4TQ.png?q=20', 'https://miro.medium.com/max/8192/1*D0PPo816oTV9Gb1I9bGLdQ.png', 'https://miro.medium.com/max/2280/1*rcEK5Szwwao_6_5gcTCkAw.png', 'https://miro.medium.com/max/60/1*cKB1BLHGotnYDVqotHKrqQ.png?q=20', 'https://miro.medium.com/max/60/1*KIcFPvN4yx_azg6H3zgAow.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3696/1*PbUM4TKMGFiK3jayH5yKMw.png', 'https://miro.medium.com/max/2344/1*ukfCbYNAuVVkBxCDtF2bvA.png', 'https://miro.medium.com/max/3024/1*u2j5koKsBoeUskxogO3srQ.png', 'https://miro.medium.com/max/2752/1*VoJ4TVN5h5La6x66nY45Ow.png', 'https://miro.medium.com/max/60/1*2RLSNDyS3kHY8HOKQCb9jg.png?q=20', 'https://miro.medium.com/max/3288/1*oEe4ei5mOeDWmPq7ajqO6Q.png', 'https://miro.medium.com/max/60/1*t897EYoHD5BYPmUpP9N_2A.png?q=20', 'https://miro.medium.com/max/60/1*cU6sUqYvP8dCZ-0lbs2y2Q.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*91vrhvLQGszAVrgtCXqjug.png?q=20', 'https://miro.medium.com/max/3098/1*2RLSNDyS3kHY8HOKQCb9jg.png', 'https://miro.medium.com/max/60/0*bvPNft9zZrIHiJyU?q=20', 'https://miro.medium.com/fit/c/96/96/1*k541TgCbh0ohl6lHVXEsUA.jpeg', 'https://miro.medium.com/max/3088/1*ZTU0aMvysNNKmZCJGeYHVw.png', 'https://miro.medium.com/max/60/1*ZTU0aMvysNNKmZCJGeYHVw.png?q=20', 'https://miro.medium.com/max/60/1*ACuA_WyCiFJJBIGyt5S9AA.png?q=20', 'https://miro.medium.com/max/60/1*Yvxk9AHGVvi-OAbIMWwC2g.png?q=20', 'https://miro.medium.com/max/60/1*z6JsOl_ciAo-CgKcGQFNZA.png?q=20', 'https://miro.medium.com/max/8192/1*ACuA_WyCiFJJBIGyt5S9AA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*i3viG6Eni1kAh1tCNACLPg.png?q=20', 'https://miro.medium.com/max/60/1*oEe4ei5mOeDWmPq7ajqO6Q.png?q=20', 'https://miro.medium.com/max/2598/1*tCWe70mP_LLUAUkzsNTevQ.png', 'https://miro.medium.com/max/60/1*PbUM4TKMGFiK3jayH5yKMw.png?q=20', 'https://miro.medium.com/max/60/1*tCWe70mP_LLUAUkzsNTevQ.png?q=20', 'https://miro.medium.com/max/3360/1*cU6sUqYvP8dCZ-0lbs2y2Q.png', 'https://miro.medium.com/max/9814/0*bvPNft9zZrIHiJyU', 'https://miro.medium.com/max/4304/1*Yvxk9AHGVvi-OAbIMWwC2g.png', 'https://miro.medium.com/max/1200/0*bvPNft9zZrIHiJyU', 'https://miro.medium.com/max/5048/1*KIcFPvN4yx_azg6H3zgAow.png', 'https://miro.medium.com/max/60/1*VoJ4TVN5h5La6x66nY45Ow.png?q=20', 'https://miro.medium.com/max/60/1*FtJUFWNNoEDIvyhpQWFa8A.png?q=20', 'https://miro.medium.com/max/724/1*gQ6RqSWb28_xZ-sArG6mAA.png', 'https://miro.medium.com/max/60/1*u2j5koKsBoeUskxogO3srQ.png?q=20', 'https://miro.medium.com/max/60/1*ABHYW7d6LpXZSC63KqfE0A.png?q=20', 'https://miro.medium.com/max/60/1*rcEK5Szwwao_6_5gcTCkAw.png?q=20', 'https://miro.medium.com/max/6464/1*cKB1BLHGotnYDVqotHKrqQ.png', 'https://miro.medium.com/max/4168/1*ABHYW7d6LpXZSC63KqfE0A.png', 'https://miro.medium.com/max/60/1*D0PPo816oTV9Gb1I9bGLdQ.png?q=20', 'https://miro.medium.com/max/5720/1*uOUngSjithAkR0vVP7V4TQ.png', 'https://miro.medium.com/fit/c/160/160/1*k541TgCbh0ohl6lHVXEsUA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3424/1*FtJUFWNNoEDIvyhpQWFa8A.png', 'https://miro.medium.com/max/5856/1*91vrhvLQGszAVrgtCXqjug.png'}",2020-03-05 00:24:41.787328,3.451563835144043
https://towardsdatascience.com/a-quick-start-of-time-series-forecasting-with-a-practical-example-using-fb-prophet-31c4447a2274,A Quick Start of Time Series Forecasting with a Practical Example using FB Prophet,"Table of Contents

Introduction

Time Series Analysis

Why Facebook Prophet?

2. The Prophet Forecasting Model

Saturating growth

Trend Change points

Seasonality, Holiday Effects, And Regressors

3. Case study: forecasting advertising spend with Prophet

4. Closing Summary

1.1 Time Series Analysis

Time series analysis is an approach to analyze time series data to extract meaningful characteristics of data and generate other useful insights applied in business situation. Generally, time-series data is a sequence of observations stored in time order. Time-series data often stands out when tracking business metrics, monitoring industrial processes and etc.

Time series analysis helps understand time based patterns of a set of metric data points which is critical for any business. Techniques of time series forecasting could answer business questions like how much inventory to maintain, how much website traffic do you expect in your e-store to how many product will be sold in the next month — all of these are important time series problems to solve. The basic objective of time series analysis usually is to determine a model that describes the pattern of the time series and could be used for forecasting.

Classical time series forecasting techniques build on stats models which requires lots of effort to tune models and expect in data and industry. The person has to tune the parameters of the method with regards to the specific problem when a forecasting model doesn’t perform as expected. Tuning these methods requires a thorough understanding of how the underlying time series models work. It’s difficult for some organizations to handling those forecasting without data science teams. And it might seem doesn’t profitable for an organization to have a bunch of expects on board if there is no need a build a complex forecasting platform or other services.

1.2 Why Facebook Prophet?

Facebook developed an open sourcing Prophet, a forecasting tool available in both Python and R. It provides intuitive parameters which are easy to tune. Even someone who lacks deep expertise in time-series forecasting models can use this to generate meaningful predictions for a variety of problems in business scenarios.

From Facebook Prophet website:

“ Producing high quality forecasts is not an easy problem for either machines or for most analysts. We have observed two main themes in the practice of creating a variety of business forecasts:

Completely automatic forecasting techniques can be brittle and they are often too inflexible to incorporate useful assumptions or heuristics.

Analysts who can product high quality forecasts are quite rare because forecasting is a specialized data science skill requiring substantial experience. ”

1.3 Highlights of Facebook Prophet

Very fast, since it’s built in Stan, a programming language for statistical inference written in C++.

An additive regression model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects: 1. A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data 2. A yearly seasonal component modeled using Fourier series 3. A weekly seasonal component using dummy variables 4. A user-provided list of important holidays.

Robust to missing data and shifts in the trend, and typically handles outliers .

Easy procedure to tweak and adjust forecast while adding domain knowledge or business insights.

2.1 The Prophet Forecasting Model

The Prophet uses a decomposable time series model with three main model components: trend, seasonality, and holidays. They are combined in the following equation:

y(t)= g(t) + s(t) + h(t) + εt

g(t): piecewise linear or logistic growth curve for modeling non-periodic changes in time series

s(t): periodic changes (e.g. weekly/yearly seasonality)

h(t): effects of holidays (user provided) with irregular schedules

εt: error term accounts for any unusual changes not accommodated by the model

Using time as a regressor, Prophet is trying to fit several linear and non linear functions of time as components. Modeling seasonality as an additive component is the same approach taken by exponential smoothing in Holt-Winters technique . Prophet is framing the forecasting problem as a curve-fitting exercise rather than looking explicitly at the time based dependence of each observation within a time series.

2.2 Saturating growth

Set a carrying capacity cap to specify the maximum achievable point due to the business scenarios or constraints: market size, total population size, maximum budget, etc.

to specify the maximum achievable point due to the business scenarios or constraints: market size, total population size, maximum budget, etc. A saturating minimum, which is specified with a column floor in the same way as the cap column specifies the maximum.

2.3 Trend Changepoints

The model could be overfitting or underfitting while working with the trend component. The input of changepoints built in Prophet allowed is increased the fit becomes more flexible.

Here, you can nicely apply your business insights: big jump of sales during holidays, cost decreasing in future by purpose and etc. A user can also manually feed the changepoints with those business insights if it is required. In the below plot, the dotted lines represent the changepoints for the given time series.

2.4 Seasonality, Holiday Effects, And Regressors

Seasonal effects s(t) are approximated by the following function:

Prophet has a built-in holiday feature which allows inputs of customized recurring events.

Finally, action time!

3. Case study: forecasting advertising spend with Prophet in Python

I took the sample data of advertising spend from a digital marketing platform. I also did some changes on purpose to make it a ‘fake’ data source in order to use in this case study.

Here, we try to use last 17 month data to predict the next 30 days ad spend.

Step 1: Import libraries and data set:

[Code]:

import pandas as pd

pd.set_option(‘display.max_columns’, None)

import numpy as np

from fbprophet import Prophet

%matplotlib inline

import matplotlib.pyplot as plt

sample=pd.read_csv(‘/…/ad_spend.csv’)

Step 2: Check data info

[Code]:

From the above, the data set contains one and half year daily advertising spend from 2017–06–01 to 2018–12–30. There are 577 rows and two columns( date and spend) in the data frame.

Let’s check the missing value:

there is no missing value (from the able below) which is great!👏

[Code]:

Step 3: Plot time-series data","['forecasting', 'fb', 'series', 'quick', 'trend', 'changes', 'example', 'practical', 'effects', 'data', 'model', 'spend', 'business', 'prophet', 'start', 'using']","The Prophet Forecasting ModelSaturating growthTrend Change pointsSeasonality, Holiday Effects, And Regressors3.
Closing Summary1.1 Time Series AnalysisTime series analysis is an approach to analyze time series data to extract meaningful characteristics of data and generate other useful insights applied in business situation.
Classical time series forecasting techniques build on stats models which requires lots of effort to tune models and expect in data and industry.
The person has to tune the parameters of the method with regards to the specific problem when a forecasting model doesn’t perform as expected.
2.1 The Prophet Forecasting ModelThe Prophet uses a decomposable time series model with three main model components: trend, seasonality, and holidays.",en,['Yang Lyla'],2019-01-10 07:38:16.146000+00:00,"{'Forecasting', 'Data Science', 'Facebook Prophet', 'Timeseries', 'Python3'}","{'https://miro.medium.com/max/1424/1*uQcPHjTpVsvF9hTCGrtyFQ.png', 'https://miro.medium.com/max/60/1*uQcPHjTpVsvF9hTCGrtyFQ.png?q=20', 'https://miro.medium.com/max/60/1*4idrEq5IEJc3uSuV4EymMg.png?q=20', 'https://miro.medium.com/max/60/1*sZGt9flML4ZWpcyiaV1SaQ.jpeg?q=20', 'https://miro.medium.com/max/1176/1*7IlyEnJkBHfg-Hs2IOjbbg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1158/1*4idrEq5IEJc3uSuV4EymMg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*5pEe9wRJCfIZ9OGUF8LQiA.png?q=20', 'https://miro.medium.com/max/1116/1*j1kJINtQsN-UROnLZIRCBw.png', 'https://miro.medium.com/max/1424/1*JhvUB6QRzRrC1MX_FPSaow.png', 'https://miro.medium.com/fit/c/96/96/1*khHHJKlJp-T01iiL8wSTHA.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2174/1*sZGt9flML4ZWpcyiaV1SaQ.jpeg', 'https://miro.medium.com/max/60/1*pb2-NsjVDMrdmJ0DBcnuFw.png?q=20', 'https://miro.medium.com/max/60/1*ZALnOr3q113L8x07Lw8T_g.png?q=20', 'https://miro.medium.com/max/60/1*f58MrKU4994qVfj-h3Ou_w.png?q=20', 'https://miro.medium.com/max/60/1*j1kJINtQsN-UROnLZIRCBw.png?q=20', 'https://miro.medium.com/max/1087/1*sZGt9flML4ZWpcyiaV1SaQ.jpeg', 'https://miro.medium.com/max/60/1*cnpHt71qOxrUeWkqGYDuFg.png?q=20', 'https://miro.medium.com/max/1398/1*5pEe9wRJCfIZ9OGUF8LQiA.png', 'https://miro.medium.com/max/1460/1*pb2-NsjVDMrdmJ0DBcnuFw.png', 'https://miro.medium.com/max/1464/1*f58MrKU4994qVfj-h3Ou_w.png', 'https://miro.medium.com/max/60/1*YzC60nVEb_kFkQl2jPJpoQ.png?q=20', 'https://miro.medium.com/max/2390/1*ZALnOr3q113L8x07Lw8T_g.png', 'https://miro.medium.com/max/1248/1*cnpHt71qOxrUeWkqGYDuFg.png', 'https://miro.medium.com/max/1834/1*YzC60nVEb_kFkQl2jPJpoQ.png', 'https://miro.medium.com/fit/c/160/160/1*khHHJKlJp-T01iiL8wSTHA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2494/1*qLwivRt7CgkqLHZ5Kas2bA.png', 'https://miro.medium.com/max/60/1*qLwivRt7CgkqLHZ5Kas2bA.png?q=20', 'https://miro.medium.com/max/60/1*JhvUB6QRzRrC1MX_FPSaow.png?q=20', 'https://miro.medium.com/max/60/1*7IlyEnJkBHfg-Hs2IOjbbg.png?q=20'}",2020-03-05 00:24:43.876187,2.0878713130950928
https://towardsdatascience.com/playing-with-time-series-data-in-python-959e2485bff8,Playing with time series data in python,"Modeling with Prophet

Facebook Prophet was released in 2017 and it is available for Python and R. Prophet is designed for analyzing time series with daily observations that display patterns on different time scales. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints, but I will stick to the basics to get a model up and running. I think that Prophet is probably a good choice for producing quick forecasts as it has intuitive parameters that can be tweaked by someone who has good domain knowledge but lacks technical skills in forecasting models. For more information on Prophet, readers can consult the official documentation here.

Before using Prophet, we rename the columns in our data to the correct format. The Date column must be called ‘ds’ and the value column we want to predict ‘y’. We used our daily summary data in the example below.

Then we import prophet, create a model and fit to the data. In prophet, the changepoint_prior_scale parameter is used to control how sensitive the trend is to changes, with a higher value being more sensitive and a lower value less sensitive. After experimenting with a range of values, I set this parameter to 0.10 up from the default value of 0.05.

To make forecasts, we need to create what is called a future dataframe. We specify the number of future periods to predict (two months in our case) and the frequency of predictions (daily). We then make predictions with the prophet model we created and the future dataframe.

That was pretty simple! The future dataframe contains the estimated household consumption for the next two months. We can visualize the prediction with a plot:

The black dots represent the actual values, the blue line indicates the forecasted values, and the light blue shaded region is the uncertainty.

As illustrated in the next figure, the region of uncertainty grows as we move further out in the future because the initial uncertainty propagates and grows over time.

Prophet also allows us to easily visualize the overall trend and the component patterns:

The yearly pattern is interesting as it seems to suggest that the household consumption increases in fall and winter, and decreases in spring and summer. Intuitively, this is exactly what we expected to see. Looking at the weekly trend, it seems that there is more consumption on Sunday than the other days of the week. Finally, the overall trend suggests that the consumption increases for a year before slowly declining. Further investigation is needed to try to explain this trend. In a next post we will try to find if there is a correlation with the weather.

LSTM prediction

The Long Short-Term Memory recurrent neural network has the promise of learning long sequences of observations. The blog article, “Understanding LSTM Networks”, does an excellent job at explaining the underlying complexity in an easy to understand way. Here’s an image depicting the LSTM internal cell architecture.

LSTM seems to be well suited for time series forecasting, and it may be. Let’s use our daily summary data once again.

LSTMs are sensitive to the scale of the input data, specifically when the sigmoid or tanh activation functions are used. It’s generally a good practice to rescale the data to the range of [0, 1] or [-1, 1], also called normalizing. We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.

Now we can split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets with 80% of the observations that we can use to train our model, leaving the remaining 20% for testing the model.

We can define a function to create a new dataset and use this function to prepare the train and test datasets for modeling.

The LSTM network expects the input data to be provided with a specific array structure in the form of: [samples, time steps, features].

Our data is currently in the form [samples, features] and we are framing the problem as two time steps for each sample. We can transform the prepared train and test input data into the expected structure as follows:

That’s all! We are now ready to design and fit our LSTM network for our example.

From the plot of loss, we can see that the model has comparable performance on both train and test datasets.

In the next figure we see that LSTM did a quite good job of fitting the test dataset.

Clustering

Last but not least, we can also do clustering with our sample data. There are quite a few different ways of performing clustering, but one way is to form clusters hierarchically. You can form a hierarchy in two ways: start from the top and split, or start from the bottom and merge. I decided to look at the latter in this post.

Let’s start with the data, we simply import the raw data and add two columns for the day of the year and the hour of the day.","['future', 'sensitive', 'train', 'series', 'trend', 'python', 'lstm', 'data', 'model', 'prophet', 'test', 'playing', 'value']","Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.
Then we import prophet, create a model and fit to the data.
LSTMs are sensitive to the scale of the input data, specifically when the sigmoid or tanh activation functions are used.
We can transform the prepared train and test input data into the expected structure as follows:That’s all!
Let’s start with the data, we simply import the raw data and add two columns for the day of the year and the hour of the day.",en,['Arnaud Zinflou'],2018-07-31 23:25:28.324000+00:00,"{'Data Science', 'Timeseries', 'Python', 'Machine Learning', 'Algorithms'}","{'https://miro.medium.com/max/60/1*t8HaMDjFP3NS6DMoGaKq1g.png?q=20', 'https://miro.medium.com/max/60/1*BBEPgHW6XqtkNZZSps4hgA.png?q=20', 'https://miro.medium.com/max/1152/1*jI-aama6wciCh3WfvAotgQ.png', 'https://miro.medium.com/max/60/1*BkKD9DjXiV_aobf93uRKPQ.png?q=20', 'https://miro.medium.com/max/54/1*8cUmaf-trYzU9uF8xKdTWQ.png?q=20', 'https://miro.medium.com/max/60/1*HwgmLro_MmA1N7Ygs97_ng.png?q=20', 'https://miro.medium.com/max/1152/1*L-QzlAHQMWh3_dHRzmm2Ng.png', 'https://miro.medium.com/max/60/1*mCzY7k5LVMoIJfYq8tjX6g.png?q=20', 'https://miro.medium.com/max/60/1*9Ro0vNC-wDN5dUd_xBBAdw.png?q=20', 'https://miro.medium.com/max/60/1*BSfU95ukixLmrB2b8EYpdA.png?q=20', 'https://miro.medium.com/max/1152/1*FHgyhUg6Szy1wYuODZzERg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/0*v527vlEy8lSVEwke.', 'https://miro.medium.com/max/60/1*-Lb9NWWRpDyr7S-ydnQjng.png?q=20', 'https://miro.medium.com/max/60/1*YPfaYS3oUqv9dWac0hNRpQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*safjXyVMwf9xS-nIaIKlIQ.png?q=20', 'https://miro.medium.com/max/60/1*uzn0FWknmDirBcdCiGzMJQ.png?q=20', 'https://miro.medium.com/max/1102/1*safjXyVMwf9xS-nIaIKlIQ.png', 'https://miro.medium.com/max/60/1*1yALSQMztFJWpVRPymi9NQ.png?q=20', 'https://miro.medium.com/max/1152/1*lqGcReA3oq9eygmUVdWd1g.png', 'https://miro.medium.com/max/1152/1*BBEPgHW6XqtkNZZSps4hgA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1152/1*BSfU95ukixLmrB2b8EYpdA.png', 'https://miro.medium.com/max/60/1*UioIKxe5DQRI_shJYkEdyQ.png?q=20', 'https://miro.medium.com/max/60/1*vCwi2cS5Y3OcxC9B9MMy5Q.png?q=20', 'https://miro.medium.com/max/1212/1*uIr_VV_OfsnWFXCYxhn-8w.png', 'https://miro.medium.com/max/56/1*J92-E3pStWER9Mxfncm_VQ.png?q=20', 'https://miro.medium.com/max/784/1*vCwi2cS5Y3OcxC9B9MMy5Q.png', 'https://miro.medium.com/max/1224/1*HwgmLro_MmA1N7Ygs97_ng.png', 'https://miro.medium.com/max/60/1*ejgJXusSQYv5Tgd_h01QxA.png?q=20', 'https://miro.medium.com/max/1152/1*4q9UE3-5R3d-nOXXKIwPiw.png', 'https://miro.medium.com/max/60/1*Hka1ex0x-HhoxNDni_yyBQ.png?q=20', 'https://miro.medium.com/max/1152/1*uzn0FWknmDirBcdCiGzMJQ.png', 'https://miro.medium.com/max/1118/1*8cUmaf-trYzU9uF8xKdTWQ.png', 'https://miro.medium.com/max/1152/1*80sHEjUJzU5IRCZWlNKyKg.png', 'https://miro.medium.com/max/1152/1*QtyMm1AEp0PvTlrQexugSA.png', 'https://miro.medium.com/max/60/1*4q9UE3-5R3d-nOXXKIwPiw.png?q=20', 'https://miro.medium.com/max/1016/1*6zD0EO22rpHW9LpIZ8m3lQ.png', 'https://miro.medium.com/max/60/1*hnLOB5EA47n0AECoiDENGw.png?q=20', 'https://miro.medium.com/max/60/1*lqGcReA3oq9eygmUVdWd1g.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*FHgyhUg6Szy1wYuODZzERg.png?q=20', 'https://miro.medium.com/max/1114/1*1yALSQMztFJWpVRPymi9NQ.png', 'https://miro.medium.com/max/1184/1*YfGlKJDnBJxItMrgLrYxAQ.png', 'https://miro.medium.com/max/60/1*jI-aama6wciCh3WfvAotgQ.png?q=20', 'https://miro.medium.com/max/60/1*1o6xUgm3zojoQMK4C6SN8g.png?q=20', 'https://miro.medium.com/max/1036/1*hnLOB5EA47n0AECoiDENGw.png', 'https://miro.medium.com/max/1196/1*Hka1ex0x-HhoxNDni_yyBQ.png', 'https://miro.medium.com/max/1210/1*1o6xUgm3zojoQMK4C6SN8g.png', 'https://miro.medium.com/max/60/1*QtyMm1AEp0PvTlrQexugSA.png?q=20', 'https://miro.medium.com/max/1152/1*7UWXMMSlSvNvHmWV6oBRjg.png', 'https://miro.medium.com/max/1040/1*t8HaMDjFP3NS6DMoGaKq1g.png', 'https://miro.medium.com/fit/c/96/96/0*v527vlEy8lSVEwke.', 'https://miro.medium.com/max/696/1*J92-E3pStWER9Mxfncm_VQ.png', 'https://miro.medium.com/max/520/1*wEdwo63tGQd52WTgV_zDvg.png', 'https://miro.medium.com/max/1152/1*mCzY7k5LVMoIJfYq8tjX6g.png', 'https://miro.medium.com/max/60/1*6zD0EO22rpHW9LpIZ8m3lQ.png?q=20', 'https://miro.medium.com/max/1152/1*ejgJXusSQYv5Tgd_h01QxA.png', 'https://miro.medium.com/max/1222/1*YPfaYS3oUqv9dWac0hNRpQ.png', 'https://miro.medium.com/max/60/1*YfGlKJDnBJxItMrgLrYxAQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*80sHEjUJzU5IRCZWlNKyKg.png?q=20', 'https://miro.medium.com/max/60/1*uIr_VV_OfsnWFXCYxhn-8w.png?q=20', 'https://miro.medium.com/max/60/1*wEdwo63tGQd52WTgV_zDvg.png?q=20', 'https://miro.medium.com/max/2204/1*safjXyVMwf9xS-nIaIKlIQ.png', 'https://miro.medium.com/max/878/1*UioIKxe5DQRI_shJYkEdyQ.png', 'https://miro.medium.com/max/1192/1*bLhgvQAN3qa-MuHLsvSBUw.png', 'https://miro.medium.com/max/1222/1*BkKD9DjXiV_aobf93uRKPQ.png', 'https://miro.medium.com/max/1152/1*-Lb9NWWRpDyr7S-ydnQjng.png', 'https://miro.medium.com/max/60/1*L-QzlAHQMWh3_dHRzmm2Ng.png?q=20', 'https://miro.medium.com/max/1206/1*QqMeV4cwiS9jID92waOs9Q.png', 'https://miro.medium.com/max/1058/1*9Ro0vNC-wDN5dUd_xBBAdw.png', 'https://miro.medium.com/max/60/1*bLhgvQAN3qa-MuHLsvSBUw.png?q=20', 'https://miro.medium.com/max/60/1*QqMeV4cwiS9jID92waOs9Q.png?q=20', 'https://miro.medium.com/max/60/1*7UWXMMSlSvNvHmWV6oBRjg.png?q=20'}",2020-03-05 00:24:50.969361,7.092246770858765
https://towardsdatascience.com/get-a-glimpse-of-future-using-time-series-forecasting-using-auto-arima-and-artificial-intelligence-273efabec6aa,Get a glimpse of future using time series forecasting using Auto-ARIMA and Artificial Intelligence,"Get a glimpse of future using time series forecasting using Auto-ARIMA and Artificial Intelligence

Time Series Forecasting using Auto-ARIMA in python.

AI and future

Currently, there is a lot of development going on in Artificial intelligence research to get an accurate glimpse of the future. If any mathematical model predicts future data taking input as only time then that terminology called as time series forecasting. There are many machine learning and deep learning algorithms which can perform time series predictions like LSTM and ARIMA. Before going into technical part we will see the need of time series forecasting.

The need of Time Series Forecasting

When it comes to time series forecasting everyone’s dream is predict stock prices and become rich in one day. But there are lots of other important applications of time series forecasting some of them are listed below,

Risk Management

In short in any small or big business risk management is very important. Which means if any company know when unplanned and unwanted events will occur they can manage the resources accordingly to avoid loss. For example, suppose a company wants to know how much attrition will happen in next month or next quarter so that they can allocate essential human resources required. This will saves the companies time and resources.

Climate change

Predicting climate change is one of the most important applications of time series forecasting because we can manage unwanted natural disasters like floods, drought, volcano eruptions, etc. In short, we can save millions of lives using time series forecasting.

What is ARIMA?

ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. This is one of the easiest and effective machine learning algorithm to performing time series forecasting. This is the combination of Auto Regression and Moving average. First, let’s understand AR part of ARIMA. Autoregression is a time series model that uses observations from previous time steps as input to the regression equation to predict the value at the next time step. In simple words, it performs regression in previous time step t-1 to predict t. Now MA stands for moving average which is also called as rolling mean. Basically, we are calculating the simple average in a particular time frame and dividing it with the total number of time frames taken.

What is Auto-ARIMA?

Before training simple ARIMA model we have to figure out p(AR), q(MA), d(Back Timeshift) which is not as easy as fine-tuning other machine learning models. We have to reduce stationarity and get those values which so time-consuming as well as boring.

Auto-ARIMA uses brute force and tries different combinations of p, q, and d and then returns the best model after evaluation. It uses mean squared error to evaluate the best model. It also uses Akaike Information Criteria (AIC) and Bayesian information criterion (BIC) which are statistical measures of goodness of fit and the simplicity of the model.

Implementation of Auto-ARIMA in python

There is a python library called the pyramid which is required for performing auto arima on AirPassanger data to predict no of passengers in next month. Simply use pip to install pyramid as shown below,

pip install pyramid-arima

First, we will import important libraries

importing libraries

After that, we will load data and convert it into time series data by setting the index of the data frame to time as shown below,

Plotting the time series data is also important to see the trend and seasonality in data. We will plot our data using matplotlib function.

After this, we will split data into training and testing set for validation and evaluation purposes.

Now we will train ARIMA.

After this simply we will do prediction and evaluation of the model using traditional r2score.

And we are done for today, for greater accuracy try set max values to more higher numbers. For full code is click here.

So that’s it if you want to have the glimpse of future you have to do the above steps and then draw the final conclusion. There are lots of good models out there to perform time series forecasting like LSTM, SARIMAX, etc we will see them soon in my upcoming articles until then if you like this article don’t forget to give claps and if you have any questions write a comment until then happy learning.","['intelligence', 'forecasting', 'glimpse', 'future', 'predict', 'series', 'autoarima', 'artificial', 'learning', 'data', 'model', 'important', 'using', 'uses']","Get a glimpse of future using time series forecasting using Auto-ARIMA and Artificial IntelligenceTime Series Forecasting using Auto-ARIMA in python.
If any mathematical model predicts future data taking input as only time then that terminology called as time series forecasting.
Before going into technical part we will see the need of time series forecasting.
The need of Time Series ForecastingWhen it comes to time series forecasting everyone’s dream is predict stock prices and become rich in one day.
In short, we can save millions of lives using time series forecasting.",en,['Veer Khot'],2018-12-15 16:10:16.245000+00:00,"{'Forecasting', 'Auto Arima', 'Artificial Intelligence', 'Timeseries', 'Machine Learning'}","{'https://miro.medium.com/max/1330/1*zzX4K4RlaRHTwcj9IBHHbw.jpeg', 'https://miro.medium.com/max/3280/1*2rhKFE4Pi-m900Rii9fpiQ.jpeg', 'https://miro.medium.com/max/60/1*AS6r2cdAZHpLWCwjlgMZOA.png?q=20', 'https://miro.medium.com/max/2336/1*MQX_Dy2rx0HLr4LbspePsg.png', 'https://miro.medium.com/max/60/1*2rhKFE4Pi-m900Rii9fpiQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*xW2aRQMRkd1wC56rqxLJ4A.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3632/1*LEeRKvmSGB_ZkN48kJf0pg.png', 'https://miro.medium.com/max/60/1*LEeRKvmSGB_ZkN48kJf0pg.png?q=20', 'https://miro.medium.com/max/60/1*zzX4K4RlaRHTwcj9IBHHbw.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3320/1*AS6r2cdAZHpLWCwjlgMZOA.png', 'https://miro.medium.com/max/1200/1*2rhKFE4Pi-m900Rii9fpiQ.jpeg', 'https://miro.medium.com/max/60/1*VJiSZ9xO1GSqoLEYxJUaIQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*kxd0mYXCf2oGZbaTZV4Ryw.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1422/1*xW2aRQMRkd1wC56rqxLJ4A.jpeg', 'https://miro.medium.com/max/60/1*RWDXd2I4V8m4RfPS-Ax_Yw.png?q=20', 'https://miro.medium.com/max/2760/1*RWDXd2I4V8m4RfPS-Ax_Yw.png', 'https://miro.medium.com/max/2816/1*eBIrUPeE9tC8K3lPu6o5_A.png', 'https://miro.medium.com/max/60/1*MQX_Dy2rx0HLr4LbspePsg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2280/1*VJiSZ9xO1GSqoLEYxJUaIQ.png', 'https://miro.medium.com/fit/c/160/160/1*kxd0mYXCf2oGZbaTZV4Ryw.jpeg', 'https://miro.medium.com/max/60/1*eBIrUPeE9tC8K3lPu6o5_A.png?q=20'}",2020-03-05 00:24:52.679349,1.7089836597442627
https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56,Forecasting Exchange Rates Using ARIMA In Python,"Forecasting Exchange Rates Using ARIMA In Python

How ARIMA can forecast fx rates time series data

Nearly all sectors use time series data to forecast future time points. Forecasting future can assist analysts and management in making better calculated decisions to maximise returns and minimise risks. I will be demonstrating how we can forecast exchange rates in this article. If you are new to finance and want to understand what exchange rates are then please read my article “Best way To Learn Finance? Understand Market Data”. It provides a basic overview of market data. Exchange rates are dependent on a range of factors such as supply and demand, government policies, country’s growth rates etc. For more information on economical indicators that can impact exchange rates, please have a look at my article “Everything You Need To Know To Assess And Compare Countries”.

Forecasting Exchange Rates

Recently, a number of technological advancements have been introduced that can forecast future time points quickly, efficiently and accurately. One of them is introduction of statistical and machine learning (ML) models in Python. I provided an overview of basics of python in my article “Python From Scratch”.

In this article, I will use ARIMA model to forecast exchange rates.

In my blog “How do I predict time series?” and “Understanding Auto Regressive Moving Average Model — ARIMA”, I outlined the concept of time series and how ARIMA works. In this blog, I will be using Python programming language with Jupyter notebook to explain how to use ARIMA in python to forecast exchange rates.

Please read FinTechExplained disclaimer.

Using Pandas To Load Exchange Rates Data

Pandas is one of the most popular Python libraries. It is built on type of Numpy python library and offers a range of features including:

Object Creation, Data analysis and data loading. It has inbuilt Statistic functions, can merge/join/union multiple collections. It can also help us in grouping, pivoting and plotting the data. Additionally Pandas is very efficient library to load data from csv, HDF5 and excel. Lastly a range of styling and formatting can be applied to the data.

We will be using pandas to:

1. Load time series exchange rates from a csv (comma separated) file

2. View first 10 records

3. View basic statistical information on the data

Load time series exchange rates from a csv file

I have prepared a file which includes daily closing GBP/USD exchange rates since 31/12/1998. File is stored here. This file contains data with two columns: Data and GBP/USD Close

Ensure file is saved in the same location as your python notebook.

Type in following lines to load the file and view first 10 records:

To import a library, do: Import <library name> as <alias>

Note: GetData(fileName) is a method that takes in file name as an argument.

Press Alt + Enter to view basic statistical information ont he data

The image below shows the first 10 records of the csv file by calling GetData(fileName) method:

Let’s get useful statistics out and plot the exchange rates

Type in: exchangeRatesSeries.describe() to see the stats as shown below:

describe() shows a number of useful metrics including:

count — Number of records, mean — expected value, std — Standard deviation telling us dispersion of data around the mean, min — Minimum value in the set, max — Maximum value in the set along with a range of percentiles. Percentiles can help us understand probability distribution of our data.

Plotting the loaded data

Html styling can be added to change feel-and-look in Python. I am setting the plot colour to green:

#plot the time series data

exchangeRatesSeries.plot(color=’green’)

Notice, passing color=’green’ creates a green line graph.

Type in: exchangeRatesSeries.hist() to show historgram.

Histograms can help us understand distribution of data which in return helps us in forecasting a variable:

matplotlib can help us with plotting data. We can also import matplotlib by writing:

from matplotlib import pyplot

Then plot time series by writing:

pyplot.plot(exchangeRateSeries)

ARIMA With StatsModels Package

StatsModels is a powerful python library that is rich with statistical models. StatsModels library contains a number of models which can be used to forecast and predict data. This library holds a number of diagnostic tools too. We are going to use ARIMA model in StatsModels package to forecast exchange rates.

ARIMA Introduction

ARIMA model has 3 parameters:

P — Auto regressive feature of the model

D — Differencing order

Q — Moving average feature of the model

Once we import statsmodels, use the tsa.arima_model and give it an alias of ARIMA:

from statsmodels.tsa.arima_model import ARIMA

For in depth details of the parameters, please visit: http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html

ARIMA parameters can be changed to attain different forecasting behaviour. I have defined a user defined method that takes in training set and value for each of the three parameter.The function first creates ARIMA model. It then executes fit() and then forecast() on the model. Forecast() returns a predicted value.

Copy and paste these lines into your notebook:

ARIMA(…) creates ARIMA model. Fits() fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter and Forecast() returns an estimated value based on the fitted ARIMA model.

We can think of Fit() as a process that generates best fit line curve that gives least error. More on this topic is covered in my blog: How Good Is My Predicted Model — Regression Analysis?

p, q, d parameters can be tweeked to get better results.

This is an example of how we can pass in time series data and use ARIMA to predict a value based on the actual observed data:

By running the script, we see the predicted value:

We passed in random values as training set. ARIMA model then fitted itself and predicted the next value as 15.219305

We can also pass in exogenous variables, dates, frequency of time series etc to the ARIMA model.

Lastly, Let’s Use ARIMA In Python To Forecast Exchange Rates

Now that we understand how to use python Pandas to load csv data and how to use StatsModels to predict value, let’s combine all of the knowledge acquired in this blog to forecast our sample exchange rates.

Copy and paste this code. It is a combination of all of the concepts which we have learnt in this blog.

Press Alt+Enter. Python will start calling ARIMA model in a loop with the actual data. 70% of data is used to train the model and the rest 30% is used to test the accuracy. Each time a newly predicted value is produced.

Actual and predicted values will be printed on the notebook.

Finally actual vs predicted values will be plotted on the chart

View Mean Squared Error

I have also imported an additional library sklearn which I will be using in my future blogs.

Copy paste this line:

“from sklearn.metrics import mean_squared_error” to import the library.

Finally print Mean Squared Error:

Mean squared error calculates an average of the difference between actual and predicted data and tells you how good your model is. More information can be found in my blog: How Good Is My Predictive Model — Regression Analysis

By running the code below, we can view the actual, forecasted values along with a line graph and total mean squared error:

As you can see, we printed actual and predicted values. Additionally, we plotted predicted values in red with MSE of 1.551.

For complete notebook, please visit here.

Further Improvements

Forecasting exchange rates can be improved by:

Constantly updating model parameters

By inputting additional factors that impact exchange rates and their correlations into account

Model parameters can also be updated via machine learning and optimisation techniques.

Lastly factors and their correlations can be stressed to ensure forecasted exchange rates take extreme scenarios into account.

Final Notes

This article demonstrated how to use python to forecast exchange rates using ARIMA model. Financial markets can move in any direction and this makes it very hard, if not impossible, to accurately predict exchange rates. Having said that, the sole purpose of forecasting exchange rates via ARIMA is to help us in taking calculated decisions that maximise returns and minimise risks. Forecasted exchange rates are dependent on the assumptions imposed by ARIMA model which are based on auto regression, integrated and moving average concepts.

ARIMA is a simple yet powerful model. It assumes that the historic values dictate behaviour of present. It also assumes that the data does not contain anomalies, is stationary and model parameters along with error term is constant.

Although ARIMA does not take stresses in market data as input, economical and political conditions, or correlations of all risk factors to forecast exchange rates but the simple example demonstrated above can be useful for forecasting movements of stable currencies in normal conditions in which past behaviour dictates present and values.

Please let me know if you have any feedback.","['rates', 'forecast', 'forecasting', 'arima', 'series', 'python', 'exchange', 'data', 'model', 'predicted', 'value', 'using']","Forecasting Exchange Rates Using ARIMA In PythonHow ARIMA can forecast fx rates time series dataNearly all sectors use time series data to forecast future time points.
Exchange rates are dependent on a range of factors such as supply and demand, government policies, country’s growth rates etc.
In this blog, I will be using Python programming language with Jupyter notebook to explain how to use ARIMA in python to forecast exchange rates.
View basic statistical information on the dataLoad time series exchange rates from a csv fileI have prepared a file which includes daily closing GBP/USD exchange rates since 31/12/1998.
Final NotesThis article demonstrated how to use python to forecast exchange rates using ARIMA model.",en,['Farhad Malik'],2019-01-11 20:43:57.527000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Fintech', 'Trading'}","{'https://miro.medium.com/max/1134/1*vNx8lwqEx7iQ-yRRInOVmA.png', 'https://miro.medium.com/max/844/1*-I1LL5pIZOJQuoTNgDf4kA.png', 'https://miro.medium.com/fit/c/96/96/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/max/946/1*JHFve4qD2YOLA3C5e08p5w.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/14720/0*Qq9eArJqx5ah79HE', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/58/1*vNx8lwqEx7iQ-yRRInOVmA.png?q=20', 'https://miro.medium.com/max/1150/1*HPucCRa6aT6fYhwrtjUulA.png', 'https://miro.medium.com/max/60/1*e039YWssYj6DooPL6sqHqA.png?q=20', 'https://miro.medium.com/max/60/1*HPucCRa6aT6fYhwrtjUulA.png?q=20', 'https://miro.medium.com/max/60/1*-I1LL5pIZOJQuoTNgDf4kA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1112/1*Azyp0p4gUYk3hMAoBwG-dg.png', 'https://miro.medium.com/max/60/1*Azyp0p4gUYk3hMAoBwG-dg.png?q=20', 'https://miro.medium.com/max/1200/0*Qq9eArJqx5ah79HE', 'https://miro.medium.com/max/60/1*JHFve4qD2YOLA3C5e08p5w.png?q=20', 'https://miro.medium.com/max/1274/1*e039YWssYj6DooPL6sqHqA.png', 'https://miro.medium.com/fit/c/160/160/1*7MhP4LsHsl3G_oE3LBVlsg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*Qq9eArJqx5ah79HE?q=20'}",2020-03-05 00:24:59.108309,6.428959608078003
https://medium.com/@deeksha.sharma25/import-sql-dump-in-postgres-docker-container-in-10-min-ca16d9264f86,Import SQL dump in postgres docker container in 10 min,"Recently, I started working on a visualisation problem for an existing data set which was a sql dump from postgres database. To view the schema, relationships, attributes of this given database, I need to have postgres db and be able to retrieve these details, right? Well not really, Docker is cool enough to not give you that much of pain.

Follow these steps and get the details of this sql dump in 10 min. If Docker is already installed on your machine, then jump to Step2 directly.

Follow these instructions to install Docker on any platform Get the most recent Docker image of postgres using the below command. Ensure that your current directory contains the “dump.sql” file from postgres db. We bind mount a volume from the current directory so that when “my-postgres” container is started, it has access to the sql dump file.

docker run --name my-postgres -v `pwd`:/docker-entrypoint-initdb.d -d postgres

Note: Please note that this is the official docker image of postgres from Docker hub. The default user is “postgres”. If your database dump has some specific user privileges, for example tables can only be created by admin, then your container will exit by throwing an error. To resolve this, replace the existing username with “postgres” in your .sql file.

3. Get inside the postgres docker container.

docker exec -it my-postgres bash

4. List all databases for postgres user.

psql -U postgres -l

5. Choose the relevant db you want to use.

psql -U postgres -d <database-name>

6. Get all the tables of this database using command /d as shown below.

$<database-name>=# /d

7. To view the schema of a specific table.

$ \d <table-name>

8. To quit the sql terminal on *nux/Mac machine.","['min', 'dump', 'import', 'd', 'postgres', 'docker', 'sql', 'container', 'mypostgres', 'db', 'view', 'database', 'using']","Recently, I started working on a visualisation problem for an existing data set which was a sql dump from postgres database.
Follow these steps and get the details of this sql dump in 10 min.
Follow these instructions to install Docker on any platform Get the most recent Docker image of postgres using the below command.
docker run --name my-postgres -v `pwd`:/docker-entrypoint-initdb.d -d postgresNote: Please note that this is the official docker image of postgres from Docker hub.
Get inside the postgres docker container.",en,['Deeksha Sharma'],2018-05-12 07:02:23.305000+00:00,"{'Docker', 'DevOps', 'Postgresql'}","{'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/80/80/1*RC4BkmLFS6z_vhguAbBL4w.jpeg', 'https://miro.medium.com/fit/c/160/160/1*kclLdTuOxS4FkW8wQEShLw.png', 'https://miro.medium.com/fit/c/96/96/1*kclLdTuOxS4FkW8wQEShLw.png', 'https://miro.medium.com/fit/c/80/80/1*lJAwiC07Xql5EDO38zNI4g.png', 'https://miro.medium.com/fit/c/80/80/1*MkOGcZi_VVG2jD-QIvIZGA.jpeg'}",2020-03-05 00:25:00.458166,1.3488550186157227
https://medium.com/google-cloud/containers-four-ways-457f4b7dd898,Containers Four Ways,"At one of the local meet-ups I attend containers and containerization is a relatively common topic of discussion. Google is a big advocate of containers so it probably isn’t a surprise that there are four different ways to run a container on Google Cloud Platform.

Kubernetes and Google Container Engine

The most obvious way to run a container on Google Cloud is using Kubernetes with Google Container Engine. Kubernetes is an open source container orchestration tool that started at Google. I like that when using Kubernetes you describe what a healthy running system should look like. You write out files describing what sets of containers you want (pods), how many of each set you need, and how they should communicate with each other. Kubernetes schedules the containers on VMs and takes care of the networking.

Kubernetes and Container Engine are a good choice when your system has multiple components with different scaling characteristics. For example, you might have an authentication service and a data processing service. With Kubernetes, you can easily accommodate the different scaling characteristics of each of these services. You can put the containers that run each service in different pods and create different auto-scaling rules for each type of pod. Kubernetes gives you the advantage of auto-scaling logic and a degree of auto-healing. If a VM hosting your containers goes down Kubernetes will reschedule the containers that were running on that VM automatically on one of the remaining hosts. And finally, Kubernetes supports rolling updates which is great if you practice continuous deployment on your application.

App Engine Flex

If you only have one container and that container runs a web app then App Engine Flex is a great way to run it. App Engine Flex gives you many of the advantages of App Engine (auto-scaling, auto-healing, integrated diagnostics, and simple deployment) but you aren’t limited to the restricted runtimes App Engine used to use. Instead, there are base Dockerfile based images for each supported language. And if the existing container images don’t work for you then you can use your own container image by using custom run times.

App Engine Flex is still in beta but it is worth a look if your application runs a web site or web backend and you want to run it in a fully managed platform. With App Engine Flex you get all the advantages of a container based platform (fast start up time and portability) but you don’t have to write the Dockerfile yourself if you don’t want to. Also Flex has great support for updating your app seamlessly.

Container Image for Managed Instance Groups

If you only have one container you need to run you can also use a container image running on Managed Instance Groups. Container images on Managed Instance Groups. This feature is currently in Alpha so you must request to be added to the white list to use it.

To use this method for running containers on GCP you have to write your own Dockerfile, build it, and push it to Google Container Registry. You can also use a publicly available image from Docker Hub. This way to run containers is a good match for container workloads that aren’t for web sites, such as data processing. It is also a good choice when you want full control over how containers are distributed over your VMs. It supports both running a single container on each VM and running multiple containers. It does not currently support rolling updates, which is another reason it isn’t ideal for websites.

Google Compute Engine

The final way to run containers on Google Cloud Platform is by running your containers on plain old Compute Engine VMs. Running your containers this way gives you the most control over how they are run. But you have to install all the infrastructure for running the containers yourself. If you need very tight control over exactly how your containers run or you aren’t ready to move to a more managed tool this another option for running containers on GCP.

Conclusion

That is four different ways to run containers on Google Cloud Platform. Hopefully there’s one that will work for you.","['kubernetes', 'different', 'way', 'engine', 'container', 'ways', 'run', 'google', 'containers', 'app', 'running']","Google is a big advocate of containers so it probably isn’t a surprise that there are four different ways to run a container on Google Cloud Platform.
App Engine Flex gives you many of the advantages of App Engine (auto-scaling, auto-healing, integrated diagnostics, and simple deployment) but you aren’t limited to the restricted runtimes App Engine used to use.
Google Compute EngineThe final way to run containers on Google Cloud Platform is by running your containers on plain old Compute Engine VMs.
If you need very tight control over exactly how your containers run or you aren’t ready to move to a more managed tool this another option for running containers on GCP.
ConclusionThat is four different ways to run containers on Google Cloud Platform.",en,['Aja Hammerly'],2017-07-14 16:22:40.439000+00:00,"{'Google Cloud Platform', 'Containers', 'Google Compute Engine', 'Kubernetes', 'Cloud Computing'}","{'https://miro.medium.com/fit/c/80/80/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg', 'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/max/72/1*FUjLiCANvATKeaJEeg20Rw.png', 'https://miro.medium.com/fit/c/80/80/1*za37g0oQyyg1od7wEzB5NA.jpeg', 'https://miro.medium.com/fit/c/80/80/2*ettXEQz0weveNd9aTPa0qw.jpeg', 'https://miro.medium.com/fit/c/160/160/0*XjDhFI7kwqVXg0nl.jpeg', 'https://miro.medium.com/fit/c/96/96/0*XjDhFI7kwqVXg0nl.jpeg', 'https://miro.medium.com/fit/c/160/160/1*FUjLiCANvATKeaJEeg20Rw.png'}",2020-03-05 00:25:01.185502,0.7273354530334473
https://medium.com/@ssola/building-microservices-with-python-part-2-9f951199094a,"Building Microservices with Python, Part 2","In the previous article about Microservices with Python, I was talking about how to create a basic API with some simple steps. You can run that directly with Python, but let’s say we have to integrate it with other systems, such as a Database, ElasticSearch or RabbitMQ.

As I mentioned in the previous article, you can find all the code I am generating on those articles in this GitHub repo: https://github.com/ssola/python-flask-microservice

Third part is available in this link.

What is Docker?

Docker is the world’s leading software container platform. Developers use Docker to eliminate “works on my machine” problems when collaborating on code with co-workers. Operators use Docker to run and manage apps side-by-side in isolated containers to get better compute density.

Docker allow us to have different container for each one of our dependencies. In this example we are going to include two dependencies in our project:

Elasticsearch

RabbitMQ

But before starting, I need to explain some concepts:

Image

A Docker image is a read-only template with instructions for creating a Docker container. For example, an image might contain an Ubuntu operating system with Apache web server and your web application installed.

Container

A Docker container is a runnable instance of a Docker image. You can run, start, stop, move, or delete a container using Docker API or CLI commands. When you run a container, you can provide configuration metadata such as networking information or environment variables.

Registry

A docker registry is a library of images. A registry can be public or private, and can be on the same server as the Docker daemon or Docker client, or on a totally separate server.

Creating your docker-compose

From this step onwards I am assuming you have Docker installed on your machine. In case you do not have it installed, just follow this link.

It is a common practice to put the docker-compose.yml and Dockerfile in the root of your project. With this approach, you can share your development environment with anyone cloning the project.

Docker Compose allow you to create many containers needed for your service. For instance, in the Docker Compose we can define that we need a MySQL instance and an Elasticsearch up-and-running. We can set both on the same file, then with a single command, we can bring up or down those services.

Dockerfile allow you to create the recipe for a new container. In this case, let’s imagine we need to create a new container to run our application. With Dockerfile we can define to:

Install Python 3.6

Clone my project

Make it run

Understanding the Dockerfile

The Dockerfile allow you to define a recipe to build your image. Based on a given one, like alpine, you can set a list of commands to be executed to achieve some state, for instance, running a python app.

This is the definition of our recipe:

Recipe for a Python 3.5 image

It basically get an alpine image with a python 3.5 already installed. This is nice because we can save some time, we only need to create a directory where to put our files, install the dependencies of our application and that is it.

This is a simple example, but for a production ready image we need to think about:

Setting environment variables depending on production/staging environment.

Tune the image with production ready settings on Flask.

Store the image in some private registry to be able to do immutable deployments.

Defining my dependencies

Open the docker-compose.yml file. Now, we are going to define the dependencies I stated above, Elasticsearch and RabbitMQ.

Most of the time you do not need to create your images. Fortunately, we have many of them publicly available in DockerHub.

Let’s start with the Elasticsearch one. Be careful, when looking for an image in DockerHub check the version of the applications you want to install. I found so many Elasticsearch 1.7 images when the current version is 5.2.0.

In this case, I chose the official image from Elastic elasticsearch:5-alpine .

Probably your first question is, what that Alpine is? Alpine is a base image based on Alpine Linux. It is a super minimal distribution that allows us to create super small containers. It is a good idea to search for images built on top of that base image.

In our docker-compose.yml we are going to add these lines:

And the RabbitMQ dependency too, after the elasticsearch one:

With these few lines we have a composition of two containers. We can build up or down both services at the same time. But before building and running our services we should understand what we just did.

Image : It defines which image should be use to build this service

: It defines which image should be use to build this service Environment : You can define some environment variable that will be used on the image. For instance we can define the user and pass for RabbitMQ

: You can define some environment variable that will be used on the image. For instance we can define the user and pass for RabbitMQ Ports : You can define the port forwarding from the image to your machine

: You can define the port forwarding from the image to your machine Command : If needed you can execute a command after starting the image

: If needed you can execute a command after starting the image Volumes: You can define a mapping between the image filesystem and your host filesystem. This is useful if you want to share the Elasticsearch content between containers

Now we can execute the command docker-compose up -d this will bring up two containers, one with the Elasticsearch and the other one with RabbitMQ. The -d means it will detach the process from your session.","['elasticsearch', 'create', 'environment', 'docker', 'microservices', 'python', 'image', 'need', 'container', 'define', 'instance', 'building']","In the previous article about Microservices with Python, I was talking about how to create a basic API with some simple steps.
You can run that directly with Python, but let’s say we have to integrate it with other systems, such as a Database, ElasticSearch or RabbitMQ.
ContainerA Docker container is a runnable instance of a Docker image.
With Dockerfile we can define to:Install Python 3.6Clone my projectMake it runUnderstanding the DockerfileThe Dockerfile allow you to define a recipe to build your image.
This is the definition of our recipe:Recipe for a Python 3.5 imageIt basically get an alpine image with a python 3.5 already installed.",en,['Sergio Sola'],2017-12-22 14:44:54.025000+00:00,"{'Flask', 'Python', 'Microservices', 'Docker', 'DevOps'}","{'https://miro.medium.com/fit/c/80/80/1*Kp-ufsm5xJmlNzrAzCtBYw.jpeg', 'https://miro.medium.com/max/3396/1*PsSQBD_2QPyKsvknchw5zg.jpeg', 'https://miro.medium.com/max/1200/1*PsSQBD_2QPyKsvknchw5zg.jpeg', 'https://miro.medium.com/fit/c/80/80/1*PH4B9mGLVkKo1RG0yLZyYQ.jpeg', 'https://miro.medium.com/max/762/1*B6Rz27SaoKAlqeSHRWVkjw.png', 'https://miro.medium.com/max/60/1*PsSQBD_2QPyKsvknchw5zg.jpeg?q=20', 'https://miro.medium.com/max/46/1*B6Rz27SaoKAlqeSHRWVkjw.png?q=20', 'https://miro.medium.com/max/5208/1*jxwtjo4JTOUuQz3bWUKJRg.png', 'https://miro.medium.com/fit/c/96/96/1*rGBvpqyVY2MncHmjWCoXAw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*RIY1J-PXwS8EemkGvOpE3Q@2x.jpeg', 'https://miro.medium.com/fit/c/160/160/1*rGBvpqyVY2MncHmjWCoXAw.jpeg', 'https://miro.medium.com/max/60/1*jxwtjo4JTOUuQz3bWUKJRg.png?q=20'}",2020-03-05 00:25:03.498360,2.3128581047058105
https://medium.com/hydramicroservices/up-and-running-with-docker-and-redis-14ac069938cb,Up and running with Docker and Redis,"Install Docker and launch Redis on your Mac!

By far the easiest and most useful way to launch Redis is simply by installing Docker and running Redis as a container. If that sounds complex, hang in there — you won’t believe how easy that actually is!

First, visit the Docker website and download Docker CE for your Mac, Windows or Linux box. Once you have Docker installed you should be able to confirm it’s properly installed using the docker version command.

Next, let’s prepare Docker to run containers in a configuration called a swarm.

With Docker running, we’re ready to pull in Redis.

To do this we’ll utilize a Docker Stack for organizing containers within our cluster. This will only require creating three small files, a configuration file and two shell scripts.

Firstly, let’s create a Docker compose file, compose.yml, which will describe our stack.

Above we’re specifying that we’re using a network called servicenet and defining a single service called redis .

Next, let’s create our two shell scripts. The first, stackup.sh will launch our stack and the second, stackdn.sh will tear our stack down when we no longer need it.

stackup.sh

stackdn.sh

We launch our stack using stackup.sh:

Above we see that the stackup.sh script created a new network and a service called test_redis. It does that by reading the compose.yml we created earlier.

Using the docker ps command we’re able to see the running container.","['called', 'redis', 'lets', 'shell', 'launch', 'docker', 'stackupsh', 'running', 'using', 'stack']","Install Docker and launch Redis on your Mac!
By far the easiest and most useful way to launch Redis is simply by installing Docker and running Redis as a container.
Once you have Docker installed you should be able to confirm it’s properly installed using the docker version command.
With Docker running, we’re ready to pull in Redis.
To do this we’ll utilize a Docker Stack for organizing containers within our cluster.",en,['Carlos Justiniano'],2018-08-06 14:43:11.067000+00:00,"{'Mac', 'Redis', 'Docker'}","{'https://miro.medium.com/max/60/1*uEAyoy8hjxcEorhu10VS8w.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*H7yIqOfLMIrBrEEUgUZ1DA.jpeg', 'https://miro.medium.com/max/60/1*eZmLjN7VWFguDKxpH9MxoA.png?q=20', 'https://miro.medium.com/max/60/1*dms4-9f_9GYIOmlQbsFDmw.png?q=20', 'https://miro.medium.com/max/1520/1*maFk59qoQKPkqhljQmwyUw.png', 'https://miro.medium.com/fit/c/160/160/1*Ioxw9_BddEpe8z3dQMcAfw.png', 'https://miro.medium.com/max/1200/1*eZmLjN7VWFguDKxpH9MxoA.png', 'https://miro.medium.com/max/200/1*N1NNMQU6CUltSBj2atE2tQ.png', 'https://miro.medium.com/max/60/1*3Rfajvi7OOT2yTl-ERXBCA.png?q=20', 'https://miro.medium.com/max/1252/1*dms4-9f_9GYIOmlQbsFDmw.png', 'https://miro.medium.com/fit/c/96/96/2*bgLMi0t7E_P9tQmifNyhig.jpeg', 'https://miro.medium.com/max/60/1*maFk59qoQKPkqhljQmwyUw.png?q=20', 'https://miro.medium.com/max/1624/1*9g-utlk910hlM6Ke9-7uiQ.png', 'https://miro.medium.com/max/2764/1*uEAyoy8hjxcEorhu10VS8w.png', 'https://miro.medium.com/max/2296/1*3Rfajvi7OOT2yTl-ERXBCA.png', 'https://miro.medium.com/fit/c/80/80/2*bgLMi0t7E_P9tQmifNyhig.jpeg', 'https://miro.medium.com/max/58/1*9g-utlk910hlM6Ke9-7uiQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*bgLMi0t7E_P9tQmifNyhig.jpeg', 'https://miro.medium.com/max/2674/1*eZmLjN7VWFguDKxpH9MxoA.png'}",2020-03-05 00:25:05.112309,1.6139490604400635
https://medium.com/@ssola/building-microservices-with-python-part-i-5240a8dcc2fb,"Building Microservices with Python , Part I","Currently I am working in my current job as a Software Engineer at HelloFresh on the DataWarehouse Team. I am working on data pipelines and building tools around our infrastructure, like Documentr.

By the way, we are hiring in those positions: Data Engineer and Backend Engineer.

But at the end, I am a Software Engineer interested in many different areas. This post it is going to be focus on Backend Development and how I am building microservices on a personal project I am working.

You can find all the code in this repo: https://github.com/ssola/python-flask-microservice

Purpose

Nowadays it is a common practice to work in smaller applications, sharing the responsibility among many different services. I believe it is critical to have some standard tools between the teams working solving those problems.

In my opinion a Microservices should have some basic features:

Easy to start coding your logic, stop worrying about tools/patterns.

Documentation, an essential feature to share how your service it is going to work. In this case, Swagger works pretty well.

Serializing your input/output in a way shared among all applications. You need to chose a technology like Avro/Protobuf. This is mandatory to be sure all services are sharing the same entities.

Events, probably your application it is going to generate events that can be consumed by others. For instance, in my project, every time a new room is inserted into the system, that piece of information is serialized with avro and spread among all the other services.

Stack and Patterns

In this basic setup we are going to include those packages:

Flask (as a Framework)

(as a Framework) connexion (helpful tool to generate routes and Swagger docs)

(helpful tool to generate routes and Swagger docs) Flask-Injector (Dependency Injection package)

(Dependency Injection package) Avro (or any data serialization package)

In this case, I chose Flask because I find it super useful to build small services without all the learning curve of Django. I just need something to help to do the routing and I will include whatever I need on it.

Connexion is a project from Zalando that adds a layer on top of Flask to help you building your RESTFul API in a simpler manner, with the great benefit to have at the end Swagger docs. Connexion gives you as well an elegant solution to protect your service behind oAuth2 and a way to versioning your API.

Dependency Injection is a nice way to inject the dependencies your need in your methods/classes. For this purpose, I chose Flask-Injector, as you can see by the name it’s completely integrated on Flask. With this tool using the decorator @inject I can have the service I need, for instance, ElasticSearch or SQLAlchemy.

Did you have problems with services sending corrupted data or some payload with the wrong schema? Well, that can be solved using some Serialization tool like Avro or Protobuf. These tools help you to ensure whatever you are receiving or sending has the proper schema.

I am not covering all the topics related to Microservices, but this is a good starting point, and you are covering some of the important pieces of the puzzle.

Building your more than a Hello World service

Starting your environment

I am going to specify a step-by-step guide with al the steps you need to do to have a simple service up-and-running on your machine.

First of all, create a new virtual environment with Python 3.6.0. If you are not used to working with virtual environments in Python, stop reading and jump to this other article.

Then create a new project on your favorite IDE, I like to work with PyCharm. In the root of your project create a new requirements.txt file with these dependencies:

Flask

connexion

Flask-Injector

fastavro

Starting with Connexion

After installing the dependencies, we can start coding our app.py script on the same path we placed the requirements file.

As you can see, we are already using the package connexion. It is pretty simple what it is doing there.

Creates a new application defining to which port it will start (later on we can change which server we want to use to execute the service) Defines to which directory your swagger configurations are placed. You add a new api (you can have more than one in the same app), mainly the yaml file where you are going to write your routes and definitions.

At this point, we need to read a little bit how the connexion yaml file works.

Let’s create our first endpoint on the swagger/my_super_app.yaml file:

In this example, you can start understanding some features of connexion.

We have a get endpoint on the /v1.0/items/ path. When this endpoit it’s called it will execute the method search on the module api.items This module is exposed on the __init__.py file in the api module. This endpoint will return an array of objects of type Item , we defined this item as an object with an id and a name

Here you can see the code on the api.items module:

Executing this endpoint, it will return a json like this:

{

""0"": {

""name"": ""First item""

}

}

And going to your http://localhost:9090/ui you will find the Swagger docs.

Swagger docs

Without that much effort, we have an up-and-running simple API returning items. Obviously, this is a silly example. So the next step is to make use of injection to inject an ItemProvider in our search method.

Working with Flask Injector

Dependency Injection it is a great pattern to help us to organize our dependencies among objects.

In software engineering, dependency injection is a technique whereby one object supplies the dependencies of another object. A dependency is an object that can be used (a service). An injection is the passing of a dependency to a dependent object (a client) that would use it. The service is made part of the client’s state.[1] Passing the service to the client, rather than allowing a client to build or find the service, is the fundamental requirement of the pattern.

In our previous example, we have in the very same file a list of Items , as you can imagine that is not the way we should retrieve data. We can connect to databases to get this information. To achieve that, we can use the Repository Pattern or create a Data Service to make this example simple enough; we are going to create a Data Service where we can retrieve some data.

This simple example can be easily extended to get the data from any database or third party API.

Now, let’s bind this service and inject it in our search method. First of all, we need to do some changes on our app.py , we need to add the bindings on the Flask Injector.

Here, as you can see, we have a new method called configure , this will handle all the bindings to create the container. In this case, we are creating a new ItemsProvider with some default items.

Now, in our api.items we can inject this ItemsProvider .

This is great as well for testing. When we need to test our search method we can create new bindings to a mocked ItemsProvider. We are simplifying our dependencies and improving our code for testing purposes.

As I said, this is not a complicated example. But probably you can see now how you can inject SQLAlchemy into your ItemsProvider to get dat from a database.

Serializing your payloads

A common pitfall building services are sharing wrong payloads. For instance, you expect an Item object with a name and an id. But working with plain JSON is not possible to enforce it.

We can do it right now with your connexion yaml, it will throw exceptions if something is wrong. But in this case, we need to ensure the data between different services using for instance RabbitMQ.

We can use JSON as I said, but then for each application, you will need to have all the required validations. If your platform works with several programming languages that can be a hassle.

With some tools like Avro or Protobuf, we can have a common serialization objects among different programming languages. A good practice would be to have a repo where you have all your Avro/Protobuf serializers. Those can be used almost for any programming language.

In this case, Avro gives us:

Rich data structures.

A compact, fast, binary data format.

A container file, to store persistent data.

Remote procedure call (RPC).

Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically typed languages.

A simple way to work with Avro and Python is using this library called fastavro.

You can find many examples in the GitHub repo.

Conclusion

With these small steps we can have a microservice up-and-running pretty quickly. With some nice features as:

Documentation

Dependency Injection

Global serializer

As you can imagine, this is not everything we need to do to have a production ready microservice. Here we have the steps to start building a solid skeleton.

In the next post I am going to write about other important features on microservices, like:

Logging & Metrics

CI/CD

Service Discovery

Dev environment with Docker

Please, let me know with your comments if you find any flaw or you have better ideas about how to improve those ideas.

You can continue reading part 2 here or part 3 here.

Recommended Books

Building Microservices: Designing Fine-Grained Systems

This is a very interesting book explaining some of the basic of microservices concepts.

Very useful to start getting all the concepts clear in your mind.","['create', 'way', 'start', 'going', 'need', 'microservices', 'python', 'simple', 'data', 'service', 'building', 'injection', 'swagger']","This post it is going to be focus on Backend Development and how I am building microservices on a personal project I am working.
In my opinion a Microservices should have some basic features:Easy to start coding your logic, stop worrying about tools/patterns.
If you are not used to working with virtual environments in Python, stop reading and jump to this other article.
A simple way to work with Avro and Python is using this library called fastavro.
Recommended BooksBuilding Microservices: Designing Fine-Grained SystemsThis is a very interesting book explaining some of the basic of microservices concepts.",en,['Sergio Sola'],2018-06-11 08:22:15.358000+00:00,"{'Flask', 'Python', 'Avro', 'Programming', 'Microservices'}","{'https://miro.medium.com/max/760/1*P-FpHBimwwBBZ_MNnkbT4w.png', 'https://miro.medium.com/fit/c/80/80/1*LvTplBafBE22hyL4HqdMIQ.png', 'https://miro.medium.com/max/46/1*P-FpHBimwwBBZ_MNnkbT4w.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*rGBvpqyVY2MncHmjWCoXAw.jpeg', 'https://miro.medium.com/max/60/1*xpc9FGmlLHq6zejClQry7g.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*rGBvpqyVY2MncHmjWCoXAw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*W_eqMnrJ8bsRz-qJZmRnhA.jpeg', 'https://miro.medium.com/max/3968/1*xpc9FGmlLHq6zejClQry7g.png', 'https://miro.medium.com/max/1200/1*xpc9FGmlLHq6zejClQry7g.png', 'https://miro.medium.com/fit/c/80/80/2*1p0-7K-keK9AJa_mhPPQFQ.png'}",2020-03-05 00:25:06.988791,1.876481533050537
https://medium.com/hacksnextdoor/set-up-your-first-microservice-using-flask-and-docker-part-2-d8e078357500,Set up your first microservice using Flask and Docker: Part 2,"Since you’ve built your first service now its time to package it up and 🚢 it for all your friends to see! But how do you exactly do it? No worries I’ll be showing you the industry standard for packaging up a service and making it seamless to deploy to any cloud service (Now, Heroku, AWS, Google Cloud, etc) Learning how to use docker not only works for python services, but any a web app you may want deploy.

What is Docker?

Docker is the company driving the container movement and the only container platform provider to address every application across the hybrid cloud.

A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings.

How to integrate Docker

Go into your project directory in this case if you are cd my-flask-microservice

Create the following Dockerfile:

# Use an official Python runtime as a parent image

FROM python:2.7-slim



# Set the working directory to /app

WORKDIR /app



# Copy the current directory contents into the container at /app

ADD . /app



# Install any needed packages specified in requirements.txt

RUN pip install --trusted-host pypi.python.org -r requirements.txt



# Make port 80 available to the world outside this container

EXPOSE 80



# Define environment variable

ENV NAME World



# Run app.py when the container launches

CMD [""python"", ""app.py""]

Now run the build command. This creates a Docker image, which we’re going to tag using -t so it has a friendly name.

docker build -t friendlyhello .

Run the app, mapping your machine’s port 4000 to the container’s published port 80 using -p :

docker run -p 4000:80 friendlyhello

Now let’s run the app in the background, in detached mode by adding option -d :

docker run -d -p 4000:80 friendlyhello

You get the long container ID for your app and then are kicked back to your terminal. Your container is running in the background. You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands):

$ docker container ls

CONTAINER ID IMAGE COMMAND CREATED

1fa4ab2cf395 friendlyhello ""python app.py"" 28 seconds ago

You’ll see that CONTAINER ID matches what’s on http://localhost:4000 .

Congrats! You’ve setup you first docker container. 🎉

Now use docker container stop to end the process, using the CONTAINER ID , like so:

docker container stop 1fa4ab2cf395

Reference: https://docs.docker.com/get-started/part2/#tag-the-image","['set', 'id', 'port', 'docker', 'python', 'p', 'flask', 'container', 'app', 'run', 'service', 'using', 'microservice']","Docker is the company driving the container movement and the only container platform provider to address every application across the hybrid cloud.
This creates a Docker image, which we’re going to tag using -t so it has a friendly name.
You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands):$ docker container lsCONTAINER ID IMAGE COMMAND CREATED1fa4ab2cf395 friendlyhello ""python app.py"" 28 seconds agoYou’ll see that CONTAINER ID matches what’s on http://localhost:4000 .
You’ve setup you first docker container.
🎉Now use docker container stop to end the process, using the CONTAINER ID , like so:docker container stop 1fa4ab2cf395Reference: https://docs.docker.com/get-started/part2/#tag-the-image",en,['H A M Ii'],2018-08-08 15:04:58.599000+00:00,"{'Containers', 'Docker', 'Deployment', 'Python'}","{'https://miro.medium.com/fit/c/80/80/1*Kaci3GC9dEuP8PSidRn6zQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*PH4B9mGLVkKo1RG0yLZyYQ.jpeg', 'https://miro.medium.com/max/1200/1*X67_cjNBb6i767D76DvK_w.jpeg', 'https://miro.medium.com/max/242/1*QwEj1lBo5jatYB5IWfDqbg.png', 'https://miro.medium.com/fit/c/96/96/1*hvMgH9U8MCgjBErUV7KOMg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*gLnwzgPmzGw0RUOUmQQfQg.png', 'https://miro.medium.com/max/60/1*X67_cjNBb6i767D76DvK_w.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*hvMgH9U8MCgjBErUV7KOMg.jpeg', 'https://miro.medium.com/fit/c/80/80/2*JBsnXShZXNO_wrreu_ifaQ.jpeg', 'https://miro.medium.com/max/2560/1*X67_cjNBb6i767D76DvK_w.jpeg'}",2020-03-05 00:25:07.949662,0.9608712196350098
https://medium.com/@raymasson/how-to-develop-an-application-with-a-docker-container-and-a-volume-286437e08349,How to use volumes to make dynamic changes to code running in containers,"Short description

I have developed a small python application in order to test docker containers.

Some things must be done to run an application in a docker container:

1- Create a Dockerfile

2- Build a docker image

3- Run the docker container

The issue here is that every time there is a change in the source code, the steps 2 and 3 must be repeated.

It can be avoided with volumes. Thus, every developer can work with docker containers for testing on the fly their changes.

Run the application in a docker container



cd python-identidock

docker build -t identidock .

docker run -d -p 5000:5000 identidock git clone https://github.com/raymasson/python-identidock cd python-identidockdocker build -t identidock .docker run -d -p 5000:5000 identidock

Using a volume

Running an application in a docker container is nice. But, it implies that every time a change is done on the source code, the docker image must be built again and the docker container must be ran.

This can be avoided with a volume.

docker run -d -p 5000:5000 -v ""$PWD""/app:/app identidock

The -v option allows to use a volume. Every changes made on the host in the directory “$PWD”/app are applied to the directory /app in the running docker container.

So, you can change your source code, refresh the internet browser page and you’ll directly see changes.","['d', 'change', 'docker', 'changes', 'p', 'volumes', 'run', 'identidock', 'build', 'source', 'dynamic', 'application', 'containers', 'running', 'code']","docker run -d -p 5000:5000 identidock git clone https://github.com/raymasson/python-identidock cd python-identidockdocker build -t identidock .docker run -d -p 5000:5000 identidockUsing a volumeRunning an application in a docker container is nice.
But, it implies that every time a change is done on the source code, the docker image must be built again and the docker container must be ran.
docker run -d -p 5000:5000 -v ""$PWD""/app:/app identidockThe -v option allows to use a volume.
Every changes made on the host in the directory “$PWD”/app are applied to the directory /app in the running docker container.
So, you can change your source code, refresh the internet browser page and you’ll directly see changes.",en,['Raymond Masson'],2017-01-17 13:35:05.374000+00:00,"{'Docker', 'DevOps', 'Python'}","{'https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png', 'https://miro.medium.com/fit/c/80/80/1*FnGEmZk3Ny4uV7HAFVOulg.jpeg', 'https://miro.medium.com/fit/c/160/160/1*LcLOC-byefYBlOqkcT6d7A.jpeg', 'https://miro.medium.com/fit/c/96/96/1*LcLOC-byefYBlOqkcT6d7A.jpeg', 'https://miro.medium.com/max/60/1*IIqrVXQ9Xyu-hW_t1oiweQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*pPLuhaIv_JKl6PXaCGPWGA.png', 'https://miro.medium.com/fit/c/80/80/2*Nhx2FiKECTHrP-DpUnOqMg.jpeg', 'https://miro.medium.com/max/1528/1*IIqrVXQ9Xyu-hW_t1oiweQ.png'}",2020-03-05 00:25:08.629608,0.6799459457397461
https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b,Continuous Numeric Data,"Introduction

“Money makes the world go round” is something which you cannot ignore whether to choose to agree or disagree with it. A more apt saying in today’s digital revolutionary age would be “Data makes the world go round”. Indeed data has become a first class asset for businesses, corporations and organizations irrespective of their size and scale. Any intelligent system regardless of their complexity needs to be powered by data. At the heart of any intelligent system, we have one or more algorithms based on machine learning, deep learning or statistical methods which consume this data to gather knowledge and provide intelligent insights over a period of time. Algorithms are pretty naive by themselves and cannot work out of the box on raw data. Hence the need for engineering meaningful features from raw data is of utmost importance which can be understood and consumed by these algorithms.

A gentle refresher of the machine learning pipeline

Any intelligent system basically consists of an end-to-end pipeline starting from ingesting raw data, leveraging data processing techniques to wrangle, process and engineer meaningful features and attributes from this data. Then we usually leverage techniques like statistical models or machine learning models to model on these features and then deploy this model if necessary for future usage based on the problem to be solved at hand. A typical standard machine learning pipeline based on the CRISP-DM industry standard process model is depicted below.

A standard machine learning pipeline (source: Practical Machine Learning with Python, Apress/Springer)

Ingesting raw data and building models on top of this data directly would be foolhardy since we wouldn’t get desired results or performance and also algorithms are not intelligent enough to automatically extract meaningful features from raw data (there are automated feature extraction techniques which are enabled nowadays with deep learning methodologies to some extent, but more on that later!).

Our main area of focus falls under the data preparation aspect as pointed out in the figure above, where we deal with various methodologies to extract meaningful attributes or features from the raw data after it has gone through necessary wrangling and pre-processing.

Motivation

Feature engineering is an essential part of building any intelligent system. Even though you have a lot of newer methodologies coming in like deep learning and meta-heuristics which aid in automated machine learning, each problem is domain specific and better features (suited to the problem) is often the deciding factor of the performance of your system. Feature Engineering is an art as well as a science and this is the reason Data Scientists often spend 70% of their time in the data preparation phase before modeling. Let’s look at a few quotes relevant to feature engineering from several renowned people in the world of Data Science.

“Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.” — Prof. Andrew Ng.

This basically reinforces what we mentioned earlier about data scientists spending close to 80% of their time in engineering features which is a difficult and time-consuming process, requiring both domain knowledge and mathematical computations.

“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.” — Dr. Jason Brownlee

This gives us an idea about feature engineering being the process of transforming data into features to act as inputs for machine learning models such that good quality features help in improving the overall model performance. Features are also very much dependent on the underlying problem. Thus, even though the machine learning task might be same in different scenarios, like classification of emails into spam and non-spam or classifying handwritten digits, the features extracted in each scenario will be very different from the other.

Prof. Pedro Domingos from the University of Washington, in his paper titled, “A Few Useful Things to Know about Machine Learning” tells us the following.

“At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.” — Prof. Pedro Domingos

The final quote which should motivate you about feature engineering is from renowned Kaggler, Xavier Conort. Most of you already know that tough real-world machine learning problems are often posted on Kaggle regularly which is usually open to everyone.

“The algorithms we used are very standard for Kagglers. …We spent most of our efforts in feature engineering. … We were also very careful to discard features likely to expose us to the risk of over-fitting our model.” — Xavier Conort

Understanding Features

A feature is typically a specific representation on top of raw data, which is an individual, measurable attribute, typically depicted by a column in a dataset. Considering a generic two-dimensional dataset, each observation is depicted by a row and each feature by a column, which will have a specific value for an observation.

A generic dataset snapshot

Thus like in the example in the figure above, each row typically indicates a feature vector and the entire set of features across all the observations forms a two-dimensional feature matrix also known as a feature-set. This is akin to data frames or spreadsheets representing two-dimensional data. Typically machine learning algorithms work with these numeric matrices or tensors and hence most feature engineering techniques deal with converting raw data into some numeric representations which can be easily understood by these algorithms.

Features can be of two major types based on the dataset. Inherent raw features are obtained directly from the dataset with no extra data manipulation or engineering. Derived features are usually obtained from feature engineering, where we extract features from existing data attributes. A simple example would be creating a new feature “Age” from an employee dataset containing “Birthdate” by just subtracting their birth date from the current date.

There are diverse types and formats of data including structured and unstructured data. In this article, we will discuss various feature engineering strategies for dealing with structured continuous numeric data. All these examples are a part of one of my recent books ‘Practical Machine Learning with Python’ and you can access relevant datasets and code used in this article on GitHub. A big shout out also goes to Gabriel Moreira who helped me by providing some excellent pointers on feature engineering techniques.

Feature Engineering on Numeric Data

Numeric data typically represents data in the form of scalar values depicting observations, recordings or measurements. Here, by numeric data, we mean continuous data and not discrete data which is typically represented as categorical data. Numeric data can also be represented as a vector of values where each value or entity in the vector can represent a specific feature. Integers and floats are the most common and widely used numeric data types for continuous numeric data. Even though numeric data can be directly fed into machine learning models, you would still need to engineer features which are relevant to the scenario, problem and domain before building a model. Hence the need for feature engineering still remains. Let’s leverage python and look at some strategies for feature engineering on numeric data. We load up the following necessary dependencies first (typically in a Jupyter notebook).

import pandas as pd

import matplotlib.pyplot as plt

import numpy as np

import scipy.stats as spstats %matplotlib inline

Raw Measures

Like we mentioned earlier, raw numeric data can often be fed directly to machine learning models based on the context and data format. Raw measures are typically indicated using numeric variables directly as features without any form of transformation or engineering. Typically these features can indicate values or counts. Let’s load up one of our datasets, the Pokémon dataset also available on Kaggle.

poke_df = pd.read_csv('datasets/Pokemon.csv', encoding='utf-8') poke_df.head()

Snapshot of our Pokemon dataset

Pokémon is a huge media franchise surrounding fictional characters called Pokémon which stands for pocket monsters. In short, you can think of them as fictional animals with superpowers! This dataset consists of these characters with various statistics for each character.

Values

If you closely observe the data frame snapshot in the above figure, you can see that several attributes represent numeric raw values which can be used directly. The following snippet depicts some of these features with more emphasis.

poke_df[['HP', 'Attack', 'Defense']].head()

Features with (continuous) numeric data

Thus, you can directly use these attributes as features which are depicted in the above data frame. These include each Pokémon’s HP (Hit Points), Attack and Defense stats. In fact, we can also compute some basic statistical measures on these fields.

poke_df[['HP', 'Attack', 'Defense']].describe()

Basic descriptive statistics on numeric features

With this you can get a good idea about statistical measures in these features like count, average, standard deviation and quartiles.

Counts

Another form of raw measures include features which represent frequencies, counts or occurrences of specific attributes. Let’s look at a sample of data from the millionsong dataset which depicts counts or frequencies of songs which have been heard by various users.

popsong_df = pd.read_csv('datasets/song_views.csv',

encoding='utf-8')

popsong_df.head(10)

Song listen counts as a numeric feature

It is quite evident from the above snapshot that the listen_count field can be used directly as a frequency\count based numeric feature.

Binarization

Often raw frequencies or counts may not be relevant for building a model based on the problem which is being solved. For instance if I’m building a recommendation system for song recommendations, I would just want to know if a person is interested or has listened to a particular song. This doesn’t require the number of times a song has been listened to since I am more concerned about the various songs he\she has listened to. In this case, a binary feature is preferred as opposed to a count based feature. We can binarize our listen_count field as follows.

watched = np.array(popsong_df['listen_count'])

watched[watched >= 1] = 1

popsong_df['watched'] = watched

You can also use scikit-learn's Binarizer class here from its preprocessing module to perform the same task instead of numpy arrays.

from sklearn.preprocessing import Binarizer bn = Binarizer(threshold=0.9)

pd_watched = bn.transform([popsong_df['listen_count']])[0]

popsong_df['pd_watched'] = pd_watched

popsong_df.head(11)

Binarizing song counts

You can clearly see from the above snapshot that both the methods have produced the same result. Thus we get a binarized feature indicating if the song was listened to or not by each user which can be then further used in a relevant model.

Rounding

Often when dealing with continuous numeric attributes like proportions or percentages, we may not need the raw values having a high amount of precision. Hence it often makes sense to round off these high precision percentages into numeric integers. These integers can then be directly used as raw values or even as categorical (discrete-class based) features. Let’s try applying this concept in a dummy dataset depicting store items and their popularity percentages.

items_popularity = pd.read_csv('datasets/item_popularity.csv',

encoding='utf-8') items_popularity['popularity_scale_10'] = np.array(

np.round((items_popularity['pop_percent'] * 10)),

dtype='int')

items_popularity['popularity_scale_100'] = np.array(

np.round((items_popularity['pop_percent'] * 100)),

dtype='int')

items_popularity

Rounding popularity to different scales

Based on the above ouputs, you can guess that we tried two forms of rounding. The features depict the item popularities now both on a scale of 1–10 and on a scale of 1–100. You can use these values both as numerical or categorical features based on the scenario and problem.

Interactions

Supervised machine learning models usually try to model the output responses (discrete classes or continuous values) as a function of the input feature variables. For example, a simple linear regression equation can be depicted as

where the input features are depicted by variables

having weights or coefficients denoted by

respectively and the goal is to predict the response y.

In this case, this simple linear model depicts the relationship between the output and inputs, purely based on the individual, separate input features.

However, often in several real-world scenarios, it makes sense to also try and capture the interactions between these feature variables as a part of the input feature set. A simple depiction of the extension of the above linear regression formulation with interaction features would be

where the features represented by

denote the interaction features. Let’s try engineering some interaction features on our Pokémon dataset now.

atk_def = poke_df[['Attack', 'Defense']]

atk_def.head()

From the output data frame, we can see that we have two numeric (continuous) features, Attack and Defence . We will now build features up to the 2nd degree by leveraging scikit-learn .

from sklearn.preprocessing import PolynomialFeatures pf = PolynomialFeatures(degree=2, interaction_only=False,

include_bias=False)

res = pf.fit_transform(atk_def)

res

Output

------ array([[ 49., 49., 2401., 2401., 2401.],

[ 62., 63., 3844., 3906., 3969.],

[ 82., 83., 6724., 6806., 6889.],

...,

[ 110., 60., 12100., 6600., 3600.],

[ 160., 60., 25600., 9600., 3600.],

[ 110., 120., 12100., 13200., 14400.]])

The above feature matrix depicts a total of five features including the new interaction features. We can see the degree of each feature in the above matrix as follows.

pd.DataFrame(pf.powers_, columns=['Attack_degree',

'Defense_degree'])

Looking at this output, we now know what each feature actually represents from the degrees depicted here. Armed with this knowledge, we can assign a name to each feature now as follows. This is just for ease of understanding and you should name your features with better, easy to access and simple names.

intr_features = pd.DataFrame(res, columns=['Attack', 'Defense',

'Attack^2',

'Attack x Defense',

'Defense^2'])

intr_features.head(5)

Numeric features with their interactions

Thus the above data frame represents our original features along with their interaction features.

Binning

The problem of working with raw, continuous numeric features is that often the distribution of values in these features will be skewed. This signifies that some values will occur quite frequently while some will be quite rare. Besides this, there is also another problem of the varying range of values in any of these features. For instance view counts of specific music videos could be abnormally large (Despacito we’re looking at you!) and some could be really small. Directly using these features can cause a lot of issues and adversely affect the model. Hence there are strategies to deal with this, which include binning and transformations.

Binning, also known as quantization is used for transforming continuous numeric features into discrete ones (categories). These discrete values or numbers can be thought of as categories or bins into which the raw, continuous numeric values are binned or grouped into. Each bin represents a specific degree of intensity and hence a specific range of continuous numeric values fall into it. Specific strategies of binning data include fixed-width and adaptive binning. Let’s use a subset of data from a dataset extracted from the 2016 FreeCodeCamp Developer\Coder survey which talks about various attributes pertaining to coders and software developers.

fcc_survey_df = pd.read_csv('datasets/fcc_2016_coder_survey_subset.csv',

encoding='utf-8') fcc_survey_df[['ID.x', 'EmploymentField', 'Age', 'Income']].head()

Sample attributes from the FCC coder survey dataset

The ID.x variable is basically a unique identifier for each coder\developer who took the survey and the other fields are pretty self-explanatory.

Fixed-Width Binning

Just like the name indicates, in fixed-width binning, we have specific fixed widths for each of the bins which are usually pre-defined by the user analyzing the data. Each bin has a pre-fixed range of values which should be assigned to that bin on the basis of some domain knowledge, rules or constraints. Binning based on rounding is one of the ways, where you can use the rounding operation which we discussed earlier to bin raw values.

Let’s now consider the Age feature from the coder survey dataset and look at its distribution.

fig, ax = plt.subplots()

fcc_survey_df['Age'].hist(color='#A9C5D3', edgecolor='black',

grid=False)

ax.set_title('Developer Age Histogram', fontsize=12)

ax.set_xlabel('Age', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)

Histogram depicting developer age distribution

The above histogram depicting developer ages is slightly right skewed as expected (lesser aged developers). We will now assign these raw age values into specific bins based on the following scheme

Age Range: Bin

---------------

0 - 9 : 0

10 - 19 : 1

20 - 29 : 2

30 - 39 : 3

40 - 49 : 4

50 - 59 : 5

60 - 69 : 6

... and so on

We can easily do this using what we learnt in the Rounding section earlier where we round off these raw age values by taking the floor value after dividing it by 10.

fcc_survey_df['Age_bin_round'] = np.array(np.floor(

np.array(fcc_survey_df['Age']) / 10.)) fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]

Binning by rounding

You can see the corresponding bins for each age have been assigned based on rounding. But what if we need more flexibility? What if we want to decide and fix the bin widths based on our own rules\logic? Binning based on custom ranges will help us achieve this. Let’s define some custom age ranges for binning developer ages using the following scheme.

Age Range : Bin

---------------

0 - 15 : 1

16 - 30 : 2

31 - 45 : 3

46 - 60 : 4

61 - 75 : 5

75 - 100 : 6

Based on this custom binning scheme, we will now label the bins for each developer age value and we will store both the bin range as well as the corresponding label.

bin_ranges = [0, 15, 30, 45, 60, 75, 100]

bin_names = [1, 2, 3, 4, 5, 6] fcc_survey_df['Age_bin_custom_range'] = pd.cut(

np.array(

fcc_survey_df['Age']),

bins=bin_ranges)

fcc_survey_df['Age_bin_custom_label'] = pd.cut(

np.array(

fcc_survey_df['Age']),

bins=bin_ranges,

labels=bin_names)

# view the binned features

fcc_survey_df[['ID.x', 'Age', 'Age_bin_round',

'Age_bin_custom_range',

'Age_bin_custom_label']].iloc[10a71:1076]

Custom binning scheme for developer ages

Adaptive Binning

The drawback in using fixed-width binning is that due to us manually deciding the bin ranges, we can end up with irregular bins which are not uniform based on the number of data points or values which fall in each bin. Some of the bins might be densely populated and some of them might be sparsely populated or even empty! Adaptive binning is a safer strategy in these scenarios where we let the data speak for itself! That’s right, we use the data distribution itself to decide our bin ranges.

Quantile based binning is a good strategy to use for adaptive binning. Quantiles are specific values or cut-points which help in partitioning the continuous valued distribution of a specific numeric field into discrete contiguous bins or intervals. Thus, q-Quantiles help in partitioning a numeric attribute into q equal partitions. Popular examples of quantiles include the 2-Quantile known as the median which divides the data distribution into two equal bins, 4-Quantiles known as the quartiles which divide the data into 4 equal bins and 10-Quantiles also known as the deciles which create 10 equal width bins. Let’s now look at the data distribution for the developer Income field.

fig, ax = plt.subplots()

fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3',

edgecolor='black', grid=False)

ax.set_title('Developer Income Histogram', fontsize=12)

ax.set_xlabel('Developer Income', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)

Histogram depicting developer income distribution

The above distribution depicts a right skew in the income with lesser developers earning more money and vice versa. Let’s take a 4-Quantile or a quartile based adaptive binning scheme. We can obtain the quartiles easily as follows.

quantile_list = [0, .25, .5, .75, 1.]

quantiles = fcc_survey_df['Income'].quantile(quantile_list)

quantiles

Output

------

0.00 6000.0

0.25 20000.0

0.50 37000.0

0.75 60000.0

1.00 200000.0

Name: Income, dtype: float64

Let’s now visualize these quantiles in the original distribution histogram!

fig, ax = plt.subplots()

fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3',

edgecolor='black', grid=False) for quantile in quantiles:

qvl = plt.axvline(quantile, color='r')

ax.legend([qvl], ['Quantiles'], fontsize=10) ax.set_title('Developer Income Histogram with Quantiles',

fontsize=12)

ax.set_xlabel('Developer Income', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)

Histogram depicting developer income distribution with quartile values

The red lines in the distribution above depict the quartile values and our potential bins. Let’s now leverage this knowledge to build our quartile based binning scheme.

quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']

fcc_survey_df['Income_quantile_range'] = pd.qcut(

fcc_survey_df['Income'],

q=quantile_list)

fcc_survey_df['Income_quantile_label'] = pd.qcut(

fcc_survey_df['Income'],

q=quantile_list,

labels=quantile_labels)



fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range',

'Income_quantile_label']].iloc[4:9]

Quantile based bin ranges and labels for developer incomes

This should give you a good idea of how quantile based adaptive binning works. An important point to remember here is that the resultant outcome of binning leads to discrete valued categorical features and you might need an additional step of feature engineering on the categorical data before using it in any model. We will cover feature engineering strategies for categorical data shortly in the next part!

Statistical Transformations

We talked about the adverse effects of skewed data distributions briefly earlier. Let’s look at a different strategy of feature engineering now by making use of statistical or mathematical transformations.We will look at the Log transform as well as the Box-Cox transform. Both of these transform functions belong to the Power Transform family of functions, typically used to create monotonic data transformations. Their main significance is that they help in stabilizing variance, adhering closely to the normal distribution and making the data independent of the mean based on its distribution

Log Transform

The log transform belongs to the power transform family of functions. This function can be mathematically represented as

which reads as log of x to the base b is equal to y. This can then be translated into

which indicates as to what power must the base b be raised to in order to get x. The natural logarithm uses b=e where e = 2.71828 popularly known as Euler’s number. You can also use base b=10 used popularly in the decimal system.

Log transforms are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. This tends to make the skewed distribution as normal-like as possible. Let’s use log transform on our developer Income feature which we used earlier.

fcc_survey_df['Income_log'] = np.log((1+ fcc_survey_df['Income']))

fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_log']].iloc[4:9]

Log transform on developer income

The Income_log field depicts the transformed feature after log transformation. Let’s look at the data distribution on this transformed field now.

income_log_mean = np.round(np.mean(fcc_survey_df['Income_log']), 2) fig, ax = plt.subplots()

fcc_survey_df['Income_log'].hist(bins=30, color='#A9C5D3',

edgecolor='black', grid=False)

plt.axvline(income_log_mean, color='r')

ax.set_title('Developer Income Histogram after Log Transform',

fontsize=12)

ax.set_xlabel('Developer Income (log scale)', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)

ax.text(11.5, 450, r'$\mu$='+str(income_log_mean), fontsize=10)

Histogram depicting developer income distribution after log transform

Based on the above plot, we can clearly see that the distribution is more normal-like or gaussian as compared to the skewed distribution on the original data.

Box-Cox Transform

The Box-Cox transform is another popular function belonging to the power transform family of functions. This function has a pre-requisite that the numeric values to be transformed must be positive (similar to what log transform expects). In case they are negative, shifting using a constant value helps. Mathematically, the Box-Cox transform function can be denoted as follows.

Such that the resulted transformed output y is a function of input x and the transformation parameter λ such that when λ = 0, the resultant transform is the natural log transform which we discussed earlier. The optimal value of λ is usually determined using a maximum likelihood or log-likelihood estimation. Let’s now apply the Box-Cox transform on our developer income feature. First we get the optimal lambda value from the data distribution by removing the non-null values as follows.

income = np.array(fcc_survey_df['Income'])

income_clean = income[~np.isnan(income)]

l, opt_lambda = spstats.boxcox(income_clean)

print('Optimal lambda value:', opt_lambda)

Output

------

Optimal lambda value: 0.117991239456

Now that we have obtained the optimal λ value, let us use the Box-Cox transform for two values of λ such that λ = 0 and λ = λ(optimal) and transform the developer Income feature.

fcc_survey_df['Income_boxcox_lambda_0'] = spstats.boxcox(

(1+fcc_survey_df['Income']),

lmbda=0)

fcc_survey_df['Income_boxcox_lambda_opt'] = spstats.boxcox(

fcc_survey_df['Income'],

lmbda=opt_lambda)



fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_log',

'Income_boxcox_lambda_0',

'Income_boxcox_lambda_opt']].iloc[4:9]

Developer income distribution after Box-Cox transform

The transformed features are depicted in the above data frame. Just like we expected, Income_log and Income_boxcox_lamba_0 have the same values. Let’s look at the distribution of the transformed Income feature after transforming with the optimal λ.

income_boxcox_mean = np.round(

np.mean(

fcc_survey_df['Income_boxcox_lambda_opt']),2) fig, ax = plt.subplots()

fcc_survey_df['Income_boxcox_lambda_opt'].hist(bins=30,

color='#A9C5D3', edgecolor='black', grid=False)

plt.axvline(income_boxcox_mean, color='r')

ax.set_title('Developer Income Histogram after Box–Cox Transform',

fontsize=12)

ax.set_xlabel('Developer Income (Box–Cox transform)', fontsize=12)

ax.set_ylabel('Frequency', fontsize=12)

ax.text(24, 450, r'$\mu$='+str(income_boxcox_mean), fontsize=10)

Histogram depicting developer income distribution after Box-Cox transform

The distribution looks more normal-like similar to what we obtained after the log transform.

Conclusion

Feature engineering is a very important aspect of machine learning and data science and should never be ignored. While we have automated feature engineering methodologies like deep learning as well as automated machine learning frameworks like AutoML (which still stresses that it requires good features to work well!). Feature engineering is here to stay and even some of these automated methodologies often require specific engineered features based on the data type, domain and the problem to be solved.

We looked at popular strategies for feature engineering on continuous numeric data in this article. In the next part, we will look at popular strategies for dealing with discrete, categorical data and then move on to unstructured data types in future articles. Stay tuned!","['raw', 'numeric', 'feature', 'continuous', 'income', 'learning', 'engineering', 'based', 'data', 'features', 'values']","In this article, we will discuss various feature engineering strategies for dealing with structured continuous numeric data.
Feature Engineering on Numeric DataNumeric data typically represents data in the form of scalar values depicting observations, recordings or measurements.
Here, by numeric data, we mean continuous data and not discrete data which is typically represented as categorical data.
Integers and floats are the most common and widely used numeric data types for continuous numeric data.
We looked at popular strategies for feature engineering on continuous numeric data in this article.",en,"['Dipanjan', 'Dj']",2019-03-27 13:43:20.095000+00:00,"{'Feature Engineering', 'Data Science', 'Python', 'Machine Learning', 'Tds Feature Engineering'}","{'https://miro.medium.com/fit/c/160/160/1*Wy0cxXZpX-FrMWeXsqOOpg.png', 'https://miro.medium.com/max/60/1*o4Y35xo-01zui0TBe40CeQ.png?q=20', 'https://miro.medium.com/max/54/1*Qyb6YD_LbXtBe8ZXOarPJg.png?q=20', 'https://miro.medium.com/max/892/1*o4Y35xo-01zui0TBe40CeQ.png', 'https://miro.medium.com/max/800/1*b93f6b8rQfRdPRpvk5aXYA.png', 'https://miro.medium.com/max/60/1*dHU2YSkyw_pOBHcRUb7T3g.png?q=20', 'https://miro.medium.com/max/1296/1*dHU2YSkyw_pOBHcRUb7T3g.png', 'https://miro.medium.com/max/60/1*wN2EpH0MhOeF3HFVawzSfg.png?q=20', 'https://miro.medium.com/max/152/1*cFqSK0nMRhq8CEEdTQr4WA.png', 'https://miro.medium.com/max/140/1*7F3HR9uxHmEqdJbeoy2WVQ.png', 'https://miro.medium.com/max/60/1*I12zyebq9bL7uTLgT8CwBQ.png?q=20', 'https://miro.medium.com/max/60/1*lPpz9UPNyCcrLcZyBCk1kA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/138/1*I12zyebq9bL7uTLgT8CwBQ.png', 'https://miro.medium.com/max/138/1*wYFBF-DGcDTQlxukVhZRTg.png', 'https://miro.medium.com/max/60/1*FSamGcAXuEWvvFkHQ8McFQ.png?q=20', 'https://miro.medium.com/max/1628/1*UEVbo3a2WWK2sY1CH5RI9g.png', 'https://miro.medium.com/max/1292/1*u8poCsS-SGl8jTkbV-MbCg.png', 'https://miro.medium.com/max/60/1*aWMAGdYezTJPaVtkuvv3bA.png?q=20', 'https://miro.medium.com/max/60/1*R6ajYFdrsCZ6b3CVj_ZH4Q.png?q=20', 'https://miro.medium.com/max/60/1*31fhDxmXoGrfxbk64gg4Cw.png?q=20', 'https://miro.medium.com/max/60/1*wYFBF-DGcDTQlxukVhZRTg.png?q=20', 'https://miro.medium.com/max/60/1*7F3HR9uxHmEqdJbeoy2WVQ.png?q=20', 'https://miro.medium.com/max/60/1*ZICx_7zQc_QefDORO4HXKA.png?q=20', 'https://miro.medium.com/max/60/1*A3skXIYAF6nij6LQ6tgpNg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1130/1*jkv52GnJQ7tFkygkmI97yg.png', 'https://miro.medium.com/max/1566/1*VLYNEYrgmDc1Z78FhOaa0w.png', 'https://miro.medium.com/max/818/1*ZICx_7zQc_QefDORO4HXKA.png', 'https://miro.medium.com/max/866/1*o4JmVum7d9QPjn9hevYXuA.png', 'https://miro.medium.com/max/652/1*y7WTSKnFg1RG5-CVPoccOg.png', 'https://miro.medium.com/max/60/1*K7cvymgrj7zUecDw0H2bHQ.png?q=20', 'https://miro.medium.com/max/826/1*31fhDxmXoGrfxbk64gg4Cw.png', 'https://miro.medium.com/max/60/1*o4JmVum7d9QPjn9hevYXuA.png?q=20', 'https://miro.medium.com/max/1200/1*_ARjrrFLdgNf-vodDI6PDQ.jpeg', 'https://miro.medium.com/max/60/1*Jq8qpv6zhOEDL8ozxgpmGw.png?q=20', 'https://miro.medium.com/max/310/1*Jq8qpv6zhOEDL8ozxgpmGw.png', 'https://miro.medium.com/max/1600/1*7rPKhSdT9s8bc_nqfPKKcQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*OvvWVA31p5OE3z8CEwVrxA.png?q=20', 'https://miro.medium.com/max/60/1*2T5rbjOBGVFdSvtlhCqlNg.png?q=20', 'https://miro.medium.com/max/798/1*K7cvymgrj7zUecDw0H2bHQ.png', 'https://miro.medium.com/max/60/1*j7Wp0PxjYdvFmdf415jVWQ.png?q=20', 'https://miro.medium.com/max/2560/1*_ARjrrFLdgNf-vodDI6PDQ.jpeg', 'https://miro.medium.com/max/806/1*NV2JXvMfqfVQwUPUkh0Y9A.png', 'https://miro.medium.com/max/570/1*aWMAGdYezTJPaVtkuvv3bA.png', 'https://miro.medium.com/max/360/1*wN2EpH0MhOeF3HFVawzSfg.png', 'https://miro.medium.com/max/680/1*OvvWVA31p5OE3z8CEwVrxA.png', 'https://miro.medium.com/max/60/1*u8poCsS-SGl8jTkbV-MbCg.png?q=20', 'https://miro.medium.com/max/798/1*SFKE_AVc2SZEeV5xPanAXg.png', 'https://miro.medium.com/max/298/1*Qyb6YD_LbXtBe8ZXOarPJg.png', 'https://miro.medium.com/fit/c/96/96/1*Wy0cxXZpX-FrMWeXsqOOpg.png', 'https://miro.medium.com/max/60/1*_ARjrrFLdgNf-vodDI6PDQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*7rPKhSdT9s8bc_nqfPKKcQ.png?q=20', 'https://miro.medium.com/max/60/1*cFqSK0nMRhq8CEEdTQr4WA.png?q=20', 'https://miro.medium.com/max/60/1*y7WTSKnFg1RG5-CVPoccOg.png?q=20', 'https://miro.medium.com/max/60/1*jkv52GnJQ7tFkygkmI97yg.png?q=20', 'https://miro.medium.com/max/60/1*VLYNEYrgmDc1Z78FhOaa0w.png?q=20', 'https://miro.medium.com/max/1668/1*2T5rbjOBGVFdSvtlhCqlNg.png', 'https://miro.medium.com/max/584/1*FSamGcAXuEWvvFkHQ8McFQ.png', 'https://miro.medium.com/max/60/1*b93f6b8rQfRdPRpvk5aXYA.png?q=20', 'https://miro.medium.com/max/82/1*lPpz9UPNyCcrLcZyBCk1kA.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*NV2JXvMfqfVQwUPUkh0Y9A.png?q=20', 'https://miro.medium.com/max/832/1*R6ajYFdrsCZ6b3CVj_ZH4Q.png', 'https://miro.medium.com/max/60/1*SFKE_AVc2SZEeV5xPanAXg.png?q=20', 'https://miro.medium.com/max/60/1*UEVbo3a2WWK2sY1CH5RI9g.png?q=20', 'https://miro.medium.com/max/1420/1*j7Wp0PxjYdvFmdf415jVWQ.png', 'https://miro.medium.com/max/484/1*A3skXIYAF6nij6LQ6tgpNg.png'}",2020-03-05 00:25:16.108359,7.126070499420166
https://towardsdatascience.com/cnn-application-on-structured-data-automated-feature-extraction-8f2cd28d9a7e,CNN application on structured data-Automated Feature Extraction,"Deep Learning based Automated Feature Engineering

Written: 13th Aug 2018 by Sourish Dey

https://www.linkedin.com/in/sourish-dey-03420b1a/

Importance Feature Engineering:

In my previous article, I discussed the importance of the creation of rich features from the limited number of features. Indeed, the real quality of machine learning/deep learning model comes from extensive feature engineering than from the modeling technique itself. While specific machine learning technique may work best for particular business problem/dataset, features are the universal drivers/critical components for any modeling project. Extracting as much information as possible from the available data sets is crucial to creating an effective solution.

In this article I will discuss about a not so popular method of feature engineering in industry(at least for structured data) — generating features from structured data using CNN(yes you heard it correct, Convolutional Neural Network), a family of modern deep learning model, extensively used in the area of computer vision problem. We will also test this method on a small data to create features, hands-on- as if It’s Not Run, It’s Not Done.

Why Automated feature engineering:

Traditionally analysts/data scientists used to create features using a manual process from domain/business knowledge. Often it’s called handcrafted feature engineering. While in data science we can’t deny the importance of domain knowledge, this type of feature engineering has some drawbacks:

1. Tedious: Manual feature engineering can be a tedious process. How many can new features be created from a list of parent variables? For example from the date variable, a data scientist can create 4 new features (month, year, hour, and day in the week) can be created. However, another data scientist can create 5 additional features(weekend indicator, festive season indicator, X-mass month indicator, seasonality index, week no in a month and so on). Is there any relationship/interaction with any other variable? So manual feature engineering is limited both by human time constraints and imagination: we simply cannot conceive of every possible feature that will be useful.

2. Influence of Human Bias: More often than not any human being working on a particular domain/ modeling project, builds up deep bias for some features(especially if it’s created by that analyst earlier!), irrespective of it adds value to the model or not.

Here it comes the power of automated feature engineering. Here no of features can be created is practically infinite and without any human bias. Also, this captures all possible complex non-linear interactions among features. Off course we can apply dimension reduction/feature selection technique at any point in time to get rid of redundant/zero importance features.

Business Problem and CNN relevance

Here the business objective is to predict the probability of credit default based on credit card owner’s payment status, balance and payment history monitored for past few vintages(last 6 months from the predicted period). For sake of simplicity let’s ignore the lag period(generally there is a lag between customer information extraction and data upload in the system for analysis). The motivation of this business problem and data preparation is from https://medium.com/@guaisang/credit-default-prediction-with-logistic-regression-b5bd89f2799f.

I have not used cross-sectional components(eg. Gender, Education etc.) in the data and only kept time series variables(balance, payment history etc.) to introduce CNN in this business problem. The goal of this article is to conceptualize and implement CNN on this structured data and generate 100 new features from this data using CNN model. You can get the data and entire code here.

About the data:

The datasets utilizes a binary variable, ‘default payment next month’ (Yes = 1, No = 0), as the response variable. There are 18 features(excluding ID) in this set:

1:6 =X1 the repayment status in the last month of the prediction period; . . .;X6 = the repayment status in the 6th month before prediction period. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. 0 means there is no transaction

X7 = amount of bill statement in last month of reference time frame;…. X12 = amount of bill statement in 6th month before the reference time frame;

X13 = amount of bill statement in last month of reference time frame;…. X18 = amount of payment in 6th month before the reference time frame;

Representation of data in CNN format:

Deep Learning methods, specifically CNNs, have seen a lot of success in the domain of image-based data, where the data offers a clearly structured topology in the regular lattice of pixels.Although detailed discussion about convolutional neural network (CNN, or ConvNet) is beyond scope of this article, let’s take a look at what makes CNNs so popular?

Deep Learning methods, specifically CNNs, have seen a lot of success in the domain of image-based data, where the data offers a clearly structured topology in the regular lattice of pixels. Although detailed discussion about CNN or ConvNet is beyond scope of this article, let’s take a look what makes CNNs so popular?

Reduced Parameters(parameter sharing): During convolution operation (layer), each output value (convolved feature in the figure) is not required to connected to every neuron in the previous layer (image in the figure), but only to those, called receptive fields, where the convolution kernel is applied currently. This characteristic of convolution layer is called Local Connectivity.Also same weights are applied over the convolving until the next update of the parameters. This characteristic of convolution layer is called Parameter Sharing.These reduced the number of parameters needed dramatically compared to usual ANN(artificial neural network) structure, where there is a connection between every single pair of input/output neuron.

Shift/Translation In-variance: It means that when the input shifts the output also shifts but stays otherwise unchanged. To be specific to an image, you can recognize an object as an object, even when its appearance varies in some way.

So in nutshell to apply CNN on any data, the data needs to be prepared in a way, so that there are local patterns and the same local patterns are relevant everywhere. To specific our data we have time series horizons of events(balance, payments etc.) for an individual customer, which makes the data full of local patterns for her and hence these parameters can be shared across customers. We just need to prepare the data suitable to feed into a CNN- feature matrix of for individual customers, same like an image frame matrix of d * w * n(d, w and n is the depth, width and no of the channel of an image frame).

To elaborate if we have N customers with m features(here balance/payment etc.) across t time horizons(here month1, month2 etc.) for p different trades(trades are different lines of credit, such as retail cards, mortgage etc.). So simply m features, t time horizons and p trades for individual customers are analogous to width, depth and no of channels of an image feature array. Precisely for our data the dimension for each customer will be m * t * p and for the entire data will be N*m*t*p. Now this makes the originally structured data into image frame type data, ideal to apply CNN to extract complex non-linear local patterns across customers by a convolution operation.

Data preparation:

The original data set was consisting of one trade- retail card. To make it more relevant and more challenging I created dummy data set(tagging target variable status as original data) for one additional trades — mortgage. The data is sparser than retail card data and values may not be 100% logical. The final data set(CreditDefault.csv) is kept here.

So we have 30k distinct customer ids and total 60k observations, first 30k observations are of the retail card, next 30k is for the mortgage.

A detailed solution including data preparation code is at https://github.com/nitsourish/CNN-automated-Feature-Extraction/blob/master/CNN_feature_extraction.ipynb. This is the tricky part of this problem is to prepare the data in the format(image like format), so that we can apply CNN.Precisely for our data the dimension for each customer will be 3(no of features) * 6(time horizons) * 2(trades) and there will be 30000 frames of this dimension. So for the entire data it will be an array of will be 30000 * 3(width) * 6(depth) * 2(no of channels). So the data will be perfect to train a Convolutional network.

Pipeline- CNN Feature Extraction

After preparation of channel specific data, we see the dimension:

shape of channel1(retail)data: (30000, 3, 6, 1)

shape of channel2(mortgage)data: (30000, 3, 6, 1)

After merging these two arrays the data is proper to feed in CNN as the input volume to extract complex features with non-linear interaction.

shape of input volume(layer)data: (30000, 3, 6, 2)

Now the data looks like image frame data image like data (30000 image frame of volume 3*6*2). Now we need to map target variable(default payment next month) against customer ID in one hot encoding format.

X = np.concatenate((retail, mort),axis=3) Y = event_id[['default payment next month']].values Y = to_categorical(Y,2)

In our data we have payment default rate is: 28.402671 percent

In the data we have 28.40% payment_default rate. Although event rate is not 50%, we can say it’s fairly balanced data. so no need to do any weight correction

Feature extraction from CNN

To extract features from CNN model first we need to train the CNN network with last sigmoid/logistic dense layer(here dimension 2) w.r.t. target variable. The objective of the training network is to identify the correct weights for the network by multiple forward and backward iterations, which eventually try to minimize binary cross entropy(misclassification cost). Here I will use keras framework with back-end as TensorFlow(tf). For our business problem AUC is the evaluation metric and we will do the optimization by maximizing AUC value in each epoch(Keras doesn’t have default AUC metric. However we can leverage tf to create customized evaluation function for AUC.)

CNN Architecture

We built following CNN with 4 conv layers(conv + activation + pooling(optional)) and 2 FC(fully connected layers) before reaching to the last sigmoid output layer.

We trained our model for epochs = 100 utilizing early stopping to prevent overfitting.It took less than 10 mins time to train using my NVIDIA GTX GeForce 1060 GPU with val_auc = 0.8317.

Feature Extraction Methodology

Now by using a pre-trained model, we can directly apply these weights on a data, removing the last sigmoid/logistic layer(in this problem until the dense layer of dimension 100). We can use this to any new data of same business problem to calculate these features.We need to just feed forward the network and it will directly map final weights to calculate features at some intermediate layer, without building the network again.This is a kind of transfer learning.Based on our requirement of no of features we can extract features at any intermediate dense layer with the desired dimension. For this problem, we will extract feature using an intermediate model till ‘feature_dense’ layer of the CNN.

#Preparing Indermediate model-removing last sigmoid layer

intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('feature_dense').output)

intermediate_layer_model.summary()

The output layer dimension is 100, so if we predict using this network on a new data with the same input dimension, we can create 100 new complex features.

#predict to get featured data

feauture_engg_data = intermediate_layer_model.predict(X)

feauture_engg_data = pd.DataFrame(feauture_engg_data)

print('feauture_engg_data shape:', feauture_engg_data.shape)

feauture_engg_data shape: (30000, 100)

We successfully captured 100 complex interactions among raw variables.

Further Exploration on feauture_engg_data

· Let’s have some sneak-peak whether these features have any value. Although there are a few better ways, for sake of simplicity let’s capture some bivariate relationship of these new features with the target variable ‘default payment next month’.Although for categorical target variable Correlation is not 100% correct method, we will calculate the correlation of all new values with the target as an approximation of the variables which may be important for the final model.

#Sorting correlation values in decsending order

new_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)

#Let's see top 10 correlation values

new_corrs[:10]

[('PAY_1', 0.32479372847862253),

('PAY_2', 0.26355120167216467),

('PAY_3', 0.2352525137249163),

('feat_78', -0.22173805212223915),

('PAY_4', 0.21661363684242424),

('PAY_5', 0.20414891387616674),

('feat_86', -0.20047655459374053),

('feat_6', -0.19189993720885604),

('PAY_6', 0.1868663616535449),

('feat_3', -0.17394080015462873)]

We can see 4 newly created variables(feat_78,feat_86,feat_6,feat_3) are in top 10, in terms of correlation(pseudo although) with ‘default payment next month’. Although based on this nothing concretely can be said about the predictive strength of these variable, at least these variable are worthy of further investigation.

We can also look at the KDE(kernel density estimation) plot of the highest correlated variables, in terms of absolute magnitude correlation.

Emphasizing CNN for feature extraction

Now the critical ask is for this problem why CNN/deep learning method is only used for feature extraction. Why not use it as a classifier also. Two main reasons is there:

Local pattern: The data we used in this business problem(which is applicable to most of the scenarios where we use structural data), consists cross sectional(static)variables as well, where we will not find any local pattern. So we can’t use CNN on the entire data as classifier. Of course for the data which holds Shift/Translation Invariance property, we can use CNN as end classifier.

Infrastructural challenge: Building in large-scale industrial data and bringing into production requires significant infrastructural support(GPUs, AWS etc.), which may not available to every organization.

Latency in model run: Even with proper infrastructure forward propagation of decently complex deep learning models takes significantly higher time than highly effective classifiers like XGBoost, RF, SVM etc. Indeed some application, which requires blazing fast real-time prediction, the best solution, in that case, is to extract feature with an automated technique like CNN feature extraction and then to add relatively simpler classifier.

Automated Feature Extraction + Classifier

Regulatory challenge: Even with the immense focus on machine learning/AI, organizations especially into the domain of banking/insurance/finance have to go through internal/external regulatory processes before implementation of predictive models and it may require painstaking persuasion to make these latest techniques on the production system. On the contrary, if the end classifier is the traditional model, there may not be a challenge on a modeling methodology perspective.

Improvement and Road Ahead:

As modern Artificial Intelligence and Deep Learning guru, Andrew Ng accurately said, “…applied machine learning is basically feature engineering.”, the importance of creating the meaningful feature space cannot be overstated and automated feature engineering techniques aim to help the data scientist with the problem of feature creation seamlessly building hundreds or thousands of new features from raw data set. However, the purpose is not to replace data scientist. Rather will allow her to focus on other valuable parts of the machine learning pipeline, such as feature selection, exploring new methodology, delivering robust models into production etc. We just discussed one of the potential methods of automated feature engineering. Surely there are a few other comparable techniques. Love to explore those in future. Meanwhile eager to hear the other applications of CNN on structured data(relational database), in any other business problems.","['application', 'feature', 'cnn', 'payment', 'dataautomated', 'features', 'problem', 'learning', 'variable', 'data', 'month', 'model', 'structured', 'extraction']","Indeed, the real quality of machine learning/deep learning model comes from extensive feature engineering than from the modeling technique itself.
The goal of this article is to conceptualize and implement CNN on this structured data and generate 100 new features from this data using CNN model.
so no need to do any weight correctionFeature extraction from CNNTo extract features from CNN model first we need to train the CNN network with last sigmoid/logistic dense layer(here dimension 2) w.r.t.
Emphasizing CNN for feature extractionNow the critical ask is for this problem why CNN/deep learning method is only used for feature extraction.
Indeed some application, which requires blazing fast real-time prediction, the best solution, in that case, is to extract feature with an automated technique like CNN feature extraction and then to add relatively simpler classifier.",en,['Sourish Dey'],2018-09-17 13:11:33.989000+00:00,"{'Feature Engineering', 'Deep Learning', 'Artificial Intelligence', 'Convolutional Network', 'Machine Learning'}","{'https://miro.medium.com/max/60/0*NzdNrYJRGIrlwoEt?q=20', 'https://miro.medium.com/max/60/1*weeUPXG6sohyFcmqlDwCSg.png?q=20', 'https://miro.medium.com/max/2800/0*JLEJUX1xVlsz5Yfg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*WDHHTENJh1cBE__R?q=20', 'https://miro.medium.com/max/60/0*JLEJUX1xVlsz5Yfg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/52/0*g87jZNuFnRTKNAjA?q=20', 'https://miro.medium.com/max/598/0*WDHHTENJh1cBE__R', 'https://miro.medium.com/max/693/1*zUATaXMAmKof27rPyBRWsg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/7434/1*weeUPXG6sohyFcmqlDwCSg.png', 'https://miro.medium.com/max/60/1*zUATaXMAmKof27rPyBRWsg.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*YQRqyrjzmy8-isgY', 'https://miro.medium.com/max/2304/1*bM1cVQkJ58iYpw3_o65Wpg.png', 'https://miro.medium.com/max/2304/1*pG9Rc-fmvDzfTWB43hWlOQ.jpeg', 'https://miro.medium.com/max/2304/0*NzdNrYJRGIrlwoEt', 'https://miro.medium.com/max/60/1*pG9Rc-fmvDzfTWB43hWlOQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*bM1cVQkJ58iYpw3_o65Wpg.png?q=20', 'https://miro.medium.com/max/1386/1*zUATaXMAmKof27rPyBRWsg.png', 'https://miro.medium.com/max/1190/0*g87jZNuFnRTKNAjA', 'https://miro.medium.com/fit/c/160/160/0*YQRqyrjzmy8-isgY'}",2020-03-05 00:25:18.037825,1.928466558456421
https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd,Improving the Random Forest in Python Part 1,"In a previous post we went through an end-to-end implementation of a simple random forest in Python for a supervised regression problem. Although we covered every step of the machine learning process, we only briefly touched on one of the most critical parts: improving our initial machine learning model. The model we finished with achieved decent performance and beat the baseline, but we should be able to better the model with a couple different approaches. This article is the first of two that will explore how to improve our random forest machine learning model using Python and the Scikit-Learn library. I would recommend checking out the introductory post before continuing, but the concepts covered here can also stand on their own.

How to Improve a Machine Learning Model

There are three general approaches for improving an existing machine learning model:

Use more (high-quality) data and feature engineering Tune the hyperparameters of the algorithm Try different algorithms

These are presented in the order in which I usually try them. Often, the immediate solution proposed to improve a poor model is to use a more complex model, often a deep neural network. However, I have found that approach inevitably leads to frustration. A complex model is built over many hours, which then also fails to deliver, leading to another model and so on. Instead, my first question is always: “Can we get more data relevant to the problem?”. As Geoff Hinton (the father of deep neural networks) has pointed out in an article titled ‘The Unreasonable Effectiveness of Data”, the amount of useful data is more important to the problem than the complexity of the model. Others have echoed the idea that a simple model and plenty of data will beat a complex model with limited data. If there is more information that can help with our problem that we are not using, the best payback in terms of time invested versus performance gained is to get that data.

This post will cover the first method for improving ML models, and the second approach will appear in a subsequent article. I will also write end-to-end implementations of several algorithms which may or may not beat the random forest (If there are any requests for specific algorithms, let me know in the comments)! All of the code and data for this example can be found on the project GitHub page. I have included plenty of code in this post, not to discourage anyone unfamiliar with Python, but to show how accessible machine learning has become and to encourage anyone to start implementing these useful models!

Problem Recap

As a brief reminder, we are dealing with a temperature prediction problem: given historical data, we want to predict the maximum temperature tomorrow in our city. I am using Seattle, WA, but feel free to use the NOAA Climate Data Online Tool to get info for your city. This task is a supervised, regression machine learning problem because we have the labels (targets) we want to predict, and those labels are continuous values (in contrast to unsupervised learning where we do not have labels, or classification, where we are predicting discrete classes). Our original data used in the simple model was a single year of max temperature measurements from 2016 as well as the historical average max temperature. This was supplemented by the predictions of our “meteorologically-inclined” friend, calculated by randomly adding or subtracting 20 degrees from the historical average.

Our final performance using the original data was an average error of 3.83 degrees compared to a baseline error of 5.03 degrees. This represented a final accuracy of 93.99%.

Getting More Data

In the first article, we used one year of historical data from 2016. Thanks to the NOAA (National Atmospheric and Oceanic Administration), we can get data going back to 1891. For now, let’s restrict ourselves to six years (2011–2016), but feel free to use additional data to see if it helps. In addition to simply getting more years of data, we can also include more features. This means we can use additional weather variables that we believe will be useful for predicting the max temperature. We can use our domain knowledge (or advice from the experts), along with correlations between the variable and our target to determine which features will be helpful. From the plethora of options offered by the NOAA (seriously, I have to applaud the work of this organization and their open data policy), I added average wind speed, precipitation, and snow depth on the ground to our list of variables. Remember, because we are predicting the maximum temperature for tomorrow, we can’t actually use the measurement on that day. We have to shift it one day into the past, meaning we are using today’s precipitation total to predict the max temperature tomorrow. This prevents us from ‘cheating’ by having information from the future today.

The additional data was in relatively good shape straight from the source, but I did have to do some slight modifications before reading it into Python. I have left out the “data munging” details to focus on the Random Forest implementation, but I will release a separate post showing how to clean the data. I use the R statistical language for munging because I like how it makes data manipulation interactive, but that’s a discussion for another article. For now, we can load in the data and examine the first few rows.

# Pandas is used for data manipulation

import pandas as pd # Read in data as a dataframe

features = pd.read_csv('data/temps_extended.csv')

features.head(5)

Expanded Data Subset

The new variables are:

ws_1: average wind speed from the day before (mph)

prcp_1: precipitation from the day before (in)

snwd_1: snow depth on the ground from the day before (in)

Before we had 348 days of data. Let’s look at the size now.

print('We have {} days of data with {} variables'.format(*features.shape)) We have 2191 days of data with 12 variables.

There are now over 2000 days of historical temperature data (about 6 years). We should summarize the data to make sure there are no anomalies that jump out in the numbers.

round(features.describe, 2)

Expanded Data Summary

Nothing appears immediately anomalous from the descriptive statistics. We can quickly graph all of the variables to confirm this. I have left out the plotting code because while the matplotlib library is very useful, the code is non-intuitive and it can be easy to get lost in the details of the plots. (All the code is available for inspection and modification on GitHub).

First up are the four temperature plots.

Expanded Data Temperature Plots

Next we can look at the historical max average temperature and the three new variables.

Expanded Data Additional Variables

From the numerical and graphical inspection, there are no apparent outliers n our data. Moreover, we can examine the plots to see which features will likely be useful. I think the snow depth will be the least helpful because it is zero for the majority of days, likewise, the wind speed looks to be too noisy to be of much assistance. From prior experience, the historical average max temperature and prior max temperature will probably be most important, but we will just have to see!

We can make one more exploratory plot, the pairplot, to visualize the relationships between variables. This plots all the variables against each other in scatterplots allowing us to inspect correlations between features. The code for this impressive-looking plot is rather simple compared to the above graphs!

# Create columns of seasons for pair plotting colors

seasons = [] for month in features['month']:

if month in [1, 2, 12]:

seasons.append('winter')

elif month in [3, 4, 5]:

seasons.append('spring')

elif month in [6, 7, 8]:

seasons.append('summer')

elif month in [9, 10, 11]:

seasons.append('fall') # Will only use six variables for plotting pairs

reduced_features = features[['temp_1', 'prcp_1', 'ws_1', 'average', 'friend', 'actual']]

reduced_features['season'] = seasons # Use seaborn for pair plots

import seaborn as sns

sns.set(style=""ticks"", color_codes=True); # Create a custom color palete

palette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange']) # Make the pair plot with a some aesthetic changes

sns.pairplot(reduced_features, hue = 'season', diag_kind = 'kde', palette= palette, plot_kws=dict(alpha = 0.7),

diag_kws=dict(shade=True))

Pairplots

The diagonal plots show the distribution of each variable because graphing each variable against itself would just be a straight line! The colors represent the four seasons as shown in the legend on the right. What we want to concentrate on are the trends between the actual max temperature and the other variables. These plots are in the bottom row, and to see a specific relationship with the actual max, move to the row containing the variable. For example, the plot in the bottom left shows the relationship between the actual max temperature and the max temperature from the previous day (temp_1). This is a strong positive correlation, indicating that as the max temperature one day prior increases, the max temperature the next day also increases

Data Preparation

The data has been validation both numerically and graphically, and now we need to put it in a format understandable by the machine learning algorithm. We will perform exactly the same data formatting procedure as in the simple implementation:

One-hot encode categorical variables (day of the week) Separate data into features (independent varibles) and labels (targets) Convert dataframes to Numpy arrays Create random training and testing sets of features and labels

We can do all of those steps in a few lines of Python.

# One Hot Encoding

features = pd.get_dummies(features) # Extract features and labels

labels = features['actual']

features = features.drop('actual', axis = 1) # List of features for later use

feature_list = list(features.columns) # Convert to numpy arrays

import numpy as np features = np.array(features)

labels = np.array(labels) # Training and Testing Sets

from sklearn.model_selection import train_test_split train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)

We set a random seed (of course it has to be 42) to ensure consistent results across runs. Let’s quickly do a check of the sizes of each array to confirm everything is in order.

print('Training Features Shape:', train_features.shape)

print('Training Labels Shape:', train_labels.shape)

print('Testing Features Shape:', test_features.shape)

print('Testing Labels Shape:', test_labels.shape) Training Features Shape: (1643, 17)

Training Labels Shape: (1643,)

Testing Features Shape: (548, 17)

Testing Labels Shape: (548,)

Good to go! We have about 4.5 years of training data and 1.5 years of testing data. However, before we can get to the fun part of modeling, there is one additional step.

Establish a New Baseline

In the previous post, we used the historical average maximum temperature as our target to beat. That is, we evaluated the accuracy of predicting the max temperature tomorrow as the historical average max temperature on that day. We already know even the model trained on a single year of data can beat that baseline so we need to raise our expectations. For a new baseline, we will use the model trained on the original data. To make a fair comparison, we need to test it against the new, expanded test set. However, the new test set has 17 features, whereas the original model was only trained on 14 features. We first have to remove the 3 new features from the test set and then evaluate the original model. The original random forest has already been trained on the original data and code below shows preparing the testing features and evaluating the performance (refer to the notebook for the model training).

# Find the original feature indices

original_feature_indices = [feature_list.index(feature) for feature in feature_list if feature not in ['ws_1', 'prcp_1', 'snwd_1']] # Create a test set of the original features

original_test_features = test_features[:, original_feature_indices] # Make predictions on test data using the model trained on original data

predictions = rf.predict(original_test_features) # Performance metrics

errors = abs(predictions - test_labels) print('Metrics for Random Forest Trained on Original Data')

print('Average absolute error:', round(np.mean(errors), 2), 'degrees.') # Calculate mean absolute percentage error (MAPE)

mape = 100 * (errors / test_labels) # Calculate and display accuracy

accuracy = 100 - np.mean(mape)

print('Accuracy:', round(accuracy, 2), '%.') Metrics for Random Forest Trained on Original Data

Average absolute error: 4.3 degrees.

Accuracy: 92.49 %.

The random forest trained on the single year of data was able to achieve an average absolute error of 4.3 degrees representing an accuracy of 92.49% on the expanded test set. If our model trained with the expanded training set cannot beat these metrics, then we need to rethink our method.

Training and Evaluating on Expanded Data

The great part about Scikit-Learn is that many state-of-the-art models can be created and trained in a few lines of code. The random forest is one example:

# Instantiate random forest and train on new features

from sklearn.ensemble import RandomForestRegressor rf_exp = RandomForestRegressor(n_estimators= 1000, random_state=100)

rf_exp.fit(train_features, train_labels)

Now, we can make predictions and compare to the known test set targets to confirm or deny that our expanded training dataset was a good investment:

# Make predictions on test data

predictions = rf_exp.predict(test_features) # Performance metrics

errors = abs(predictions - test_labels) print('Metrics for Random Forest Trained on Expanded Data')

print('Average absolute error:', round(np.mean(errors), 2), 'degrees.') # Calculate mean absolute percentage error (MAPE)

mape = np.mean(100 * (errors / test_labels)) # Compare to baseline

improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape

print('Improvement over baseline:', round(improvement_baseline, 2), '%.') # Calculate and display accuracy

accuracy = 100 - mape

print('Accuracy:', round(accuracy, 2), '%.') Metrics for Random Forest Trained on Expanded Data

Average absolute error: 3.7039 degrees.

Improvement over baseline: 16.67 %.

Accuracy: 93.74 %.

Well, we didn’t waste our time getting more data! Training on six years worth of historical measurements and using three additional features has netted us a 16.41% improvement over the baseline model. The exact metrics will change depending on the random seed, but we can be confident that the new model outperforms the old model.

Why does a model improve with more data? The best way to answer this is to think in terms of how humans learn. We increase our knowledge of the world through experiences, and the more times we practice a skill, the better we get. A machine learning model also “learns from experience” in the sense that each time it looks at another training data point, it learns a little more about the relationships between the features and labels. Assuming that there are relationships in the data giving the model more data will allow it to better understand how to map a set of features to a label. For our case, as the model sees more days of weather measurements, it better understands how to take those measurements and predict the maximum temperature on the next day. Practice improves human abilities and machine learning model performance alike.

Feature Reduction

In some situations, we can go too far and actually use too much data or add too many features. One applicable example is a machine learning prediction problem involving building energy which I am currently working on. The problem is to predict building energy consumption in 15-minute intervals from weather data. For each building, I have 1–3 years of historical weather and electricity use data. Surprisingly, I found as I included more data for some buildings, the prediction accuracy decreased. Asking around, I determined some buildings had undergone retrofits to improve energy efficiency in the middle of data collection, and therefore, recent electricity consumption differed significantly from before the retrofit. When predicting current consumption, using data from before the modification actually decreased the performance of my models. More recent data from after the change was more relevant than the older data, and for several buildings, I ended up decreasing the amount of historical data to improve performance!

For our problem, the length of the data is not an issue because there have been no major changes affecting max temperatures in the six years of data (climate change is increasing temperatures but on a longer timescale). However, it may be possible we have too many features. We saw earlier that some of the features, especially our friend’s prediction, looked more like noise than an accurate predictor of the maximum temperature. Extra features can decrease performance because they may “confuse” the model by giving it irrelevant data that prevents it from learning the actual relationships. The random forest performs implicit feature selection because it splits nodes on the most important variables, but other machine learning models do not. One approach to improve other models is therefore to use the random forest feature importances to reduce the number of variables in the problem. In our case, we will use the feature importances to decrease the number of features for our random forest model, because, in addition to potentially increasing performance, reducing the number of features will shorten the run time of the model. This post does not touch on more complex dimensionality reductions such as PCA (principal components analysis) or ICA (independent component analysis). These do a good job of decreasing the number of features while not decreasing information, but they transform the features such that they no longer represent our measured variables. I like machine learning models to have a blend of interpretability and accuracy, and I generally therefore stick to methods that allow me to understand how the model is making predictions.

Feature Importances

Finding the feature importances of a random forest is simple in Scikit-Learn. The actual calculation of the importances is beyond this blog post, but this occurs in the background and we can use the relative percentages returned by the model to rank the features.

The following Python code creates a list of tuples where each tuple is a pair, (feature name, importance). The code here takes advantage of some neat tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. Don’t worry if you do not understand these entirely, but if you want to become skilled at Python, these are tools you should have in your arsenal!

# Get numerical feature importances

importances = list(rf_exp.feature_importances_) # List of tuples with variable and importance

feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)] # Sort the feature importances by most important first

feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True) # Print out the feature and importances

[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances] Variable: temp_1 Importance: 0.83

Variable: average Importance: 0.06

Variable: ws_1 Importance: 0.02

Variable: temp_2 Importance: 0.02

Variable: friend Importance: 0.02

Variable: year Importance: 0.01

Variable: month Importance: 0.01

Variable: day Importance: 0.01

Variable: prcp_1 Importance: 0.01

Variable: snwd_1 Importance: 0.0

Variable: weekday_Fri Importance: 0.0

Variable: weekday_Mon Importance: 0.0

Variable: weekday_Sat Importance: 0.0

Variable: weekday_Sun Importance: 0.0

Variable: weekday_Thurs Importance: 0.0

Variable: weekday_Tues Importance: 0.0

Variable: weekday_Wed Importance: 0.0

These stats definitely prove that some variables are much more important to our problem than others! Given that there are so many variables with zero importance (or near-zero due to rounding), it seems like we should be able to get rid of some of them without impacting performance. First, let’s make a quick graph to represent the relative differences in feature importances. I left this plotting code in because it’s a little easier to understand.

# list of x locations for plotting

x_values = list(range(len(importances))) # Make a bar chart

plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2) # Tick labels for x axis

plt.xticks(x_values, feature_list, rotation='vertical') # Axis labels and title

plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');

Expanded Model Variable Importances

We can also make a cumulative importance graph that shows the contribution to the overall importance of each additional variable. The dashed line is drawn at 95% of total importance accounted for.

# List of features sorted from most to least important

sorted_importances = [importance[1] for importance in feature_importances]

sorted_features = [importance[0] for importance in feature_importances] # Cumulative importances

cumulative_importances = np.cumsum(sorted_importances) # Make a line graph

plt.plot(x_values, cumulative_importances, 'g-') # Draw line at 95% of importance retained

plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed') # Format x ticks and labels

plt.xticks(x_values, sorted_features, rotation = 'vertical') # Axis labels and title

plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');

Cumulative Feature Importances

We can now use this to remove unimportant features. 95% is an arbitrary threshold, but if it leads to noticeably poor performance we can adjust the value. First, we need to find the exact number of features to exceed 95% importance:

# Find number of features for cumulative importance of 95%

# Add 1 because Python is zero-indexed

print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1) Number of features for 95% importance: 6

We can then create a new training and testing set retaining only the 6 most important features.

# Extract the names of the most important features

important_feature_names = [feature[0] for feature in feature_importances[0:5]]

# Find the columns of the most important features

important_indices = [feature_list.index(feature) for feature in important_feature_names] # Create training and testing sets with only the important features

important_train_features = train_features[:, important_indices]

important_test_features = test_features[:, important_indices] # Sanity check on operations

print('Important train features shape:', important_train_features.shape)

print('Important test features shape:', important_test_features.shape) Important train features shape: (1643, 6)

Important test features shape: (548, 6)

We decreased the number of features from 17 to 6 (although to be fair, 7 of those features were created from the one-hot encoding of day of the week so we really only had 11 pieces of unique information). Hopefully this will not significantly decrease model accuracy and will considerably decrease the training time.

Training and Evaluating on Important Features

Now we go through the same train and test procedure as we did with all the features and evaluate the accuracy.

# Train the expanded model on only the important features

rf_exp.fit(important_train_features, train_labels); # Make predictions on test data

predictions = rf_exp.predict(important_test_features) # Performance metrics

errors = abs(predictions - test_labels) print('Average absolute error:', round(np.mean(errors), 2), 'degrees.') # Calculate mean absolute percentage error (MAPE)

mape = 100 * (errors / test_labels) # Calculate and display accuracy

accuracy = 100 - np.mean(mape)

print('Accuracy:', round(accuracy, 2), '%.') Average absolute error: 3.821 degrees.

Accuracy: 93.56 %.

The performance suffers a minor increase of 0.12 degrees average error using only 6 features. Often with feature reduction, there will be a minor decrease in performance that must be weighed against the decrease in run-time. Machine learning is a game of making trade-offs, and run-time versus performance is usually one of the critical decisions. I will quickly do some bench-marking to compare the relative run-times of the two models (see Jupyter Notebook for code).

Model Tradeoffs

Overall, the reduced features model has a relative accuracy decrease of 0.131% with a relative run-time decrease of 35.1%. In our case, run-time is inconsequential because of the small size of the data set, but in a production setting, this trade-off likely would be worth it.

Conclusions

Instead of developing a more complex model to improve our random forest, we took the sensible step of collecting more data points and additional features. This approach was validated as we were able to decrease the error of compared to the model trained on limited data by 16.7%. Furthermore, by reducing the number of features from 17 to 6, we decreased our run-time by 35% while suffering only a minor decrease in accuracy. The best way to summarize these improvements is with another graph. The model trained on one year of training data is on the left, the model using six years of data and all features is in the middle, and the model on the right used six years of data but only a subset of the most important features.

Model Comparisons

This example has demonstrated the effectiveness of increasing the amount of data. While most people make the mistake of immediately moving to a more powerful model, we have learned most problems can be improving by collecting more relevant data points. In further parts of this series, we will take a look at the other ways to improve our model, namely, hyperparameter tuning and using different algorithms. However, getting more data is likely to have the largest payoff in terms of time invested versus increase in performance in this situation. The next time you see someone rushing to implement a complex deep learning model after failing on the first model, nicely ask them if they have exhausted all sources of data. Chances are, if there is still data out there relevant to their problem, they can get better performance and save time in the process!

As always, I appreciate any comments and constructive feedback. I can be reached at wjk68@case.edu","['forest', 'feature', 'temperature', 'improving', 'importance', 'python', 'learning', 'performance', 'data', 'model', 'random', 'features']","In a previous post we went through an end-to-end implementation of a simple random forest in Python for a supervised regression problem.
This article is the first of two that will explore how to improve our random forest machine learning model using Python and the Scikit-Learn library.
Metrics for Random Forest Trained on Original DataAverage absolute error: 4.3 degrees.
The random forest is one example:# Instantiate random forest and train on new featuresfrom sklearn.ensemble import RandomForestRegressor rf_exp = RandomForestRegressor(n_estimators= 1000, random_state=100)rf_exp.fit(train_features, train_labels)Now, we can make predictions and compare to the known test set targets to confirm or deny that our expanded training dataset was a good investment:# Make predictions on test datapredictions = rf_exp.predict(test_features) # Performance metricserrors = abs(predictions - test_labels) print('Metrics for Random Forest Trained on Expanded Data')print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')
One approach to improve other models is therefore to use the random forest feature importances to reduce the number of variables in the problem.",en,['Will Koehrsen'],2018-01-08 15:36:48.513000+00:00,"{'Machine Learning', 'Python', 'Learning', 'Data Science'}","{'https://miro.medium.com/max/874/1*eDslQfIazaCV6QR1WlRN2A.png', 'https://miro.medium.com/max/60/1*TYVhgJY3prVTQnI-S9t7NQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/1572/1*4osw3tOIxsvD_V1VgK0mfg.png', 'https://miro.medium.com/max/60/1*8YBlxmui6Rupof9jONvWjA.png?q=20', 'https://miro.medium.com/max/60/1*8KwPmC0U67WxCPR8gpBN5g.png?q=20', 'https://miro.medium.com/max/60/1*x0gwCC_4CsqEijfHy8W8bw.jpeg?q=20', 'https://miro.medium.com/max/2076/1*8KwPmC0U67WxCPR8gpBN5g.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*4osw3tOIxsvD_V1VgK0mfg.png?q=20', 'https://miro.medium.com/max/1776/1*wUlERtPqRqggWHRsRrsL4Q.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1200/1*x0gwCC_4CsqEijfHy8W8bw.jpeg', 'https://miro.medium.com/max/60/1*xpY2T_tTW5D12QrpTtjNZg.png?q=20', 'https://miro.medium.com/max/3840/1*x0gwCC_4CsqEijfHy8W8bw.jpeg', 'https://miro.medium.com/max/792/1*TYVhgJY3prVTQnI-S9t7NQ.png', 'https://miro.medium.com/max/36/1*BZOjMNEyRq-smBojUcXi-Q.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1956/1*p3sJL4mX-6AbBBM5Mx3KNQ.png', 'https://miro.medium.com/max/1154/1*BZOjMNEyRq-smBojUcXi-Q.png', 'https://miro.medium.com/max/60/1*6WMKg6WeqFkIA-QLSQcj6g.png?q=20', 'https://miro.medium.com/max/60/1*eDslQfIazaCV6QR1WlRN2A.png?q=20', 'https://miro.medium.com/max/910/1*8YBlxmui6Rupof9jONvWjA.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/800/1*6WMKg6WeqFkIA-QLSQcj6g.png', 'https://miro.medium.com/max/60/1*wUlERtPqRqggWHRsRrsL4Q.png?q=20', 'https://miro.medium.com/max/60/1*p3sJL4mX-6AbBBM5Mx3KNQ.png?q=20', 'https://miro.medium.com/max/2076/1*xpY2T_tTW5D12QrpTtjNZg.png'}",2020-03-05 00:25:24.655401,6.616572141647339
https://medium.com/@shachiakyaagba_41915/integrating-folium-with-dash-5338604e7c56,Integrating Folium with Dash,"Dashboard applications are a very useful intelligence tool for displaying interactive data visualizations in a wide range of interesting and intuitive ways. On the back end, the dashboard applications connected to your database, api and any other repositories for your files. On the front end it displays this information in forms of interconnected graphs and tables. Dashboards can be customized to meet the specific needs of your organization and as such they have been widely adopted by the private, public and non-profit sectors for both internal and external processes.

What is Dash?

Dash is a Python framework for building analytic web applications (wait for it….) without JavaScript. It is built on top of Plotly.js, React and Flask. Dash takes your python analytic code and, within its framework, generates the relevant html code which displays the front end of your web app. The most important quality of dash, however is that it is extremely user-friendly.

In my first Dash app I wanted to create an interactive map of New York City that displayed locations of job openings depending on a salary range selected by the user. The salary scale would be in the form of a slider and the map would be updated each time the slider was moved in order to accommodate the change in salary range.

To generate the map I looked to the Folium library. It also has the distinct trait of being very intuitive. The basic process is as follows:

Import Folium and any other dependencies you need.

2. Pass in the coordinates for the base map (in my case, New York City )

3. Generate base map.

4. Generate markers for job locations and add to map

After generating the map my challenge was integrating it with dash. Dash has its built in core components and can easily be integrated with some libraries and apis using this core component framework. Folium, unfortunately, isn’t one of them. So I hit the internet in search of a solution and found one!

Enter the Iframe

An iframe (short for inline frame) is an HTML element that allows an external webpage to be embedded in an HTML document. Unlike traditional frames, which were used to create the structure of a webpage, iframes can be inserted anywhere within a webpage layout.

Since Dash generates html code, it follows that its html component should include an iframe tag, and it does. If I can convert my folium map to an html document I should be able to pass that html doc into Dash’s iframe component and have it displayed on my app. This can be done as follows:

Save the newly created map as an html file.

2. Set up your Dash environment and pass the html file through iframe component.

3. Test your Dash app.

Conclusion.

In my quest to find a way to integrate my map generator with dash, I stumbled into something much bigger: the iframe. From the concept of the iframe it follows that anything that can be converted to html (not just folium) can be integrated with dash. This makes for endless possibilities for things that can be incorporated to your web app e.g videos, animations, external dashboards, the list goes on and on and on.","['folium', 'web', 'pass', 'dash', 'salary', 'integrating', 'webpage', 'map', 'iframe', 'html', 'app']","Dash takes your python analytic code and, within its framework, generates the relevant html code which displays the front end of your web app.
Generate markers for job locations and add to mapAfter generating the map my challenge was integrating it with dash.
Since Dash generates html code, it follows that its html component should include an iframe tag, and it does.
Test your Dash app.
From the concept of the iframe it follows that anything that can be converted to html (not just folium) can be integrated with dash.",en,['Shachia Kyaagba'],2018-09-08 03:49:53.792000+00:00,"{'Dashboard', 'Iframe', 'Maps', 'Python3', 'Dash'}","{'https://miro.medium.com/max/60/1*g2brVQjvsWF_6Vvq9PnPqg.png?q=20', 'https://miro.medium.com/max/2224/1*jqs_LwM-qgSv-QuFOheERQ.png', 'https://miro.medium.com/max/60/1*7UEbp-Jy4LYUkhCiman-QQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*TZDv06-5rE5-V9HNhg6I9A.jpeg', 'https://miro.medium.com/max/1982/1*7UEbp-Jy4LYUkhCiman-QQ.png', 'https://miro.medium.com/max/60/1*Cr0VhmCS1r76ecfg6PUV8w.png?q=20', 'https://miro.medium.com/max/2246/1*9vdvg9JqiqvbJSWos1-pHw.png', 'https://miro.medium.com/max/60/1*Oz0_cLH1_XCeDSBVlmfnfg.png?q=20', 'https://miro.medium.com/max/60/1*jqs_LwM-qgSv-QuFOheERQ.png?q=20', 'https://miro.medium.com/freeze/max/60/1*emnal6GioazQGu0NedUVPQ.gif?q=20', 'https://miro.medium.com/max/2206/1*g2brVQjvsWF_6Vvq9PnPqg.png', 'https://miro.medium.com/fit/c/80/80/1*dLCgcignlYa9NehXUdTkBw.jpeg', 'https://miro.medium.com/max/2242/1*U-ozXr5kRAY6htO9mPRvoQ.png', 'https://miro.medium.com/max/60/1*ntJmfOjiQLxNfCz36U2fyQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*60Njujwh8b1sZ019pvdijw.jpeg', 'https://miro.medium.com/max/2194/1*ntJmfOjiQLxNfCz36U2fyQ.png', 'https://miro.medium.com/max/1200/1*vgL7OvFYvsghn_V0S3XqrQ.png', 'https://miro.medium.com/fit/c/96/96/1*60Njujwh8b1sZ019pvdijw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*4DftZU6XNUAH5bwKRJIePA.png', 'https://miro.medium.com/max/712/1*emnal6GioazQGu0NedUVPQ.gif', 'https://miro.medium.com/max/2202/1*Oz0_cLH1_XCeDSBVlmfnfg.png', 'https://miro.medium.com/max/2880/1*vgL7OvFYvsghn_V0S3XqrQ.png', 'https://miro.medium.com/max/60/1*9vdvg9JqiqvbJSWos1-pHw.png?q=20', 'https://miro.medium.com/max/2878/1*Cr0VhmCS1r76ecfg6PUV8w.png', 'https://miro.medium.com/max/60/1*vgL7OvFYvsghn_V0S3XqrQ.png?q=20', 'https://miro.medium.com/max/60/1*U-ozXr5kRAY6htO9mPRvoQ.png?q=20'}",2020-03-05 00:25:26.306246,1.6498448848724365
https://medium.com/@aliciagilbert.itsimplified/build-stunning-interactive-web-data-dashboards-with-python-plotly-and-dash-fcbdc09ba318,Build Stunning Interactive Web Data Dashboards with Python Plotly and Dash,"Data Visualization and Python

Data visualizations put large or complex data into graphical format so that patterns, trends and correlations can be more easily seen. A major part of Exploratory Data Analysis or EDA is data visualization. Some EDA techniques include:

Plotting the raw data with histograms or scatter plots. Plotting simple statistics with boxplots.

Python has some very nice libraries for data visualization. Matplotlib, Seaborn and ggplot are all very widely used. However, these libraries produce only static image files of your graphics. If you want your plots to be interactive, Python’s plotly is great for that.

Plotly is free and open source! This library produces html files that allow the user to hover over chart objects to view data labels and values, and to click on legend items to isolate traces. Click here for examples of how to use plotly to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.

To add even more interactivity, use Dash. Dash, built on top of plotly and Flask, ties UI elements like dropdowns, radio items, sliders and datepickers to your plotly graph. According to the documentation, Dash is a Python framework for building analytical web applications. No JavaScript required. It is also free and open source.

Dash Basics and Required Installations

Dash apps are composed of its layout and its interactive components. Dash offers two component libraries (dash_html_components and dash_core_components) that eliminate the need to know HTML, CSS or JavaScript to produce beautiful web data dashboards.

The Dash installation steps are as followed:

Here is a simple Dash application that displays “Hello World”:

Though it is not necessary, I am choosing here to pass a flask app instance into Dash. Doing it this way, by exposing this server variable, you can deploy Dash apps like you would any Flask app. See this website for more information.

This code makes use of the dash_html_components library only. It displays on a webpage the text “Hello World” formatted with the H1 HTML tag. Dash provides all of the available HTML tags as user-friendly Python classes. For more information on available HTML tags go here. To view the Dash app, run the code with

and then visit http://127.0.0.1:8050/ in a web browser.

Let’s extend the above code by adding a simple plotly scatter plot to the dashboard with Dash:

This code makes use of both the dash_html_components and the dash_core_components libraries. Notice here, I imported a couple more libraries. In particular, I imported plotly.graph_objs as go. Plotly charts are described declaratively with objects in plotly.graph_objs and dictionaries. Every aspect of a plotly chart (the colors, the grids, the data, and so on) has a corresponding key-value attribute in these objects. This page contains an extensive list of these attributes.

After running the code and then visiting the URL http://127.0.0.1:8050/ in a web browser, you should see a page that looks like this:

Hover over the dots to see the scatter plot data values. Sweet! Now let’s add a Dash interactive component and then connect it with a callback.

There is quite a bit going on here. My interactive component is a dropdown to select one of two different datasets, random_y1 or random_y2. Depending on which dataset the user selects, the callback @app.callback will select that dataset to display on the scatter plot. I had to import Input and Output from the dash.dependecies library to handle the callback. This may not be obvious, but I initialized the scatter plot to display [0,0] on the y-axis so that you can more clearly see the change in scatter plot upon selection.

After running the code and then visiting the URL http://127.0.0.1:8050/ in a web browser, you should see a page that looks like this:

Putting similar concepts together, I created a very nice Dash web app that you can try out here.

Data For My Dash Web Application

Data used for my Dash web application was taken from the US Department of Justice website which can be accessed here. To get the data from the HTML table into my web app, I used Python for web scraping. There is a very nice tutorial on this topic located here. I pretty much followed these instructions and it worked like a charm. Of course, I tweaked some things to grab exactly what I needed. The output was a .csv file in which I imported into a pandas DataFrame for use in my dashboard.

It is worth noting here that I did not import the Population column or any of the rate columns for use in my dashboard. Also, since there was no data prior to 2013 for the revised definition of rape, I excluded that column as well.

Deploying My Dash Web Application

As mentioned previously, I chose to pass a flask app instance into Dash. Therefore, I was able to deploy my Dash web application like I would any Flask app.

To deploy a Flask app, you will need a server configured to run your app. I used Digital Ocean as my host server provider. This company allows deployment of your Flask app for a very low cost without any restrictions on installations. You have to configure your “droplet” yourself, but the small cost of deployment, in my opinion, is worth the set-up time. For information on deploying a Flask app on Digital Ocean using nginx and uWSGI click here.

My Dash web application shows a scatter plot in which a user can interact with by selecting from the dropdown a crime type he/she wants to display the data for. Scatter plots are great for showing correlations. Upon displaying the data for property crime and larceny on the scatter plot, you can see a decreasing trend over time. It also shows a stacked bar chart in which a user can hide or show traces by selecting or deselecting the trace element from the legend. Stacked bars do a good job of featuring the total and also how the total for each category value is divided into parts. It is clear from my stacked bar chart that property crime is the largest segment every year. Lastly it shows a boxplot with the same hide and show functionality as the stacked bar chart. By hiding some boxplots in this chart you can more easily see outliers for burglary and larceny. In all 3 charts, a user can hover over plot objects to view data labels and values.

Very, very cool.

In Conclusion

You can use plotly and Dash to create beautiful interactive web data applications. I had a lot of fun with this project. The complete code for this app can be accessed here.

Thanks guys for reading and I hope I have provided you some value.","['web', 'stunning', 'plotly', 'python', 'dashboards', 'dash', 'scatter', 'flask', 'plots', 'data', 'build', 'interactive', 'plot', 'app', 'code']","This may not be obvious, but I initialized the scatter plot to display [0,0] on the y-axis so that you can more clearly see the change in scatter plot upon selection.
Data For My Dash Web ApplicationData used for my Dash web application was taken from the US Department of Justice website which can be accessed here.
To get the data from the HTML table into my web app, I used Python for web scraping.
Deploying My Dash Web ApplicationAs mentioned previously, I chose to pass a flask app instance into Dash.
In ConclusionYou can use plotly and Dash to create beautiful interactive web data applications.",en,['Alicia Gilbert'],2018-11-06 15:34:37.270000+00:00,"{'Dashboard', 'Python', 'Data Visualization'}","{'https://miro.medium.com/fit/c/80/80/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/0*qcudLU_9t-bcUPvI?q=20', 'https://miro.medium.com/fit/c/96/96/1*5stX-tI0dqEx5o6Nj4UV8Q.jpeg', 'https://miro.medium.com/fit/c/80/80/2*muI32AIHBFPtlVP9FUWptg.jpeg', 'https://miro.medium.com/max/44/0*gzBWpaNlO2VBl0lO?q=20', 'https://miro.medium.com/max/32/0*0ZQf7R8KzLL1G4wA?q=20', 'https://miro.medium.com/max/449/0*gpSDbH8V3Ih7QqlN', 'https://miro.medium.com/max/1204/0*qcudLU_9t-bcUPvI', 'https://miro.medium.com/max/60/0*U0-OjQcDpjxPQ_t4?q=20', 'https://miro.medium.com/max/60/0*DK0p91Hl8Ci7bgmi?q=20', 'https://miro.medium.com/max/60/0*gpSDbH8V3Ih7QqlN?q=20', 'https://miro.medium.com/max/898/0*gpSDbH8V3Ih7QqlN', 'https://miro.medium.com/fit/c/160/160/1*5stX-tI0dqEx5o6Nj4UV8Q.jpeg', 'https://miro.medium.com/max/1204/0*DK0p91Hl8Ci7bgmi', 'https://miro.medium.com/max/864/0*U0-OjQcDpjxPQ_t4', 'https://miro.medium.com/max/632/0*Cdm6ilsgS7FWPHn1', 'https://miro.medium.com/max/856/0*gzBWpaNlO2VBl0lO', 'https://miro.medium.com/max/60/0*Cdm6ilsgS7FWPHn1?q=20', 'https://miro.medium.com/fit/c/80/80/2*cO1mkMg_MHj7XGB73SnsFw.jpeg', 'https://miro.medium.com/max/1064/0*0ZQf7R8KzLL1G4wA'}",2020-03-05 00:25:27.473924,1.1676781177520752
https://medium.com/@olegkomarov_77860/how-to-embed-a-dash-app-into-an-existing-flask-app-ea05d7a2210b,How to embed a Dash app into an existing Flask app,"Photo by Jezael Melgoza on Unsplash

Dash (by Plotly) is an open source, simple-to-use Python framework for building beautiful data-driven web applications.

If you’re interested in knowing more about the framework, head over to “Introducing Dash”, the official release announcement, or check out the many examples from their gallery.

What’s relevant though is that Dash uses Flask for the back end.

If you’re familiar with Flask, you will know that it has a minimalistic core, it’s highly customizable and comes with easy-to-add extensions.

And so, it’s only natural to think that Dash apps should be easy to integrate into an existing Flask app.

Table of Contents

Existing solutions

Resources on Dash integration with an existing Flask app are sparse and incomplete.

Searching for “dash integration flask” brings up code from the official docs on deployment. B̵u̵t̵ ̵i̵t̵’̵s̵ ̵h̵a̵r̵d̵l̵y̵ ̵e̵v̵e̵n̵ ̵a̵n̵ ̵e̵x̵a̵m̵p̵l̵e̵. The Dash team did a great job and addressed requests by the community. Notheless, the new examples are only using a bare-bones Flask setup.

Other search results point to StackOverflow questions like Running a Dash app within a Flask app or to the Dash’s github issue that requests A simple working example for embedding dash in flask under a path.

These resources are still insufficient and usually have several of the following problems:

examples are not self-contained, difficult to test, disorganized

require an unrealistically simplified Flask app

do not address specific basic requirements like authentication

And so, if you also have spent hours searching, reading and testing through dozens of nested links, without much success, you probably came to the same conclusion.

We need a documented strategy for embedding a Dash app into an existing Flask app!

A self-contained example

I created an easily testable and self-contained example which integrates a Dash app into a realistic Flask project.

To test it

Clone the github repository:

git clone https://github.com/okomarov/dash_on_flask

cd dash_on_flask

touch .envrc

Add config details to the .envrc file (not committed):

export FLASK_APP=dashapp

export FLASK_ENV=development export DATABASE_URL=sqlite:///$PWD/app.db export SECRET_KEY=secret_key_change_as_you_wish_make_it_long_123

Load config details, create a virtual environment (optional, will get back to this), install python dependencies, initialize the database, and run the app:

source .envrc

pip install -r requirements.txt flask db init

flask db migrate -m 'init'

flask db upgrade flask run

Finally, check the app at http://127.0.0.1:5000/dashboard!

You should be re-directed to a sign-in form, with a link to register first. That is, the dashboard requires authentication:

Sample from sign in form

This is exactly what we want! Register first, then sign in, and go back to to the /dashboard.

Now we have an independently built Dash app, served by our Flask app, playing well with our authentication and other extensions.

Implementation details

The app adds the simple stock plotter displayed below (modified to use yahoo prices):

Adapted from Dash’s Stock Ticker example

to a Flask app which uses:

the application factory pattern and blueprints

a database to manage users (sqlite with Flask-SQLAlchemy and Flask-Migrate)

authentication (Flask-Login)

This is a standard setup with most Flask applications. Steps and code on how to implement the infrastructure used in my repo are adapted directly from the excellent Flask Mega Tutorial by Miguel Grinberg.

There are a couple of things to note. So let’s have a look at the folder structure:

.

├── app/

│ ├── __init__.py

│ ├── extensions.py

│ ├── forms.py

│ ├── models.py

│ ├── templates/

│ │ └── ...

│ └── webapp.py

├── app.db

├── config.py

├── dashapp.py

├── migrations/

│ └── ...

├── .envrc

└── requirements.txt

First, the app/ folder contains everything related to our Flask app, or the server, as referred by the Dash documentation. We create the main Blueprint and the routes in webapp.py (edited for clarity):

Full code here

and define the app factory in __init__.py :

Second, dashapp.py is the file which we run with flask run . That file creates the server Flask app which is re-used by the Dash app.

Moreover, the file is also used to define the layout and callbacks of the Dash app and to protect its routes/views (edited for clarity):

Older code, see below for proper Dash app organization

Factor out the Dash app into its own subfolder

As you probably noticed, the file that instantiates the app (sometimes called run.py ) is more cluttered than usual. For instance, the Dash app should really belong to its own folder.

So, if you are not using the application factory pattern, you are probably better off with the simpler structure described in this answer.

Following the workaround explained in this answer, we can put the Dash app code into its own subfolder under the /app :

.

├── app

│ ├── __init__.py

│ ├── dashapp1

│ │ ├── assets/

│ │ ├── callbacks.py

│ │ └── layout.py

│ ⋮

│ └── webapp.py

├── dashapp.py

⋮

where layout and callbacks are in their separate files (side by side editing is easier than having callbacks underneath layout):

New layout and callbacks organization for Dash app

The Dash app instatiation is now all done inside __init__.py (for details check the github repository) and our main dashapp.py is just:

from app import create_app server = create_app()

Feels good!

Multiple Dash apps on a single Flask server

Now that we know how to factor out our Dash app into its own subfolder we can instantiate multiple Dash apps under the same Flask server.

The full solution is on github under the branch feature/multiple_dash_apps.

As a side note, if you are not using the factory pattern to start the Flask server, skip this section and follow the official documention in how to structure a multi-page app.

Now, suppose you have another Dash app which you created under /app/dashapp1 :

.

├── app

│ ├── __init__.py

│ ├── dashapp1

│ │ ├── assets/

│ │ ├── callbacks.py

│ │ └── layout.py

│ ├── dashapp2

│ │ ├── assets/

│ │ ├── callbacks.py

│ │ └── layout.py

│ ⋮

│ └── webapp.py

├── dashapp.py

⋮

We can adapt our /app/__init__.py to:

Instantiating multiple Dash app under the same flask server

We can now launch our Flask server and navigate to <base_url>/dashboard for the first Dash app or to <base_url>/example for the second app!

Deploy to Heroku for Free

Fork the Github repository In the forked repository, edit the app.json and replace the value of the ""repository"": ""https://github.com/okomarov/dash_on_flask"" with the URL of the forked repository. Click on the “Deploy to Heroku” button

In the new screen, if you don’t have a Heroku account create one. Then in the app creation screen, pick a name for the app and click on Deploy.

App creation in Heroku with deploy button

Outstanding points

Jinja templating: when accessing the dashboard route, the Dash app is loaded into its own page instead of a div element that can be fitted into our Jinja templates. This answer describes a workaround on how to re-use our Jinja templating for the Dash app.

Notes on setup

direnv

To run the example we need to create some environment variables, which are then automatically imported by the config.py . We place those variables into a file called .envrc and source them when required.

I let direnv auto-magically load project-specific environment variables defined in the .envrc file when I cd into the folder.

virtualenvwrapper

Before installing the requirements, I recommend creating a virtual environment. One way to do this is by running from the project root:

python3 -m venv env

source env/bin/activate

Another way among the many, is to manage and organize your environments in one place with virtualenvwrapper, which is what I use in conjunction with the direnv integration.

Log of updates

Initial post — 2019/01/02

Better Dash app files organization — 2019/02/21

Multiple Dash apps under a single Flask server — 2019/05/05

Added Flask context to Dash apps — 2019/07/09

Deploy to Heroku — 2019/11/15","['file', 'existing', 'server', 'github', 'embed', 'dash', 'flask', 'repository', 'apps', 'heroku', 'app', 'code']","And so, it’s only natural to think that Dash apps should be easy to integrate into an existing Flask app.
We need a documented strategy for embedding a Dash app into an existing Flask app!
Now we have an independently built Dash app, served by our Flask app, playing well with our authentication and other extensions.
Multiple Dash apps on a single Flask serverNow that we know how to factor out our Dash app into its own subfolder we can instantiate multiple Dash apps under the same Flask server.
Log of updatesInitial post — 2019/01/02Better Dash app files organization — 2019/02/21Multiple Dash apps under a single Flask server — 2019/05/05Added Flask context to Dash apps — 2019/07/09Deploy to Heroku — 2019/11/15",en,['Oleg Komarov'],2020-01-07 00:34:27.593000+00:00,"{'Web Development', 'Flask', 'Python', 'Business Intelligence', 'Dash'}","{'https://miro.medium.com/fit/c/80/80/2*5VLESjo09eI1gMxevfyehg.jpeg', 'https://miro.medium.com/max/2936/1*2mVKlkKzQOF1AEPp-4fNMA.png', 'https://miro.medium.com/max/54/1*5bJQ5ZGI3n5B-jTdO5dN9w.png?q=20', 'https://miro.medium.com/max/60/0*HymSbKwaJsMjE4yk?q=20', 'https://miro.medium.com/fit/c/80/80/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/60/1*2mVKlkKzQOF1AEPp-4fNMA.png?q=20', 'https://miro.medium.com/max/46/1*E9gjU2cggfaChFuYxLIqCg.png?q=20', 'https://miro.medium.com/fit/c/160/160/0*txqvJq7hFA8PLbWD.', 'https://miro.medium.com/max/1200/0*HymSbKwaJsMjE4yk', 'https://miro.medium.com/max/996/1*5bJQ5ZGI3n5B-jTdO5dN9w.png', 'https://miro.medium.com/fit/c/80/80/2*tiQVZEZxHMPcnVmEmN7UtA.jpeg', 'https://miro.medium.com/max/60/1*dYgDFdm28oM9BcrIsvuYoA.png?q=20', 'https://miro.medium.com/fit/c/96/96/0*txqvJq7hFA8PLbWD.', 'https://miro.medium.com/max/780/1*E9gjU2cggfaChFuYxLIqCg.png', 'https://miro.medium.com/max/1318/1*dYgDFdm28oM9BcrIsvuYoA.png', 'https://miro.medium.com/max/60/1*-zG8hKjlRca9iYj4jKPtxg.png?q=20', 'https://miro.medium.com/max/588/1*-zG8hKjlRca9iYj4jKPtxg.png', 'https://miro.medium.com/max/7566/0*HymSbKwaJsMjE4yk'}",2020-03-05 00:25:28.542167,1.067244529724121
https://towardsdatascience.com/python-for-finance-dash-by-plotly-ccf84045b8be,Python for Finance: Dash by Plotly,"Python for Finance: Dash by Plotly

Expanding Jupyter Notebook Stock Portfolio Analyses with Interactive Charting in Dash by Plotly.

By Carlos Muza on Unsplash.

Part 2 of Leveraging Python for Stock Portfolio Analyses.

In part 1 of this series I discussed how, since I’ve become more accustomed to using pandas , that I have signficantly increased my use of Python for financial analyses. During the part 1 post, we reviewed how to largely automate the tracking and benchmarking of a stock portfolio’s performance leveraging pandas and the Yahoo Finance API . At the end of that post you had generated a rich dataset, enabling calculations such as the relative percentage and dollar value returns for portfolio positions versus equally-sized S&P 500 positions during the same holding periods. You could also determine how much each position contributed to your overall portfolio return and, perhaps most importantly, if you would have been better off investing in an S&P 500 ETF or index fund. Finally, you used Plotly for visualizations, which made it much easier to understand which positions drove the most value, what their YTD momentum looked like relative to the S&P 500, and if any had traded down and you might want to consider divesting, aka hit a “Trailing Stop”.

I learned a lot as part of building this initial process in Jupyter notebook, and I also found it very helpful to write a post which walked through the notebook, explained the code and related my thinking behind each of the visualizations. This follow-up post will be shorter than the prior one and more direct in its purpose. While I’ve continued to find the notebook that I created helpul to track my stock portfolio, it had always been my intention to learn and incorporate a Python framework for building analytical dashboards / web applications. One of the most important use cases for me is having the ability to select specific positions and a time frame, and then dynamically evaluate the relative performances of each position. In the future, I will most likely expand this evaluation case to positions I do not own but am considering acquiring. For the rest of this year, I’m looking to further develop my understanding of building web applications by also learning Flask , deploying apps with Heroku , and ideally developing some type of a data pipeline to automate the extracting and loading of new data for the end web application. While I’m still rather early on in this process, in this post I will discuss the extension of the notebook I discussed last time with my initial development using Dash by Plotly , aka Dash .

Dash by Plotly.

If you have read or reference part 1, you will see that once you created the master dataframe, you used Plotly to generate the visualizations which evaluate portfolio performance relative to the S&P 500. Plotly is a very rich library and I prefer to create visualizations using Plotly relative to other Python visualization libraries such as Seaborn and Matplotlib . Building on this, my end goal is to have an interactive dashboard / web app for my portfolio analysis. I’m continuing to search for the optimal solution for this, and in the meantime I’ve begun exploring the use of Dash . Plotly defines Dash as a Python framework for building web applications with the added benefit that no JavaScript is required. As indicated on the landing page which I link to, it’s built on top of Plotly.js, React, and Flask.

The initial benefit that I’ve seen thus far is that, once you’re familiar and comfortable with Plotly , Dash is a natural extension into dashboard development. Rather than simply house your visualizations within the Jupyter notebook where you conduct your analysis, I definitely see value in creating a stand-alone and interactive web app. Dash provides increased interactivity and the ability to manipulate data with “modern UI elements like dropdowns, sliders and graphs”. This functionality directionally supports my ultimate goal for my stock portfolio analyses, including the ability to conduct ‘what if analyses’, as well as interactively research potential opportunities and quickly understand key drivers and scenarios. With all of this considered, the learning curve with Dash , at least for me, is not insignificant.

Jose Portilla’s “Interactive Python Dashboards with Plotly and Dash”

To short circuit the time that it would have taken for me to read through and extensively troubleshoot Dash’s documentation, I enrolled in Jose Portilla’s Plotly and Dash course on Udemy. The detail page for that course can be found here. I have taken a few of Jose’s courses and am currently taking his Flask course. I view him as a very sound and helpful instructor – while he generally does not presume extensive programming experience as prerequisites for his courses, in this Dash course he does recommend at least a strong familiarity with Python . In particular, having a solid understanding of Plotly's syntax for visualization, including using pandas , are highly recommended. After taking the course, you will still be scratching the surface in terms of what you can build with Dash . However, I found the course to be a very helpful jump start, particularly because Jose uses datareader and financial data and examples, including dynamically pulling stock price charts.

Porting Data from Jupyter Notebook to Interact with it in Dash.

Getting Started

Similar to part 1, I created another repo on GitHub with all of the files and code required to create the final Dash dashboard.

Below is a summary of what is included and how to get started:

Investment Portfolio Python Notebook_Dash_blog_example.ipynb — this is very similar to the Jupyter notebook from part 1; the additions include the final two sections: a ‘Stock Return Comparisons’ section, which I built as a proof-of-concept prior to using Dash , and ‘Data Outputs’, where I create csv files of the data the analyses generate; these serve as the data sources used in the Dash dashboard. Sample stocks acquisition dates_costs.xlsx — this is the toy portfolio file, which you will use or modify for your portfolio assessments. requirements.txt — this should have all of the libraries you will need. I recommend creating a virtual environment in Anaconda, discussed further below. Mock_Portfolio_Dash.py — this has the code for the Dash dashboard which we’ll cover below.

As per my repo’s README file, I recommend creating a virtual environment using Anaconda. Here’s a quick explanation and a link to more detail on Anaconda virtual environments:

I recommend Python 3.6 or greater so that you can run the Dash dashboard locally with the provided csv files. Here is a very thorough explanation on how to set up virtual environments in Anaconda .

Last, as mentioned in part 1, once your environment is set up, in addition to the libraries in the requirements file, if you want the Yahoo Finance datareader piece to run in the notebook, you will also need to pip install fix-yahoo-finance within your virtual environment.

Working with Dash

If you have followed along thus far in setting up a virtual environment using Python 3.6, and have installed the necessary libraries, you should be able to run the Python file with the Dash dashboard code.

For those who are less familiar: once in your virtual environment, you will need to change directory, cd, to where you have the repo’s files saved. As a quick example, if you open Anaconda Prompt and you are in your Documents folder, and the files are saved on your Desktop, you could do the following:

cd .. # This will take you up one folder in the directory. cd Desktop # this will take you to your Desktop. dir # This is the windows command to display all files in the directory. You should see the Mock_Portfolio_Dash.py file listed. python Mock_Portfolio_Dash.py # this will run the Dash file

# You will then go to your browser and input the URL where Python says your dashboard is running on localhost.

If you would like the full explanation on the Jupyter notebook and generating the portfolio data set, please refer to part 1. At the end of the Jupyter notebook, you will see the below code in the ‘Data Outputs’ section. These minor additions will send CSV files into your local directory. The first is the full portfolio dataset, from which you can generate all of the visualizations, and the second provides the list of tickers you will use in the first, new stock chart’s dropdown selection.

# Generate the base file that will be used for Dash dashboard. merged_portfolio_sp_latest_YTD_sp_closing_high.to_csv('analyzed_portfolio.csv')

I’ll highlight some key aspects of the Mock Portfolio Python file and share how to run the dashboard locally.","['file', 'virtual', 'stock', 'dashboard', 'portfolio', 'plotly', 'python', 'dash', 'finance', 'data', 'notebook']","Python for Finance: Dash by PlotlyExpanding Jupyter Notebook Stock Portfolio Analyses with Interactive Charting in Dash by Plotly.
Plotly defines Dash as a Python framework for building web applications with the added benefit that no JavaScript is required.
The initial benefit that I’ve seen thus far is that, once you’re familiar and comfortable with Plotly , Dash is a natural extension into dashboard development.
python Mock_Portfolio_Dash.py # this will run the Dash file# You will then go to your browser and input the URL where Python says your dashboard is running on localhost.
merged_portfolio_sp_latest_YTD_sp_closing_high.to_csv('analyzed_portfolio.csv')I’ll highlight some key aspects of the Mock Portfolio Python file and share how to run the dashboard locally.",en,['Kevin Boller'],2018-08-04 20:27:08.694000+00:00,"{'Data Science', 'Python', 'Machine Learning', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/max/3810/0*pBf3K0f-fL3-hUNY.png', 'https://miro.medium.com/max/60/0*jICjTiwfGMojvl_l.jpg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/1*uVrRexmzbS0KHV8QRisX_Q.png', 'https://miro.medium.com/max/1200/0*jICjTiwfGMojvl_l.jpg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/1*uVrRexmzbS0KHV8QRisX_Q.png', 'https://miro.medium.com/max/60/0*pBf3K0f-fL3-hUNY.png?q=20', 'https://miro.medium.com/max/3960/0*jICjTiwfGMojvl_l.jpg'}",2020-03-05 00:25:35.200097,6.656930685043335
https://towardsdatascience.com/a-short-python-tutorial-using-the-open-source-plotly-dash-library-part-i-e59fb1f1a457,A short Python tutorial using the open-source Plotly “Dash” library (Part I),"Of late, I’ve become a major fan of Plotly’s “Dash” Python library for data visualization projects. Using Dash, it is possible to create beautiful graphs and figures, then easily distribute them as lightweight web apps, without ever needing to write a single line of HTML or Javascript. I don’t think it’s hyperbolic to say that within the next 18 months, the Dash library will be one of the most important Python data-visualization libraries available — and it’s all open-source (MIT Licensed).

On a recent project, I’ve been working with some of the more intermediate features of Dash, and wanted to create a few tutorials to share some of the things I’ve learned. Today, I want to focus on learning how to create dynamic data tables in Dash (based off of the original Plotly code here). In subsequent tutorials we’ll look at more advanced features, such as using Dash callback functions to enable/disable certain components of our base app.

For the purposes of this tutorial, we’ll use data on 2011 US agriculture exports per state and a second dataset on the 2014 population for every US state, both available on Github.

2011 US Agricultural Export Data

2014 Population by US State

Let’s say we want to figure out a way to visualize the beef, pork, and poultry exports of each state (in later tutorials, we’ll look at cross-referencing the export data with the State’s population). We’ll walk through each part of the app step-by-step — if you’re in a hurry, jump to the final code embed at the end for the final product.","['opensource', 'short', 'create', 'say', 'plotly', 'python', 'population', 'dash', 'tutorials', 'tutorial', 'data', 'ive', 'state', 'library', 'using']","Of late, I’ve become a major fan of Plotly’s “Dash” Python library for data visualization projects.
Using Dash, it is possible to create beautiful graphs and figures, then easily distribute them as lightweight web apps, without ever needing to write a single line of HTML or Javascript.
I don’t think it’s hyperbolic to say that within the next 18 months, the Dash library will be one of the most important Python data-visualization libraries available — and it’s all open-source (MIT Licensed).
Today, I want to focus on learning how to create dynamic data tables in Dash (based off of the original Plotly code here).
In subsequent tutorials we’ll look at more advanced features, such as using Dash callback functions to enable/disable certain components of our base app.",en,['Daniel Barker'],2018-09-17 20:41:25.080000+00:00,"{'Data Science', 'Python', 'Plotly', 'Data Visualization', 'Computer Science'}","{'https://miro.medium.com/max/2046/1*8aYmXZtROGp7ri4L_vJ-zA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*bgdQybrl-s1d6udkSZPH6Q.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2048/1*dh37jHa1cxp8adKmYleuUw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/1*brwX2PSgW6yjaW3FjwtytQ.png', 'https://miro.medium.com/max/60/1*dh37jHa1cxp8adKmYleuUw.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*brwX2PSgW6yjaW3FjwtytQ.png', 'https://miro.medium.com/max/2452/1*pVTfpV30FC1QeQ2AO0M9_w.png', 'https://miro.medium.com/max/60/1*8aYmXZtROGp7ri4L_vJ-zA.png?q=20', 'https://miro.medium.com/max/2438/1*mAZBPxixqkuoYItScdXcQg.png', 'https://miro.medium.com/max/1023/1*8aYmXZtROGp7ri4L_vJ-zA.png', 'https://miro.medium.com/max/60/1*pVTfpV30FC1QeQ2AO0M9_w.png?q=20', 'https://miro.medium.com/max/5000/1*bgdQybrl-s1d6udkSZPH6Q.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/1*mAZBPxixqkuoYItScdXcQg.png?q=20'}",2020-03-05 00:25:37.027272,1.827174186706543
https://towardsdatascience.com/dash-a-beginners-guide-d118bd620b5d,Dash: A Beginner’s Guide,"As a data scientist, one of the most integral aspects of our job is to relay and display data to “non-data scientists”, in formats that provide visually actionable data. In my opinion, one of the coolest parts of our job is interacting with data, especially when there is visual interaction. At times where we might want to build an interactive application, one of the options available to us is a framework called Dash. Dash is an open source Python framework for building web applications, created and maintained by the people at Plotly. Dash’s web graphics are completely interactive because the framework is built on top of Ploty.js, a JavaScript library written and maintained by Ploty. This means that after importing the Dash framework into a Python file you can build a web application writing strictly in Python with no other languages necessary.

If you’re new to coding, you might be saying to yourself, “A framework sounds like a library…what’s the difference?” I’m glad you asked!

“Inversion of Control is a key part of what makes a framework different to a library. A library is essentially a set of functions that you can call, these days usually organized into classes. Each call does some work and returns control to the client. A framework embodies some abstract design, with more behavior built in. In order to use it you need to insert your behavior into various places in the framework either by subclassing or by plugging in your own classes. The framework’s code then calls your code at these points.”

- Martin Fowler -

The key difference between a library and a framework is that “control” is inverted. When you call a method from a library, you are in control. However, with a framework the control is inverted; the framework calls you. One easy way to think of this concept is the Hollywood Principle - “Don’t call us, we’ll call you.”

“Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements.” -https://github.com/plotly/dash

Let’s break that down into simpler terms. Declarative programming is a technique of building computer programs in which the programmer merely declares properties of the desired result although not how to compute it. This style basically tells you what the program should accomplish but not the specific sequence of steps to follow. The “how” is left up to the program’s interpreter. I’ll make an analogy based on the assumption that you have friends whom possess baking skills. Declarative Programming is like asking your friend to make you a cake. You don’t care how they make it; the process is up to them. One example of a declarative language is SQL, a database query language.","['web', 'code', 'beginners', 'framework', 'control', 'python', 'dash', 'data', 'interactive', 'library', 'declarative', 'guide']","At times where we might want to build an interactive application, one of the options available to us is a framework called Dash.
Dash is an open source Python framework for building web applications, created and maintained by the people at Plotly.
Dash’s web graphics are completely interactive because the framework is built on top of Ploty.js, a JavaScript library written and maintained by Ploty.
This means that after importing the Dash framework into a Python file you can build a web application writing strictly in Python with no other languages necessary.
However, with a framework the control is inverted; the framework calls you.",en,['Andrew Sproul'],2018-11-15 14:53:35.381000+00:00,"{'Data Science', 'Web Applications', 'Python', 'Dash', 'Visualization'}","{'https://miro.medium.com/freeze/max/60/1*xeQbH0GDSmFq14f2-q4qdw.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*t0mrWYoGDUY1GhpauhQyug.gif?q=20', 'https://miro.medium.com/max/940/1*UspfBsAdMdU_rZk0O-JnoQ.gif', 'https://miro.medium.com/fit/c/96/96/1*EtsiZujUjpC6oo7kfEKWew.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/160/160/1*EtsiZujUjpC6oo7kfEKWew.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1280/1*9lN_XTuSxuDHn2kYc6PqXA.png', 'https://miro.medium.com/max/7680/1*bltvHu9x0gb7bqIsNzsFrg.gif', 'https://miro.medium.com/max/1490/1*xeQbH0GDSmFq14f2-q4qdw.gif', 'https://miro.medium.com/max/60/1*9lN_XTuSxuDHn2kYc6PqXA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*k6hmqmlEPWjMLObQq3iaNQ.jpeg?q=20', 'https://miro.medium.com/freeze/max/60/1*dxLNQHmaNBIh0i6Ds_d1jQ.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*8pAScaJTQH3nLC8CtmwYoQ.gif?q=20', 'https://miro.medium.com/max/1674/1*t0mrWYoGDUY1GhpauhQyug.gif', 'https://miro.medium.com/max/1032/1*LkV2JcYcgoullSuSrJHf8A.gif', 'https://miro.medium.com/freeze/max/60/1*bltvHu9x0gb7bqIsNzsFrg.gif?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/freeze/max/1200/1*bltvHu9x0gb7bqIsNzsFrg.gif', 'https://miro.medium.com/max/1472/1*k6hmqmlEPWjMLObQq3iaNQ.jpeg', 'https://miro.medium.com/freeze/max/60/1*UspfBsAdMdU_rZk0O-JnoQ.gif?q=20', 'https://miro.medium.com/max/1490/1*8pAScaJTQH3nLC8CtmwYoQ.gif', 'https://miro.medium.com/max/2624/1*dxLNQHmaNBIh0i6Ds_d1jQ.gif', 'https://miro.medium.com/freeze/max/60/1*LkV2JcYcgoullSuSrJHf8A.gif?q=20'}",2020-03-05 00:25:42.804156,5.7768847942352295
https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23,"Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard","Creating an interactive visualization application in Bokeh

Sometimes I learn a data science technique to solve a specific problem. Other times, as with Bokeh, I try out a new tool because I see some cool projects on Twitter and think: “That looks pretty neat. I’m not sure when I’ll use it, but it could come in handy.” Nearly every time I say this, I end up finding a use for the tool. Data science requires knowledge of many different skills and you never know where that next idea you will use will come from!

In the case of Bokeh, several weeks after trying it out, I found a perfect use case in my work as a data science researcher. My research project involves increasing the energy efficiency of commercial buildings using data science, and, for a recent conference, we needed a way to show off the results of the many techniques we apply. The usual suggestion of a powerpoint gets the job done, but doesn’t really stand out. By the time most people at a conference see their third slide deck, they have already stopped paying attention. Although I didn’t yet know Bokeh very well, I volunteered to try and make an interactive application with the library, thinking it would allow me to expand my skill-set and create an engaging way to show off our project. Skeptical, our team prepared a back-up presentation, but after I showed them some prototypes, they gave it their full support. The final interactive dashboard was a stand-out at the conference and will be adopted by our team for future use:

Example of Bokeh Dashboard built for my research

While not every idea you see on Twitter is probably going to be helpful to your career, I think it’s safe to say that knowing more data science techniques can’t possibly hurt. Along these lines, I started this series to share the capabilities of Bokeh, a powerful plotting library in Python that allows you to make interactive plots and dashboards. Although I can’t share the dashboard for my research, I can show the basics of building visualizations in Bokeh using a publicly available dataset. This third post is a continuation of my Bokeh series, with Part I focused on building a simple graph, and Part II showing how to add interactions to a Bokeh plot. In this post, we will see how to set up a full Bokeh application and run a local Bokeh server accessible in your browser!

This article will focus on the structure of a Bokeh application rather than the plot details, but the full code for everything can be found on GitHub. We will continue to use the NYCFlights13 dataset, a real collection of flight information from flights departing 3 NYC airports in 2013. There are over 300,000 flights in the dataset, and for our dashboard, we will focus primarily on exploring the arrival delay information.

To run the full application for yourself, make sure you have Bokeh installed ( using pip install bokeh ), download the bokeh_app.zip folder from GitHub, unzip it, open a command window in the directory, and type bokeh serve --show bokeh_app . This will set-up a local Bokeh server and open the application in your browser (you can also make Bokeh plots available publicly online, but for now we will stick to local hosting).

Final Product

Before we get into the details, let’s take a look at the end product we’re aiming for so we can see how the pieces fit together. Following is a short clip showing how we can interact with the complete dashboard:","['conference', 'iii', 'dashboard', 'python', 'making', 'science', 'local', 'dataset', 'visualization', 'data', 'bokeh', 'application', 'interactive', 'using', 'complete']","Creating an interactive visualization application in BokehSometimes I learn a data science technique to solve a specific problem.
Data science requires knowledge of many different skills and you never know where that next idea you will use will come from!
In the case of Bokeh, several weeks after trying it out, I found a perfect use case in my work as a data science researcher.
In this post, we will see how to set up a full Bokeh application and run a local Bokeh server accessible in your browser!
This article will focus on the structure of a Bokeh application rather than the plot details, but the full code for everything can be found on GitHub.",en,['Will Koehrsen'],2018-04-02 21:31:17.018000+00:00,"{'Python', 'Data Visualization', 'Towards Data Science', 'Programming', 'Bokeh'}","{'https://miro.medium.com/max/60/1*fnxAzaoSwqrhX2K7RZJdeg.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/freeze/max/60/1*nN5-hITqzDlhelSJ2W9x5g.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/340/1*MvlTa19t4B5MLhY6329B7Q.png', 'https://miro.medium.com/freeze/max/60/1*6orEuCOf0HsnCp_wzKPs3A.gif?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*wWPUyFSC0LlX960L3FTeDQ.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1600/1*6orEuCOf0HsnCp_wzKPs3A.gif', 'https://miro.medium.com/max/1600/1*nN5-hITqzDlhelSJ2W9x5g.gif', 'https://miro.medium.com/max/2618/1*CUyrsJpP5lkvVdheseAYXQ.png', 'https://miro.medium.com/max/3400/1*wWPUyFSC0LlX960L3FTeDQ.jpeg', 'https://miro.medium.com/max/3542/1*fnxAzaoSwqrhX2K7RZJdeg.png', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*wWPUyFSC0LlX960L3FTeDQ.jpeg', 'https://miro.medium.com/max/30/1*MvlTa19t4B5MLhY6329B7Q.png?q=20', 'https://miro.medium.com/max/60/1*CUyrsJpP5lkvVdheseAYXQ.png?q=20'}",2020-03-05 00:25:49.249251,6.444096088409424
https://towardsdatascience.com/how-to-build-a-complex-reporting-dashboard-using-dash-and-plotl-4f4257c18a7f,How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.","['cells', 'doppelganger', 'dashboard', 'plotly', 'columns', 'dash', 'callback', 'formatting', 'data', 'build', 'conditional', 'reporting', 'table', 'value', 'using']","A method to select either a condensed data table or the complete data table.
One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table.
If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.
There is a bug in the Dash data table code in which conditional formatting does not work properly.
I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.).",en,['David Comfort'],2019-03-13 14:21:44.055000+00:00,"{'Dashboard', 'Data Science', 'Data Visualization', 'Towards Data Science', 'Dash'}","{'https://miro.medium.com/max/28/1*76Ni1D-TvMa7Q2C9eH92FQ.png?q=20', 'https://miro.medium.com/max/4428/1*bUz81xK2IGo9abzsIzz3gw.png', 'https://miro.medium.com/max/60/1*4Pqj0pzEyUSkdUuMPa6NFA.png?q=20', 'https://miro.medium.com/max/888/1*PaIwCRRB855x2xhTHpyb7Q.png', 'https://miro.medium.com/max/3404/1*7vFIZxM1K4oHE861ahdqVg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/1400/1*76Ni1D-TvMa7Q2C9eH92FQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/42/1*7vFIZxM1K4oHE861ahdqVg.png?q=20', 'https://miro.medium.com/max/4304/1*2xze0C6T3SQweRhRQwbOzg.png', 'https://miro.medium.com/max/60/1*6iox1sjUoB9CcADal-3WAA.png?q=20', 'https://miro.medium.com/max/60/1*2xze0C6T3SQweRhRQwbOzg.png?q=20', 'https://miro.medium.com/max/5156/1*4Pqj0pzEyUSkdUuMPa6NFA.png', 'https://miro.medium.com/max/3420/1*XrmB2nwuHPxtpCICXWDcVA.png', 'https://miro.medium.com/max/44/1*XrmB2nwuHPxtpCICXWDcVA.png?q=20', 'https://miro.medium.com/max/2160/1*6iox1sjUoB9CcADal-3WAA.png', 'https://miro.medium.com/max/4344/1*9_7J0kgJSj-fOj47OwTkQA.png', 'https://miro.medium.com/max/60/1*bUz81xK2IGo9abzsIzz3gw.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/38/1*ZBUumyprilJgMy0HB9OmGg.png?q=20', 'https://miro.medium.com/max/4244/1*PXT7I6DAcaaVG0s4g_6vnw.png', 'https://miro.medium.com/max/58/1*izIFUYyYhIi8qahMLIsHRg.png?q=20', 'https://miro.medium.com/max/1200/1*izIFUYyYhIi8qahMLIsHRg.png', 'https://miro.medium.com/max/4468/1*izIFUYyYhIi8qahMLIsHRg.png', 'https://miro.medium.com/fit/c/96/96/1*_QxQ53O2e3otKd2WD4t03Q.jpeg', 'https://miro.medium.com/max/60/1*PaIwCRRB855x2xhTHpyb7Q.png?q=20', 'https://miro.medium.com/max/4484/1*9UpPeoKW37MEFAjetTA4Iw.png', 'https://miro.medium.com/max/3764/1*EFUNX2eoDCBmDKQxGzEeAA.png', 'https://miro.medium.com/max/60/1*PXT7I6DAcaaVG0s4g_6vnw.png?q=20', 'https://miro.medium.com/max/60/1*9_7J0kgJSj-fOj47OwTkQA.png?q=20', 'https://miro.medium.com/max/60/1*9UpPeoKW37MEFAjetTA4Iw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*EFUNX2eoDCBmDKQxGzEeAA.png?q=20', 'https://miro.medium.com/max/2564/1*ZBUumyprilJgMy0HB9OmGg.png', 'https://miro.medium.com/fit/c/160/160/1*_QxQ53O2e3otKd2WD4t03Q.jpeg'}",2020-03-05 00:25:52.294286,3.045034885406494
https://medium.com/@plotlygraphs/7-new-dash-apps-made-by-the-dash-community-196998112ce3,7 New Dash Apps Made by the Dash Community,"We are always looking for new and creative ways that the community is utilizing Dash and Plotly to make their web apps.

If you’re interested in learning to make these apps yourself, a great starting point is our workshop series. Our next workshops, in Boston, will offer advanced training for Dash and R Shiny April 14–15. Chris Parmer, the creator of Dash, will be discussing this open source library for creating analytic web apps with Python. You’ll learn how to create Dash apps like the ones in this post, plus more advanced ones and the best usage of the latest Dash features.

In the meantime, check out these 7 fresh Dash apps made by the Dash community. Alternatively, you can view a variety of apps at the Dash Gallery maintained by Plotly.

Dash: build beautiful web-based interfaces in Python

1. Great Balls of Fire: NASA Fireballs Open API

This Dash app was made using fireball data from 1988 to 2017 from NASA’s Jet Propulsion Laboratory.

Dash app | Dash author: Ivan Nieto | BitBucket code

2. Zika Outbreak Explorer

This is a Dash that utilizes Plotly’s crossfilter to look at Zika incidents over time.

Dash app | Dash author: Charley Ferrari | GitHub code

3. Finding Bigfoot

Here’s an exploratory Dash app based on a Bigfoot Sightings from data.world. It demonstrates several plots (including a map), a grid layout built with Bootstrap, interactions with an input field, and caching.

Dash app | Dash author: Timothy Renner | GitHub code

4. Climate Change Life Events

Climate history and possible futures showing your important life events, inspired by Sophie Lewis.

Dash app | Dash author: Greg Schivley | GitHub code

5. Earthquake Explorer

A Plotly Dash application showing earthquake data from the US Geological Survey.

Dash app | Dash author: Giacomo Debidda | GitHub code

6. Street Quality Identification Device

The technology consists of taking pictures of the streets (with a camera installed on a vehicle) every second, and combine this information with an accelerometer data, measuring the ride quality of the streets. The first city to use this technology is the city of Syracuse and the result is below.

Dash app | Dash author: Adriano M. Yoshino| GitHub Code

7. Visualizing MRI Data

Visualization tool for (f)MRI data-sets using Plotly Dash. Submitted to the TransIP VPS competition — and it won the 1st prize!

Dash app | Dash author: Lukas Snoek | GitHub Code","['web', 'community', 'zika', 'github', 'plotly', 'dash', 'data', 'author', 'apps', 'using', 'app']","You’ll learn how to create Dash apps like the ones in this post, plus more advanced ones and the best usage of the latest Dash features.
In the meantime, check out these 7 fresh Dash apps made by the Dash community.
Dash app | Dash author: Ivan Nieto | BitBucket code2.
Dash app | Dash author: Greg Schivley | GitHub code5.
Dash app | Dash author: Lukas Snoek | GitHub Code",en,[],2018-03-28 14:39:52.097000+00:00,"{'Rstudio', 'Rstats', 'Dash', 'Data Visualization'}","{'https://miro.medium.com/max/2880/1*6K8V_xTLvm9kt_-g4WFwQg.png', 'https://miro.medium.com/max/1758/1*BbfcHv3SgRguzyMbyN8Cxg.gif', 'https://miro.medium.com/max/2740/1*SJ22azhYSYOntsUgiBuBpQ.gif', 'https://miro.medium.com/max/1350/1*pUiXXPR4QkBwf_1wpngcug.gif', 'https://miro.medium.com/freeze/max/60/1*BbfcHv3SgRguzyMbyN8Cxg.gif?q=20', 'https://miro.medium.com/max/2874/1*9ZWGz_xayrvnWx1QpoumYw.gif', 'https://miro.medium.com/max/522/1*SlkVlKItoWYktcVgy2XH5w.png', 'https://miro.medium.com/fit/c/80/80/2*LN8gIwMva9YVJ3AYv1RklQ.png', 'https://miro.medium.com/max/2876/1*4u-KabmUGsiKUW7cGR3rgQ.gif', 'https://miro.medium.com/max/600/1*wUVA5RV_ROAZGAX7G6tIpw.png', 'https://miro.medium.com/max/1044/1*SlkVlKItoWYktcVgy2XH5w.png', 'https://miro.medium.com/max/2416/1*WoqE3FJAuGpEXoSF1AJaNg.gif', 'https://miro.medium.com/freeze/max/60/1*SJ22azhYSYOntsUgiBuBpQ.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*9ZWGz_xayrvnWx1QpoumYw.gif?q=20', 'https://miro.medium.com/freeze/max/60/1*WoqE3FJAuGpEXoSF1AJaNg.gif?q=20', 'https://miro.medium.com/fit/c/160/160/2*LN8gIwMva9YVJ3AYv1RklQ.png', 'https://miro.medium.com/max/60/1*6K8V_xTLvm9kt_-g4WFwQg.png?q=20', 'https://miro.medium.com/max/60/1*SlkVlKItoWYktcVgy2XH5w.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*LU8DrYH0werAkSh2K9VPDg.png', 'https://miro.medium.com/fit/c/80/80/2*NpP9YJcwOtdmjVL8C9Sdjg.jpeg', 'https://miro.medium.com/fit/c/96/96/2*LN8gIwMva9YVJ3AYv1RklQ.png', 'https://miro.medium.com/freeze/max/60/1*4u-KabmUGsiKUW7cGR3rgQ.gif?q=20', 'https://miro.medium.com/freeze/max/54/1*pUiXXPR4QkBwf_1wpngcug.gif?q=20'}",2020-03-05 00:25:54.686481,2.391195774078369
https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a,Building a Search Engine with BERT and TensorFlow,"Building a Search Engine with BERT and TensorFlow

In this experiment, we use a pre-trained BERT model checkpoint to build a general-purpose text feature extractor, which we apply to the task of nearest neighbour search.

T-SNE decomposition of BERT text representations (Reuters-21578 benchmark, 6 classes)

Feature extractors based on deep Neural Probabilistic Language Models such as BERT may extract features that are relevant for a wide array of downstream NLP tasks. For that reason, they are sometimes referred to as Natural Language Understanding (NLU) modules.

These features may also be utilized for computing the similarity between text samples, which is useful for instance-based learning algorithms (e.g. K-NN). To illustrate this, we will build a nearest neighbour search engine for text, using BERT for feature extraction.

The plan for this experiment is:

getting the pre-trained BERT model checkpoint extracting a sub-graph optimized for inference creating a feature extractor with tf.Estimator exploring vector space with T-SNE and Embedding Projector implementing a nearest neighbour search engine accelerating search queries with math example: building a movie recommendation system

Questions and Answers

What is in this guide?

This guide contains implementations of two things: a BERT text feature extractor and a nearest neighbour search engine.

Who is this guide for?

This guide should be useful for researchers interested in using BERT for natural language understanding tasks. It may also serve as a worked example of interfacing with tf.Estimator API.

What does it take?

For a reader familiar with TensorFlow it should take around 30 minutes to finish this guide.

Show me the code.

The code for this experiment is available in Colab here. Also, check out the repository I set up for my BERT experiments: it contains bonus stuff!

Now, let’s start.

Step 1: getting the pre-trained model

We start with a pre-trained BERT checkpoint. For demonstration purposes, I will be using the uncased English model pre-trained by Google engineers. To train a model for a different language, check out my other guide.

For configuring and optimizing the graph for inference we will make use of the awesome bert-as-a-service repository. This repository allows for serving BERT models for remote clients over TCP.

Having a remote BERT-server is beneficial in multi-host environments. However, in this part of the experiment we will focus on creating a local

(in-process) feature extractor. This is useful if one wishes to avoid additional latency and potential failure modes introduced by a client-server architecture.

Now, let us download the model and install the package.

Step 2: optimizing the inference graph

Normally, to modify the model graph we would have to do some low-level TensorFlow programming. However, thanks to bert-as-a-service, we can configure the inference graph using a simple CLI interface.

There are a couple of parameters there to look out for.

For each text sample, BERT encoding layers output a tensor of shape [sequence_len, encoder_dim] with one vector per token. We need to apply some sort of pooling if we are to obtain a fixed representation.

POOL_STRAT parameter defines the pooling strategy applied to the encoder layer number POOL_LAYER. The default value ‘REDUCE_MEAN’ averages the vectors for all tokens in a sequence. This strategy works best for most sentence-level tasks when the model is not fine-tuned. Another option is NONE, in which case no pooling is applied at all. This is useful for word-level tasks such as Named Entity Recognition or POS tagging. For a detailed discussion of these options check out the Han Xiao’s blog post.

SEQ_LEN affects the maximum length of sequences processed by the model. Smaller values will increase the model inference speed almost linearly.

Running the above command will put the model graph and weights into a GraphDef object which will be serialized to a pbtxt file at GRAPH_OUT. The file will often be smaller than the pre-trained model because the nodes and variables required for training will be removed. This results in a very portable solution: for example, the english model only takes 380 MB after serializing.

Step 3: creating a feature extractor

Now, we will use the serialized graph to build a feature extractor using the tf.Estimator API. We will need to define two things: input_fn and model_fn

input_fn manages getting the data into the model. That includes executing the whole text preprocessing pipeline and preparing a feed_dict for BERT.

First, each text sample is converted into a tf.Example instance containing the necessary features listed in INPUT_NAMES. The bert_tokenizer object contains the WordPiece vocabulary and performs the text preprocessing. After that, the examples are re-grouped by feature name in a feed_dict.

tf.Estimators have a fun feature which makes them rebuild and reinitialize the whole computational graph at each call to the predict function. So, in order to avoid the overhead, we will pass a generator to the predict function, and the generator will yield the features to the model in a never-ending loop. Ha-ha.","['graph', 'tensorflow', 'bert', 'feature', 'pretrained', 'extractor', 'engine', 'inference', 'search', 'model', 'building', 'text', 'using']","Building a Search Engine with BERT and TensorFlowIn this experiment, we use a pre-trained BERT model checkpoint to build a general-purpose text feature extractor, which we apply to the task of nearest neighbour search.
To illustrate this, we will build a nearest neighbour search engine for text, using BERT for feature extraction.
This guide contains implementations of two things: a BERT text feature extractor and a nearest neighbour search engine.
This guide should be useful for researchers interested in using BERT for natural language understanding tasks.
Step 1: getting the pre-trained modelWe start with a pre-trained BERT checkpoint.",en,['Denis Antyukhov'],2020-03-02 21:36:43.208000+00:00,"{'Information Retrieval', 'Machine Learning', 'Deep Learning', 'Naturallanguageprocessing'}","{'https://miro.medium.com/fit/c/96/96/2*UO1x-7AIjLqr9REtxBveIg.jpeg', 'https://miro.medium.com/max/60/1*9oQzTBGJ1qr1jfzciLGbdA.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/5280/1*GQl-cYKkjtI_YDayjY8RyA.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2680/1*9oQzTBGJ1qr1jfzciLGbdA.png', 'https://miro.medium.com/max/1200/1*amiNjYHGJDVreoF5HTp2Gg.png', 'https://miro.medium.com/max/60/1*GsOkDVQkQKEnyso0XucLyA.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*UO1x-7AIjLqr9REtxBveIg.jpeg', 'https://miro.medium.com/max/60/1*amiNjYHGJDVreoF5HTp2Gg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/3600/1*amiNjYHGJDVreoF5HTp2Gg.png', 'https://miro.medium.com/max/60/1*GQl-cYKkjtI_YDayjY8RyA.png?q=20', 'https://miro.medium.com/max/60/1*6n49V1YYpD2bGOErcid2YQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/3920/1*6n49V1YYpD2bGOErcid2YQ.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/3800/1*GsOkDVQkQKEnyso0XucLyA.png'}",2020-03-05 00:25:59.216142,4.528743028640747
https://medium.com/@abhijitdaskgp/setup-tensor-flow-and-keras-with-gpu-support-on-windows-pc-2a13f5f15f9f,Setup Tensor flow and Keras with GPU support on Windows PC,"I have started using python and machine learning for few months back and even though I have some remote background on this, I found it very extensive in terms of all the materials available for learning — starting from introduction to machine learning to how to implement deep network in practical examples using ROS (Robot Operating System). So, I started to read whatever I found from google and very soon I feel little overwhelmed. Anyway, I started concentrating only on supervised learning with available data set and at first I was using Deep Learning Toolbox (formerly Neural Network Toolbox) from MATLAB . I got some meaningful results and then started to think about incorporating the real ‘big-data’. I immediately found MATLAB probably not best application for large data even though I am introduced with few tools especially for large data handling such as datastorage, filedatastorage, tall arrays etc (I have contacted MATLAB support to confirm that I am using these tools right). So, on one weekend, I started to play with python and Keras to implement the same data set and yes, I figured why it is so popular among developers, not only because it is free, it’s way more flexible than MATLAB. Python (numpy, pandas) can handle large data much better than MATLAB (in my experience). I also found having GPU support with Keras is more straight forward than MATLAB as the later requires Parallel Computing Toolbox in addition to Deep Learning Toolbox (see below).

Anyway, I thought it would be good for me if I can summarize my learning from last few weeks that I or some newbie like me can refer it to in future if needed. There are lot of similar tutorials are out there and I am not conveying something new, rather summarizing some of the key steps that a beginner can follow to get up to speed in this topic pretty quick (I tried to summarize the way I wish to have these at the very first day).

After quite a bit trial and error on using GPU with Keras on multiple PCs, I found that the following steps are helpful if you want to configure everything at a first try —

Install Visual Studio — It will be fine if you install the free community version if it. Follow this link to see which version you need for your case. If you wish to get an ISO copy of the installer for future reference use this link . You just need the basic install as shown in the image below

Install CUDA Toolkit and CUDNN libraries — Then you need to install NVDIA CUDA tool set and CUDNN libraries directly from respective links. You will need NVDIA developer account for downloading CUDNN libraries. Not sure why though. You may take a look into NVDIA’s other libraries if applicable. If you are not sure how to install these libraries just follow the Section 4 of Ankit Bhatia’s blog. Most often the confusing part is to choose the right version that is compatible with Tensorflow. Just follow this link to make sure you are on the right path.

Add Environment Variables — Next step is to set-up the environment variable for CUDA libraries on Windows. Again follow the Section 5 if required.

Install Anaconda — Next you need Anaconda if you haven’t already installed it. As Keras and Tensor flow now support Python 3.x you can choose to install either python 2.x or python 3.x. Let’s assume we installed 3.x. Also make sure to check the PATH environment variable option during installation, otherwise if you can do it later manually

Setup Conda Environment — After this the concept of conda environment comes into the picture. Conda environment is just a virtual setup for your program/project/application so that you have a clean dependency structure. So, once we set up a new conda environment, we need to wither clone existing Tensorflow and Keras setup or install it again for that environment. Either way is fine, but I prefer just to reinstall as then you don’t need to worry about a specific toolbox version that is applicable to a specific project. In any case, as you need a Tensorflow package for GPU support. You will get most of the conda environment related command from here.

Install Tensorflow and Keras — From now just follow the instruction described here by Donald Kinghorn, starting from the section Install TensorFlow-GPU from the Anaconda Cloud Repositories

Here are the commands that you need to use one by one —

Line# 1 and 2 are simple enough to follow. On Line# 3-6, you may choose to install tensorflow-gpu directly (pip install tensorflow-gpu==1.10). In that case, just make sure to use pip instead of conda command. Also make sure CUDA, CUDNN and Tensorflow versions are compatible with each other. If you follow the above mentioned code, then you will get (as of today) Tensorflow ver. 9.0, CUDA ver. 9.0 and CUDNN ver. 7.1.4 which are compatible to each other. Installing cudatoolkit and cudnn seems redundant as we have already installed them previously. But, I found these steps make the configuration robust (not exactly sure why). In line 7, you may choose to install Spyder or any other python editor. After executing line #10, you should see an output something like the figure below where GPU is identified as gpu:0 by Tensorflow (in my case it is 1050 Ti with compute capability 6.1). Note that Tensorflow supports NVDIA GPU that has compute capability 3.5 or higher.

From now by default GPU will be used as the primary computation resource under the conda environment tflow-gpu. In addition you can enforce the use of GPU or CPU by adding the following commands as needed in your script —

To view the graphics card usage while training, open a command prompt and cd to NVSMI directory under NVDIA installation folder. In my PC it is located at C:\Program Files\NVIDIA Corporation\NVSMI. If you run nvidia-smi.exe, you should be able to see the GPU usage as shown below.

To automatically update the memory usage can use the command nvidia-smi dmon -d 3 for every 3 second update (in Linux the similar command is watch -n 3 nvidia-smi). You will see an output something like below where the column ‘sm’ and ‘mem’ represent the % utilization of CUDA SM(Streaming Multiprocessor) cores and graphics memory.

You can list all the available GPUs by using the command nvidia-smi -L.

If you have multiple GPUs, you may follow this to set them up appropriately.

Some noteworthy references —

The Best Way To Install Tensorflow with Gpu Support on Windows 10 (without Installing Cuda) by Donald Kinghorn

https://medium.com/@raza.shahzad/setting-up-tensorflow-gpu-keras-in-conda-on-windows-10-75d4fd498198 by Shahzad Raza

Set up GPU Accelerated Tensorflow & Keras on Windows 10 with Anaconda by Ankit Bhatia","['sure', 'flow', 'tensorflow', 'install', 'environment', 'pc', 'tensor', 'setup', 'keras', 'need', 'python', 'learning', 'follow', 'windows', 'gpu', 'support']","I also found having GPU support with Keras is more straight forward than MATLAB as the later requires Parallel Computing Toolbox in addition to Deep Learning Toolbox (see below).
As Keras and Tensor flow now support Python 3.x you can choose to install either python 2.x or python 3.x.
So, once we set up a new conda environment, we need to wither clone existing Tensorflow and Keras setup or install it again for that environment.
In any case, as you need a Tensorflow package for GPU support.
Some noteworthy references —The Best Way To Install Tensorflow with Gpu Support on Windows 10 (without Installing Cuda) by Donald Kinghornhttps://medium.com/@raza.shahzad/setting-up-tensorflow-gpu-keras-in-conda-on-windows-10-75d4fd498198 by Shahzad RazaSet up GPU Accelerated Tensorflow & Keras on Windows 10 with Anaconda by Ankit Bhatia",en,['Abhijit Das'],2019-01-07 03:06:16.293000+00:00,"{'Keras', 'Machine Learning', 'TensorFlow', 'Gpu'}","{'https://miro.medium.com/max/60/1*-vdWkgw8gf54BUVE9TVS8g.png?q=20', 'https://miro.medium.com/max/1470/1*9pWTJK3Xi4MJgVmvVXXdDg.png', 'https://miro.medium.com/max/60/1*0FIltfEWvvUXFEWqZs_4-A.png?q=20', 'https://miro.medium.com/max/60/1*b9guhbBzbmIYFh4uhWJkgA.png?q=20', 'https://miro.medium.com/max/1090/1*ZbmJWJHkL9rsXZHc7gFAZg.png', 'https://miro.medium.com/max/2864/1*4kqGFhlnFBfm-WStuOJSwQ.png', 'https://miro.medium.com/max/60/1*9pWTJK3Xi4MJgVmvVXXdDg.png?q=20', 'https://miro.medium.com/max/60/1*4kqGFhlnFBfm-WStuOJSwQ.png?q=20', 'https://miro.medium.com/max/1074/1*0FIltfEWvvUXFEWqZs_4-A.png', 'https://miro.medium.com/fit/c/80/80/1*IUnkky_B5C2krQs9oYVk6A.jpeg', 'https://miro.medium.com/fit/c/80/80/1*0o8GFrfHDdzKfBkeLHNAog.jpeg', 'https://miro.medium.com/max/1200/1*4kqGFhlnFBfm-WStuOJSwQ.png', 'https://miro.medium.com/fit/c/96/96/1*7RPwaCYw15cS17oL6gGRYA.jpeg', 'https://miro.medium.com/max/1366/1*GPRVuz552eY8WEMLWp-UlQ.png', 'https://miro.medium.com/max/60/1*M84i0-ahEO4EjMJf9ZgFvg.png?q=20', 'https://miro.medium.com/max/1846/1*YUAXFJdnzh-5fADdSv4Tww.png', 'https://miro.medium.com/max/778/1*b9guhbBzbmIYFh4uhWJkgA.png', 'https://miro.medium.com/fit/c/160/160/1*7RPwaCYw15cS17oL6gGRYA.jpeg', 'https://miro.medium.com/fit/c/80/80/1*XjNrsNAvNzWZH2PNr7nKlw.jpeg', 'https://miro.medium.com/max/60/0*hbrDVLdg5wUS6W_u.png?q=20', 'https://miro.medium.com/max/60/1*ZbmJWJHkL9rsXZHc7gFAZg.png?q=20', 'https://miro.medium.com/max/998/0*hbrDVLdg5wUS6W_u.png', 'https://miro.medium.com/max/60/1*GPRVuz552eY8WEMLWp-UlQ.png?q=20', 'https://miro.medium.com/max/2144/1*M84i0-ahEO4EjMJf9ZgFvg.png', 'https://miro.medium.com/max/60/1*YUAXFJdnzh-5fADdSv4Tww.png?q=20', 'https://miro.medium.com/max/1368/1*-vdWkgw8gf54BUVE9TVS8g.png'}",2020-03-05 00:26:01.173744,1.956599235534668
https://medium.com/nybles/create-your-first-image-recognition-classifier-using-cnn-keras-and-tensorflow-backend-6eaab98d14dd,"Create your first Image Recognition Classifier using CNN, Keras and Tensorflow backend","With the dawn of a new era of A.I., machine learning, and robotics, its time for the machines to perform tasks characteristic of human intelligence. Machines use their own senses to do things like planning, pattern recognizing, understanding natural language, learning and solving problems. And Image Recognition is one of its senses!!!

From Automated self-driven cars to Boosting augmented reality applications and gaming, from Image and Face Recognition on Social Networks to Its application in various Medical fields, Image Recognition has emerged as a powerful tool and has become a vital for many upcoming inventions.

So, why not create our own Image Recognition Classifier, and that too with a few lines of code, thanks to the modern day machine learning libraries. Let’s get started !!

Getting Started — Dog or Cat

Well, not asking what you like more. Lets first create a simple image recognition tool that classifies whether the image is of a dog or a cat. The idea is to create a simple Dog/Cat Image classifier and then applying the concepts on a bigger scale.

Tools And Technologies

Anaconda — Anaconda is a free and open source distribution of the Python and R programming languages for data science and machine learning related applications, that aims to simplify package management and deployment. You can download it from the link below according to your system https://www.anaconda.com/download/ Spyder — Spyder is an open source cross-platform IDE for scientific programming in the Python language. It comes installed with anaconda. If not, install it using anaconda navigator. Tensorflow — TensorFlow is an open-source software library for dataflow programming across a range of tasks. Download link — https://www.tensorflow.org/install/install_windows Keras — Keras is an open source neural network library written in Python. Activate Tensorflow env and install keras using ‘pip install keras’. CNN — Convolution Neural network , a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery. Here is a very good explanation to what it actually is -https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/

Plan of Attack

Its time to get into action and here is the plan —

Collecting the Dataset

Importing Libraries and Splitting the Dataset

Building the CNN

Full Connection

Data Augmentation

Training our Network

Testing

Step 1 — Collecting the Dataset

In order to train our machine, we need a huuuuggge amount of data so that our model can learn from them by identifying out certain relations and common features related to the objects.

Fortunately many such datasets are available on internet. Here is a link for the cats and dogs dataset which consist of 10000 images — 5000 of each. This will help in training as well testing our classifier.

https://www.superdatascience.com/pages/machine-learning

Download the link Convolutional-Neural-Networks.zip under Section-40

Step 2 — Importing Libraries and Splitting the Dataset

To use the powers of the libraries, we first need to import them.

After importing the libraries, we need to split our data into two parts- taining_set and test_set.

In our case, the dataset is already split into two parts. The training set has 4000 image each of dogs and cats while the test set has 1000 images of each.

Step 3 — Buliding the CNN

This is most important step for our network. It consists of three parts -

Convolution Polling Flattening

The primary purpose of Convolution is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data.

Since every image can be considered as a matrix of pixel values. Consider a 5 x 5 image whose pixel values are only 0 and 1 (note that for a grayscale image, pixel values range from 0 to 255, the green matrix below is a special case where pixel values are only 0 and 1):

Also, consider another 3 x 3 matrix as shown below:

Then, the Convolution of the 5 x 5 image and the 3 x 3 matrix can be computed as shown in the animation in Figure 5 below:

The obtained matrix is also known as the feature map. An additional operation called ReLU is used after every Convolution operation. The next step is of pooling.

Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.

After pooling comes flattening. Here the matrix is converted into a linear array so that to input it into the nodes of our neural network.

Let’s come to the code.

So now our CNN network looks like this

Step 4 — Full Connection

Full connection is connecting our convolutional network to a neural network and then compiling our network.

Here we have made 2 layer neural network with a sigmoid function as an activation function for the last layer as we need to find the probability of the object being a cat or a dog.

So now the final network looks something like this -

Step 5 — Data Augmentation

While training your data, you need a lot of data to train upon. Suppose we have a limited number of images for our network. What to do now??

You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.

The same tennis ball, but translated.

So, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.

Data augmentation is a way we can reduce overfitting on models, where we increase the amount of training data using information only in our training data. The field of data augmentation is not new, and in fact, various data augmentation techniques have been applied to specific problems.

And here goes the code

Now we have a huge amount of data and its time for the training.

Step 6 — Training our Network

So, we completed all the steps of construction and its time to train our model.

If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer.

With increasing number of epochs, the accuracy will increase.

Step 7 — Testing

Now lets test a random image.

And, yes !! our network correctly predicted the image of the dog!! Though it is not 100% accurate but it will give correct predictions most of the times. Try adding more convolutional and pooling layers, play with the number of nodes and epochs, and you might get high accuracy result.

You can even try it with your own image and see what it predicts. Whether you look close to a dog or a cat.

Summary

So, we created a simple Image Recognition Classifier. The same concept can applied to a diverse range of objects with a lot of training data and appropriate network. You can change the dataset with the images of your friends and relatives and work upon the network to make a Face Recognition Classifier.

So, now you know how to build it,

“Be quiet, darling. Let pattern recognition have its way.”

― William Gibson, The Peripheral

However there are many APIs available which can be automatically embedded into our application. They have been trained on a large dataset and powerful machines. If you don’t want to go deep in machine learning, you can use them.

But for machine learning enthusiasts, this is a very good start for them to learn the basics and dive to an ocean of infinite possibilities!!","['machine', 'create', 'network', 'tensorflow', 'recognition', 'neural', 'image', 'need', 'keras', 'learning', 'data', 'backend', 'training', 'cnn', 'matrix', 'using', 'classifier']","So, why not create our own Image Recognition Classifier, and that too with a few lines of code, thanks to the modern day machine learning libraries.
Lets first create a simple image recognition tool that classifies whether the image is of a dog or a cat.
Download link — https://www.tensorflow.org/install/install_windows Keras — Keras is an open source neural network library written in Python.
CNN — Convolution Neural network , a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery.
Data augmentation is a way we can reduce overfitting on models, where we increase the amount of training data using information only in our training data.",en,['Yash Agarwal'],2019-05-24 11:58:38.157000+00:00,"{'Dog Vs Cat Classifier', 'Convolutional Network', 'Machine Learning', 'Keras', 'Image Recognition'}","{'https://miro.medium.com/max/3234/1*3mNRYX8Lz2SGP_cVq-IzHA.jpeg', 'https://miro.medium.com/max/750/1*LTPAzUjUentTWoXXmqoT9g.jpeg', 'https://miro.medium.com/fit/c/96/96/0*t-VsS5gC6zICZbTh.', 'https://miro.medium.com/max/60/1*ciqcx2s4COFk8so5Wb6h0A.png?q=20', 'https://miro.medium.com/max/1308/1*3qwjNjfD0STfGCDjiPL5hQ.png', 'https://miro.medium.com/max/60/1*Fm-xFCVAjnB4WPH5NV0eRQ.png?q=20', 'https://miro.medium.com/max/1500/1*LTPAzUjUentTWoXXmqoT9g.jpeg', 'https://miro.medium.com/max/1878/1*DoRz0KaV1jwCttVDxqJflw.png', 'https://miro.medium.com/max/60/1*8gABzAFwVXbYJ3uOW7EiMA.png?q=20', 'https://miro.medium.com/max/60/1*LTPAzUjUentTWoXXmqoT9g.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/1*9Tk9E8QmZoXGgHvff0Apow.png', 'https://miro.medium.com/max/60/1*3qwjNjfD0STfGCDjiPL5hQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*dmbNkD5D-u45r44go_cf0g.png', 'https://miro.medium.com/max/1858/1*aAz7Nrx4IkdEViyBknpH9Q.png', 'https://miro.medium.com/max/60/1*yMS_YeWNjbPZVEvLtGflbQ.png?q=20', 'https://miro.medium.com/max/1102/1*yer5IvRX3QIcLwfx1LhdWw.png', 'https://miro.medium.com/max/60/0*KdJv2eWBC1qUs3Po?q=20', 'https://miro.medium.com/max/1400/1*A5pq2mfrYptkbixW48JSKw.jpeg', 'https://miro.medium.com/max/1236/1*8gABzAFwVXbYJ3uOW7EiMA.png', 'https://miro.medium.com/fit/c/80/80/1*tYPCQwsf2EX3ElNzU6J1dw.jpeg', 'https://miro.medium.com/max/60/1*L07HTRw7zuHGT4oYEMlDig.jpeg?q=20', 'https://miro.medium.com/fit/c/160/160/0*t-VsS5gC6zICZbTh.', 'https://miro.medium.com/max/1254/1*ciqcx2s4COFk8so5Wb6h0A.png', 'https://miro.medium.com/max/60/1*PSLmC5odgP3-c3uQx6FS0g.png?q=20', 'https://miro.medium.com/max/60/1*3mNRYX8Lz2SGP_cVq-IzHA.jpeg?q=20', 'https://miro.medium.com/max/60/1*k10IptuhvAIIMGUukDIY3g.png?q=20', 'https://miro.medium.com/max/60/1*aAz7Nrx4IkdEViyBknpH9Q.png?q=20', 'https://miro.medium.com/max/60/1*DoRz0KaV1jwCttVDxqJflw.png?q=20', 'https://miro.medium.com/max/60/1*67NywA8WidTMXGCTxVhqXw.png?q=20', 'https://miro.medium.com/max/1344/1*k10IptuhvAIIMGUukDIY3g.png', 'https://miro.medium.com/max/1274/1*67NywA8WidTMXGCTxVhqXw.png', 'https://miro.medium.com/max/536/0*KdJv2eWBC1qUs3Po', 'https://miro.medium.com/max/254/0*JsCxaZb5AIm6YFlB', 'https://miro.medium.com/max/148/0*TG6RffpzcJBzYsEk', 'https://miro.medium.com/max/138/1*B47JYzys3hvfekauok6ksA@2x.png', 'https://miro.medium.com/max/60/0*TG6RffpzcJBzYsEk?q=20', 'https://miro.medium.com/max/1976/0*6ED-178t3tjE0Wo6', 'https://miro.medium.com/fit/c/80/80/1*o_qXLKr41WaBHwbmAKvAFg.png', 'https://miro.medium.com/max/60/0*JsCxaZb5AIm6YFlB?q=20', 'https://miro.medium.com/max/1362/1*PSLmC5odgP3-c3uQx6FS0g.png', 'https://miro.medium.com/max/526/1*Fm-xFCVAjnB4WPH5NV0eRQ.png', 'https://miro.medium.com/max/60/1*yer5IvRX3QIcLwfx1LhdWw.png?q=20', 'https://miro.medium.com/max/1636/1*L07HTRw7zuHGT4oYEMlDig.jpeg', 'https://miro.medium.com/max/60/0*6ED-178t3tjE0Wo6?q=20', 'https://miro.medium.com/max/60/1*A5pq2mfrYptkbixW48JSKw.jpeg?q=20', 'https://miro.medium.com/max/1134/1*yMS_YeWNjbPZVEvLtGflbQ.png'}",2020-03-05 00:26:02.273145,1.0994007587432861
https://medium.com/@kahana.hagai/docker-compose-with-node-js-and-mongodb-dbdadab5ce0a,Docker compose with Node.js and MongoDB,"Setup the Node.js application

At first, we need to obtain a simple application. In this post we will not build the application, but use the detailed guide provided by scotch.io as an example application. The guide creates an application called ‘easy-node-authentication’ and provides an example of a login form that utilizes several passport.js authentication capabilities.

We forked that code base and trimmed it down to only use a “local” passport login. This allows us to persist user information on MongoDB storage which can be a nice example of a 2-tier application. The express app and the MongoDB storage.

Use the following command to download the modified code:

First, define the location of the database to be a local mongo. Go to file ‘config/database.js’ and modify it to contain the following configuration:

// config/database.js

module.exports = {

'url' : 'mongodb://mongo:27017'

};

Build application docker image

To build a docker image create a Dockerfile in the root directory of the application code base:

FROM node:carbon # Create app directory

WORKDIR /usr/src/app # Install app dependencies

COPY package*.json ./

RUN npm install # Copy app source code

COPY . . #Expose port and start application

EXPOSE 8080

CMD [ ""npm"", ""start"" ]

Now, we can build our image:

$ docker build -t hagaik/auth-app .

At this point, we have a docker image with our application code. However, the application needs a running MongoDB to persist the user data and credentials.

Build Compose File:

That is where docker-compose comes in handy; When we want to be running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services.

Create a compose file called ‘docker-compose.yml’ :

version: ""2""

services:

web:

build: .

ports:

- ""8080:8080""

depends_on:

- mongo

mongo:

image: mongo

ports:

- ""27017:27017""

This defines two dockers:

web which represents our application docker mongo which represents the persistence layer docker

By default, “web” service can reach “mongo” service by using the service’s name. That is why we configured the database URI to mongodb://mongo:27017

To run the two dockers using the compose file execute the command:

$ docker-compose up

Open a browser on http://localhost:8080/ to see our application:

From there we can Signup and create an account:

Let’s validate that the user has been persistent in our MongoDB docker that is part of the composed work:



CONTAINER ID IMAGE COMMAND...

9a47337d30fe easynodeauthentication_web ""npm start""...

c0f31757662b mongo ""docker...""...

$ docker exec -it c0f31757662b /bin/bash

$ mongo

MongoDB shell version v3.4.10

connecting to: mongodb://127.0.0.1:27017

MongoDB server version: 3.4.10

Welcome to the MongoDB shell.

> show dbs

admin 0.000GB

local 0.000GB

passport 0.000GB

> use passport

switched to db passport

> show collections

users

> db.users.find({})

{ ""_id"" : ObjectId(""5a1f982615336e0010c38dd1""), ""local"" : { ""password"" : ""$2a$08$rDBEIW7FBnH2LPk6E9ptFuv.J71/HrBZkQfdPjpiWDK/FUMHTto4y"", ""email"" : "" $ docker psCONTAINER ID IMAGE COMMAND...9a47337d30fe easynodeauthentication_web ""npm start""...c0f31757662b mongo ""docker...""...$ docker exec -it c0f31757662b /bin/bash$ mongoMongoDB shell version v3.4.10connecting to: mongodb://127.0.0.1:27017MongoDB server version: 3.4.10Welcome to the MongoDB shell.> show dbsadmin 0.000GBlocal 0.000GBpassport 0.000GB> use passportswitched to db passport> show collectionsusers> db.users.find({}){ ""_id"" : ObjectId(""5a1f982615336e0010c38dd1""), ""local"" : { ""password"" : ""$2a$08$rDBEIW7FBnH2LPk6E9ptFuv.J71/HrBZkQfdPjpiWDK/FUMHTto4y"", ""email"" : "" johndoe@example.com "" }, ""__v"" : 0 }

We use docker ps command to list all of the running dockers. From there, we can see the mongo docker id and login to it with docker exec -i <docker-id/name> /bin/bash .

Once we are logged to the docker, we run the mongo shell, list the databases, switch to passport database and list all of the users to find the new record of johndoe@example.com.

Persist data in a dedicated volume

In our current setup, we will lose our user data when we delete the mongo container or rebuild it. This is because MongoDB storage is part of the mongo container.

If we want to ensure that the data surpass the lifetime of the mongo container, we should use a named volume to store our mongo files.

The named volume will remain after the container is deleted and can be attached to other docker containers:

version: ""2""

services:

web:

build: .

ports:

- ""8080:8080""

depends_on:

- mongo

mongo:

image: mongo

ports:

- ""27017:27017""

volumes:

- data-volume:/data/db

volumes:

data-volume:

The only addition is the volumes section that defines a named volume call data-volume and in mongo service we added the volumes. That field maps the created data-volume into ‘/data/db’ folder where the mongo storage file resides.

We can run to list the current running containers:

$ docker ps

To find the MongoDB container id and run the inspect command:

$ docker inspect -f '{{ .Mounts }}' <containerid>

[{volume easynodeauthentication_data-volume /var/lib/docker/volumes/easynodeauthentication_data-volume/_data /data/db local rw true}]

The first value easynodeauthentication_data-volume is the volume name. The seconds value is the path of the volume in the host file system (For Mac the Docker is running inside a VM. As such the path is relative to the VM, and not to your host Mac.). The third value is the mapping on the docker container file system, which match to our docker-compose YAML configuration.

Summary

That is it. We have a build a simple application that persists data in MongoDB. We were able to Dockerize that application and use docker-compose to lunch both the application and MongoDB in a single command. Lastly, we used volumes to ensure the persisted data remains after the MongoDB container is destroyed. We can close both dockers with:

$docker-compose stop

and continue with our day.","['file', 'version', 'mongodb', 'shell', 'nodejs', 'volume', 'docker', 'mongo', 'container', 'data', 'application', 'compose']","ports:- ""8080:8080""depends_on:- mongomongo:image: mongoports:- ""27017:27017""This defines two dockers:web which represents our application docker mongo which represents the persistence layer dockerBy default, “web” service can reach “mongo” service by using the service’s name.
From there, we can see the mongo docker id and login to it with docker exec -i <docker-id/name> /bin/bash .
The third value is the mapping on the docker container file system, which match to our docker-compose YAML configuration.
We were able to Dockerize that application and use docker-compose to lunch both the application and MongoDB in a single command.
Lastly, we used volumes to ensure the persisted data remains after the MongoDB container is destroyed.",en,['Hagai Kahana'],2017-12-03 17:04:26.679000+00:00,"{'Docker Compose', 'Docker', 'Mongodb'}","{'https://miro.medium.com/max/60/1*ShmKHPXwKd-qrFFlLSxSag.png?q=20', 'https://miro.medium.com/max/2536/1*3xtmqPLRgCRU-ZqOtH95tQ.png', 'https://miro.medium.com/max/8610/1*EduoeF2hdn4kYSHDXpDgPQ.jpeg', 'https://miro.medium.com/max/48/1*SpNSwog-tYeTXn2bEn2_RQ.png?q=20', 'https://miro.medium.com/max/60/1*oaqKLJhk2EkoRy5tycN41w.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*VjL0cwbQRb17XX8adxUMuw.jpeg', 'https://miro.medium.com/max/1200/1*EduoeF2hdn4kYSHDXpDgPQ.jpeg', 'https://miro.medium.com/max/2208/1*oaqKLJhk2EkoRy5tycN41w.png', 'https://miro.medium.com/max/368/1*SpNSwog-tYeTXn2bEn2_RQ.png', 'https://miro.medium.com/max/4712/1*ShmKHPXwKd-qrFFlLSxSag.png', 'https://miro.medium.com/fit/c/80/80/1*DebRBkExPVqvOvejj_DuLQ.jpeg', 'https://miro.medium.com/fit/c/160/160/0*00FXPndEWTV06HUu.jpg', 'https://miro.medium.com/fit/c/80/80/0*3Qkmfp5h7q2kZPls', 'https://miro.medium.com/fit/c/96/96/0*00FXPndEWTV06HUu.jpg', 'https://miro.medium.com/max/60/1*EduoeF2hdn4kYSHDXpDgPQ.jpeg?q=20', 'https://miro.medium.com/max/60/1*3xtmqPLRgCRU-ZqOtH95tQ.png?q=20'}",2020-03-05 00:26:04.151427,1.877317190170288
https://medium.com/@itunpredictable/apache-airflow-on-docker-for-complete-beginners-cf76cf7b2c9a,Apache Airflow on Docker for Complete Beginners,"Airflow — it’s not just a word Data Scientists use when they fart. It’s a powerful open source tool originally created by Airbnb to design, schedule, and monitor ETL jobs. But what exactly does that mean, and why is the community so excited about it?

Background: OLTP vs. OLAP, Analytics Needs, and Warehouses

Every company starts out with some group of tables and databases that are operation critical. These might be an orders table, a users table, and an items table if you’re an e-commerce company: your production application uses those tables as a backend for your day-to-day operations. This is what we call OLTP, or Online Transaction Processing. A new user signs up and a row gets added to the users table — mostly insert, update, or delete operations.

As companies mature (this point is getting earlier and earlier these days), they’ll want to start running analytics. How many users do we have? How have our order counts been growing over time? What are our most popular items? These are more complex questions and will tend to require aggregation (sum, average, maximum) as well as a few joins to other tables. We call this OLAP, or Online Analytical Processing.

The most important differences between OLTP and OLAP operations is what their priorities are:

OLTP’s priority is maintaining data integrity and processing a large number of transactions in a short time span

OLAP’s priority is query speed, and transactions tend to be batched and at regular intervals (ETL Jobs, which we’ll look at later)

A great diagram from MarkLogic explaining the differences between OLTP and OLAP

Accordingly, OLTP related queries tend to be simpler, require little aggregation and few joins, while OLAP queries tend be larger, more complex, and require more aggregations and joins.

In your e-commerce app, you might want to insert a new row into the orders table when a user makes a new order. These queries are pretty simple: they’ll probably boil down to something like INSERT INTO users VALUES x,y,z .

Answering those analytics-type OLAP questions is a different story though: they tend to involve much more data (aggregating order value over an entire table, for example) and require more joins to other database tables. If we wanted to see the total order value for users who live in New York, we’d need to use both an aggregation (sum of order value from the orders table) and a join (filter for New York addresses from the users table).

Early on in the company lifecycle, OLAP type queries and OLTP type queries will be run on your normal production database — because that’s all you have. But over time, OLAP usually starts to become too burdensome to run on your production tables:

OLAP queries are more computationally expensive (aggregations, joins)

OLAP often requires intermediate steps like data cleaning and featurization

Analytics usually runs at regular time intervals, while OLTP is usually event based (e.g. a user does something, so we hit the database)

For these reasons and more, Bill Inmon, Barry Devlin, and Paul Murphy developed the concept of a Data Warehouse in the 1980’s. A warehouse is basically just another database, but it’s not used for production (OLTP) — it’s just for analytics (decision support, in their language). It’s designed and populated with all of the above analytics concerns instead of the OLTP requirements for running your app.

ETL Jobs and Cron

Where things get interesting is how you actually get data into your warehouse. A typical modern warehouse setup will have many different types of data, like for example:

Daily snapshots of production tables (to lessen the load on production)

Product lifecycle events (consumed from some stream like Kafka)

Dimension and fact tables (from the Star Schema)

These all come from different places, and require unique jobs to get the data from the source (extract), do whatever you need to get it ready for analytics (transform), and deposit it in the warehouse (load). This process is called ETL (Extract, Transform, Load), and each individual job we need is called an ETL job. So how exactly do we build those?

The classic approach (other than expensive vendors) has usually been Cron Jobs. Cron is a utility that runs in your terminal and lets you run programs at specific intervals (say, 2AM every other day). The code required for the ETL job is packaged into a file and scheduled on Cron. The problem, though, is that Cron wasn’t built for this type of work, and it shows. Some of the major issues that data teams face scheduling ETL jobs with Cron:

ETL jobs fail all the time for a million reasons, and Cron makes it very difficult to debug for a number of reasons

ETL tends to have a lot of dependencies (on past jobs, for example) and Cron isn’t built to account for that

Data is getting larger, and modern distributed data stacks (HDFS, Hive, Presto) don’t always work well with Cron

Unsurprisingly, data teams have been trying to find more sophisticated ways to schedule and run ETL jobs. Airflow is one of the things we’ve come up with, and it’s pretty great.

Introduction to Airflow and Its Core Concepts

Now we can go back to our original definition of Airflow, and it should make more sense. Airflow lets you:

Design complex, sophisticated ETL jobs with multiple layers of dependencies

Schedule jobs to run at any time and wait for their dependencies to finish, with the option to run distributed workloads (using Celery)

Monitor all of your jobs, know exactly where and when they failed, and get detailed logs on what the errors were

It’s important to note that Airflow is not an ETL tool, even though that happens to be what it’s used most for. It’s a general purpose workflow scheduler, and that will become clearer as we delve into different Airflow concepts.

Operators

Airflow Operators are different types of things that you can do in workflows. If you want to run some Python, you’d use the Python Operator, and if you want to interact with MySQL you’d use the MySQL Operator. You can also define your own Operators by extending Airflow’s Base Operator class (or any of the others).

Tasks

Once you actually create an instance of an Operator, it’s called a Task in Airflow. Tasks are what make up workflows in Airflow, but here they’re called DAGs.

DAGs

DAG stands for a Directed Acyclic Graph — it may sound scary, but a DAG is basically just a workflow where tasks lead to other tasks. Tasks can be upstream or downstream of other tasks, which sets a sort of order for how they need to get executed.

(A sample DAG from my Sneaker Data Pipeline)

The “Directed” part means that order matters, like in the above example: we need to create a staging table before we can insert into it, and we need to create a target table before we can insert into it. DAGs are “Acyclic” — that means that they are not cyclical, and have a clear start and end point.

DAGs are the main way you interact with Airflow: you build them from a bunch of different tasks (we’ll get there), and Airflow executes them in order. It’s relatively simple.

XComs

Airflow doesn’t really want you to communicate between Tasks, but if you need to you can use Airflow’s XComs, an abbreviation for cross communication. An XCom is sort of like a centralized Key / Value repository: you can push to it from a Task, and and pull from it from a Task. Airflow automatically populates values like which DAG and which Task an XCom belongs to, which makes it easier to use than just storing data in a table.

Variables

Airflow Variables are like XComs, but they’re global and not designed for communicating between tasks. Variables make the most sense for settings and configuration, but you can even upload JSON files as variables.

Recap

Airflow Operators let you create Tasks, which you organize into DAGs. You can communicate between Tasks with XComs, and set global state with Variables. That’s most of what you need to know. Here’s a simple version of what a DAG might look like:

Practical Needs for Running Airflow

Assuming you already know what your DAGs will look like and what you want them to accomplish, there are a few things you’ll need to understand and get running first.

Connections

If you’re using Airflow for ETL (as you probably are), you’ll need to connect your Airflow deployment to whatever databases you intend to work with. Airflow Connections are, well, connections to your databases.

Creating Connections in Airflow is relatively simple, and just involve your typical username, password, host, and port setup. You can use the Airflow UI to add Connections, or create them (more) automatically through environment variables. If you export AIRFLOW_CONN_MY_DB and pass a database URI, that will create a connection called MyDB.

The Airflow Connections UI

Hooks

Airflow Hooks are — ready for this — connections to your Connections. They’re pretty much identical to the conn object you might be familiar with if you use Python with SQL, but Airflow makes it simpler by allowing you to just pass it a Connection ID. If you want a Task to insert data to a Postgres Database, for example, you’d create a PostgresHook from your Postgres Connection, and use the built in methods like insert_rows .

The Airflow Database

Airflow uses its own internal database to store credentials and your Airflow history (which DAGs you ran, when you ran them, etc.). It ships with SQLite, but you can use whatever you want. You usually get this initialized by running airflow initdb .

Webserver

One of Airflow’s awesome-est features is the Webserver, which is the front end GUI that it provides to end users. Running airflow webserver will get it started (it usually runs on your localhost’s port 8080), and it’s got a ton of cool stuff. Some really clutch features that you might find yourself using:

Seeing the structure of your DAG in a graph format

Checking on all of your DAG runs and seeing when they failed

Adding and editing Connections

Looking at how long your tasks typically take in one of your DAGs

The Webserver definitely gives Cron a run for its money, and is probably the most compelling feature Airflow has to offer over Cron for beginners.

Scheduler

The Airflow Scheduler is what takes care of actually running all of the DAGs that you’ve created: making sure things run in order, quickly, and how you want them to. The interesting part of the Scheduler though — which you’d normally run with airflow scheduler — is how Airflow actually executes your tasks. They’re called Executors, and they’re (surprise!) configurable.

The default Executor for Airflow is the Sequential Executor, which (gasp!) just executes your Tasks locally in order. But if you want to scale out Airflow to something more production ready, especially using multiple workers, Airflow has other options. The Celery Executor uses Python’s Celery package to queue tasks as messages, and the Dask Executor lets you run Airflow Tasks on a Dask Cluster. There’s also a Mesos Executor.

The Best Way to Actually Get Airflow Started

You can install Airflow pretty simply through pip , but there’s a lot of configuration to do upfront. The standard way companies get Airflow going is through Docker, and in particular this image from some user named puckel. If you’re not familiar with Docker, check out FreeCodeCamp’s conceptual introduction and DigitalOcean’s tutorial on how to get it going.

Puckel’s docker-airflow repo has everything you need to get Airflow up and running, and some basic templates for expanding it out to multiple containers with Docker Compose. The getting started guide isn’t exactly comprehensive, and it can be really frustrating for beginners to get Airflow working. Here are a few things you’ll need to do, and issues you might run into.

Read and understand the Dockerfile and Entrypoint.sh script

Docker Images (like this Airflow one) are built with a Dockerfile, which is sort of like a blueprint for what your Docker Image (and eventual containers) should look like. Dockerfiles will run shell commands, create environment variables, install packages, and all that good stuff you need to create the Container of Your Dreams™️.

A pain point for beginners using this Airflow Docker Image is that a lot of the interesting configuration doesn’t actually happen in the Dockerfile: it happens in this little script called entrypoint.sh . An entrypoint script is just a series of commands that you can have your container run when it starts up, and you put it in the Dockerfile as ENTRYPOINT [“entrypoint.sh”] . A bunch of very important variables and configurations are defined there (like Fernet Keys, which we’ll look at in a bit). It also tells the container to immediately run airflow initdb , airflow webserver , and airflow scheduler , so you don’t have to run those manually.

You’re going to run into issues setting up Airflow: there’s no way around it. But you’ll be able to debug much faster if you understand how the Docker Image is being created and what’s being run on your container(s).

2) Decide on how you want to deploy and test

The typical structure for building the Airflow Docker Image, which is also how this repo is designed, is two fold:

A bunch of local files exist that are your “source of truth”

When Docker Images are built, all of the local data and files are copied over to containers

What this means that you’d make edits to any local files like script/entrypoint.sh or config/airflow.cfg , and then build your image. The Dockerfile will automatically copy all of those local files over, so the new versions will be the only ones that appear in your containers. Since you’ll constantly be running the same Docker Build and Docker Run commands, it’s convenient to create a Makefile as a shortcut.

There are other ways to do this though. If you want to run Airflow on multiple containers, you can use the Docker Compose files included in the repo. For testing and configuration, that might be overkill, but it also can help you avoid some of the issues below.

3) Create a COPY command in the Dockerfile so your DAGs will appear

When you build your Docker Image using the Dockerfile from puckel/docker-airflow , it copies over the local stuff (your entrypoint.sh script, your airflow config files, etc.) into the eventual container.

If you build manually and don’t use the included docker-compose files, the Dockerfile in this repo is missing an important COPY statement: you need to copy your dags directory into the container too. It’s as simple as adding COPY dags ${AIRFLOW_HOME}/dags to your Dockerfile, where dags is your local directory and ${AIRFLOW_HOME}/dags is the target directory in your containers.

4) Address Fernet Key issues

When you add Connections to Airflow, it stores your credentials in that internal SQLite database we mentioned above. Airflow encrypts those credentials using Fernet Encryption, and you need to generate a key for it to work. You then point Airflow to that key in your airflow.cfg file, and all is well. Here’s the way that puckel/docker-airflow does it:

The Fernet Key is generated with a Python command and exported as an environment variable in entrypoint.sh

In the airflow.cfg file, the fernet_key parameter is set to $FERNET_KEY , which is the name of the environment variable exported above

There’s an open issue (created by yours truly) with this process, though: if you build your Docker Image through plain old Docker Build and Docker Run commands, it doesn’t work. The commands as they exist in entrypoint.sh aren’t correctly exporting the Fernet Key for whatever reason (I haven’t figured it out yet), and so your Connections will not work without the properly encrypted credentials.

The error message will be binascii.Error: Incorrect padding , but that’s actually the wrong error — the key doesn’t exist. Someday I’ll create a pull request to fix the message. Someday.

You can create a simple workaround in the meanwhile by exporting your Fernet Key manually when you create a container (this likely won’t work if you’re using Docker Compose). Just copy the command from entrypoint.sh that’s supposed to work — export FERNET_KEY = python -c “from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY) — and run it in your container. I run this through a Docker Exec command in my Makefile so I don’t need to do it manually each time I build.

(Note: I’m pretty new to Docker, so there’s a good chance I’ve made some errors above. Send some feedback on Twitter!)

Next Steps

These are the basics, from the conceptual to the practical parts of deployment. If you’re planning on getting started with Airflow and want more information, there a few other tutorials that will take you deeper in the Airflow lifecycle (past setup). Check out:","['create', 'etl', 'apache', 'jobs', 'beginners', 'docker', 'need', 'tasks', 'data', 'airflow', 'run', 'order', 'complete']","The Celery Executor uses Python’s Celery package to queue tasks as messages, and the Dask Executor lets you run Airflow Tasks on a Dask Cluster.
A pain point for beginners using this Airflow Docker Image is that a lot of the interesting configuration doesn’t actually happen in the Dockerfile: it happens in this little script called entrypoint.sh .
It also tells the container to immediately run airflow initdb , airflow webserver , and airflow scheduler , so you don’t have to run those manually.
Since you’ll constantly be running the same Docker Build and Docker Run commands, it’s convenient to create a Makefile as a shortcut.
If you want to run Airflow on multiple containers, you can use the Docker Compose files included in the repo.",en,['Justin Gage'],2019-02-10 16:00:01.529000+00:00,"{'Data Science', 'Data Engineering', 'Python', 'Apache Airflow', 'Docker'}","{'https://miro.medium.com/fit/c/160/160/2*E4kgArhzUvBCN8lDYaE4qg.png', 'https://miro.medium.com/max/60/0*ODcUeRaRVWluh62Y?q=20', 'https://miro.medium.com/max/60/0*WNuvTzU6c3s75s4r.png?q=20', 'https://miro.medium.com/max/950/0*3tHSMYRFDJDjLPjP.jpg', 'https://miro.medium.com/fit/c/96/96/2*E4kgArhzUvBCN8lDYaE4qg.png', 'https://miro.medium.com/max/50/0*3tHSMYRFDJDjLPjP.jpg?q=20', 'https://miro.medium.com/max/1120/0*VDHg9W08_Ot4XIlj.jpg', 'https://miro.medium.com/fit/c/80/80/1*6rWvker9exEx1aQP5m9mUw.png', 'https://miro.medium.com/max/60/1*WhpNH7QSvONqUF6FHC7Jyw.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/0*R8Xd1sLROPnEck-a.jpg', 'https://miro.medium.com/max/60/0*VDHg9W08_Ot4XIlj.jpg?q=20', 'https://miro.medium.com/max/3200/0*ODcUeRaRVWluh62Y', 'https://miro.medium.com/max/60/0*X_3mIpuv8g81mTk0.png?q=20', 'https://miro.medium.com/max/3840/0*WNuvTzU6c3s75s4r.png', 'https://miro.medium.com/max/14566/1*WhpNH7QSvONqUF6FHC7Jyw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*Px-hnY7RCTPtEZW-wPi2Yg.jpeg', 'https://miro.medium.com/max/1378/0*X_3mIpuv8g81mTk0.png', 'https://miro.medium.com/max/1200/0*ODcUeRaRVWluh62Y'}",2020-03-05 00:26:06.157832,2.0054051876068115
https://medium.com/python-pandemonium/develop-your-first-etl-job-in-python-using-bonobo-eaea63cc2d3c,Develop your first ETL job in Python using bonobo,"In this post I am going to discuss how you can write ETL jobs in Python by using Bonobo library. Before I get into the library itself, allow me to discuss about ETL itself and why is it needed?

What is ETL?

ETL is actually short form of Extract, Transform and Load, a process in which data is acquired, changed/processes and then finally get loaded into data warehouse/database(s).

You can extract data from data sources like Files, Website or some Database, transform the acquired data and then load the final version into database for business usage.

You may ask, Why ETL?, well, what ETL does, many of you might already been doing one way or other by writing different functions/scripts to perform tasks but one of the main advantage of ETLs is visualizing your entire data flow pipeline thus help you make decisions according to that.

OK enough talk, let’s get into writing our first ever ETL in Python.

Python Bonobo

The python library I am going to use is bonobo. It’s one of many available libraries out there. The reason to pick is that I found it relatively easy for new comers. It required Python 3.5+ and since I am already using Python 3.6 so it works well for me.

You can install it via pip by issuing the command:

pip install bonobo

Alright, the library is installed and let’s write a very very simple ETL job. I am using the example given on their Github Repo.

import bonobo def generate_data():

yield 'foo'

yield 'bar'

yield 'baz' def uppercase(x: str):

return x.upper() def output(x: str):

print(x) graph = bonobo.Graph(

generate_data,

uppercase,

output,

) if __name__ == '__main__':

bonobo.run(graph)

It is not doing something amazing, all it is doing is converting the input data into upper case and then printing it. The visualizing part is left, for that bonobo provides a way to use graphviz library for graphs. Once installed you will execute following two commands: the first one to generate dot file and the next one to generate graph image file from dot file.

bonobo inspect --graph etl.py > etl.dot

and then

dot -o etl.png -T png etl.dot

which will result graph image like below:

You can also see what happened with your input data by executing bonobo run . For the example above it shows like:

OK so this was a toy example let’s make an ETL of some real data. I am going to make ETL of a a couple of real estate website records. I am using Redfin and Zillow for this post. The goal is to capture price from each site and then transform them into a standardized format for later usage. In our case it’s to convert price string into float after cleaning and save it into a text file. Below is the code:

import bonobo

import requests

from bs4 import BeautifulSoup

price = ''

status = ''

url = '

r = requests.get(url, headers=headers)

if r.status_code == 200:

html = r.text.strip()

soup = BeautifulSoup(html, 'lxml')

price_status_section = soup.select('.home-summary-row')

if len(price_status_section) > 1:

price = price_status_section[1].text.strip()

return price def scrape_zillow():price = ''status = ''url = ' https://www.zillow.com/homedetails/41-Norton-Ave-Dallas-PA-18612/2119501298_zpid/' r = requests.get(url, headers=headers)if r.status_code == 200:html = r.text.strip()soup = BeautifulSoup(html, 'lxml')price_status_section = soup.select('.home-summary-row')if len(price_status_section) > 1:price = price_status_section[1].text.strip()return price

price = ''

status = ''

url = '

r = requests.get(url, headers=headers)

if r.status_code == 200:

html = r.text.strip()

soup = BeautifulSoup(html, 'lxml')

price_section = soup.find('span', {'itemprop': 'price'})

if price_section:

price = price_section.text.strip()

return price def scrape_redfin():price = ''status = ''url = ' https://www.redfin.com/TX/Dallas/2619-Colby-St-75204/unit-B/home/32251730' r = requests.get(url, headers=headers)if r.status_code == 200:html = r.text.strip()soup = BeautifulSoup(html, 'lxml')price_section = soup.find('span', {'itemprop': 'price'})if price_section:price = price_section.text.strip()return price def extract():

yield scrape_zillow()

yield scrape_redfin() def transform(price: str):

t_price = price.replace(',', '').lstrip('$')

return float(t_price) def load(price: float):

with open('pricing.txt', 'a+', encoding='utf8') as f:

f.write((str(price) + '

'))

headers = {

'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',

'referrer': '

}

# scrape_redfin()

graph = bonobo.Graph(

extract,

transform,

load,

)

bonobo.run(graph) if __name__ == '__main__':headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36','referrer': ' https://google.com' # scrape_redfin()graph = bonobo.Graph(extract,transform,load,bonobo.run(graph)

I am not touching the scraping part as I already covered it here. Beside that, as you can see that I did nothing fancy. I created two separated functions and then used yield for making them available for ETL, then, I cleaned the data and then load used for saving into the file after transformation into float .

Conclusion

In this post you learnt how you can use bonobo libraries to write ETL jobs in Python language. This was a very basic demo. Visit the official site and see goodies like these as well. Like always, code is available on Github.

This article originally published here.

Click here to subscribe my newsletter for future posts.","['url', 'etl', 'def', 'python', 'price', 'job', 'bonobo', 'status', 'data', 'library', 'develop', 'using']","In this post I am going to discuss how you can write ETL jobs in Python by using Bonobo library.
OK enough talk, let’s get into writing our first ever ETL in Python.
Python BonoboThe python library I am going to use is bonobo.
It required Python 3.5+ and since I am already using Python 3.6 so it works well for me.
ConclusionIn this post you learnt how you can use bonobo libraries to write ETL jobs in Python language.",en,['Adnan Siddiqi'],2018-09-03 18:35:28.269000+00:00,"{'Etl', 'Python', 'here', 'Data Science'}","{'https://miro.medium.com/fit/c/80/80/1*Qp7L50ru1UEhVWSnpptrRQ.jpeg', 'https://miro.medium.com/fit/c/80/80/2*5whHj7P5v5x70Zf08K8uuQ.png', 'https://miro.medium.com/fit/c/160/160/1*nUYOwBlsmc2xTLmGRzBTCQ.png', 'https://miro.medium.com/max/60/1*iF-Hmq4x2Y8VrvDJg3ktTQ.png?q=20', 'https://miro.medium.com/max/1052/1*RlDMjFno0C2nDuZBTc7CVA.png', 'https://miro.medium.com/max/1680/1*iF-Hmq4x2Y8VrvDJg3ktTQ.png', 'https://miro.medium.com/max/60/1*RlDMjFno0C2nDuZBTc7CVA.png?q=20', 'https://miro.medium.com/max/60/1*aUjlv_fdSIO7xJiiclyyzA.jpeg?q=20', 'https://miro.medium.com/max/1432/1*aUjlv_fdSIO7xJiiclyyzA.jpeg', 'https://miro.medium.com/fit/c/160/160/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/fit/c/96/96/0*3dPRYUGiYI3tqys4.png', 'https://miro.medium.com/max/716/1*aUjlv_fdSIO7xJiiclyyzA.jpeg'}",2020-03-05 00:26:07.128377,0.9695448875427246
https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638,Data Traffic Control with Apache Airflow,"by Nicolas GOLL PERRIER (Data Engineer)

Leboncoin is the 7th most accessed website in France; this web traffic activity generates a lot of different events that need to be collected, transformed, reconciled, analysed and understood in order to fully grasp our customers and market, and improve upon our classified ads services.

In order to handle the diversity, volume, and complexity of events, we build workflows to ensure that our data is processed in an orderly and coherent way.

A good analogy for this job is the flight traffic control at the core of any airport. Each transiting dataset is like a plane carrying valuable information requiring to go from point A to point B. Our responsibility is to ensure on time and accurate transit of those “planes”, but we can neither control the “weather”, nor the behavior of those precious “passengers”.

So we need to be vigilant and establish the best flight paths for the streams and batches of data we ingest and transform, re-shuffle the priorities, stall some in-flight datasets at a certain “altitude” (postponing non critical information availability), while re-allocating “fuel reserves” (CPU, Memory, Bandwidth…) depending on the actual and current business priorities of the company.

From Airstrip to Airport

In the beginning, we were able to somewhat cope with the growth of the site with traditional Business Intelligence tools and practices (good old Extract Transform Load tools, cron-based orchestration, limited dashboards…). But as Leboncoin kept growing, we had to adapt our data-operations accordingly.

Therefore, we had to completely rethink our data architecture several years ago to:

Scale our data pipelines to process workloads of up to several terabytes a day efficiently

Adapt to the platform’s technological and organisational shift to a micro-service architecture

Quickly detect failures and inconsistencies in the many data processes run each day/hour

Respond to internal or external faults without impacting the quality, conformity, and availability of actionable information to our business users

Flying through the data-hurricane

Nowadays, we have different kinds of data processing batches running every hour of the day: Spark transformations, Kafka topics S3 archiving processes, Data-Warehouse loads, dataset indexing, business extracts, regulatory extracts, Machine Learning training and predictions… Too many to manually ensure that all our data stores are properly up to date and missing no data-points.

Moreover, data engineering and analytics are downstream the information system. Every incident, failure, bug, inconsistency will at some point end-up in our pipelines: so some input dataset may have changed its format without notice, a third-party data-source might become unavailable during the night, a real-life event triggered an unexpected increase in a specific customer activity, which produces more events and takes longer to process…

An information system is a living organism. We might prevent some of those things from happening, and we have contingency plans, but we can’t predict everything, especially in a quickly growing and evolving organisation.

So when dealing with our large set of data transformations, we need to guarantee to our customers, both internal and external, that those transformation will run consistently and quickly enough to provide relevant, accurate, and up to date insights. And if something were to fail, quickly assess the impact of the failure on all depending systems, reports, and dashboards.

Several tools such as Amazon S3, Apache Spark or Redshift have helped us tackle most the scaling issues, but as data processing workflows kept accumulating and were getting more and more complex, we needed a proper way to ensure that those ran like clockwork. And we were fortunate enough to try on Apache Airflow early on, just after it was open-sourced.

Airflow: Pre-flight checklist

Airflow has been developed with data-engineering challenges in mind. It is above all a DAG (Directed Acyclic Graph, a fancy way of saying “a workflow that does not loop”) management platform, or a worfklow orchestrator if you prefer.

Airflow

Airflow enables you to define, schedule, run, monitor and manage your workflows in pure Python code, while also providing the tools and UI to handle those workflow operations

It provides:

Retry mechanisms to ensure that each and every anomaly can be detected, and automatically or manually healed over time (with as little human intervention as possible)

to ensure that each and every anomaly can be detected, and automatically or manually healed over time (with as little human intervention as possible) Priority aware work queue management , ensuring that the most important tasks are run first and complete as soon as possible

, ensuring that the most important tasks are run first and complete as soon as possible Resource pooling system to ensure that, in a high concurrency environment, thresholds can be set to avoid overloading input or output systems

system to ensure that, in a high concurrency environment, thresholds can be set to avoid overloading input or output systems Backfill capabilities to identify “missing” past runs, and automatically re-create and run them

capabilities to identify “missing” past runs, and automatically re-create and run them Full history of metrics and statistics to view the evolution of each task performance over time, and even assess data-delivery SLAs over time

to view the evolution of each task performance over time, and even assess data-delivery SLAs over time An horizontally scalable set of alternatives to the way tasks are dispatched and run on a distributed infrastructure

set of alternatives to the way tasks are dispatched and run on a distributed infrastructure A centralized, secure place to store and view logs and configuration parameters for all task runs

Fig 1 — Example representation of a DAG in Airflow

All these features allow us to run more than 10,000 automated tasks each day without breaking a sweat. Even in the case of a major failure, recovery requires very little human labor, as the system eventually heals itself automatically.

Airflow’s architecture relies on several components, and a few auxiliary tools:","['ensure', 'information', 'apache', 'way', 'system', 'set', 'tools', 'control', 'tasks', 'business', 'data', 'airflow', 'run', 'traffic']","In order to handle the diversity, volume, and complexity of events, we build workflows to ensure that our data is processed in an orderly and coherent way.
A good analogy for this job is the flight traffic control at the core of any airport.
Each transiting dataset is like a plane carrying valuable information requiring to go from point A to point B.
Moreover, data engineering and analytics are downstream the information system.
And we were fortunate enough to try on Apache Airflow early on, just after it was open-sourced.",en,[],2019-01-09 15:50:03.932000+00:00,"{'Docker', 'Apache Airflow', 'Workflow', 'Data Science'}","{'https://miro.medium.com/max/3830/1*2ib--9ZoykuNWP1-WsHgGw.png', 'https://miro.medium.com/max/60/1*l6_dBPFyE6_zHZlXKi_vmA.png?q=20', 'https://miro.medium.com/max/60/1*2ib--9ZoykuNWP1-WsHgGw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*0pzfzuelW-sIjb4NvmDNkw.png', 'https://miro.medium.com/fit/c/80/80/1*8qPdGWQ1EXm8OdRHn40Hzg.png', 'https://miro.medium.com/max/60/1*hc7poeqQZnp1Am8Ymjvo7A.png?q=20', 'https://miro.medium.com/max/1600/1*nRsm4kiN3iyvERBj1cskNg.jpeg', 'https://miro.medium.com/fit/c/80/80/0*a3Vk6VD_G3QKa6CK.jpg', 'https://miro.medium.com/max/60/1*rqCsV4VoT2lwrWCjog4M_A.png?q=20', 'https://miro.medium.com/max/800/1*nRsm4kiN3iyvERBj1cskNg.jpeg', 'https://miro.medium.com/max/60/1*nRsm4kiN3iyvERBj1cskNg.jpeg?q=20', 'https://miro.medium.com/max/3830/1*gIkU0UOaVM915SGM8svYKA.png', 'https://miro.medium.com/fit/c/80/80/2*QLSGq3mwL45M-6it0EkHRQ.png', 'https://miro.medium.com/max/3830/1*34ecMevVoXc05zqm2GRbZA.png', 'https://miro.medium.com/max/3830/1*hc7poeqQZnp1Am8Ymjvo7A.png', 'https://miro.medium.com/max/60/1*qaDjtlyLS7l9Ah5-n7KBjg.png?q=20', 'https://miro.medium.com/max/60/1*QGSF1R5N6S3Z-VAqczz74w.png?q=20', 'https://miro.medium.com/fit/c/160/160/2*QLSGq3mwL45M-6it0EkHRQ.png', 'https://miro.medium.com/max/3830/1*l6_dBPFyE6_zHZlXKi_vmA.png', 'https://miro.medium.com/max/512/1*rqCsV4VoT2lwrWCjog4M_A.png', 'https://miro.medium.com/max/2006/1*bMV6HxMdei1jH-qlAxCKZQ.png', 'https://miro.medium.com/max/3830/1*qaDjtlyLS7l9Ah5-n7KBjg.png', 'https://miro.medium.com/max/1280/1*VKE1UvkZA6_LJyCNliFbzg.jpeg', 'https://miro.medium.com/max/60/1*gIkU0UOaVM915SGM8svYKA.png?q=20', 'https://miro.medium.com/max/3830/1*QGSF1R5N6S3Z-VAqczz74w.png', 'https://miro.medium.com/max/60/1*bMV6HxMdei1jH-qlAxCKZQ.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*QLSGq3mwL45M-6it0EkHRQ.png', 'https://miro.medium.com/max/60/1*34ecMevVoXc05zqm2GRbZA.png?q=20', 'https://miro.medium.com/max/60/1*VKE1UvkZA6_LJyCNliFbzg.jpeg?q=20'}",2020-03-05 00:26:08.328477,1.2000999450683594
https://medium.com/wbaa/airflow-ha-environment-c60ddca825a9,Apache Airflow HA environment,"Routing the workload

Workers

We use two types of workers: regular workers, and ETL workers. The first can access the Hadoop environments; the second has access to the internal data sources. Our environment is in a restricted security zone, so not every server has access to the internal data sources. That’s why we’ve had to introduce the ETL worker as a middleman. This system has a Hadoop client for accessing HDFS, and Sqoop for ingesting the data from the internal source systems.

Queues

We have defined five queues in our setup. The default queue can be used for tasks that aren’t bound to one cluster, where it doesn’t matter on which cluster the queue is executed (read and report operations). All but the ETL workers will listen to the default queue. The next two queues are the a_queue and b_queue, mapped to the different workers in the data-centers in sites A and B, respectively.

This allows us to monitor in which data-center the Airflow task will create and modify data on HDFS/Hive so that we can replicate/sync the data to the other Hadoop cluster. The data-replication will be part of all DAGs that are creating and modifying data. For this we will probably create an Airflow operator.

1. Task

2. Data transformation (new table in Hive)

3. Replicate data

Airflow allows you to start the Airflow worker process with specific queues to which it should listen. The worker will then only pickup tasks from that specified queue or list of queues. No additional airflow configuration is needed.

To start the worker listening to a specific queue requires only the queue and site to be specified:

Command : airflow worker — queues=default, sitea

This will work perfectly in a greenfield situation. We can schedule a task for a specific queue and route it to one of the workers associated to the queue. But what if we configure all of our DAGs to use a specific site, say A? If this worker were to go down, or be purposely taken offline, we would then have to change the target queue in the DAG to the site B queue in order to get the task executed again, which is less than ideal, but something we still need to investigate in our lab environment.

Possible solutions could be:

Create a check if the work for that queue is still online, if not switch to a secondary queue/worker node. Such a change can be made in the Airflow code or by building an appropriate check directly in the DAG (as a “check worker” operator).

Use the default queue, but add a return value to indicate which worker picked up the task (necessary for the data-sync)

Here’s an example of a simple Bash operator DAG that uses the site A queue:

Message service

We use Redis as a message queue service in this test setup. Airflow supports multiple messaging services like RabbitMQ, AWS, SQS, etc. Although Redis is fast and easy to setup, the lack of security will likely mean our message service will need to change to RabbitMQ in our production environment, with SSL and Kerberos enabled. We already have a running RabbitMQ cluster running (without SSL and Kerberos) in our lab environment, so the next steps would be to enable security and change the Airflow configuration to use RabbitMQ instead of Redis.

For our current setup we will continue with Redis as our message service.","['task', 'ha', 'apache', 'environment', 'rabbitmq', 'setup', 'data', 'airflow', 'queue', 'site', 'specific', 'worker', 'workers']","Routing the workloadWorkersWe use two types of workers: regular workers, and ETL workers.
Replicate dataAirflow allows you to start the Airflow worker process with specific queues to which it should listen.
To start the worker listening to a specific queue requires only the queue and site to be specified:Command : airflow worker — queues=default, siteaThis will work perfectly in a greenfield situation.
We can schedule a task for a specific queue and route it to one of the workers associated to the queue.
But what if we configure all of our DAGs to use a specific site, say A?",en,['Wb Advanced Analytics'],2017-06-01 10:27:22.165000+00:00,"{'Airflow', 'Python', 'High Availability'}","{'https://miro.medium.com/max/284/1*KYx8AKboqQvL2QINcWnWSw.png', 'https://miro.medium.com/max/1073/1*lbh6fE9bKd6EileLat3iww.jpeg', 'https://miro.medium.com/fit/c/80/80/1*zVlP8CJMlBOnYW-h5DYs5w.png', 'https://miro.medium.com/fit/c/96/96/1*ZXqPJou9Na6Ef1vf9TynPg.png', 'https://miro.medium.com/max/60/1*LLvHP2lKfg-xPugWQbQHnA.jpeg?q=20', 'https://miro.medium.com/max/286/1*KYx8AKboqQvL2QINcWnWSw.png', 'https://miro.medium.com/max/60/1*lbh6fE9bKd6EileLat3iww.jpeg?q=20', 'https://miro.medium.com/max/1970/1*LLvHP2lKfg-xPugWQbQHnA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*ZXqPJou9Na6Ef1vf9TynPg.png', 'https://miro.medium.com/fit/c/80/80/1*bG7OmNZLnPxxscHVkKnMtg@2x.jpeg', 'https://miro.medium.com/max/2146/1*lbh6fE9bKd6EileLat3iww.jpeg', 'https://miro.medium.com/fit/c/160/160/1*RJtXkxNFpuSwT-dquGNLFQ.png'}",2020-03-05 00:26:09.598896,1.269465684890747
https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7,A Beginner’s Guide to Data Engineering — Part I,"Motivation

The more experienced I become as a data scientist, the more convinced I am that data engineering is one of the most critical and foundational skills in any data scientist’s toolkit. I find this to be true for both evaluating project or job opportunities and scaling one’s work on the job.

In an earlier post, I pointed out that a data scientist’s capability to convert data into value is largely correlated with the stage of her company’s data infrastructure as well as how mature its data warehouse is. This means that a data scientist should know enough about data engineering to carefully evaluate how her skills are aligned with the stage and need of the company. Furthermore, many of the great data scientists I know are not only strong in data science but are also strategic in leveraging data engineering as an adjacent discipline to take on larger and more ambitious projects that are otherwise not reachable.

Despite its importance, education in data engineering has been limited. Given its nascency, in many ways the only feasible path to get training in data engineering is to learn on the job, and it can sometimes be too late. I am very fortunate to have worked with data engineers who patiently taught me this subject, but not everyone has the same opportunity. As a result, I have written up this beginner’s guide to summarize what I learned to help bridge the gap.

Organization of This Beginner’s Guide

The scope of my discussion will not be exhaustive in any way, and is designed heavily around Airflow, batch data processing, and SQL-like languages. That said, this focus should not prevent the reader from getting a basic understanding of data engineering and hopefully it will pique your interest to learn more about this fast-growing, emerging field.

Part I (this post) is designed to be a high-level introductory post. Using a combination of personal anecdotes and expert insights, I will contextualize what data engineering is, why it is challenging, and how it can help you or your organization to scale. The primary audience of this post is for aspiring data scientists who need to learn the basics to evaluate job opportunities or early-stage founders who are about to build the company’s first data team.

is designed to be a high-level introductory post. Using a combination of personal anecdotes and expert insights, I will contextualize what data engineering is, why it is challenging, and how it can help you or your organization to scale. The primary audience of this post is for aspiring data scientists who need to learn the basics to evaluate job opportunities or early-stage founders who are about to build the company’s first data team. Part II is more technical in nature. This post will focus on Airflow, Airbnb’s open-sourced tool for programmatically author, schedule, and monitor workflows. Specifically, I will demonstrate how to write a Hive batch job in Airflow, how to design table schemas using techniques such as star schema, and finally highlight some best practices around building ETLs. This post is suitable for starting data scientists and starting data engineers who are trying to hone their data engineering skills.

is more technical in nature. This post will focus on Airflow, Airbnb’s open-sourced tool for programmatically author, schedule, and monitor workflows. Specifically, I will demonstrate how to write a Hive batch job in Airflow, how to design table schemas using techniques such as star schema, and finally highlight some best practices around building ETLs. This post is suitable for starting data scientists and starting data engineers who are trying to hone their data engineering skills. Part III will be the final post of this series, where I will describe advanced data engineering patterns, higher level abstractions, and extended frameworks that would make building ETLs a lot easier and more efficient. A lot of these patterns are taught to me by Airbnb’s experienced data engineers who learned the hard way. I find these insights particularly useful for seasoned data scientists and seasoned data engineers who are trying to further optimize their workflows.

Given that I am now a huge proponent for learning data engineering as an adjacent discipline, you might find it surprising that I had the completely opposite opinion a few years ago — I struggled a lot with data engineering during my first job, both motivationally and emotionally.

My First Industry Job out of Graduate School

Right after graduate school, I was hired as the first data scientist at a small startup affiliated with the Washington Post. With endless aspirations, I was convinced that I will be given analysis-ready data to tackle the most pressing business problems using the most sophisticated techniques.

Shortly after I started my job, I learned that my primary responsibility was not quite as glamorous as I imagined. Instead, my job was much more foundational — to maintain critical pipelines to track how many users visited our site, how much time each reader spent reading contents, and how often people liked or retweeted articles. It was certainly important work, as we delivered readership insights to our affiliated publishers in exchange for high-quality contents for free.

Secretly though, I always hope by completing my work at hand, I will be able to move on to building fancy data products next, like the ones described here. After all, that is what a data scientist is supposed to do, as I told myself. Months later, the opportunity never came, and I left the company in despair. Unfortunately, my personal anecdote might not sound all that unfamiliar to early stage startups (demand) or new data scientists (supply) who are both inexperienced in this new labor market.

Image credit: Me building ETL pipelines diligently (guy in blue in the middle)

Reflecting on this experience, I realized that my frustration was rooted in my very little understanding of how real life data projects actually work. I was thrown into the wild west of raw data, far away from the comfortable land of pre-processed, tidy .csv files, and I felt unprepared and uncomfortable working in an environment where this is the norm.

Many data scientists experienced a similar journey early on in their careers, and the best ones understood quickly this reality and the challenges associated with it. I myself also adapted to this new reality, albeit slowly and gradually. Over time, I discovered the concept of instrumentation, hustled with machine-generated logs, parsed many URLs and timestamps, and most importantly, learned SQL (Yes, in case you were wondering, my only exposure to SQL prior to my first job was Jennifer Widom’s awesome MOOC here).

Nowadays, I understand counting carefully and intelligently is what analytics is largely about, and this type of foundational work is especially important when we live in a world filled with constant buzzwords and hypes.

The Hierarchy of Analytics

Among the many advocates who pointed out the discrepancy between the grinding aspect of data science and the rosier depictions that media sometimes portrayed, I especially enjoyed Monica Rogati’s call out, in which she warned against companies who are eager to adopt AI:

Think of Artificial Intelligence as the top of a pyramid of needs. Yes, self-actualization (AI) is great, but you first need food, water, and shelter (data literacy, collection, and infrastructure).

This framework puts things into perspective. Before a company can optimize the business more efficiently or build data products more intelligently, layers of foundational work need to be built first. This process is analogous to the journey that a man must take care of survival necessities like food or water before he can eventually self-actualize. This rule implies that companies should hire data talents according to the order of needs. One of the recipes for disaster is for startups to hire its first data contributor as someone who only specialized in modeling but have little or no experience in building the foundational layers that is the pre-requisite of everything else (I called this “The Hiring Out-of-Order Problem”).

Source: Monica Rogati’s fantastic Medium post “The AI Hierarchy of Needs”

Unfortunately, many companies do not realize that most of our existing data science training programs, academic or professional, tend to focus on the top of the pyramid knowledge. Even for modern courses that encourage students to scrape, prepare, or access raw data through public APIs, most of them do not teach students how to properly design table schemas or build data pipelines. As a result, some of the critical elements of real-life data science projects were lost in translation.

Luckily, just like how software engineering as a profession distinguishes front-end engineering, back-end engineering, and site reliability engineering, I predict that our field will be the same as it becomes more mature. The composition of talent will become more specialized over time, and those who have the skill and experience to build the foundations for data-intensive applications will be on the rise.

What does this future landscape mean for data scientists? I would not go as far as arguing that every data scientist needs to become an expert in data engineering. However, I do think that every data scientist should know enough of the basics to evaluate project and job opportunities in order to maximize talent-problem fit. If you find that many of the problems that you are interested in solving require more data engineering skills, then it is never too late then to invest more in learning data engineering. This is in fact the approach that I have taken at Airbnb.

Building Data Foundations & Warehouses

Regardless of your purpose or interest level in learning data engineering, it is important to know exactly what data engineering is about. Maxime Beauchemin, the original author of Airflow, characterized data engineering in his fantastic post The Rise of Data Engineer:

Data engineering field could be thought of as a superset of business intelligence and data warehousing that brings more elements from software engineering. This discipline also integrates specialization around the operation of so called “big data” distributed systems, along with concepts around the extended Hadoop ecosystem, stream processing, and in computation at scale.

Among the many valuable things that data engineers do, one of their highly sought-after skills is the ability to design, build, and maintain data warehouses. Just like a retail warehouse is where consumable goods are packaged and sold, a data warehouse is a place where raw data is transformed and stored in query-able forms.

Source: Jeff Hammerbacher’s slide from UC Berkeley CS 194 course

In many ways, data warehouses are both the engine and the fuels that enable higher level analytics, be it business intelligence, online experimentation, or machine learning. Below are a few specific examples that highlight the role of data warehousing for different companies in various stages:

Building Analytics at 500px : In this post, Samson Hu explains the challenges 500px faced when it tried to grow beyond product-market fit. He describes in details the process how he built the data warehouse from the ground and up.

: In this post, Samson Hu explains the challenges 500px faced when it tried to grow beyond product-market fit. He describes in details the process how he built the data warehouse from the ground and up. Scaling Airbnb’s Experimentation Platform : Jonathon Parks demonstrates how Airbnb’s data engineering team built specialized data pipelines to power internal tools like the experimentation reporting framework. This work is critical in shaping and scaling Airbnb’s product development culture.

: Jonathon Parks demonstrates how Airbnb’s data engineering team built specialized data pipelines to power internal tools like the experimentation reporting framework. This work is critical in shaping and scaling Airbnb’s product development culture. Using ML to Predict the Value of Homes on Airbnb: Written by myself, I explain why building batch training, offline scoring machine learning models requires a lot of upfront data engineering work. Notably, many tasks associated with feature engineering, building, and backfilling training data resemble data engineering works.

Without these foundational warehouses, every activity related to data science becomes either too expensive or not scalable. For example, without a properly designed business intelligence warehouse, data scientists might report different results for the same basic question asked at best; At worst, they could inadvertently query straight from the production database, causing delays or outages. Similarly, without an experimentation reporting pipeline, conducting experiment deep dives can be extremely manual and repetitive. Finally, without data infrastructure to support label collection or feature computation, building training data can be extremely time consuming.

ETL: Extract, Transform, and Load

All of the examples we referenced above follow a common pattern known as ETL, which stands for Extract, Transform, and Load. These three conceptual steps are how most data pipelines are designed and structured. They serve as a blueprint for how raw data is transformed to analysis-ready data. To understand this flow more concretely, I found the following picture from Robinhood’s engineering blog very useful:

Source: Vineet Goel’s “Why Robinhood uses Airflow?” Medium Post

Extract : this is the step where sensors wait for upstream data sources to land (e.g. a upstream source could be machine or user-generated logs, relational database copy, external dataset … etc). Upon available, we transport the data from their source locations to further transformations.

: this is the step where sensors wait for upstream data sources to land (e.g. a upstream source could be machine or user-generated logs, relational database copy, external dataset … etc). Upon available, we transport the data from their source locations to further transformations. Transform : This is the heart of any ETL job, where we apply business logic and perform actions such as filtering, grouping, and aggregation to translate raw data into analysis-ready datasets. This step requires a great deal of business understanding and domain knowledge.

: This is the heart of any ETL job, where we apply business logic and perform actions such as filtering, grouping, and aggregation to translate raw data into analysis-ready datasets. This step requires a great deal of business understanding and domain knowledge. Load: Finally, we load the processed data and transport them to a final destination. Often, this dataset can be either consumed directly by end-users or it can be treated as yet another upstream dependency to another ETL job, forming the so called data lineage.

While all ETL jobs follow this common pattern, the actual jobs themselves can be very different in usage, utility, and complexity. Here is a very simple toy example of an Airflow job:

Source: Arthur Wiedmer’s workshop from DataEngConf SF 2017

The example above simply prints the date in bash every day after waiting for a second to pass after the execution date is reached, but real-life ETL jobs can be much more complex. For example, we could have an ETL job that extracts a series of CRUD operations from a production database and derive business events such as a user deactivation. Another ETL can take in some experiment configuration file, compute the relevant metrics for that experiment, and finally output p-values and confidence intervals in a UI to inform us whether the product change is preventing from user churn. Yet another example is a batch ETL job that computes features for a machine learning model on a daily basis to predict whether a user will churn in the next few days. The possibilities are endless!

Choosing ETL Frameworks

When it comes to building ETLs, different companies might adopt different best practices. Over the years, many companies made great strides in identifying common problems in building ETLs and built frameworks to address these problems more elegantly.

In the world of batch data processing, there are a few obvious open-sourced contenders at play. To name a few: Linkedin open sourced Azkaban to make managing Hadoop job dependencies easier. Spotify open sourced Python-based framework Luigi in 2014, Pinterest similarly open sourced Pinball and Airbnb open sourced Airflow (also Python-based) in 2015.

Different frameworks have different strengths and weaknesses, and many experts have made comparisons between them extensively (see here and here). Regardless of the framework that you choose to adopt, a few features are important to consider:

Source: Marton Trencseni’s comparison between Luigi, Airflow, and Pinball

Configuration : ETLs are naturally complex, and we need to be able to succinctly describe the data flow of a data pipeline. As a result, it is important to evaluate how ETLs are authored. Is it configured on a UI, a domain specific language, or code? Nowadays, the concept of configuration as code is gaining popularity, because it allows users to expressively build pipelines programmatically that are customizable.

: ETLs are naturally complex, and we need to be able to succinctly describe the data flow of a data pipeline. As a result, it is important to evaluate how ETLs are authored. Is it configured on a UI, a domain specific language, or code? Nowadays, the concept of configuration as code is gaining popularity, because it allows users to expressively build pipelines programmatically that are customizable. UI, Monitoring, Alerts: Long running batch processes inevitably can run into errors (e.g. cluster failures) even when the job itself does not have bugs. As a result, monitoring and alerting are crucial in tracking the progress of long running processes. How well does a framework provide visual information for job progress? Does it surface alerts or warnings in a timely and accurate manner?

Long running batch processes inevitably can run into errors (e.g. cluster failures) even when the job itself does not have bugs. As a result, monitoring and alerting are crucial in tracking the progress of long running processes. How well does a framework provide visual information for job progress? Does it surface alerts or warnings in a timely and accurate manner? Backfilling: Once a data pipeline built, we often need to go back in time and re-process the historical data. Ideally, we do not want to build two separate jobs, one for backfilling historical data and another for computing current or future metrics. How easy does a framework support backfilling? Can it do so in a way that is standardized, efficient, and scalable? All these are important questions to consider.

Naturally, as someone who works at Airbnb, I really enjoy using Airflow and I really appreciate how it elegantly addresses a lot of the common problems that I encountered during data engineering work. Given that there are already 120+ companies officially using Airflow as their de-facto ETL orchestration engine, I might even go as far as arguing that Airflow could be the standard for batch processing for the new generation start-ups to come.

Two Paradigms: SQL- v.s. JVM-Centric ETL

As we can see from the above, different companies might pick drastically different tools and frameworks for building ETLs, and it can be a very confusing to decide which tools to invest in as a new data scientist.

This was certainly the case for me: At Washington Post Labs, ETLs were mostly scheduled primitively in Cron and jobs are organized as Vertica scripts. At Twitter, ETL jobs were built in Pig whereas nowadays they are all written in Scalding, scheduled by Twitter’s own orchestration engine. At Airbnb, data pipelines are mostly written in Hive using Airflow.

During my first few years working as a data scientist, I pretty much followed what my organizations picked and take them as given. It was not until much later when I came across Josh Will’s talk did I realize there are typically two ETL paradigms, and I actually think data scientists should think very hard about which paradigm they prefer before joining a company.

Video Source: Josh Wills’ Keynote @ DataEngConf SF 2016

JVM-centric ETL is typically built in a JVM-based language (like Java or Scala). Engineering data pipelines in these JVM languages often involves thinking data transformation in a more imperative manner, e.g. in terms of key-value pairs. Writing User Defined Functions (UDFs) are less painful because one does not need to write them in a different language, and testing jobs can be easier for the same reason. This paradigm is quite popular among engineers.

is typically built in a JVM-based language (like Java or Scala). Engineering data pipelines in these JVM languages often involves thinking data transformation in a more imperative manner, e.g. in terms of key-value pairs. Writing User Defined Functions (UDFs) are less painful because one does not need to write them in a different language, and testing jobs can be easier for the same reason. This paradigm is quite popular among engineers. SQL-centric ETL is typically built in languages like SQL, Presto, or Hive. ETL jobs are often defined in a declarative way, and almost everything centers around SQL and tables. Writing UDFs sometimes is troublesome because one has to write it in a different language (e.g. Java or Python), and testing can be a lot more challenging due to this. This paradigm is popular among data scientists.

As a data scientist who has built ETL pipelines under both paradigms, I naturally prefer SQL-centric ETLs. In fact, I would even argue that as a new data scientist, you can learn much more quickly about data engineering when operating in the SQL paradigm. Why? Because learning SQL is much easier than learning Java or Scala (unless you are already familiar with them), and you can focus your energy on learning DE best practices than learning new concepts in a new domain on top of a new language.

Wrapping Up Beginner’s Guide — Part I

In this post, we learned that analytics are built upon layers, and foundational work such as building data warehousing is an essential prerequisite for scaling a growing organization. We briefly discussed different frameworks and paradigms for building ETLs, but there are so much more to learn and discuss.

In the second post of this series, I will dive into the specifics and demonstrate how to build a Hive batch job in Airflow. Specifically, we will learn the basic anatomy of an Airflow job, see extract, transform, and load in actions via constructs such as partition sensors and operators. We will learn how to use data modeling techniques such as star schema to design tables. Finally, I will highlight some ETL best practices that are extremely useful.

If you found this post useful, stay tuned for Part II and Part III.","['scientists', 'etl', 'beginners', 'etls', 'post', 'engineering', 'job', 'data', 'airflow', 'built', 'building', 'guide']","This post is suitable for starting data scientists and starting data engineers who are trying to hone their data engineering skills.
This post is suitable for starting data scientists and starting data engineers who are trying to hone their data engineering skills.
If you find that many of the problems that you are interested in solving require more data engineering skills, then it is never too late then to invest more in learning data engineering.
Building Data Foundations & WarehousesRegardless of your purpose or interest level in learning data engineering, it is important to know exactly what data engineering is about.
Notably, many tasks associated with feature engineering, building, and backfilling training data resemble data engineering works.",en,['Robert Chang'],2018-06-24 22:44:11.141000+00:00,"{'Software Development', 'Coding', 'Startup', 'Data Science'}","{'https://miro.medium.com/max/60/1*Uo56hrm9r5L_5fmPsY7I9A.png?q=20', 'https://miro.medium.com/max/2756/1*2XybEH3eav63pBIu-tlRlw.png', 'https://miro.medium.com/max/3824/1*a-gxXI6hPZe9uuRYl1MYWQ.png', 'https://miro.medium.com/max/1200/1*gjgczPgeWlqWEVHtKYpJUg.png', 'https://miro.medium.com/max/60/1*x1yKCJqTFFftEfn6G4jTlA.png?q=20', 'https://miro.medium.com/max/60/1*tcDY4JKmvgfR0x_x0gpS_Q.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*EguVA0HsIGqUy0gaDS1VgA.png', 'https://miro.medium.com/max/5036/1*x1yKCJqTFFftEfn6G4jTlA.png', 'https://miro.medium.com/max/2440/1*tcDY4JKmvgfR0x_x0gpS_Q.png', 'https://miro.medium.com/fit/c/80/80/2*QPeODLfb9YFIKX8_69WPlQ.jpeg', 'https://miro.medium.com/max/60/1*a-gxXI6hPZe9uuRYl1MYWQ.png?q=20', 'https://miro.medium.com/max/60/1*WszG7tFQbuQrYAHnNzxnlg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*EguVA0HsIGqUy0gaDS1VgA.png', 'https://miro.medium.com/fit/c/80/80/2*2JSGiTWBUC51kncKQICFow.jpeg', 'https://miro.medium.com/max/3608/1*gjgczPgeWlqWEVHtKYpJUg.png', 'https://miro.medium.com/max/60/1*gjgczPgeWlqWEVHtKYpJUg.png?q=20', 'https://miro.medium.com/max/60/1*2XybEH3eav63pBIu-tlRlw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*EguVA0HsIGqUy0gaDS1VgA.png', 'https://miro.medium.com/max/4040/1*WszG7tFQbuQrYAHnNzxnlg.png', 'https://miro.medium.com/max/2860/1*Uo56hrm9r5L_5fmPsY7I9A.png'}",2020-03-05 00:26:10.517070,0.9171750545501709
https://towardsdatascience.com/python-data-transformation-tools-for-etl-2cb20d76fcd0,Python Data Transformation Tools for ETL,"Disclaimer: I’m not an ETL expert, and I welcome any comments, advice, or criticism from those who are more experienced in this field.

The other day, I went on Reddit to ask if I should use Python for ETL related transformations, and the overwhelming response was yes.

However, while my fellow Redditors enthusiastically supported using Python, they advised looking into libraries outside of Pandas — citing concerns about Pandas performance with large datasets.

After doing some research, I found a ton of Python libraries built for data transformation: some improve Pandas performance, while others offer their own solutions.

I couldn’t find a comprehensive list of these tools, so I thought I’d compile one using the research I did — if I missed something or got something wrong, please let me know!

Pandas

Website: https://pandas.pydata.org/

Overview

Pandas certainly doesn’t need an introduction, but I’ll give it one anyway.

Pandas adds the concept of a DataFrame into Python, and is widely used in the data science community for analyzing and cleaning datasets. It is extremely useful as an ETL transformation tool because it makes manipulating data very easy and intuitive.

Pros

Widely used for data manipulation

Simple, intuitive syntax

Integrated well with other Python tools including visualization libraries

Support for common data formats (read from SQL databases, CSV files, etc.)

Drawbacks

Since it loads all data into memory, it isn’t scalable and can be a bad choice for very large (larger than memory) datasets

Further Reading

Dask

Website: https://dask.org/

Overview

According to their website, “Dask is a flexible library for parallel computing in Python.”

Essentially, Dask extends common interfaces such as Pandas for use in distributed environments — for instance, the Dask DataFrame mimics Pandas.

Pros

Scalability — Dask can run on your local machine and scale up to a cluster

Ability to work with datasets that don’t fit in memory

Increased performance with the same functionality, even on the same hardware (thanks to parallel computing)

Minimal code changes to switch from Pandas

Designed to integrate with other Python libraries

Drawbacks

There are other ways to improve the performance of Pandas (often more significantly) than parallelism

There is little benefit if the computations you’re doing are small

Some functions aren’t implemented in the Dask DataFrame

Further Reading

modin

Website: https://github.com/modin-project/modin

Overview

Modin is similar to Dask in that it tries to increase the efficiency of Pandas by using parallelism and enabling distributed DataFrames. Unlike Dask, Modin is based on Ray, a task-parallel execution framework.

The main upside to Modin over Dask is that Modin automatically handles distributing your data across your machine’s cores (no configuration necessary).

Pros

Scalability — this is provided more so by Ray than Modin

Increased performance with exact same functionality, even on the same hardware

Minimal code changes to switch from Pandas (changing the import statement)

Provides all the Pandas functionality — more of a “drop-in” solution than Dask is

Drawbacks

There are other ways to improve the performance of Pandas (often more significantly) than parallelism

There is little benefit if the computations you’re doing are small

Further Reading

petl

Website: https://petl.readthedocs.io/en/stable/

Overview

petl includes many of the features pandas has, but is designed more specifically for ETL thus lacking extra features such as those for analysis. petl has tools for all three parts of ETL, but this post focuses solely on transforming data.

Although petl offers the ability to transform tables, other tools such as pandas seem to be more widely used for transformation and well-documented, making petl less appealing for this purpose.

Pros

Minimize use of system memory, enabling it to scale to millions of rows

Useful for migrating between SQL databases

Lightweight and efficient

Drawbacks

By minimizing use of system memory, petl executes slower — it is not recommended for applications where performance is important

Less used than the other solutions on this list for data manipulation

Further Reading

PySpark

Website: http://spark.apache.org/

Overview

Spark is designed for processing and analyzing big data, and offers APIs in numerous languages. The primary advantage of using Spark is that Spark DataFrames use distributed memory and make use of lazy execution, so they can process much larger datasets using a cluster — which isn’t possible with tools like Pandas.

Spark is a good choice for ETL if the data you’re working with is very large, and speed and size in your data operations.

Pros

Scalability and support for larger datasets

Spark DataFrames are very similar to those of Pandas in terms of syntax

Querying using SQL syntax via Spark SQL

Compatible with other popular ETL tools, including Pandas (you can actually convert a Spark DataFrame to a Pandas DataFrame, enabling you to work with all sorts of other libraries)

Compatible with Jupyter Notebooks

Built-in support for SQL, streaming, and graph processing

Drawbacks

Requires a distributed file system such as S3

Using data formats like CSVs limits lazy execution, requiring transforming the data to other formats like Parquet

Lack of direct support for data visualization tools like Matplotlib and Seaborn, both of which are well-supported by Pandas

Further Reading","['etl', 'transformation', 'pandas', 'tools', 'used', 'python', 'performance', 'data', 'memory', 'using', 'dask']","However, while my fellow Redditors enthusiastically supported using Python, they advised looking into libraries outside of Pandas — citing concerns about Pandas performance with large datasets.
After doing some research, I found a ton of Python libraries built for data transformation: some improve Pandas performance, while others offer their own solutions.
It is extremely useful as an ETL transformation tool because it makes manipulating data very easy and intuitive.
ProsWidely used for data manipulationSimple, intuitive syntaxIntegrated well with other Python tools including visualization librariesSupport for common data formats (read from SQL databases, CSV files, etc.)
Spark is a good choice for ETL if the data you’re working with is very large, and speed and size in your data operations.",en,['Hassan Syyid'],2019-08-14 19:02:59.629000+00:00,"{'Data Integration', 'Data Analysis', 'Big Data', 'Python'}","{'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/2160/1*fUO28EIHi1bkZPhjZ451tQ.jpeg', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*lesZyv9LGdUoEO_1StvHIw.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*eRrr_in0dx-UGnwueTRkXQ.png', 'https://miro.medium.com/max/60/1*XjAHPtbB9sCcJiFRIyGzWg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*fUO28EIHi1bkZPhjZ451tQ.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1080/1*fUO28EIHi1bkZPhjZ451tQ.jpeg', 'https://miro.medium.com/max/2400/1*XjAHPtbB9sCcJiFRIyGzWg.png', 'https://miro.medium.com/fit/c/160/160/2*eRrr_in0dx-UGnwueTRkXQ.png', 'https://miro.medium.com/max/60/1*wvsRsWpqqZjiu54WXg-Fkw.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1420/1*lesZyv9LGdUoEO_1StvHIw.png', 'https://miro.medium.com/max/670/1*wvsRsWpqqZjiu54WXg-Fkw.png'}",2020-03-05 00:26:17.228671,6.71160101890564
https://medium.com/weareservian/rstudio-and-bigquery-in-under-30-minutes-c73303f00786,RStudio and BigQuery in under 30 minutes,"I’ve recently had a chance to play with some of the newer tech stacks being used for Big Data and ML/AI across the major cloud platforms. Whilst there are strengths and weaknesses in all of the tools, one of the challenges I’ve become aware of is accessing scalable compute from within RStudio, a common IDE used by data scientists to produce analytics using the R and Python languages.

I’ve put this short guide together to show a clear example of just how easy it is to provision an RStudio instance on GCP and use that instance to access the scalable power of BigQuery to perform complex analytics. You might use this guide to run your own proof-of-concept or to perform ad-hoc data analysis for projects or assignments.

Oh, and the best bit? Because this is all done on the cloud, you can follow this guide from login to query in less than 30 minutes using the web-based console. You could, of course, deploy all of this programmatically, but we’ll keep it simple for now.

Let’s get started.

Create a project

A “project” is the top-level container for resources you deploy in GCP — you might be more used to hearing these called accounts or subscriptions in AWS and Azure respectively. You will need to set one up before you can deploy instances or use BigQuery.

I’m not going to cover the setup of a project — Google has done a great job of this for us:

https://cloud.google.com/resource-manager/docs/creating-managing-projects

Deploy RStudio

The Marketplace on GCP allows us to deploy ready-made images for everything from hosting websites through to running advanced machine learning algorithms. In this example, we’re going to deploy an RStudio Server Pro image, which can be found here:

https://console.cloud.google.com/marketplace/details/rstudio-launcher-public/rstudio-server-pro-for-gcp

Starting from the “Home” dashboard, we can search for “Marketplace” or access this through the menu on the left hand side:

Searching for resources, such as the Marketplace, in the GCP portal

From there, we can search for “RStudio” in the search field in the middle of the page. This will return all related Marketplace results:

You can search the GCP marketplace for all sorts of pre-built images, such as one for RStudio

Either by following the link above, or the steps listed here, you should have found yourself at the marketplace page for “RStudio Server Pro for GCP”:

Each marketplace image has a page explaining a little bit about its purpose and pricing

Now, click on the big blue “Launch on Compute Engine” button. This will take you to a screen where you get to define options for the deployment of your RStudio server:

During deployment of an image, you can specify options that customise your instance, and see detailed pricing information

I recommend increasing your vCPUs to 2, using the “n1-standard-2” VM size, but otherwise, all of the default options should be sufficient. Once you are ready to deploy, scroll down the page and press “Deploy”.

We’re nearly there.

Next, you’ll be shown a page which will update you live on the provisioning of your RStudio server:

The deployment manager shows the current state of your deployments, as well as any warnings, advice or information that is relevant

Once this is complete, I recommend you read all of the information in the panel on the right-hand side — it will give you information such as the username and password for RStudio, as well as some helpful tips about how to configure and manage your instance once deployed.

That’s it — you now have a functional RStudio instance running on GCP!

You can access your RStudio instance using the “site address” shown in the deployment page

You can log in with the username and password shown back on the right-hand side of the deployment page.

Set up RStudio for BigQuery

First, you’re going to need to make a few small changes to enable RStudio to talk to BigQuery.

First, you will need to install the “readr” package — this helps display results returned from BigQuery. To do so, you can install it using the UI as shown, or by running the command install.packages(“readr"") :

Installing packages and libraries in RStudio can be done via the console or via the UI

This may take 2–3 minutes to install, and will you will see updates appear in the console window.

Once done, you need to load the “BigRQuery” package by running the command library(bigrquery) , then defining your GCP project ID, as shown:

The RStudio console shows output from your actions within the IDE, such as installing packages or running commands.

You are now ready to run your first query!

Run your query

Next up, we’re going to define the query we want to run. In this example, we’re going to use a public dataset for our queries, but will move on later to running a query against private data you’ve uploaded.

First, define the SQL statement you wish to run with the following command:

sql <- “#standardSQL SELECT year,month,day,weight_pounds FROM `publicdata.samples.natality` LIMIT 5;""

That’s it! You can now execute your query and get your results:

After being redirected to a sign-in page, you will need to copy and paste the given code into RStudio

You may be prompted to log in to your Google account to get access to BigQuery. In the example above, I’ve chosen to save my credentials and entered my authorization code, but you may choose not to do so if you are using a shared RStudio environment.

Bonus: Uploading and querying custom datasets

In the example given above, we’ve used one of the many provided public datasets. However, in the real world, you likely need to use your own data, such as a web traffic report or a list of transactions. Here we will show you how to upload your own data into GCP for use with BigQuery and RStudio.

We’re going to primarily be following this guide from Google:

https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv

First, you’ll need to create some storage. Navigate to the storage browser and click “Create bucket”:

Creating storage buckets is easy via the storage browser in the GCP portal

You can use the default settings, but to save some money and maintain data sovereignty, I’ve selected a bucket region within Australia using the regional storage class:

Whilst I’ve tweaked my settings shown, the default options will be sufficient for most users

Once created, you can upload your data — I’m using a 1000 row CSV file which contains randomly generated sales transactional-style data:

The GCS browser allows you to easily upload and manage files in your bucket

Next, we’re going to open the BigQuery UI, and after creating a dataset if we need to, we’re going to click the small “+” icon to create a new table:

You can create new datasets and tables via the BigQuery web UI — Please note the newer UI may look slightly different

Once you’ve filled in your details, and let BigQuery handle the schema detection, we can hit “Create table”.

That’s it — your data is loaded in and accessible via BigQuery, which we can test directly via the web-based UI:

Or we can test via RStudio instance we provisioned earlier, simply by updating our SQL query:

By updating the contents of the “SQL” variable and re-executing the “query_exec” command, we can run a new query against BigQuery

Thank you for reading — I hope this short guide helps you understand the possibilities of cloud technology and how it can help you to unlock exciting new data opportunities.

Find out more about Servian’s data and analytics capabilities, here.","['marketplace', 'minutes', '30', 'instance', 'going', 'need', 'gcp', 'data', 'running', 'rstudio', 'using', 'bigquery']","Because this is all done on the cloud, you can follow this guide from login to query in less than 30 minutes using the web-based console.
That’s it — you now have a functional RStudio instance running on GCP!
You can access your RStudio instance using the “site address” shown in the deployment pageYou can log in with the username and password shown back on the right-hand side of the deployment page.
Set up RStudio for BigQueryFirst, you’re going to need to make a few small changes to enable RStudio to talk to BigQuery.
Here we will show you how to upload your own data into GCP for use with BigQuery and RStudio.",en,['Jack Latrobe'],2019-07-08 05:15:48.231000+00:00,"{'Google Cloud Platform', 'Cloud', 'Rstudio', 'Bigquery', 'Analytics'}","{'https://miro.medium.com/max/3200/0*Fyv9mGQe1yemIjN_', 'https://miro.medium.com/max/60/0*Fyv9mGQe1yemIjN_?q=20', 'https://miro.medium.com/fit/c/160/160/1*TK3jvU0pNDG0BAub4IPWaw.jpeg', 'https://miro.medium.com/max/60/0*bhI6I74yt1FFFvan?q=20', 'https://miro.medium.com/max/3200/0*56zIwopId0D7mWdl', 'https://miro.medium.com/max/3200/0*nMt_zZyzBFRIVcyt', 'https://miro.medium.com/max/3200/0*4lb1BjzWKJ1EepvU', 'https://miro.medium.com/max/5670/1*a1-sE2HcrFMXSQASxEnQaA.png', 'https://miro.medium.com/max/60/0*WiYuC-0Ffa-Oqqwm?q=20', 'https://miro.medium.com/max/60/0*M-dW8mnV9dBrbnwZ?q=20', 'https://miro.medium.com/fit/c/160/160/1*Vsqo59XpzZ_epuiPU63jRQ.png', 'https://miro.medium.com/max/3200/0*bhI6I74yt1FFFvan', 'https://miro.medium.com/max/60/0*fT6Op5aAgtrwjyXi?q=20', 'https://miro.medium.com/max/60/0*nMt_zZyzBFRIVcyt?q=20', 'https://miro.medium.com/max/60/0*4lb1BjzWKJ1EepvU?q=20', 'https://miro.medium.com/max/60/0*dxbC8219c0lMcWXb?q=20', 'https://miro.medium.com/max/60/0*56zIwopId0D7mWdl?q=20', 'https://miro.medium.com/max/1200/0*bhI6I74yt1FFFvan', 'https://miro.medium.com/max/60/0*kuZ7zNs66Lir0icJ?q=20', 'https://miro.medium.com/max/3200/0*M-dW8mnV9dBrbnwZ', 'https://miro.medium.com/fit/c/80/80/0*gBtU1x6cultK10FP.jpg', 'https://miro.medium.com/max/3200/0*fT6Op5aAgtrwjyXi', 'https://miro.medium.com/max/60/0*jStAFpun3feVJgm2?q=20', 'https://miro.medium.com/max/60/1*a1-sE2HcrFMXSQASxEnQaA.png?q=20', 'https://miro.medium.com/max/3200/0*V-jG-PbrolPWmEr7', 'https://miro.medium.com/max/3200/0*0IAn0rHzcwdzKcq3', 'https://miro.medium.com/fit/c/80/80/0*zvmUdlH-sl9XLzAA', 'https://miro.medium.com/max/60/0*vWHwInSebCsUJcwb?q=20', 'https://miro.medium.com/max/3200/0*WiYuC-0Ffa-Oqqwm', 'https://miro.medium.com/max/3200/0*kuZ7zNs66Lir0icJ', 'https://miro.medium.com/max/392/1*Q_4p7QWmgDvwbxonpy6xYg.png', 'https://miro.medium.com/max/3200/0*vWHwInSebCsUJcwb', 'https://miro.medium.com/max/3200/0*jStAFpun3feVJgm2', 'https://miro.medium.com/fit/c/96/96/1*TK3jvU0pNDG0BAub4IPWaw.jpeg', 'https://miro.medium.com/max/3200/0*dxbC8219c0lMcWXb', 'https://miro.medium.com/fit/c/80/80/1*-gtQJ08fht22E5wx7JCQNA.png', 'https://miro.medium.com/max/60/0*0IAn0rHzcwdzKcq3?q=20', 'https://miro.medium.com/max/60/0*V-jG-PbrolPWmEr7?q=20'}",2020-03-05 00:26:18.058535,0.8298633098602295
https://medium.com/de-bijenkorf-techblog/deploying-machine-learning-models-with-kubeflow-b2cb45cf22f5,Deploying machine learning models with Kubeflow,"Kubeflow

Kubeflow’s goal is to simplify deploying machine learning workflows to Kubernetes. Currently it consists of a number of different services that give you the tools you need to develop, train, test and deploy models without having to worry (much) about Kubernetes’ internals. We currently use two services specifically: the Jupyter hub for developing models and Pipelines for creating and scheduling workflows.

This is where Kubeflow Pipelines offers a solution. Pipelines provides a simple set of commands to chain together different operations and pass data between them. Separating each operation into its own container greatly reduces the overall complexity. Additionally this gives you the freedom the choose different frameworks or languages for different steps. You could prepare your data using Pandas in Python and but write your model in R for instance. This also encourages sharing and reuse of common operations, such as loading from object storage or writing to databases.

Setup

Setting up Kubeflow can be done either through the GUI (only supports Google Cloud) or CLI. If you just want to play around with Kubeflow we recommend using the GUI which can be found here, but once you want to do a actual permanent deployment we recommend using the CLI.

Screenshot of the Kubeflow deployment via https://deploy.kubeflow.cloud/

The GUI deployment missed a few things which made us to go for the CLI deployment. For starters the initial deployment is deployed with standard machines, where we wanted to run the complete environment on pre-emptibles. Pre-emptibles are machines that can be killed after 24 hours, but only cost 20% of the price of standard instances. Another addition we really wanted was support for GPU machines (and yes you guessed correct, also pre-emptible because we love our cheap stuff ). 💵 💵

Example

To show the strength of Kubeflow Pipelines we’ll modify the Kubernetes example a little by including a pre-processing step, where we generate the text that we’ll read in the next step. In reality the first step might be where you prepare your data set and the second step where you train your model.

Let’s write a script called generate.py that writes a string to a file:

And a script called consume.py that reads a file and outputs its contents:

Again we wrap these scripts in a Docker image:

and push it to our container repository:

With Kubeflow Pipelines we can now define a workflow that runs both images as separate steps and passes the output from the first step to the second, storing the intermediate result as an artifact in object storage. In this case we’re using minio, which is deployed with Kubeflow, but you could just as easily use S3 or Google Storage. We define the task dependencies in a directed acyclic graph (DAG), which is also used to map the output of the first step to the input of the second step.

When we upload this definition we can see Pipelines understands our desired workflow:

Example of the workflow graph

And running yields the desired output:

Results for the graph shown above

This simple example shows the flexibility of piecing together simple building blocks in the form of Docker containers to create complex workflows. The DAG allows for much more intricate control flows by running steps in parallel, combining outputs across multiple execution steps, etc.

Next steps

We still haven’t touched some of the other features of Kubeflow, and new features are coming out regularly. Some of the ones we’re excited about:

Katib is Kubeflow’s engine for hyper-parameter optimization. We’re looking into combining Katib with our pipelines so we can define our workflow and then use Katib to find an optimal set of parameters. By using pipelines you could optimize any setting in any of step of your workflow, whether it’s the actual model training, or some pre-processing step.

The Python DSL for generating pipeline definitions looks promising, allowing pipeline definitions to be built up by combining modular building blocks. Common patterns could be centralized reused, reducing the lines of code needed for each definition (the raw YAML can get quite lengthy).

Seldon can be used for deploying and serving models and offers components for things like A-B testing and multi-armed bandit experiments.

Conclusion

All in all we’re very excited about Kubeflow. It has allowed us to make our ML pipelines much more production worthy and by picking and choosing from the different services it provides we can build out gradually.","['machine', 'models', 'different', 'kubeflow', 'deployment', 'workflow', 'steps', 'learning', 'simple', 'storage', 'step', 'pipelines', 'deploying', 'using']","KubeflowKubeflow’s goal is to simplify deploying machine learning workflows to Kubernetes.
This is where Kubeflow Pipelines offers a solution.
Additionally this gives you the freedom the choose different frameworks or languages for different steps.
Screenshot of the Kubeflow deployment via https://deploy.kubeflow.cloud/The GUI deployment missed a few things which made us to go for the CLI deployment.
Seldon can be used for deploying and serving models and offers components for things like A-B testing and multi-armed bandit experiments.",en,['Martijn Zwennes'],2019-05-08 14:46:41.322000+00:00,"{'Machine Learning', 'Docker', 'Kubeflow', 'Kubernetes'}","{'https://miro.medium.com/max/3292/1*JOFQZtIZ0AnBH4dv8y4mRQ.png', 'https://miro.medium.com/max/60/1*zAeZ9nRub8498OJE4R5GOA.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*C-MnX-OBkcggL3In.', 'https://miro.medium.com/max/60/1*Z5iaX83AIh-Hsw0oheXpbw.png?q=20', 'https://miro.medium.com/max/60/1*JOFQZtIZ0AnBH4dv8y4mRQ.png?q=20', 'https://miro.medium.com/max/60/1*JZs9F24-kZnXsQUgGjL_VQ.png?q=20', 'https://miro.medium.com/max/1526/1*JZs9F24-kZnXsQUgGjL_VQ.png', 'https://miro.medium.com/max/763/1*JZs9F24-kZnXsQUgGjL_VQ.png', 'https://miro.medium.com/fit/c/80/80/0*t_m-aUq4xXAKlE9T.', 'https://miro.medium.com/fit/c/160/160/1*fvsjc0fEiajP-pzFftmQ5w.jpeg', 'https://miro.medium.com/max/2948/1*zAeZ9nRub8498OJE4R5GOA.png', 'https://miro.medium.com/max/1124/1*hEt824yINOcWIjDutWcOaA.png', 'https://miro.medium.com/fit/c/80/80/1*fvsjc0fEiajP-pzFftmQ5w.jpeg', 'https://miro.medium.com/max/2398/1*Z5iaX83AIh-Hsw0oheXpbw.png', 'https://miro.medium.com/fit/c/160/160/1*RL-VRpDxmoUyIw0GDAUYMg.png', 'https://miro.medium.com/max/108/1*AXtAzDmF1yw0n9ll6wizQw.jpeg', 'https://miro.medium.com/max/60/1*hEt824yINOcWIjDutWcOaA.png?q=20', 'https://miro.medium.com/fit/c/96/96/1*fvsjc0fEiajP-pzFftmQ5w.jpeg'}",2020-03-05 00:26:18.949600,0.8900337219238281
https://medium.com/kredaro-engineering/ai-tales-building-machine-learning-pipeline-using-kubeflow-and-minio-4b88da30437b,AI Tales: Building Machine learning pipeline using Kubeflow and Minio,"Joe realizes that there are many building blocks to the production-grade Machine learning model.

It was clear for him that building tens of Machine learning applications at times is similar to the game of lego.

Most of the time, you need to reuse the existing blocks and plugin a few new ones.

You need an easy way to compose the system using various building blocks.

Joe is a Machine learning engineer, and he is good at it.

However, he gets tired of having to bug the DevOps engineers.

He needed them to allocate machines, schedule jobs, set up the environments, creating subnets, service discovery, running the necessary services.

Also, to obtain the metrics, setting up storage for the data, get the list of the allocated machines, schedule jobs, set up the environments, creating subnets, service discovery, running the necessary services and obtaining the metrics, setting up storage for the data.

Phew! The list goes on.

Joe wishes that he could have an easy way around solving the DevOps challenges associated with ML/DL.

Joe was primarily using Tensorflow. He realizes that the training of Machine learning algorithms on large amounts of data and serving the trained model as API’s for millions of customers poses a serious scaling issue.

Here are the factors because of which scaling becomes hard,

Storing vast volumes of data.

The computational overhead involved in processing large amounts of data.

Distributing Machine learning training across multiple computational resources.

Cost issues.

Joe found it hard to train on large amounts of data in a reasonable time.

Here were the primary reasons.

Throughput challenge from underlying storage system during training

Procuring of CPU, GPU, and TPU’s and other computational resources to scale up the performance.

Throttling, rate limiting, bandwidth charges and low throughput from public cloud storage systems.

Joe is tired and frustrated with trying to productionize the Machine learning pipeline.

As a last resort, Joe moves to managed services.

It made things more comfortable, and he could get started faster, DevOps was abstracted. It was all great.

Then the bills arrived!

Joe knew that using managed services made things easier.

However, the billing woes added massive pressure on his startup in its goal to be soon profitable.","['machine', 'kubeflow', 'computational', 'amounts', 'tales', 'learning', 'minio', 'ai', 'storage', 'large', 'devops', 'data', 'building', 'using', 'pipeline']","Joe realizes that there are many building blocks to the production-grade Machine learning model.
It was clear for him that building tens of Machine learning applications at times is similar to the game of lego.
Joe is a Machine learning engineer, and he is good at it.
Distributing Machine learning training across multiple computational resources.
Joe is tired and frustrated with trying to productionize the Machine learning pipeline.",en,['Karthic Rao'],2019-10-02 15:35:45.604000+00:00,"{'Deep Learning', 'Kubernetes', 'Machine Learning', 'DevOps', 'TensorFlow'}","{'https://miro.medium.com/max/60/1*dUwaPMr74Q0E1seiPrWC6g.png?q=20', 'https://miro.medium.com/max/16000/1*_KzuBTRimXDY_c8cybPXOQ.png', 'https://miro.medium.com/max/16000/1*9APuMtQl_DqgsJfS8o2Q2g.png', 'https://miro.medium.com/max/16000/1*ev2idUjA6frxWW07ph7kjQ.png', 'https://miro.medium.com/max/60/1*xvH7f5Si8ObEsc2bUKI4zg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*GW-MKzk9V-U3WLQ0aycYtw.gif?q=20', 'https://miro.medium.com/max/60/1*_886hV6a6UmJ9yBCD6sFWA.png?q=20', 'https://miro.medium.com/max/16000/1*rpT1edsmKWPhgeFQBnGftw.png', 'https://miro.medium.com/max/16000/1*fNG-RPba1jIEG26w5I4btw.png', 'https://miro.medium.com/max/16000/1*cRv7Q0shr9SHnzmCx4wvcw.png', 'https://miro.medium.com/max/60/1*BFXsOLhWfNUBGZBWU6v9Yg.png?q=20', 'https://miro.medium.com/max/16000/1*ICNFJBx6cf24OBDP1_-KqA.png', 'https://miro.medium.com/freeze/max/60/1*cvqgYAL_xdQsVtMmdZ8HEA.gif?q=20', 'https://miro.medium.com/max/60/1*EZl0yA7s5PpRzRM4WZ8UtA.png?q=20', 'https://miro.medium.com/max/1200/1*dUwaPMr74Q0E1seiPrWC6g.png', 'https://miro.medium.com/max/2200/1*sjpRhg3r3rBnIJoQ2xMpWA.gif', 'https://miro.medium.com/freeze/max/60/1*xNFxQRWZhWTvVa6bbHPJpA.gif?q=20', 'https://miro.medium.com/max/60/1*AT8hpFUOITcB_jzAVzG1pg.png?q=20', 'https://miro.medium.com/max/60/1*zvHTkUJH9-cliRS5mp_NBg.png?q=20', 'https://miro.medium.com/freeze/max/60/1*jRrL5DnWF5BXrrbW9XwzlQ.gif?q=20', 'https://miro.medium.com/max/126/1*N-nRtkDpr2vOMs4RZIzR1g.png', 'https://miro.medium.com/max/128/1*N-nRtkDpr2vOMs4RZIzR1g.png', 'https://miro.medium.com/max/2200/1*RBTCWdRIdB0G857sxGYuuA.gif', 'https://miro.medium.com/freeze/max/60/1*VK0rNJUxH98y7-R7jHWx8A.gif?q=20', 'https://miro.medium.com/max/16000/1*hj4XsxdX66GKN_mrMA0DeQ.png', 'https://miro.medium.com/max/2840/1*zvHTkUJH9-cliRS5mp_NBg.png', 'https://miro.medium.com/max/16000/1*zw17RsezX8xdQR2T4PYJfA.png', 'https://miro.medium.com/max/2200/1*MANC9V9X28ecRACFLDQNFQ.gif', 'https://miro.medium.com/max/16000/1*ah2P2zGg96wInG1EVwRwnw.png', 'https://miro.medium.com/freeze/max/60/1*sjpRhg3r3rBnIJoQ2xMpWA.gif?q=20', 'https://miro.medium.com/max/60/1*zw17RsezX8xdQR2T4PYJfA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*L-ZCYpkTr22SYnPavdZPHA.gif?q=20', 'https://miro.medium.com/max/16000/1*iyzjO_iXRndLjANnEpJNGQ.png', 'https://miro.medium.com/freeze/max/60/1*MANC9V9X28ecRACFLDQNFQ.gif?q=20', 'https://miro.medium.com/max/60/1*X2TAxZ3ID4bysBzblwTsEQ.png?q=20', 'https://miro.medium.com/max/60/1*ZB1BoAw_XbWBL303J4L4JQ.png?q=20', 'https://miro.medium.com/max/16000/1*ow6Z07aCJ4Q1IEoExMMVZw.png', 'https://miro.medium.com/max/60/1*N7zEtpUFArt1IiOAEulXww.png?q=20', 'https://miro.medium.com/max/2676/1*5j6l6mV1EzHHY3zMBPlhEA.png', 'https://miro.medium.com/max/60/1*5arGnQ9De9pXQi8CSQLqVg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*x893Kg4-hcaQdbwhg5tN6Q.jpeg', 'https://miro.medium.com/max/16000/1*sE337-IFrESg_Sl3oneGkQ.png', 'https://miro.medium.com/max/16000/1*N7zEtpUFArt1IiOAEulXww.png', 'https://miro.medium.com/max/60/1*iyzjO_iXRndLjANnEpJNGQ.png?q=20', 'https://miro.medium.com/max/16000/1*krwNGoby8krekEw8Rk5XYw.png', 'https://miro.medium.com/max/2200/1*cvqgYAL_xdQsVtMmdZ8HEA.gif', 'https://miro.medium.com/max/60/1*5j6l6mV1EzHHY3zMBPlhEA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*V-sULK-e-vwj8zRHQRIevA.png', 'https://miro.medium.com/max/60/1*cRv7Q0shr9SHnzmCx4wvcw.png?q=20', 'https://miro.medium.com/max/16000/1*ZB1BoAw_XbWBL303J4L4JQ.png', 'https://miro.medium.com/max/14372/1*4LEt6bGMZ4obgHtl0uiaNA.png', 'https://miro.medium.com/max/60/1*4LEt6bGMZ4obgHtl0uiaNA.png?q=20', 'https://miro.medium.com/max/60/1*ow6Z07aCJ4Q1IEoExMMVZw.png?q=20', 'https://miro.medium.com/max/16000/1*X2TAxZ3ID4bysBzblwTsEQ.png', 'https://miro.medium.com/max/16000/1*5arGnQ9De9pXQi8CSQLqVg.png', 'https://miro.medium.com/max/16000/1*xvH7f5Si8ObEsc2bUKI4zg.png', 'https://miro.medium.com/max/60/1*ev2idUjA6frxWW07ph7kjQ.png?q=20', 'https://miro.medium.com/max/3552/1*OZGoh9eZD0MQkwSBHcLtpQ.png', 'https://miro.medium.com/freeze/max/60/1*lp3EXdKI0tmuLlfbt6TCLw.gif?q=20', 'https://miro.medium.com/max/60/1*fNG-RPba1jIEG26w5I4btw.png?q=20', 'https://miro.medium.com/freeze/max/60/1*RBTCWdRIdB0G857sxGYuuA.gif?q=20', 'https://miro.medium.com/max/60/1*umZWqSHP89x0lnI0DSZX1g.png?q=20', 'https://miro.medium.com/max/60/1*axOJpyOdAQ_FNaOFhDAdHA.png?q=20', 'https://miro.medium.com/max/60/1*rpT1edsmKWPhgeFQBnGftw.png?q=20', 'https://miro.medium.com/max/2200/1*L-ZCYpkTr22SYnPavdZPHA.gif', 'https://miro.medium.com/max/2200/1*GW-MKzk9V-U3WLQ0aycYtw.gif', 'https://miro.medium.com/max/16000/1*_886hV6a6UmJ9yBCD6sFWA.png', 'https://miro.medium.com/max/2544/1*umZWqSHP89x0lnI0DSZX1g.png', 'https://miro.medium.com/max/2200/1*jRrL5DnWF5BXrrbW9XwzlQ.gif', 'https://miro.medium.com/fit/c/96/96/1*x893Kg4-hcaQdbwhg5tN6Q.jpeg', 'https://miro.medium.com/max/16000/1*AT8hpFUOITcB_jzAVzG1pg.png', 'https://miro.medium.com/max/60/1*7vk91VymcVN_8OGF_X7xHA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*IBxguQMNSBL0xc2eXJNoUg@2x.jpeg', 'https://miro.medium.com/max/16000/1*EZl0yA7s5PpRzRM4WZ8UtA.png', 'https://miro.medium.com/max/16000/1*dUwaPMr74Q0E1seiPrWC6g.png', 'https://miro.medium.com/max/60/1*9APuMtQl_DqgsJfS8o2Q2g.png?q=20', 'https://miro.medium.com/max/60/1*krwNGoby8krekEw8Rk5XYw.png?q=20', 'https://miro.medium.com/max/16000/1*axOJpyOdAQ_FNaOFhDAdHA.png', 'https://miro.medium.com/max/1980/1*7vk91VymcVN_8OGF_X7xHA.png', 'https://miro.medium.com/max/2200/1*xNFxQRWZhWTvVa6bbHPJpA.gif', 'https://miro.medium.com/max/60/1*ICNFJBx6cf24OBDP1_-KqA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*NNrFCH8gP99gVPcf2et_fg.gif?q=20', 'https://miro.medium.com/max/60/1*sE337-IFrESg_Sl3oneGkQ.png?q=20', 'https://miro.medium.com/max/60/1*hj4XsxdX66GKN_mrMA0DeQ.png?q=20', 'https://miro.medium.com/max/2200/1*lp3EXdKI0tmuLlfbt6TCLw.gif', 'https://miro.medium.com/max/2200/1*NNrFCH8gP99gVPcf2et_fg.gif', 'https://miro.medium.com/max/16000/1*BFXsOLhWfNUBGZBWU6v9Yg.png', 'https://miro.medium.com/fit/c/80/80/1*x893Kg4-hcaQdbwhg5tN6Q.jpeg', 'https://miro.medium.com/max/60/1*ah2P2zGg96wInG1EVwRwnw.png?q=20', 'https://miro.medium.com/max/2200/1*VK0rNJUxH98y7-R7jHWx8A.gif', 'https://miro.medium.com/fit/c/80/80/0*XOOG6H-QWpzLvBdZ.jpg', 'https://miro.medium.com/max/60/1*_KzuBTRimXDY_c8cybPXOQ.png?q=20', 'https://miro.medium.com/max/60/1*OZGoh9eZD0MQkwSBHcLtpQ.png?q=20'}",2020-03-05 00:26:21.148963,2.198362112045288
https://medium.com/kubeflow/kubeflow-v0-5-simplifies-model-development-with-enhanced-ui-and-fairing-library-78e19cdc9f50,Kubeflow v0.5 simplifies model development with enhanced UI and Fairing library,"The Kubeflow Product Management Working Group is excited to announce the release Kubeflow v0.5, which brings significant improvements to users’ model development experience!

New features in Kubeflow 0.5 include:

A Go binary, kfctl, to simplify configuring and deploying Kubeflow

An improved UI for managing notebooks that makes it easy to:

Run multiple notebooks simultaneously

Attach volumes to notebooks

The Fairing library to build, train, and deploy models from notebooks or your favorite Python IDE

Demo Kubeflow 0.5 by building, training and deploying an XGBoost model

We thought the best way to illustrate Kubeflow 0.5’s improvements was with a walkthrough demonstrating how to leverage the new notebooks enhancements for interactive development of your XGBoost models.

Deploy Kubeflow

First off, we’ll kick off a fresh deployment of Kubeflow v0.5 on Google Kubernetes Engine using the web deployment application available at https://deploy.kubeflow.cloud/. For instructions on deploying onto other platforms, please see Getting Started with Kubeflow.

The screenshot below shows an example deployment form. Note that “Project field” is a GCP Project ID, with a deployment name of our choice. In this example, we opted to use Login with a Username/Password, and picked a username and password for the deployment to use. Also note that we left the Kubeflow version to the default v0.5.0. Then we clicked “Create Deployment,” kicking off the deployment of Kubeflow to the project. It will take roughly 10 minutes to be ready after you kick it off. Click on Show Logs to view the progress messages. In case you run into errors, please see detailed instructions for deployment.

Once the deployment is ready, the deployment web app page automatically redirects to the login page of the newly deployed Kubeflow cluster, as shown below.

Create a notebook server in Kubeflow

After logging in with the username and password we chose at deployment, we arrive at the updated Kubeflow Dashboard in v0.5:

Notice the build version displayed at the bottom left of the dashboard. This gives a quick confirmation of the version of Kubeflow deployed in your cluster.

In this demo we’ll focus on notebooks. Clicking on Notebooks in the left nav takes us to the new Notebooks management interface:

This is a new Kubernetes Native web app developed by the Kubeflow Community to improve the experience of creating and managing Notebook Servers in a Kubeflow deployment.

We’ll create a new TensorFlow 1.13 notebook server using one of the pre-configured images in Kubeflow by clicking “New Server” at the top-right.

Now we’ll provide a name for the notebook server (myserver in this example), pick the default Kubeflow namespace, and pick one of the standard TensorFlow notebook server images. We picked 1.0 for CPU and 5.0Gi for the memory. The new UI makes it really easy to create and attach new volumes, as well as existing volumes, to the Notebook Server. If you have a pre-configured NFS Server volume (your Admin team might have done that), you can easily discover it and attach the existing volume(s).

Once configured, we click “Spawn” and wait for the notebook server to get ready.

At this point, the pod is getting ready and pulling the specified container image. Once ready, the “Connect” button is highlighted on the notebook server, as shown below.

Clicking on “Connect” takes us to the Jupyter notebooks:

Note that, initially, there are no notebooks or terminals running.

Run an example notebook with Kubeflow Fairing

Fairing is a Kubeflow library that makes it easy to build, train, and deploy your ML training jobs on Kubeflow or Kubernetes, directly from Python code or a Jupyter notebook.

For this example, we’ll try running through one of the new Fairing example notebooks. In order to do that easily, here are the steps we follow:

Create a new terminal.

2. Clone the fairing repo in the terminal.



$ git clone $ bash$ git clone https://github.com/kubeflow/fairing

3. In the terminal, run the following commands:

$ cd fairing/examples/prediction

$ pip3 install -r requirements.txt

4. Switch back to the notebooks view. Notice the fairing directory that shows up.

5. Browse to faring/examples/prediction directory. Click on xgboost-high-level-apis.ipynb

6. This opens the notebook in your notebook server.

7. Study the notebook and run through the notebooks cells.

Explore the notebook

The notebook is self-explanatory, and walks us through the development of an XGBoost-based model for a housing price prediction example. It illustrates how Fairing makes it extremely straightforward to develop your model.

Here are some of the core features to note:","['ui', 'enhanced', 'server', 'kubeflow', 'makes', 'web', 'deployment', 'simplifies', 'fairing', 'example', 'development', 'ready', 'model', 'notebooks', 'library', 'notebook', 'v05']","The Kubeflow Product Management Working Group is excited to announce the release Kubeflow v0.5, which brings significant improvements to users’ model development experience!
Deploy KubeflowFirst off, we’ll kick off a fresh deployment of Kubeflow v0.5 on Google Kubernetes Engine using the web deployment application available at https://deploy.kubeflow.cloud/.
For this example, we’ll try running through one of the new Fairing example notebooks.
This opens the notebook in your notebook server.
It illustrates how Fairing makes it extremely straightforward to develop your model.",en,['Thea Lamkin'],2019-04-30 00:18:58.719000+00:00,"{'Data Science', 'Kubeflow', 'Kubernetes', 'Machine Learning', 'Jupyter Notebook'}","{'https://miro.medium.com/max/60/0*SnCHA0eXUODquEqB?q=20', 'https://miro.medium.com/max/3200/0*fBFyY7MLzEFTPSc9', 'https://miro.medium.com/fit/c/160/160/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/fit/c/80/80/1*XTnyfuovNWBDIfCDuHbFpg.jpeg', 'https://miro.medium.com/max/3200/0*nUHy2BEkrWZA92eb', 'https://miro.medium.com/max/3200/0*nQ-u6_0zqGOJsLVs', 'https://miro.medium.com/max/60/0*L6iHP5ABgA73EPen?q=20', 'https://miro.medium.com/max/3200/0*IiMdSK1Jjg9Iqm9w', 'https://miro.medium.com/max/60/0*MX7rUqFyQ6lVaKjG?q=20', 'https://miro.medium.com/max/60/0*muvS6RZz5ScQeHJa?q=20', 'https://miro.medium.com/fit/c/80/80/2*8qYzMnCauVpCpfq0QH-Hrw.jpeg', 'https://miro.medium.com/max/3200/0*46KG131wIVB3-4X3', 'https://miro.medium.com/max/3200/0*L6iHP5ABgA73EPen', 'https://miro.medium.com/max/60/0*sH7J8__V1yj2wmk6?q=20', 'https://miro.medium.com/max/2914/0*muvS6RZz5ScQeHJa', 'https://miro.medium.com/max/60/0*IiMdSK1Jjg9Iqm9w?q=20', 'https://miro.medium.com/max/3200/0*QdI0ILQtHIkPYyNP', 'https://miro.medium.com/max/1200/0*SnCHA0eXUODquEqB', 'https://miro.medium.com/max/60/0*_tXj50Nu7ykInlLk?q=20', 'https://miro.medium.com/max/1840/0*_tXj50Nu7ykInlLk', 'https://miro.medium.com/max/60/0*QdI0ILQtHIkPYyNP?q=20', 'https://miro.medium.com/fit/c/80/80/2*RXJ8MDWG2MsLXAh_Do_73w.jpeg', 'https://miro.medium.com/fit/c/96/96/1*XTnyfuovNWBDIfCDuHbFpg.jpeg', 'https://miro.medium.com/max/60/0*46KG131wIVB3-4X3?q=20', 'https://miro.medium.com/max/2882/0*sH7J8__V1yj2wmk6', 'https://miro.medium.com/fit/c/160/160/1*XTnyfuovNWBDIfCDuHbFpg.jpeg', 'https://miro.medium.com/max/72/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/max/60/0*nUHy2BEkrWZA92eb?q=20', 'https://miro.medium.com/max/60/0*fBFyY7MLzEFTPSc9?q=20', 'https://miro.medium.com/max/2880/0*SINSNRcL8vcgdu8X', 'https://miro.medium.com/max/60/0*nQ-u6_0zqGOJsLVs?q=20', 'https://miro.medium.com/max/3200/0*5DBSqzN4aVUxCAMj', 'https://miro.medium.com/max/3200/0*SnCHA0eXUODquEqB', 'https://miro.medium.com/max/60/0*SINSNRcL8vcgdu8X?q=20', 'https://miro.medium.com/max/60/0*5DBSqzN4aVUxCAMj?q=20', 'https://miro.medium.com/max/3200/0*MX7rUqFyQ6lVaKjG'}",2020-03-05 00:26:22.778486,1.6285250186920166
https://medium.com/kubeflow/an-end-to-end-ml-pipeline-on-prem-notebooks-kubeflow-pipelines-on-the-new-minikf-33b7d8e9a836,An end-to-end ML pipeline on-prem: Notebooks & Kubeflow Pipelines on the new MiniKF,"Update (October 2, 2019): This tutorial has been updated to showcase the Taxi Cab end-to-end example using the new MiniKF (v20190918.1.0) that features Kubeflow v0.6.2.

Today, at Kubecon Europe 2019, Arrikto announced the release of the new MiniKF, which features Kubeflow v0.5. The new MiniKF enables data scientists to run end-to-end Kubeflow Pipelines locally, starting from their Notebook.

This is great news for data scientists, as until now, there was no easy way for one to run an end-to-end KFP example on-prem. One should have strong Kubernetes knowledge to be able to deal with some steps. Specifically, for Kubeflow’s standard Chicago Taxi (TFX) example, which will be the one we will presenting in this post, one should:

Understand K8s and be familiar with kubectl

Understand and compose YAML files

Manually create PVCs via K8s

Mount a PVC to a container to fill it up with initial data

Using MiniKF and Arrikto’s Rok data management product (MiniKF comes with a free, single-node Rok license), we showcase how to streamline all these operations to reduce time, and provide a much more friendly user experience. A data scientist starts from a Notebook, builds the pipeline, and uses Rok to take a snapshot of the local data they prepare, with a click of a button. Then, they can seed the Kubeflow Pipeline with this snapshot using only the UIs of KFP and Rok.

MiniKF greatly enhances data science experience by simplifying users’ workflow and removing the need for even a hint of K8s knowledge. It also introduces the first steps towards a unified integration of Notebooks & Kubeflow Pipelines.

In a nutshell, this tutorial will highlight the following benefits of using MiniKF, Kubeflow, and Rok:

Easy execution of a local/on-prem Kubeflow Pipelines e2e example

Seamless Notebook and Kubeflow Pipelines integration with Rok

KFP workflow execution without K8s-specific knowledge

Kubeflow’s Chicago Taxi (TFX) example on-prem tutorial

Let’s put all the above together, and watch MiniKF, Kubeflow, and Rok in action.

One very popular data science example is the Taxi Cab (or Chicago Taxi) example that predicts trips that result in tips greater than 20% of the fare. This example is already ported to run as a Kubeflow Pipeline on GCP, and included in the corresponding KFP repository. We are going to showcase the Taxi Cab example running locally, using the new MiniKF, and demonstrate Rok’s integration as well. Follow the steps below and you will run an end-to-end Kubeflow Pipeline on your laptop!

Install MiniKF

Open a terminal and run:

vagrant init arrikto/minikf

vagrant up

Open a browser, go to 10.10.10.10 and follow the instructions to get Kubeflow and Rok up and running.

For more info about how to install MiniKF, visit the MiniKF page:

https://www.arrikto.com/minikf/

Or the official Kubeflow documentation:

https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/

Create a Notebook Server

On the MiniKF landing page, click the “Connect” button next to Kubeflow to connect to the Kubeflow Dashboard:

Log in using the following credentials:

Username: user

Password: 12341234

Once at Kubeflow’s Dashboard, click on the “Notebook Servers” link on the left pane to go to the Notebook Manager UI:

You are now at the Kubeflow Notebook Manager, showing the list of Notebook Servers, which is currently empty. Click on “New Server” to create a new Notebook Server:

Enter a name for your new Notebook Server, and select Image, CPU, and RAM:

Add a new, empty Data Volume, for example of size 2GB, and name it “data” (you can give it any name you like, but then you will have to modify some commands in later steps):

Once you select all options, click “Launch” to create the Notebook Server, and wait for it to get ready:

Once the Server is ready, the “Connect” button will become active (blue color). Click on the “Connect” button to connect to your new Notebook Server:

A new tab will open up with the JupyterLab landing page:

Bring in the Pipelines code and data

Create a new terminal in JupyterLab:

Bring in a pre-filled Notebook to run all the necessary commands:

The Notebook will appear on the left pane of your JupyterLab:

Double-click the notebook file to open it and run the cells one-by-one:

Run the first cell to download the Arrikto’s pipeline code to run the Chicago Taxi Cab example on-prem:

Run the second cell to ingest the data:

Run the third cell to move the ingested data to the Data Volume of the Notebook. (Note that if you didn’t name your Data Volume “data”, then you have to slightly modify this command):

Run the fourth cell to verify that data exist inside the Data Volume. Note that you should have the taxi-cab-classification directory under the data directory, not just the files. You now have a local Data Volume populated with the data the pipeline code needs (if you didn’t give the name “data” to your Data Volume, then you have to make sure that your Data Volume has these files in it):

Run the fifth cell to compile the pipeline source code:

Run the sixth cell to verify the output of the compiled pipeline:

The file containing the compiled pipeline appears on the left pane of your JupyterLab:

Snapshot the Data Volume

In later steps, the pipeline is going to need the data that we brought into the Data Volume previously. For this reason, we need a snapshot of the Data Volume. As a best practice, we will snapshot the whole JupyterLab, and not just the Data Volume, in case the user wants to go back and reproduce their work.

We will use Rok, which is already included in MiniKF, to snapshot the JupyterLab. On the Kubeflow left pane, click on the “Snapshot Store” link to go to the Rok UI. Alternatively, you can go to the MiniKF landing page and click the “Connect” button next to Rok:

This will open the Rok login page:

Log in using the following credentials:

Username: user

Password: 12341234

This is the Rok UI landing page:

Create a new bucket to host the new snapshot. Click on the “+” button on the top left:

A dialog will appear asking for a bucket name. Give it a name and click “Next”. We will keep the bucket “Local” for this demo:

Clicking “Next” will result in a new, empty bucket appearing in the landing page. Click on the bucket, to go inside:

Once inside the bucket, click on the Camera button to take a new snapshot:

By clicking the Camera button, a dialog appears asking for the K8s resource that we want to snapshot. Choose the whole “JupyterLab” option, not just the single Data Volume (“Dataset”):

Most fields will be pre-filled with values automatically by Rok, for convenience. Select your JupyterLab from the dropdown list:

Provide a commit title and a commit message for this snapshot. This is to help you identify the snapshot version in the future, the same way you would do with your code commits in Git:

Then, choose a name for your snapshot:

Take the snapshot, by clicking the “Snapshot” button:

Once the operation completes, you will have a snapshot of your whole JupyterLab. This means you have a snapshot of the Workspace Volume and a snapshot of the Data Volume, along with all the corresponding JupyterLab metadata to recreate the environment with a single click. The snapshot appears as a file inside your new bucket. Expanding the file will let you see the snapshot of the Workspace Volume and the snapshot of the Data Volume:

Now that we have both the pipeline compiled and a snapshot of the Data Volume, let’s move on to run the pipeline and seed it with the data we prepared.

Upload the Pipeline to KFP

Before uploading the pipeline to KFP, we first need to download the compiled pipeline locally. Go back to your JupyterLab and download the compiled pipeline. To do so, right click on the file on JupyterLab’s left pane, and click “Download”:

Once the file is downloaded to your laptop, go to Kubeflow Dashboard and open the Kubeflow Pipelines UI by clicking “Pipelines” on the left pane:

This will take you to the Kubeflow Pipelines UI:

Click “+ Upload pipeline”:

On the pop-up window choose the .tar.gz file you downloaded locally and give a name to your new pipeline. Then click “Upload”:

The pipeline should get uploaded successfully and will now appear on the pipelines list with the defined name:

Click on it to view all the pipeline steps:

Create a new Experiment Run

Create a new Experiment by clicking “+ Create experiment”:

Choose an Experiment name, and click “Next”:

By clicking “Next”, the KFP UI sends you to the “Start a new run” page, where you are going to create a new Run for this Experiment. Enter a name for this Run (note that the Pipeline is already selected. If this is not the case, just select the uploaded Pipeline):

Note that the Pipeline’s parameters show up:

Seed the Pipeline with the Notebook’s Data Volume

On the “rok-url” parameter we need to specify the snapshot of the Notebook Server’s Data Volume, which we created in the previous step. Click on “Choose” to open the Rok file chooser and select the snapshot of this Data Volume:

Enter inside the bucket you created previously:

Expand the file inside the bucket:

Find the snapshot of the Data Volume and click on it. Make sure that you have chosen the Data Volume and not the Workspace Volume:

Then click on “Choose”:

The Rok URL that corresponds to the Data Volume snapshot appears on the corresponding parameter field:

Select a bucket to store the pipeline snapshot

On the “rok-register-url” parameter we need to choose where we are going to store the pipeline snapshot. For that, we need to specify a bucket and a name for the file (snapshot) that is going to be created. We can select an existing bucket and an existing file. This would create a new version of this file. Here, we will create a new bucket and a new file. First, we click on the “Choose” button to open the Rok file chooser:

We create a new bucket:

We give it a name and click “Next”:

Then we enter the bucket:

We create a new file:

We give it a name:

And click “Choose”:

The Rok URL that corresponds to the pipeline snapshot appears on the corresponding parameter field:

Run the Pipeline

Now that we have defined the data to seed the pipeline and the file to store the pipeline snapshot, we can run the pipeline. We are leaving all other parameters as is, and click “Start”:

Now, click on the Run you created to view the progress of the Pipeline:

As the Pipeline runs, we see the various steps running and completing successfully. The Pipeline is going to take 10 to 30 minutes to complete, depending on your laptop specs. During the first step, Rok will instantly clone the snapshot we provided, for the next steps to use it:

Note: If the validation step fails with:

/mnt/taxi-cab-classification/column-names.json; No such file or directory

then you have to make sure that you have given the name “data” to your Data Volume. If not, then make sure that you amended the command of cell 3 accordingly, so that the data of the pipeline gets stored to your Data Volume.

The training step is going to take a few minutes:

Pipeline snapshot with Rok

As a last step, Rok will automatically take a snapshot of the pipeline, so that we can later attach it to a Notebook and do further exploration of the pipeline results. You can see this step here:

If we go back to the Rok UI, we can see the snapshot that was taken during the pipeline run. We go to the Rok UI landing page:

We find the bucket that we chose earlier to store the snapshot, in this case “taxi-cab-pipeline-snapshot”:

And we see the pipeline snapshot that has the name that we chose earlier, in this case “pipeline-snapshot”:

Explore pipeline results inside a Notebook

To explore the pipeline results, we can create a new Notebook and attach it the pipeline snapshot. The Notebook will use a clone of this immutable snapshot. This means that you can do further exploration and experimentation using this Data Volume, without losing the results that the the pipeline run produced.

To do this, we create a new Notebook Server and add an existing Data Volume. Here, the volume will be the pipeline snapshot that lives inside Rok. We go to the Notebook Manager UI and click “New Server”:

Then, we enter a name for the new Notebook Server, and select Image, CPU, and RAM:

To add an existing Data Volume, click on “Add Volume”:

Then change its type to “Existing”:

Now, we will use the Rok file chooser to select the snapshot that was taken during the pipeline run. This snapshot contains the pipeline results. The Notebook will use a clone of this immutable snapshot, so you can mess with the volume without losing any of your critical data.

We click on the Rok file chooser icon and open the file chooser:

We find the bucket that we chose earlier to store the snapshot, in this case “taxi-cab-pipeline-snapshot”:

We click on the file (snapshot) that we created during the pipeline run, in this case “pipeline-snapshot”:

And then click “Choose”:

The Rok URL that corresponds to this snapshot appears in the “Rok URL” field:

You can change the name of the volume if you wish. Here, we change it to “analysis”:

Then, we click “Launch”:

A new Notebook Server appears in the Notebook Manager UI. Click on “Connect” to go this JupyterLab:

Inside the Notebook, you can see the Data Volume that we created by cloning the pipeline snapshot:

Inside the Data Volume you can see the input data of the pipeline “taxi-cab-classification”, and the output data of the pipeline “taxi-cab-classification-s9qwx”. Note that you will see a different alphanumerical sequence in your Notebook. Open the second folder to view the pipeline results:

You can see the results of every step of the pipeline:

Let’s go inside the analysis folder to see how the model performed:

Open the “output_display.html” file:

Click on “Trust HTML” to be able to view the file:

Here is how the model performed:

Talk to us

Join the discussion on the #minikf Slack channel, ask questions, request features, and get support for MiniKF.

To join the Kubeflow Slack workspace, please request an invite.

Watch the tutorial video

You can also watch the video of this tutorial:","['file', 'kubeflow', 'snapshot', 'rok', 'volume', 'click', 'onprem', 'ml', 'endtoend', 'notebook', 'minikf', 'data', 'run', 'pipelines', 'notebooks', 'pipeline']","The new MiniKF enables data scientists to run end-to-end Kubeflow Pipelines locally, starting from their Notebook.
Then, they can seed the Kubeflow Pipeline with this snapshot using only the UIs of KFP and Rok.
This example is already ported to run as a Kubeflow Pipeline on GCP, and included in the corresponding KFP repository.
Follow the steps below and you will run an end-to-end Kubeflow Pipeline on your laptop!
(Note that if you didn’t name your Data Volume “data”, then you have to slightly modify this command):Run the fourth cell to verify that data exist inside the Data Volume.",en,['Chris Pavlou'],2019-10-18 10:38:06.889000+00:00,"{'AI', 'Data Science', 'Kubeflow', 'Kubernetes', 'Machine Learning'}","{'https://miro.medium.com/max/3840/1*IHT-sP9AQ7ugIGlAhhijuA.png', 'https://miro.medium.com/max/60/1*dDh4bEgeHOGQoYEybSJHlg.png?q=20', 'https://miro.medium.com/max/60/1*26ubXkFksBf8XVcL39amWg.png?q=20', 'https://miro.medium.com/max/60/1*eJquQCYgAq572dIuVBG_0g.png?q=20', 'https://miro.medium.com/max/3840/1*Z9qy7EVsYcubGH7I3DqB9Q.png', 'https://miro.medium.com/max/60/1*rIqiNwLwrVtd65ENCE-Q-w.png?q=20', 'https://miro.medium.com/max/3840/1*MnaHmkYyRMLbBxlCAnEDCA.png', 'https://miro.medium.com/max/3840/1*YXM42XeseZoYX9abOZVjBg.png', 'https://miro.medium.com/max/1200/1*1HapODtfIOlG1CGY_4-0PA.png', 'https://miro.medium.com/max/60/1*7jX1Fu4xe4wJPEWNuJ-jNA.png?q=20', 'https://miro.medium.com/max/3840/1*djtqXcHwiVPjGBhBu2XVXg.png', 'https://miro.medium.com/max/60/1*bDeDA8RWeKwNdBG4_5kRdw.png?q=20', 'https://miro.medium.com/max/60/1*rnQODXlTWAr6-JVndfYc1Q.png?q=20', 'https://miro.medium.com/max/3840/1*pOcf82nLz13sQ1HPcxYS9Q.png', 'https://miro.medium.com/max/60/1*3ZBTSnfSa4DkGjdmbRRUQA.png?q=20', 'https://miro.medium.com/max/3840/1*8vDxkn4uNQqLTExd-5PUZA.png', 'https://miro.medium.com/max/3840/1*V97OJj86pwrzKafK0e20Ww.png', 'https://miro.medium.com/max/3840/1*gzGZkBB3JavfHbW9P8Nx_g.png', 'https://miro.medium.com/max/60/1*IHT-sP9AQ7ugIGlAhhijuA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*8qYzMnCauVpCpfq0QH-Hrw.jpeg', 'https://miro.medium.com/max/60/1*UpRb4JYYolZWn3W-kIXlsQ.png?q=20', 'https://miro.medium.com/max/3840/1*rLW2vIlaYHxq_X5Pe014jw.png', 'https://miro.medium.com/max/60/1*c08xhQQ55umSg29a9ZhdjA.png?q=20', 'https://miro.medium.com/max/3840/1*W_tcX3u7xnd4CQi_OfYjBw.png', 'https://miro.medium.com/max/60/1*WEKnvcRihEr3K8zT2PIf8A.png?q=20', 'https://miro.medium.com/max/60/1*Yur-cAQaMzvwppFvuc6oiQ.png?q=20', 'https://miro.medium.com/max/3840/1*UpRb4JYYolZWn3W-kIXlsQ.png', 'https://miro.medium.com/max/60/1*sq_HHYuJa2SIQVPTk1KLng.png?q=20', 'https://miro.medium.com/max/3840/1*QfRbulTk_Nant2IM5zjDkg.png', 'https://miro.medium.com/max/3840/1*t2w_Jk8rpTVLCqjatxkivw.png', 'https://miro.medium.com/max/3840/1*07tyQoeqf7syCPAnHW2oTg.png', 'https://miro.medium.com/max/3840/1*Y5ZwH92rBIe-jG2BfhsnSQ.png', 'https://miro.medium.com/max/3840/1*i4FYxntsQudQQGZD5vRwQg.png', 'https://miro.medium.com/max/3840/1*GHhiXV7S41ROacUyLQQ-qw.png', 'https://miro.medium.com/max/60/1*g3ScmGzIYZGu-nXhylXXAg.png?q=20', 'https://miro.medium.com/max/60/1*Snmn6M38d10Yrt36xbh6rQ.png?q=20', 'https://miro.medium.com/max/3840/1*eJquQCYgAq572dIuVBG_0g.png', 'https://miro.medium.com/fit/c/80/80/2*RXJ8MDWG2MsLXAh_Do_73w.jpeg', 'https://miro.medium.com/max/3840/1*Ug97LmEj2_kYW_xmz_0Rdg.png', 'https://miro.medium.com/max/3840/1*fzRWMl-7T6BV0ag9jB5H3w.png', 'https://miro.medium.com/max/3840/1*26ubXkFksBf8XVcL39amWg.png', 'https://miro.medium.com/max/60/1*u9shGVaqnbmPkEitTn2M3w.png?q=20', 'https://miro.medium.com/max/3840/1*u9shGVaqnbmPkEitTn2M3w.png', 'https://miro.medium.com/max/60/1*IxyaF8Wz6Ne0ydwhJ3ugng.png?q=20', 'https://miro.medium.com/max/60/1*CtLYAaCtTrCbaKoY9zawZQ.png?q=20', 'https://miro.medium.com/max/3840/1*S0apo2gTZ53_T08cPQjxXw.png', 'https://miro.medium.com/max/60/1*3pr6x67NMqCTQurlQup2vw.png?q=20', 'https://miro.medium.com/max/3840/1*BxofpKpWKVUj32DCGwOtKg.png', 'https://miro.medium.com/max/3840/1*ZGpeoGANfa3NeIHImYBkrg.png', 'https://miro.medium.com/max/3840/1*mZU5Sm4JLvCXobneZdNeZQ.png', 'https://miro.medium.com/max/3840/1*rpNnt6CXVXa3hARoK17Qlg.png', 'https://miro.medium.com/max/60/1*FoG-RHn3ZVJlOpF5LdKd2A.png?q=20', 'https://miro.medium.com/max/60/1*rLW2vIlaYHxq_X5Pe014jw.png?q=20', 'https://miro.medium.com/max/3840/1*HVJBoMfjWwyelxdToJ1yrA.png', 'https://miro.medium.com/max/60/1*PoFqwa55x1jx62fYfDaK9w.png?q=20', 'https://miro.medium.com/max/60/1*Q5teF5X8W6xdcQ0U4sQRmg.png?q=20', 'https://miro.medium.com/max/3840/1*qhhk4cTfZ3pH50w7yyqZig.png', 'https://miro.medium.com/max/3840/1*qf77TEhPND_h2VAFbK-uOQ.png', 'https://miro.medium.com/max/60/1*gzGZkBB3JavfHbW9P8Nx_g.png?q=20', 'https://miro.medium.com/max/60/1*Ti99oisoU9co_dJ9gOz41g.png?q=20', 'https://miro.medium.com/max/3840/1*dDh4bEgeHOGQoYEybSJHlg.png', 'https://miro.medium.com/max/3840/1*XgEf-p6dUeHtkgRlaIq2lg.png', 'https://miro.medium.com/max/60/1*m8ccxrgzyZ3QaKNEiraBxQ.png?q=20', 'https://miro.medium.com/max/60/1*YVO-1KCnmOxQy3msvYNs3A.png?q=20', 'https://miro.medium.com/max/60/1*BxofpKpWKVUj32DCGwOtKg.png?q=20', 'https://miro.medium.com/max/3840/1*DCrNwtNOckuR9KhgNHx43A.png', 'https://miro.medium.com/max/60/1*TZMfr5-HbKv78gaQqwdEXQ.png?q=20', 'https://miro.medium.com/max/60/1*aG4krhPn0t4EE98iFNB7zQ.png?q=20', 'https://miro.medium.com/max/3840/1*3pr6x67NMqCTQurlQup2vw.png', 'https://miro.medium.com/max/60/1*EY8YbaVi6nCRvXjHrZbOjA.png?q=20', 'https://miro.medium.com/max/3840/1*LCANCp0KDduADvzDEDUxyg.png', 'https://miro.medium.com/max/3840/1*fxSsRSx_aYJkRvsbg61XcQ.png', 'https://miro.medium.com/max/60/1*07tyQoeqf7syCPAnHW2oTg.png?q=20', 'https://miro.medium.com/max/3840/1*rIqiNwLwrVtd65ENCE-Q-w.png', 'https://miro.medium.com/max/60/1*qhhk4cTfZ3pH50w7yyqZig.png?q=20', 'https://miro.medium.com/max/60/1*mZU5Sm4JLvCXobneZdNeZQ.png?q=20', 'https://miro.medium.com/max/60/1*taAHbys-PzzOlLef-e3nRQ.png?q=20', 'https://miro.medium.com/max/60/1*EtIRu6Ta0rneXWwu0uM4Rw.png?q=20', 'https://miro.medium.com/max/3840/1*NoYiYVplK3aSKLikGokrpg.png', 'https://miro.medium.com/max/3840/1*Y706uToB4bFWfJ9lqy0O6Q.png', 'https://miro.medium.com/max/60/1*1HapODtfIOlG1CGY_4-0PA.png?q=20', 'https://miro.medium.com/max/3840/1*rnQODXlTWAr6-JVndfYc1Q.png', 'https://miro.medium.com/max/60/1*J05a2_9oAFgMpLG9x2hUMw.png?q=20', 'https://miro.medium.com/max/3840/1*Z08EAUmgKMCdpqEKDQ7qiQ.png', 'https://miro.medium.com/max/3840/1*EY8YbaVi6nCRvXjHrZbOjA.png', 'https://miro.medium.com/max/60/1*NoYiYVplK3aSKLikGokrpg.png?q=20', 'https://miro.medium.com/max/60/1*QNjv4XBS0pNBvSPP_l57Lg.png?q=20', 'https://miro.medium.com/max/3840/1*aG4krhPn0t4EE98iFNB7zQ.png', 'https://miro.medium.com/max/60/1*V97OJj86pwrzKafK0e20Ww.png?q=20', 'https://miro.medium.com/max/3840/1*FoG-RHn3ZVJlOpF5LdKd2A.png', 'https://miro.medium.com/max/3840/1*QNjv4XBS0pNBvSPP_l57Lg.png', 'https://miro.medium.com/max/60/1*rNuK3SSu1-eklMrSY0CMdQ.png?q=20', 'https://miro.medium.com/max/60/1*QfRbulTk_Nant2IM5zjDkg.png?q=20', 'https://miro.medium.com/max/3840/1*rNuK3SSu1-eklMrSY0CMdQ.png', 'https://miro.medium.com/max/3840/1*RQOdxaiE9ZtX46LipZiizw.png', 'https://miro.medium.com/max/60/1*F6ht97PCVgTxj6MT7TOsJQ.png?q=20', 'https://miro.medium.com/max/3840/1*EtIRu6Ta0rneXWwu0uM4Rw.png', 'https://miro.medium.com/max/3840/1*sq_HHYuJa2SIQVPTk1KLng.png', 'https://miro.medium.com/max/60/1*8vDxkn4uNQqLTExd-5PUZA.png?q=20', 'https://miro.medium.com/max/3840/1*aaTMaRj8pWAfOA_g6evNWQ.png', 'https://miro.medium.com/max/3840/1*W80W6bNQAPuAeeAb4nCs8Q.png', 'https://miro.medium.com/max/3840/1*3ZBTSnfSa4DkGjdmbRRUQA.png', 'https://miro.medium.com/max/3840/1*m8ccxrgzyZ3QaKNEiraBxQ.png', 'https://miro.medium.com/max/60/1*NoKrh1BMb92H2iKw4K6q3w.png?q=20', 'https://miro.medium.com/max/3840/1*g3ScmGzIYZGu-nXhylXXAg.png', 'https://miro.medium.com/fit/c/80/80/1*XTnyfuovNWBDIfCDuHbFpg.jpeg', 'https://miro.medium.com/max/3840/1*5GHZGfu0CzagYG3Dv82XKQ.png', 'https://miro.medium.com/max/60/1*mWLQXGWuPVKrcKjOzn3qzQ.png?q=20', 'https://miro.medium.com/max/60/1*DCrNwtNOckuR9KhgNHx43A.png?q=20', 'https://miro.medium.com/max/60/1*bJF1TD_Y7zEnWEhqkVaT4Q.png?q=20', 'https://miro.medium.com/max/3840/1*CtLYAaCtTrCbaKoY9zawZQ.png', 'https://miro.medium.com/max/3840/1*DShTZwwyxmbbqiUltsWaPQ.png', 'https://miro.medium.com/max/60/1*HVJBoMfjWwyelxdToJ1yrA.png?q=20', 'https://miro.medium.com/max/3840/1*rJdrZgFNBdqJ5csfdqFGWA.png', 'https://miro.medium.com/max/60/1*LCANCp0KDduADvzDEDUxyg.png?q=20', 'https://miro.medium.com/max/3840/1*oRHob1H7LjGrwccBbbaiZw.png', 'https://miro.medium.com/max/3840/1*Q5teF5X8W6xdcQ0U4sQRmg.png', 'https://miro.medium.com/max/3840/1*WEKnvcRihEr3K8zT2PIf8A.png', 'https://miro.medium.com/max/3840/1*PoFqwa55x1jx62fYfDaK9w.png', 'https://miro.medium.com/max/60/1*Z08EAUmgKMCdpqEKDQ7qiQ.png?q=20', 'https://miro.medium.com/max/3840/1*ahvTdn7FRVYoacq3gBTTlA.png', 'https://miro.medium.com/max/60/1*oRHob1H7LjGrwccBbbaiZw.png?q=20', 'https://miro.medium.com/max/60/1*vwlkhqxUDXDqpAfYSJFflw.png?q=20', 'https://miro.medium.com/max/3840/1*taAHbys-PzzOlLef-e3nRQ.png', 'https://miro.medium.com/max/3840/1*mWLQXGWuPVKrcKjOzn3qzQ.png', 'https://miro.medium.com/max/3840/1*vZ6QRauO9kj3a53i5cdHeQ.png', 'https://miro.medium.com/max/3840/1*IxyaF8Wz6Ne0ydwhJ3ugng.png', 'https://miro.medium.com/max/3840/1*bDeDA8RWeKwNdBG4_5kRdw.png', 'https://miro.medium.com/max/60/1*Y706uToB4bFWfJ9lqy0O6Q.png?q=20', 'https://miro.medium.com/max/3840/1*LGGMCuzEYvSe4GD2Cy7jDQ.png', 'https://miro.medium.com/max/60/1*wFS7zafJRFoYzEG3tivY2A.png?q=20', 'https://miro.medium.com/max/60/1*ZGpeoGANfa3NeIHImYBkrg.png?q=20', 'https://miro.medium.com/max/60/1*qf77TEhPND_h2VAFbK-uOQ.png?q=20', 'https://miro.medium.com/max/60/1*YXM42XeseZoYX9abOZVjBg.png?q=20', 'https://miro.medium.com/max/60/1*DShTZwwyxmbbqiUltsWaPQ.png?q=20', 'https://miro.medium.com/max/60/1*MnaHmkYyRMLbBxlCAnEDCA.png?q=20', 'https://miro.medium.com/max/60/1*Z9qy7EVsYcubGH7I3DqB9Q.png?q=20', 'https://miro.medium.com/max/60/1*vZ6QRauO9kj3a53i5cdHeQ.png?q=20', 'https://miro.medium.com/max/60/1*djtqXcHwiVPjGBhBu2XVXg.png?q=20', 'https://miro.medium.com/max/3840/1*Snmn6M38d10Yrt36xbh6rQ.png', 'https://miro.medium.com/max/60/1*5GHZGfu0CzagYG3Dv82XKQ.png?q=20', 'https://miro.medium.com/max/60/1*aaTMaRj8pWAfOA_g6evNWQ.png?q=20', 'https://miro.medium.com/max/60/1*RQOdxaiE9ZtX46LipZiizw.png?q=20', 'https://miro.medium.com/max/3840/1*ykU1ley-BGiSp_AA6QBbfw.png', 'https://miro.medium.com/max/60/1*Y5ZwH92rBIe-jG2BfhsnSQ.png?q=20', 'https://miro.medium.com/max/3840/1*FKvyEy8rkHnLdYAMIicIsA.png', 'https://miro.medium.com/max/60/1*i4FYxntsQudQQGZD5vRwQg.png?q=20', 'https://miro.medium.com/max/60/1*XgEf-p6dUeHtkgRlaIq2lg.png?q=20', 'https://miro.medium.com/max/60/1*FKvyEy8rkHnLdYAMIicIsA.png?q=20', 'https://miro.medium.com/max/60/1*S0apo2gTZ53_T08cPQjxXw.png?q=20', 'https://miro.medium.com/max/3840/1*EmQswC8Y2eaoLWgA04gq3Q.png', 'https://miro.medium.com/max/60/1*fxSsRSx_aYJkRvsbg61XcQ.png?q=20', 'https://miro.medium.com/max/3840/1*wFS7zafJRFoYzEG3tivY2A.png', 'https://miro.medium.com/fit/c/160/160/1*fT674PcwvOvndA8ETD2SJA.jpeg', 'https://miro.medium.com/max/60/1*Ug97LmEj2_kYW_xmz_0Rdg.png?q=20', 'https://miro.medium.com/max/60/1*pOcf82nLz13sQ1HPcxYS9Q.png?q=20', 'https://miro.medium.com/max/3840/1*Ti99oisoU9co_dJ9gOz41g.png', 'https://miro.medium.com/max/2880/1*1HapODtfIOlG1CGY_4-0PA.png', 'https://miro.medium.com/max/60/1*W_tcX3u7xnd4CQi_OfYjBw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/fit/c/96/96/1*fT674PcwvOvndA8ETD2SJA.jpeg', 'https://miro.medium.com/max/60/1*fzRWMl-7T6BV0ag9jB5H3w.png?q=20', 'https://miro.medium.com/max/60/1*W80W6bNQAPuAeeAb4nCs8Q.png?q=20', 'https://miro.medium.com/max/60/1*t2w_Jk8rpTVLCqjatxkivw.png?q=20', 'https://miro.medium.com/max/3840/1*Yur-cAQaMzvwppFvuc6oiQ.png', 'https://miro.medium.com/max/60/1*eMcT7x9SEdICP0rIy_t-Kw.png?q=20', 'https://miro.medium.com/max/3840/1*bJF1TD_Y7zEnWEhqkVaT4Q.png', 'https://miro.medium.com/max/3840/1*NoKrh1BMb92H2iKw4K6q3w.png', 'https://miro.medium.com/max/60/1*ykU1ley-BGiSp_AA6QBbfw.png?q=20', 'https://miro.medium.com/max/60/1*LGGMCuzEYvSe4GD2Cy7jDQ.png?q=20', 'https://miro.medium.com/max/3840/1*J05a2_9oAFgMpLG9x2hUMw.png', 'https://miro.medium.com/max/60/1*GvNbV1O_vRyGB53E1GJ9zA.png?q=20', 'https://miro.medium.com/max/3840/1*0ofYtMt4DJM3Rxa5QCcpGQ.png', 'https://miro.medium.com/max/3840/1*GvNbV1O_vRyGB53E1GJ9zA.png', 'https://miro.medium.com/max/3840/1*eMcT7x9SEdICP0rIy_t-Kw.png', 'https://miro.medium.com/max/3840/1*vwlkhqxUDXDqpAfYSJFflw.png', 'https://miro.medium.com/max/60/1*ahvTdn7FRVYoacq3gBTTlA.png?q=20', 'https://miro.medium.com/max/3840/1*F6ht97PCVgTxj6MT7TOsJQ.png', 'https://miro.medium.com/max/60/1*rJdrZgFNBdqJ5csfdqFGWA.png?q=20', 'https://miro.medium.com/max/60/1*rpNnt6CXVXa3hARoK17Qlg.png?q=20', 'https://miro.medium.com/max/72/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/max/60/1*GHhiXV7S41ROacUyLQQ-qw.png?q=20', 'https://miro.medium.com/max/3840/1*TZMfr5-HbKv78gaQqwdEXQ.png', 'https://miro.medium.com/max/3840/1*7jX1Fu4xe4wJPEWNuJ-jNA.png', 'https://miro.medium.com/max/60/1*0ofYtMt4DJM3Rxa5QCcpGQ.png?q=20', 'https://miro.medium.com/max/60/1*rGTQPnvNjlHBBCQDK8-Xqg.png?q=20', 'https://miro.medium.com/max/3840/1*c08xhQQ55umSg29a9ZhdjA.png', 'https://miro.medium.com/max/60/1*EmQswC8Y2eaoLWgA04gq3Q.png?q=20', 'https://miro.medium.com/max/3840/1*rGTQPnvNjlHBBCQDK8-Xqg.png', 'https://miro.medium.com/max/3840/1*YVO-1KCnmOxQy3msvYNs3A.png'}",2020-03-05 00:26:24.328722,1.550236463546753
https://medium.com/hydrosphere-io/train-and-deliver-machine-learning-models-to-production-with-a-single-button-push-a6f89dcb1bfb,Kubeflow Times Machine Learning — Reproducibility Step by Step,"Very often a workflow of training machine learning models and delivering them to production environment contains loads of manual work. These could be various steps depending on the type of model you are using, company’s workflow you are working within and requirements from the deployed model. Industry has already developed tools for a continuous software delivery/integration, but they cannot be directly applied for machine learning models, which designates the problem. In this article we will show a way to create a pipeline that connects machine learning workflow steps (like collecting & preparing data, model training, model deployment and so on) into a single reproducible run, which you can execute with a single command.

But before we actually start implementing such pipeline we should get on the same page about the general machine learning workflow. Let’s quickly recap the flow and its main aspects:

Research in the subject area;

Suppose we’ve already have a task we need to solve. We need to make a research in the subject area, define requirements for the solution, define data sources, define tools that we are going to use and methods we are going to apply. Data preparation;

We start with collecting the data, transforming it into a desirable form and version it. Model training;

We build a model and train it using the prepared data. We experiment using different architectures, run training several times while tuning hyper-parameters and evaluating model’s performance. Cataloguing;

Once the model has been trained we export it into a self-sufficient format. This means that model’s artifact can be independently used on other infrastructures without passing its source code and all dependencies along. We handle versioning of the model as well as extraction of the inputs and outputs of the model to use for deployment. Model deployment;

After the model has been exported we deploy it on the prepared infrastructure and provision HTTP/gRPC endpoints to it. Integration Testing;

We perform integration testing to ensure the model runs properly on the prepared environment. Production Inferencing;

We start inferencing on our model while collecting all requests and predictions during model’s lifetime. We provide different deployment variants, such as A/B or Canary deployments for different model versions. Performance Monitoring;

During inference time we automatically monitor how model behaves in the production environment; how data, flowing through the model, changes over a time, including insights to concept drifts, domain changes and anomaly detection. Model Maintenance;

This stage is dedicated to the model’s long-term live. We interpret model’s predictions and explain why a model behaves in a way it does. We use that knowledge to improve the model by retraining it further on the production traffic, so to increase its performance.

We will go over each of these steps throughout the article.

Tools

A short introduction to the tools we’re going to use:

Kubeflow is a machine learning toolkit for Kubernetes. It began as just a simpler way to run TensorFlow jobs on Kubernetes, but has since expanded to be a multi-architecture/multi-cloud framework for running and automating machine learning pipelines.

is a machine learning toolkit for Kubernetes. It began as just a simpler way to run TensorFlow jobs on Kubernetes, but has since expanded to be a multi-architecture/multi-cloud framework for running and automating machine learning pipelines. Hydrosphere.io is an open source model management platform for deploying, serving and monitoring machine learning models and ad-hoc algorithms.

To make a great picture of the future pipeline, let’s see how different tools within Kubeflow toolset and Hydrosphere.io platform map onto workflow steps.

The primary focus of Kubeflow ecosystem is to train machine learning models built in different frameworks in a distributed manner by leveraging the underlying Kubernetes cluster. This gives Kubeflow a noticeable advantage over manual set-ups of such training infrastructure. Model deployment is another part that Kubeflow manages to cover with niche (regarding frameworks) and over deployment tools. On top of that there’s a new service, called Kubeflow Pipelines which main goal is to connect different workflow steps into a single processing run.

Hydrosphere.io comprehends after-training part of the workflow. Its primary goals are model management, model delivery to production in simple and robust fashion, as well as monitoring models during inference time and gaining insights from predictions (for further model’s quality improvement).

Prerequisites

This tutorial presumes that you have an access to a Kubernetes cluster. If you don’t you can create your own single node cluster locally with Minikube. You will additionally need:

Locally:

On the Kubernetes Cluster:

By default Kubeflow will be installed in the kubeflow namespace. In this tutorial I assume that Hydrosphere.io is installed in the same namespace as Kubeflow.

PersistentVolumeClaims

We will additionally need to provide a persistent storage for workflow pods to retain intermediate files and supply them between stages.

$ kubectl apply -f - <<EOF

apiVersion: v1

kind: PersistentVolumeClaim

metadata:

name: data

spec:

accessModes:

- ReadWriteMany

resources:

requests:

storage: 20Gi

---

apiVersion: v1

kind: PersistentVolumeClaim

metadata:

name: models

spec:

accessModes:

- ReadWriteMany

resources:

requests:

storage: 20Gi

EOF

Kubeflow Pipelines Manifest

The whole workflow will be running on top of Kubeflow Pipelines. Pipelines are built upon Argo Workflows — a container-native workflow engine. To execute such workflow you have to create a Kubernetes blueprint and then apply it on the cluster to start the job. To ease the process of creating such .yaml definitions Kubeflow released a Python SDK library, which allows you to define your workflows as functions and then compile them to DSL.

First of all we will define a frame of that function. Create a root repository for this tutorial and define pipeline.py script in it.

$ mkdir mnist # create a root directory for this tutorial

$ touch pipeline.py

We’ve already created PVCs in our cluster, so we can supply pipeline definition with references to them.

Throughout the article I will provide short code snippets to this pipeline.py file, which will only contain differences from previous versions. The full code is available here.

To mention a few changes here — Kubeflow provides us with is the ability to customise the workflow from Pipelines UI. To do that you just have to supply parameters to the function like we do with data_directory and models_directory variables. There are some limitations using those parameters, but they would essentially disappear in the upcoming SDK versions. For now you can use these parameters in your function like so: a data_directory variable will be mapped to a {{workflow.parameters.data-directory}} string, when compiled into DSL, which we use while declaring volumes.

Let’s start with the actual machine learning model.

Step 1. Research in the Subject Area

In order to stay focused on the automation part I’ve chosen a well known digit recognition problem.

Objectives:

Given an image of the hand-written digit classify what digit it is.

Requirements:

The deployed model should be able to predict on batches of images;

Methods:

A fully-connected neural network (for implementation simplicity).

Tools:

Language — Python (contains various deep learning frameworks);

Framework — Tensorflow (has a great model exportation mechanism; has a pre-made classification architectures).

Data Sources:

MNIST (50000 train & 10000 test grayscale 28x28 images of handwritten digits).

Step 2. Data Preparation

Real world scenario often forces you to obtain training data from the company’s data storage. The case might consist of the following:

Gain access to the data warehouse; Analyse the schema and write a query for selection; Transform selected data into a desired form and store it for training.

With the chosen problem we don’t have such a reach possibility. We will use an original publicly available MNIST dataset. Let’s create the first executable that will download the data and store it in our PersistentVolumeClaim (PVC).

$ mkdir 01-download; cd 01-download

$ touch download.py

This downloads all 4 files, unpacks them, converts into numpy arrays and stores those arrays under base_dir directory. The directory in this case will be a PVC, mounted to the pod under path specified by MNIST_DATA_DIR environment variable (we specify that variable below).

To be able to execute this file on the cluster as part of the workflow we need to pack it into Docker container. Create a Dockerfile for that.

$ cat > Dockerfile << EOF

FROM python:3.6-slim

RUN pip install numpy==1.14.3 Pillow==5.2.0

ADD ./download.py /src/

WORKDIR /src/

ENTRYPOINT [ ""python"", ""download.py"" ]

EOF

We use python:3.6-slim as our base image, while additionally installing some python dependencies.

Once container definition has been created, we can build a Docker image from it and push it to the public Docker registry. As a username you can use your own Docker Hub account or use any available for you Docker Registry.

$ cd ..

$ docker build -t {username}/mnist-pipeline-download 01-download

$ docker push {username}/mnist-pipeline-download

Lastly let’s update pipeline definition.

Here we define a new environment variable (1) to be attached to our container, define a container operation (2) to be executed on the cluster and finally attach (3) required resources.

Step 3. Model Training

After our data is ready we can switch to model building and training. Typically at this stage you build a model, train it, evaluate, tune hyper-parameters and repeat that loop until model has a desired performance. So, let’s first create a directory for the executable.

$ mkdir 02-train; cd 02-train

$ touch train.py

The model is quite simple, we just use a pre-made fully-connected neural network classifier ( tf.estimator.DNNClassifier ). We specify, that the model consists of 2 hidden layers with 256 and 64 units respectively. learning_rate , num_steps and batch_size are parameters, configured by environment variables. num_steps is responsible for how long the model is going to be trained.

After model training finishes, we immediately store it in the models PVC in the tf.saved_model format, specifying inputs of the model. Outputs will be automatically inferred.

Let’s create a Docker image from that executable.

$ cat > Dockerfile << EOF

FROM tensorflow/tensorflow:1.9.0-py3

ADD ./train.py /src/

WORKDIR /src/

ENTRYPOINT [ ""python"", ""train.py"" ]

EOF $ cd ..

$ docker build -t {username}/mnist-pipeline-train 02-train

$ docker push {username}/mnist-pipeline-train

Update pipeline definition with training model container definition.

Similarly to the previous step we additionally define new environment variables (1) and define a container operation (2). We additionally specify, that this operation must be executed after the previous operation download completes. Otherwise they will start in parallel. At the bottom we define how much resources this container needs on the cluster (5). This instructs Kubeflow to start this container only when the required conditions are met.

Step 4. Model Cataloguing

Model Cataloguing is a term I would like to use to comprehend the following:

Metadata extraction:

— Graph definition and weights;

— Inputs and outputs definitions;

— Training data version / statistics;

— Other dependencies (look_up vocabularies, etc);

— Graph definition and weights; — Inputs and outputs definitions; — Training data version / statistics; — Other dependencies (look_up vocabularies, etc); Model artifact versioning;

Storing model in Repository;

This describes the model in great detail. We can use such information for model deployment since we know inputs and outputs of the model and can build an appropriate infrastructure. We can identify data that was used for training for the current model. Later on we can experiment with the different architectures and varying data versions to see if we can reach better performance. This also allows us to analyze how newer architecture works with previous data versions, hence compare the performance. We version models to compare their performance via A/B or Canary deployments (we will talk about this in the next sections).

We have already done half of this in the previous step when we exported the model to tf.saved_model format. Other steps require actions though. If we will do this manually we would need to create a repository where all of our models will be stored and versioned. Along with that we would also need to handle metadata that wasn't provisioned with the model.

$ mkdir 03-upload; cd 03-upload

$ touch execute.sh

At this stage we build a model artifact (retrieve all metadata needed for later inference and pack it along with the model binaries) and store the built artifact in a repository. The artifact is a Docker image with a frozen model inside it whereas the repository is a configured Docker registry (could be external). Along with that we supplement the model with the training data statistics, used for this model.

In case you are wondering why would we need to store a built model’s version in a file inside the pod (ii) → we can instruct Kubeflow to retrieve the contents of that file and use them in the subsequent pipeline steps (we will need it).

Let’s create a Docker image from that executable.

$ cat > Dockerfile << EOF

FROM python:3.6-slim

RUN apt update && apt install -y jq

RUN pip install hs==0.1.5

ADD ./execute.sh /src/

WORKDIR /src/

ENTRYPOINT [ ""bash"", ""execute.sh"" ]

EOF $ cd ..

$ docker build -t {username}/mnist-pipeline-upload 03-upload

$ docker push {username}/mnist-pipeline-upload

Update pipeline definition.

One minor difference from code snippets — we tell Kubeflow to store the contents of /model_version.txt under model_version key in the internal key-value repository (line 26). We will use this in a subsequent step.

Step 5. Model Deployment

The next step is the actual model deployment. At this step we have a built model artifact stored in our repository which is not yet exposed to the world. A typical way of exposing model outside of the cluster consists of creating a runtime for the model and deploying it to the cluster (REST API web server, which will inference on the model and return predictions). There are niche solutions like Tensorflow Serving, TensorRT, but they are focused on the specific frameworks. There are no way to use them for models built on different frameworks such as scikit-learn or MXNet.

Along with implementing a runtime there will come up a necessity of implementing service meshing between model versions to create Canary deployments (when you deploy, for example, three model versions and let the traffic flow through them in different proportions, like 30/30/40, exposing outside only one endpoint). You might want to connect your models in chain, where some models retrieve features and others infer on that features.

Implementing such runtimes, configuring an underlying infrastructure, managing model interactions is not a trivial job. Using Hydrosphere.io you only have to create an application.

An application here is a microservice with an exposed API endpoint to interact with your model. It allows you to use your most recent deployed production models via HTTP-requests, gRPC API calls or configure it as a part of Kafka streams. It uses predefined runtimes to run your model artifacts and helps you to configure a multi-staged execution from model to model. Within an application you can design a setup with canary or A/B model deployments.

$ mkdir 04-deploy; cd 04-deploy

$ touch execute.sh

Let’s quickly analyze the manifest and its main parts:

We define what resource this manifest describes; We define an application name; By defining a singular flag we state that this application is single-staged and contains just a single model version; Here we define what model will be deployed and its version; And finally we specify a runtime for the model.

That’s basically it. After the last command the model will be deployed within an application and exposed to the world.

Let’s create a Docker image from that step.

$ cat > Dockerfile << EOF

FROM python:3.6-slim

RUN pip install hs==0.1.5

ADD ./execute.sh /src/

WORKDIR /src/

ENTRYPOINT [ ""bash"", ""execute.sh"" ]

EOF $ cd ..

$ docker build -t {username}/mnist-pipeline-deploy 04-deploy

$ docker push {username}/mnist-pipeline-deploy

Update pipeline definition.

The minor difference here is located at line 19. We use output value from the previous upload container ( model_vesion key) and pass it as an argument to the entrypoint of the deploy container operation.

Step 6. Model Integration Testing

This step aims to test the model when it’s deployed into production-like infrastructure. It helps us to eliminate configuration bugs, deployment and integration issues. It’s important to write test suites that comprehend the following data:

A golden data set;

Edge cases;

Recent traffic.

Let’s create a client application which will invoke the deployed model and estimate accuracy.

$ mkdir 05-test; cd 05-test

$ touch test.py

generate_data function reads some amount of the testing data saved in our PVC. Then we use HTTP endpoint to reach our model, get predictions and store them in a list. At last we assert that accuracy of our model is above acceptable level. acceptable_accuracy reflects a corresponding environment variable.

Let’s create a Docker image from that step.

$ cat > Dockerfile << EOF

FROM python:3.6-slim

RUN pip install scikit-learn==0.20.0 requests==2.21.0 numpy==1.14.3

ADD ./test.py /src/

WORKDIR /src/

ENTRYPOINT [ ""python"", ""test.py"" ]

EOF $ cd ..

$ docker build -t {username}/mnist-pipeline-test 05-test

$ docker push {username}/mnist-pipeline-test

Update pipeline definition. Everything is similar to the whole pipeline definition.

Run Workflow

This was the last step we can implement within Kubeflow pipelines but the overall machine learning workflow does not end here. There’re a few important steps left but we will deal with them later.

We can compile the pipeline definition now.

$ python pipeline.py

This will produce a pipeline.tar.gz file. Let’s execute this pipeline on our cluster.

$ kubectl port-forward deployment ambassador 8085:80

This will allow you to reach Kubeflow UI from your local machine. Open http://localhost/pipeline/.","['machine', 'create', 'models', 'reproducibility', 'kubeflow', 'docker', 'workflow', 'learning', 'model', 'data', 'step', 'times', 'define', 'pipeline']","Very often a workflow of training machine learning models and delivering them to production environment contains loads of manual work.
But before we actually start implementing such pipeline we should get on the same page about the general machine learning workflow.
Hydrosphere.io is an open source model management platform for deploying, serving and monitoring machine learning models and ad-hoc algorithms.
The primary focus of Kubeflow ecosystem is to train machine learning models built in different frameworks in a distributed manner by leveraging the underlying Kubernetes cluster.
Run WorkflowThis was the last step we can implement within Kubeflow pipelines but the overall machine learning workflow does not end here.",en,['Ilnur Garifullin'],2019-07-11 09:32:27.629000+00:00,"{'Data Science', 'Automation', 'Kubeflow', 'Machine Learning', 'DevOps'}","{'https://miro.medium.com/max/60/1*U5pHBN78Rq2N-CxiM8ipuA.png?q=20', 'https://miro.medium.com/max/304/1*ZypSXsx6qKrlnJh5tXA4mQ.png', 'https://miro.medium.com/max/60/1*kC32uF2tDshChP2qqkqXNA.png?q=20', 'https://miro.medium.com/max/60/1*8Y2XVCtozG_LoFDAsg86Dg.png?q=20', 'https://miro.medium.com/max/60/1*GKToDh9lgXyaKEy9xOnKXA.png?q=20', 'https://miro.medium.com/max/60/1*tSYCeiJ9UTjAVrgqgpLnGQ.png?q=20', 'https://miro.medium.com/max/60/1*aEu7PKJpwwCItsWdkRydRw.png?q=20', 'https://miro.medium.com/max/60/0*sFo124lus8cOU4M3?q=20', 'https://miro.medium.com/max/60/1*sVgBfaddBZbt7l2A1gwLsQ.png?q=20', 'https://miro.medium.com/max/60/1*UaJ51-KEImV8-mnFi-HIeg.png?q=20', 'https://miro.medium.com/max/2628/1*UaJ51-KEImV8-mnFi-HIeg.png', 'https://miro.medium.com/max/5756/1*aFdiUxjQRjDu3bfyN6MdBA.png', 'https://miro.medium.com/max/3424/1*sVgBfaddBZbt7l2A1gwLsQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3200/0*wy6yHKdHlk1xcbCc', 'https://miro.medium.com/max/2560/1*U5pHBN78Rq2N-CxiM8ipuA.png', 'https://miro.medium.com/max/1200/1*7qYqYtWaUO5djE7fB2qJDQ.png', 'https://miro.medium.com/max/3480/1*kC32uF2tDshChP2qqkqXNA.png', 'https://miro.medium.com/fit/c/160/160/1*38nxX7YhQSfAh4C52D9kBQ.jpeg', 'https://miro.medium.com/fit/c/80/80/1*gD19hBZRqmiCqmENPfxVjg@2x.jpeg', 'https://miro.medium.com/max/4800/0*r0ITDc8cAw7VGurg', 'https://miro.medium.com/max/4580/1*gDXp_aABcTgWAOvtvYaQQA.png', 'https://miro.medium.com/max/4800/0*sFo124lus8cOU4M3', 'https://miro.medium.com/max/60/0*wy6yHKdHlk1xcbCc?q=20', 'https://miro.medium.com/max/3420/1*aEu7PKJpwwCItsWdkRydRw.png', 'https://miro.medium.com/max/3372/1*GKToDh9lgXyaKEy9xOnKXA.png', 'https://miro.medium.com/max/60/0*xOznhcrv-hJFazSZ?q=20', 'https://miro.medium.com/max/60/1*aFdiUxjQRjDu3bfyN6MdBA.png?q=20', 'https://miro.medium.com/max/60/0*r0ITDc8cAw7VGurg?q=20', 'https://miro.medium.com/fit/c/96/96/1*38nxX7YhQSfAh4C52D9kBQ.jpeg', 'https://miro.medium.com/max/3416/1*tSYCeiJ9UTjAVrgqgpLnGQ.png', 'https://miro.medium.com/max/3320/1*8Y2XVCtozG_LoFDAsg86Dg.png', 'https://miro.medium.com/max/60/1*7qYqYtWaUO5djE7fB2qJDQ.png?q=20', 'https://miro.medium.com/max/3436/1*0-4R_np930irdzonlshHlg.png', 'https://miro.medium.com/max/60/1*0-4R_np930irdzonlshHlg.png?q=20', 'https://miro.medium.com/max/306/1*ZypSXsx6qKrlnJh5tXA4mQ.png', 'https://miro.medium.com/max/3252/1*7qYqYtWaUO5djE7fB2qJDQ.png', 'https://miro.medium.com/fit/c/80/80/0*XOOG6H-QWpzLvBdZ.jpg', 'https://miro.medium.com/max/60/1*gDXp_aABcTgWAOvtvYaQQA.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*6L-JXDZQmjAyuhiAsyUUiQ.png', 'https://miro.medium.com/max/4800/0*xOznhcrv-hJFazSZ'}",2020-03-05 00:26:25.803118,1.4733948707580566
https://towardsdatascience.com/kubeflow-for-poets-a05a5d4158ce,Kubeflow for Poets,"Virtual machines (VMs) emulates the capabilities of a physical machine making it possible to install and run operating systems by using a hypervisor. The hypervisor is a piece of software on the physical machine (the host) that makes it possible to carry out virtualization where multiple guest machines are managed by the host machine.

Containers, on the other hand, isolate the environment for hosting an application with its own libraries and software dependencies, however, as opposed to a VM, containers on a machine all share the same operating system kernel. Docker is an example of a container.

Working with Docker

Begin by installing Docker software on the local machine to enable it to run Docker containers. Visit https://www.docker.com/get-started to get started.

Key concepts to note are:

Dockerfile: A Dockerfile is a text file that specifies how an image will be created.

A Dockerfile is a text file that specifies how an image will be created. Docker Images: Images are created by building a Dockerfile.

Images are created by building a Dockerfile. Docker Containers: Docker containers is the running instance of an image.

The diagram below highlights the process to build an image and run a Docker container.

Introducing DockerHub

DockerHub is a library for hosting Docker images.

Key Routines when Writing a Dockerfile

The following are key routines when creating a Dockerfile.

+------------+-----------------------------------------------------+

| Command | Description |

+------------+-----------------------------------------------------+

| FROM | The base Docker image for the Dockerfile. |

| LABEL | Key-value pair for specifying image metadata. |

| RUN | It execute commands on top of the current image as | | new layers. |

| COPY | Copies files from the local machine to the |

| container filesystem. |

| EXPOSE | Exposes runtime ports for the Docker container. |

| CMD | Specifies the command to execute when running the | | container. This command is overridden if another | | command is specified at runtime. |

| ENTRYPOINT | Specifies the command to execute when running the | | container. Entrypoint commands are not overridden |

| by a command specified at runtime. |

| WORKDIR | Set working directory of the container. |

| VOLUME | Mount a volume from the local machine filesystem to | | the Docker container. |

| ARG | Set Environment variable as a key-value pair when | | building the image. |

| ENV | Set Environment variable as a key-value pair that | | will be available in the container after building. |

+------------+-----------------------------------------------------+

Build and Run a Simple Docker Container

In this simple example, we have a bash script titled date-script.sh . The script assigns the current date to a variable and then prints out the date to the console. The Dockerfile will copy the script from the local machine to the docker container filesystem and execute the shell script when running the container. The Dockerfile to build the container is stored in docker-intro/hello-world .

# navigate to folder with images

cd docker-intro/hello-world

Let’s view the bash script.

cat date-script.sh #! /bin/sh

DATE=""$(date)""

echo ""Todays date is $DATE""

Let’s view the Dockerfile.

# view the Dockerfile

cat Dockerfile # base image for building container

FROM docker.io/alpine

# add maintainer label

LABEL maintainer=""dvdbisong@gmail.com""

# copy script from local machine to container filesystem

COPY date-script.sh /date-script.sh

# execute script

CMD sh date-script.sh

The Docker image will be built-off the Alpine Linux package. See https://hub.docker.com/_/alpine

The CMD routine executes the script when the container runs.

Build the Image

# build the image

docker build -t ekababisong.org/first_image .

Build output:

Sending build context to Docker daemon 2.048kB

Step 1/4 : FROM docker.io/alpine

latest: Pulling from library/alpine

6c40cc604d8e: Pull complete

Digest: sha256:b3dbf31b77fd99d9c08f780ce6f5282aba076d70a513a8be859d8d3a4d0c92b8

Status: Downloaded newer image for alpine:latest

---> caf27325b298

Step 2/4 : LABEL maintainer=""dvdbisong@gmail.com""

---> Running in 306600656ab4

Removing intermediate container 306600656ab4

---> 33beb1ebcb3c

Step 3/4 : COPY date-script.sh /date-script.sh

---> Running in 688dc55c502a

Removing intermediate container 688dc55c502a

---> dfd6517a0635

Step 4/4 : CMD sh date-script.sh

---> Running in eb80136161fe

Removing intermediate container eb80136161fe

---> e97c75dcc5ba

Successfully built e97c75dcc5ba

Successfully tagged ekababisong.org/first_image:latest

Run the Container

# show the images on the image

docker images REPOSITORY TAG IMAGE ID CREATED SIZE

ekababisong.org/first_image latest e97c75dcc5ba 32 minutes ago 5.52MB

alpine latest caf27325b298 3 weeks ago 5.52MB # run the docker container from the image

docker run ekababisong.org/first_image Todays date is Sun Feb 24 04:45:08 UTC 2019

Commands for Managing Images

+---------------------------------+--------------------------------+

| Command | Description |

+---------------------------------+--------------------------------+

| docker images | List all images on the | | machine. |

| docker rmi [IMAGE_NAME] | Remove the image with name | | IMAGE_NAME on the machine. |

| docker rmi $(docker images -q) | Remove all images from the | | machine. |

+------------+-----------------------------------------------------+

Commands for Managing Containers

+-------------------------------+----------------------------------+

| Command | Description |

+-------------------------------+----------------------------------+

| docker ps | List all containers. Append -a |

| to also list containers not | | running. |

| docker stop [CONTAINER_ID] | Gracefully stop the container | | with [CONTAINER_ID] on the | | machine. |

| docker kill CONTAINER_ID] | Forcefully stop the container |

| with [CONTAINER_ID] on the | | machine. |

| docker rm [CONTAINER_ID] | Remove the container with | | [CONTAINER_ID ] from the machine. |

| docker rm $(docker ps -a -q) | Remove all containers from the | | machine. |

+------------+-----------------------------------------------------+

Running a Docker Container

Let’s breakdown the following command for running a Docker container.

docker run -d -it --rm --name [CONTAINER_NAME] -p 8081:80 [IMAGE_NAME]

where,

-d : run the container in detached mode. This mode runs the container in the background.

: run the container in detached mode. This mode runs the container in the background. -it : run in interactive mode, with a terminal session attached.

: run in interactive mode, with a terminal session attached. --rm : remove the container when it exits.

: remove the container when it exits. --name : specify a name for the container.

: specify a name for the container. -p : port forwarding from host to the container (i.e. host: container).

Serve a Webpage on an nginx Web Server with Docker

The Dockerfile

# base image for building container

FROM docker.io/nginx

# add maintainer label

LABEL maintainer=""dvdbisong@gmail.com""

# copy html file from local machine to container filesystem

COPY html/index.html /usr/share/nginx/html

# port to expose to the container

EXPOSE 80

Build the image

# navigate to directory

cd docker-intro/nginx-server/ # build the image

docker build -t ekababisong.org/nginx_server . Sending build context to Docker daemon 2.048kB

Step 1/4 : FROM docker.io/nginx

latest: Pulling from library/nginx

6ae821421a7d: Pull complete

da4474e5966c: Pull complete

eb2aec2b9c9f: Pull complete

Digest: sha256:dd2d0ac3fff2f007d99e033b64854be0941e19a2ad51f174d9240dda20d9f534

Status: Downloaded newer image for nginx:latest

---> f09fe80eb0e7

Step 2/4 : LABEL maintainer=""dvdbisong@gmail.com""

---> Running in 084c2484893a

Removing intermediate container 084c2484893a

---> 2ced9e52fb67

Step 3/4 : COPY html/index.html /usr/share/nginx/html

---> 1d9684901bd3

Step 4/4 : EXPOSE 80

---> Running in 3f5738a94220

Removing intermediate container 3f5738a94220

---> 7f8e2fe2db73

Successfully built 7f8e2fe2db73

Successfully tagged ekababisong.org/nginx_server:latest # list images on machine

docker images REPOSITORY TAG IMAGE ID CREATED SIZE

ekababisong.org/nginx_server latest 0928acf9fcbf 18 hours ago 109MB

ekababisong.org/first_image latest 773973d28958 20 hours ago 5.53MB

Run the container

# run the container

docker run -d -it --name ebisong-nginx -p 8081:80 ekababisong.org/nginx_server # list containers

docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

b3380cc02551 ekababisong.org/nginx_server ""nginx -g 'daemon of…"" 7 seconds ago Up 4 seconds 0.0.0.0:8081->80/tcp ebisong-nginx

View Webpage on the Running Server

Open a web browser and go to: 0.0.0.0:8081

Cleanup

# shutdown the container

docker stop b3380cc02551 # remove the container

docker rm ebisong-nginx

Push Image to Dockerhub

Login to Docker and provide your userid and password .

# login to docker

docker login # tag the image

docker tag 096e538abc1e ekababisong/ebisong-nginx-server:latest # push to Dockerhub

docker push ekababisong/ebisong-nginx-server The push refers to repository [docker.io/ekababisong/ebisong-nginx-server]

db4c3e547e3f: Pushed

6b5e2ed60418: Mounted from library/nginx

92c15149e23b: Mounted from library/nginx

0a07e81f5da3: Mounted from library/nginx

latest: digest: sha256:733009c33c6cf2775fedea36a3e1032006f1fe3d5155f49d4ddc742ea1dce1f1 size: 1155

Volumes

Local directories can be mounted as a volume to a running container, instead of the container filesystem itself. With volumes, the data can be shared with the container, while persisted on the local machine. Volumes are attached with the -v label in the docker run command.

Mount a Volume to the Containerized nginx Web Server

docker run -d -it --name ebisong-nginx -p 8081:80 -v /Users/ekababisong/Documents/Talks/kubeflow-for-poets/docker-intro/nginx-server/html:/usr/share/nginx/html ekababisong.org/nginx_server

Now whatever changes are made to the file index.html is immediately seen on the web browser from the nginx server in the Docker container.

Run a Tensorflow Jupyter Image from Dockerhub

Note: This image is large and will take a while to pull from Dockerhub.

# pull the image from dockerhub

docker pull jupyter/tensorflow-notebook Using default tag: latest

latest: Pulling from jupyter/tensorflow-notebook

a48c500ed24e: Pull complete

...

edbe68d32a46: Pull complete

Digest: sha256:75f1ffa1582a67eace0f96aec95ded82ce6bf491e915af80ddc039befea926aa

Status: Downloaded newer image for jupyter/tensorflow-notebook:latest

Run the Tensorflow Jupyter Container

This command starts an ephemeral container running a Jupyter Notebook server and exposes the server on host port 8888. The server logs appear in the terminal. Visiting http://<hostname>:8888/?token=<token> in a browser loads the Jupyter Notebook dashboard page. It is ephemeral because Docker destroys the container after the notebook server exits. This is because of the --rm label in the docker run command.

# run the image

docker run --rm -p 8888:8888 jupyter/tensorflow-notebook

The image below shows a Notebook running from a Docker container.","['machine', 'kubeflow', 'docker', 'image', 'run', 'container', 'command', 'images', 'build', 'poets', 'running', 'dockerfile']","Working with DockerBegin by installing Docker software on the local machine to enable it to run Docker containers.
Docker Images: Images are created by building a Dockerfile.
Build the Image# build the imagedocker build -t ekababisong.org/first_image .
Volumes are attached with the -v label in the docker run command.
This is because of the --rm label in the docker run command.",en,['Ekaba Bisong'],2019-04-20 01:35:23.583000+00:00,"{'Deep Learning', 'Software Development', 'Kubernetes', 'Machine Learning', 'Docker'}","{'https://miro.medium.com/max/6150/1*XJRbVzLeaX5MsDeZwX5LSQ.png', 'https://miro.medium.com/max/4276/1*J52De29jInAzByroyDf5QA.png', 'https://miro.medium.com/max/4912/0*xZQFeiVq7cvEZt6j.png', 'https://miro.medium.com/max/40/1*7d63ehDojPBrMlRbRJoQFg.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/0*XtNpJ990APMgs0xH.png?q=20', 'https://miro.medium.com/max/5200/0*EX83HxqpXbx7f5WC.png', 'https://miro.medium.com/max/1500/1*NF7SSEHtoiEW7Jt2OzqBBQ.png', 'https://miro.medium.com/max/5076/0*w5MiRzHwRkXnIZM4.png', 'https://miro.medium.com/max/60/0*rnlUS_wn7bPOwTFd.png?q=20', 'https://miro.medium.com/max/4996/0*Ruvky_xi4D5btxTn.png', 'https://miro.medium.com/max/5060/1*5-Pgz2sgPFbmYT0uZGDdcg.png', 'https://miro.medium.com/max/60/0*MbZ_2hZyWrONNxZK.png?q=20', 'https://miro.medium.com/max/60/0*AU2EnUCo6aYIlzH6.png?q=20', 'https://miro.medium.com/max/3754/1*NgGy9E4UDiz6XxwZkauFDQ.png', 'https://miro.medium.com/max/60/0*N6aDODoWMXfM6Khk.png?q=20', 'https://miro.medium.com/max/60/0*hRzP-dy6qD-YSmFr.png?q=20', 'https://miro.medium.com/max/60/0*vg7BrWIBg1__136y.png?q=20', 'https://miro.medium.com/max/4996/0*8akt-bWVTw241Lqx.png', 'https://miro.medium.com/max/1200/1*PshD8n5B_7h-cmmFYpurqA.png', 'https://miro.medium.com/max/4282/1*PshD8n5B_7h-cmmFYpurqA.png', 'https://miro.medium.com/max/4924/0*rnlUS_wn7bPOwTFd.png', 'https://miro.medium.com/max/5012/0*XtNpJ990APMgs0xH.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/60/0*3wyarYiopRjL6Y3E.png?q=20', 'https://miro.medium.com/max/4570/1*C9gRZk6viJybsG8LSOkL6A.png', 'https://miro.medium.com/max/60/1*5-Pgz2sgPFbmYT0uZGDdcg.png?q=20', 'https://miro.medium.com/max/5200/0*hRzP-dy6qD-YSmFr.png', 'https://miro.medium.com/max/60/1*NgGy9E4UDiz6XxwZkauFDQ.png?q=20', 'https://miro.medium.com/max/60/0*zyCXM6mLuOVlbC6x.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/4996/0*0dRuTjTqUXexp5_Z.png', 'https://miro.medium.com/max/3304/0*LyvNyxxCDcLGLkli.png', 'https://miro.medium.com/max/5016/0*AU2EnUCo6aYIlzH6.png', 'https://miro.medium.com/max/60/1*C9gRZk6viJybsG8LSOkL6A.png?q=20', 'https://miro.medium.com/max/54/1*WXsalHwcS3b7dO5OSupF2A.png?q=20', 'https://miro.medium.com/max/60/0*0cTLmUvNI3tcENcy.png?q=20', 'https://miro.medium.com/max/60/1*XJRbVzLeaX5MsDeZwX5LSQ.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/0*xZQFeiVq7cvEZt6j.png?q=20', 'https://miro.medium.com/max/5364/1*4K-ZgDsNnZx5L6ET80PsgA.png', 'https://miro.medium.com/max/60/1*S-HVouQEX4qu3P9pg9HgFA.png?q=20', 'https://miro.medium.com/max/60/0*B-1iQ05Uxr4KdpFA.png?q=20', 'https://miro.medium.com/max/60/0*R3l1FRhWvYiRHESa.png?q=20', 'https://miro.medium.com/max/5060/0*ToX8B56lwvJafgNI.png', 'https://miro.medium.com/fit/c/160/160/2*GpEIa8d3Dycyao7Di6dzJA.png', 'https://miro.medium.com/max/60/0*wfi6xk6h-yAWnyc_.png?q=20', 'https://miro.medium.com/max/5200/0*wfi6xk6h-yAWnyc_.png', 'https://miro.medium.com/max/56/0*LyvNyxxCDcLGLkli.png?q=20', 'https://miro.medium.com/max/4368/1*S-HVouQEX4qu3P9pg9HgFA.png', 'https://miro.medium.com/max/4764/0*0cTLmUvNI3tcENcy.png', 'https://miro.medium.com/max/5056/0*N6aDODoWMXfM6Khk.png', 'https://miro.medium.com/max/5052/0*B-1iQ05Uxr4KdpFA.png', 'https://miro.medium.com/max/5060/0*YkFUXKNyVgI_izr0.png', 'https://miro.medium.com/max/5200/0*U8_FETPSk6vkYueC.png', 'https://miro.medium.com/max/60/0*GkQzKxb54_LIIjCM.png?q=20', 'https://miro.medium.com/max/60/0*0dRuTjTqUXexp5_Z.png?q=20', 'https://miro.medium.com/max/60/0*U8_FETPSk6vkYueC.png?q=20', 'https://miro.medium.com/max/60/0*8akt-bWVTw241Lqx.png?q=20', 'https://miro.medium.com/max/60/0*j0CdSxKKQcjlNAJc.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/0*EX83HxqpXbx7f5WC.png?q=20', 'https://miro.medium.com/max/5052/0*vg7BrWIBg1__136y.png', 'https://miro.medium.com/max/4980/0*3wyarYiopRjL6Y3E.png', 'https://miro.medium.com/max/5040/0*zyCXM6mLuOVlbC6x.png', 'https://miro.medium.com/max/60/1*4K-ZgDsNnZx5L6ET80PsgA.png?q=20', 'https://miro.medium.com/max/60/1*PshD8n5B_7h-cmmFYpurqA.png?q=20', 'https://miro.medium.com/max/60/0*Ruvky_xi4D5btxTn.png?q=20', 'https://miro.medium.com/max/3240/0*GkQzKxb54_LIIjCM.png', 'https://miro.medium.com/max/60/1*J52De29jInAzByroyDf5QA.png?q=20', 'https://miro.medium.com/max/60/0*YkFUXKNyVgI_izr0.png?q=20', 'https://miro.medium.com/max/1898/1*7d63ehDojPBrMlRbRJoQFg.png', 'https://miro.medium.com/max/60/0*ToX8B56lwvJafgNI.png?q=20', 'https://miro.medium.com/fit/c/96/96/2*GpEIa8d3Dycyao7Di6dzJA.png', 'https://miro.medium.com/max/1886/1*WXsalHwcS3b7dO5OSupF2A.png', 'https://miro.medium.com/max/2864/0*j0CdSxKKQcjlNAJc.png', 'https://miro.medium.com/max/60/0*w5MiRzHwRkXnIZM4.png?q=20', 'https://miro.medium.com/max/5200/0*R3l1FRhWvYiRHESa.png', 'https://miro.medium.com/max/60/1*NF7SSEHtoiEW7Jt2OzqBBQ.png?q=20', 'https://miro.medium.com/max/5008/0*MbZ_2hZyWrONNxZK.png'}",2020-03-05 00:26:33.333367,7.530249118804932
https://towardsdatascience.com/kubeflow-components-and-pipelines-33a1aa3cc338,Kubeflow Components and Pipelines,"Kubeflow Components and Pipelines

Kubeflow is a large ecosystem, a stack of different open source tools ML tools.

I want to keep things simple therefore we cover components, pipelines and experiments. With pipelines and components, you get the basics which are required to build ML workflows.

There are many more tools integrated into Kubeflow and I will cover them in the upcoming posts.

Kubeflow is originated at Google.

Making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.

source: Kubeflow website

Photo by Ryan Quintal on Unsplash

Goals

Demonstrate how to build pipelines.

pipelines. Demonstrate how to create components.

components. Demonstrate how to use components.

components. Demonstrate how to run pipelines and experiments inside of a Notebook.

pipelines and experiments inside of a Notebook. Easy to understand and ready to use examples.

Pipelines

Component

Code that performs one step in the Pipeline. In other words a containerized implementation of an ML task.

A component is analog to a function, it has a name, parameter, return values and a body. Each component in a pipeline executes independently and has to be packed as a docker image.

The components.

Graph

The representation between the components. It shows the steps your pipeline is executing.

The graph.

Pipeline

A pipeline describes the machine learning workflow, it includes the components and the graph.

The pipeline.

Run

A run is a single execution of a pipeline, for comparison all runs are kept.

A run is a single execution of a Pipeline.

Recurring runs

Recurring runs can be used to repeat a pipeline run, useful if we want to train an updated model version on new data in a scheduled manner.

Experiment

Similar to a workspace, it contains the different runs. Runs can be compared.

An overview of all runs in this specific experiment.

Component Types

Kubeflow contains two types of components, one for rapid development and one for re-usability.

Lightweight Component

Used for fast development in a notebook environment. Fast and easy cause there is no need to build container images.

Lightweight components cannot be reused.

Reusable Component

Can be reused by loading it into a Kubeflow pipeline. It is a containerized component.

Require more implementation time.

Reusable Component

In this section, you will get the basics of a reusable component.

Component Structure

A component itself is simple and consists of just a few parts:

The component logic

A component specification as yaml.

as yaml. A Dockerfile which is required to build the container.

which is required to build the container. A readme to explain the component and its inputs and outputs.

to explain the component and its inputs and outputs. A helper script to build the component and push it to a Docker repository.

Parts of a reusable Kubeflow component.

Component Specification

This specification describes the container component data model for Kubeflow Pipelines.

Written in YAML format (component.yaml).

Metadata describe the component itself, like name and description

describe the component itself, like name and description Interface defines the input and the output of the component.

defines the input and the output of the component. Implementation specifies how the component should be executed.

Handling Input

A component requires usually some kind of input, like a path to our training data or the name of our model. It can consume multiple inputs.

Define inputs in the component.yaml

in the component.yaml Define the input as arguments for our container.

the input as for our container. Parse arguments in the component logic.

The training component might require training data as input and produces a model as output.

Handling Output

The output is required to pass data between components. It is important to know that each component in a pipeline executes independently.

Components run in different processes and cannot share data.

The process for passing small data differs from large data.

For small data

Values can be passed directly as output.

For large data

Large data has to be serialized to files to be passed between components.

has to be to files to be passed between components. Upload the data to our storage system.

the data to our storage system. And pass a reference to this file to the next component.

a to this file to the next component. The next component in the pipeline will take this reference and download the serialized data.

Dockerize Component

Each component is a container image which requires a dockerfile in order to build an image.

After the image is built we push the component container image to the Google Container Registry.

Build and upload the component container image to the Google Container Registry is just a few lines of code:

# build_image.sh

image_name=gcr.io/ml-training/kubeflow/training/train

image_tag=latest full_image_name=${image_name}:${image_tag} docker build --build-arg -t ""${full_image_name}""

docker push ""$full_image_name""

It happens that with the first docker push you might get the following error message:

You don’t have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

In that case, just run the following gcloud command and push again:

$ gcloud auth configure-docker

Use the Pipeline

Load the Component

Everyone with access to the Docker repository and the component.yaml can use the component in Pipelines.

Load a component from a component.yaml URL.

The component can then be loaded based on the component.yaml.

Create the Pipeline

The dsl decorator is provided via the pipeline SDK and is used to define and interact with pipelines. dsl.pipeline defines a decorator for Python functions which returns a pipeline.



name='The name of the pipeline',

description='The description of the pipeline'

)

def sample_pipeline(parameter):

concat = operation(parameter=first) @dsl .pipeline(name='The name of the pipeline',description='The description of the pipeline'def sample_pipeline(parameter):concat = operation(parameter=first)

Compile the Pipeline

To compile the pipeline we use the compiler.Compile() function which is again part of the pipeline SDK. The compiler generates a yaml definition which is used by Kubernetes to create the execution resources.

pipeline_func = sample_pipeline

pipeline_filename = pipeline_func.__name__ + '.pipeline.zip' compiler.Compiler().compile(sample_pipeline,

pipeline_filename)

Create an Experiment

Pipelines are always part of an experiment and can be created with the Kubeflow Pipeline Client kfp.client() . Experiments cannot be removed at the moment.

client = kfp.Client() try:

experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)

except:

experiment = client.create_experiment(EXPERIMENT_NAME)



print(experiment)

Run the Pipeline

To run a pipeline we use the experiment id and the compiled pipeline created in the previous steps. client.run_pipeline runs the pipelines and provides a direct link to the Kubeflow experiment.

run_name = pipeline_func.__name__ + ' run'

run_result = client.run_pipeline(experiment.id,

run_name,

pipeline_filename)

Examples on GitHub

I created a basic pipeline which demonstrates everything presented in this post. To keep things simple the pipeline does not contain any ML specific implementation.

https://github.com/SaschaHeyer/Machine-Learning-Training/tree/master/kubeflow/reusable-component-training","['kubeflow', 'components', 'runs', 'container', 'data', 'build', 'run', 'pipelines', 'input', 'component', 'pipeline']","Kubeflow Components and PipelinesKubeflow is a large ecosystem, a stack of different open source tools ML tools.
Reusable ComponentCan be reused by loading it into a Kubeflow pipeline.
Component StructureA component itself is simple and consists of just a few parts:The component logicA component specification as yaml.
Component SpecificationThis specification describes the container component data model for Kubeflow Pipelines.
pipeline_func = sample_pipelinepipeline_filename = pipeline_func.__name__ + '.pipeline.zip' compiler.Compiler().compile(sample_pipeline,pipeline_filename)Create an ExperimentPipelines are always part of an experiment and can be created with the Kubeflow Pipeline Client kfp.client() .",en,['Sascha Heyer'],2020-01-04 09:03:52.950000+00:00,"{'AI', 'Kubeflow', 'Google', 'Machine Learning', 'Kubeflow Pipelines'}","{'https://miro.medium.com/max/60/1*u1HDPQbPkzhSL6brpPg5kw.png?q=20', 'https://miro.medium.com/max/60/1*hR0_VvZBjBeEXQlZDSNvaQ.png?q=20', 'https://miro.medium.com/max/30/1*m2oTCQZgo_pJ6-53knxfZA.png?q=20', 'https://miro.medium.com/max/1200/1*VyxpwYqKlI2qzCU4ZWxoUw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*bxLGGOqr0kw9fypivjSGXg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3286/1*YN1P-2pkbka1mOWILWCSGA.png', 'https://miro.medium.com/max/60/1*YN1P-2pkbka1mOWILWCSGA.png?q=20', 'https://miro.medium.com/freeze/max/60/1*UTXF-jw4SzeyZ9n_ES10Uw.gif?q=20', 'https://miro.medium.com/max/2066/1*lu2ey7-IPpuvPRhTVSxs7Q.png', 'https://miro.medium.com/max/4616/1*u1HDPQbPkzhSL6brpPg5kw.png', 'https://miro.medium.com/max/60/1*VyxpwYqKlI2qzCU4ZWxoUw.jpeg?q=20', 'https://miro.medium.com/max/60/1*lu2ey7-IPpuvPRhTVSxs7Q.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/8064/1*VyxpwYqKlI2qzCU4ZWxoUw.jpeg', 'https://miro.medium.com/max/3680/1*UTXF-jw4SzeyZ9n_ES10Uw.gif', 'https://miro.medium.com/fit/c/160/160/1*FJbvWVHGQjS1tkPaK6BD6A.jpeg', 'https://miro.medium.com/max/48/1*AhW372wGfahhM7_j-D53Uw.png?q=20', 'https://miro.medium.com/max/596/1*AhW372wGfahhM7_j-D53Uw.png', 'https://miro.medium.com/max/1216/1*hR0_VvZBjBeEXQlZDSNvaQ.png', 'https://miro.medium.com/max/60/1*VWjeXXKOAK1o6lZv-FC5eQ.png?q=20', 'https://miro.medium.com/max/1956/1*bxLGGOqr0kw9fypivjSGXg.png', 'https://miro.medium.com/fit/c/96/96/1*FJbvWVHGQjS1tkPaK6BD6A.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/766/1*m2oTCQZgo_pJ6-53knxfZA.png', 'https://miro.medium.com/max/3048/1*VWjeXXKOAK1o6lZv-FC5eQ.png'}",2020-03-05 00:26:39.244568,5.9101667404174805
https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c,"Kubeflow v0.6: support for artifact tracking, data versioning & multi-user","The Kubeflow Community is excited to announce the release of Kubeflow v0.6, which introduces new primitives to enable artifact tracking, and data versioning in Istio-based multi-user environments.

Early user comments

Per Jeff Fogarty, an Innovation Engineer at US Bank and a Kubeflow user and contributor, “Multi-user functionality is a foundational building block, especially for on-prem environments, and we are excited to integrate this enhancement into our deployment.” Laura Schornack, who is using Kubeflow as a part of the City Scholars partnership between Chase and the University of Illinois added, “Data versioning, especially for Kubeflow Pipelines, allows us to snapshot datasets and recreate models quickly. This significantly simplifies workflows and improves productivity.”

New feature overview

For artifact tracking, Kubeflow v0.6 introduces a new metadata component, along with a metadata API and initial corresponding clients. These allow users to track their artifacts and execution contexts through an end-to-end ML workflow. Users are now able to interact with the metadata component from inside their Notebooks or from Kubeflow Pipelines. In addition, metadata comes with an intuitive web UI to view a list of artifacts and detailed information of each individual artifact.

Kubeflow v0.6 extends the Kubeflow Pipelines’ Domain Specific Language (DSL) by adding new primitives that enable data and execution context versioning in Kubeflow Pipelines. Kubeflow Pipelines now expose two new resources: Persistent Volume, and Volume Snapshot. Both are integrated via standards-based Kubernetes PVC primitives backed by the latest functionality of the Container Storage Interface (CSI).

v0.6 also delivers a secure architecture for multi-user support by leveraging a new integration with Istio. The architecture provides flexible options to integrate Kubeflow with authentication services in cloud and on-prem environments.

In addition, Kubeflow 0.6 delivers several documentation updates and valuable new operational and configuration capabilities, most notably the introduction of Kustomize as a complete replacement of ksonnet. The following provides more details on the major deliveries in Kubeflow 0.6:

Artifact Tracking

Superior model builders need to track the intermediate and final artifacts during their end-to-end ML workflows. When artifact tracking is combined with versioning, the model builders have access to completely reproducible results, which accelerates their model iteration and re-creation.

Kubeflow v0.6 introduces several enhancements including:

an initial metadata schema to track artifacts related to execution contexts,

a Metadata API for storing and retrieving metadata,

a new Metadata component for storing and serving this metadata,

an initial client libraries for end-users to interact with the Metadata service from their Notebooks or Pipelines code.

The new architecture allows for the definition of arbitrary artifacts to be tracked. Kubeflow v0.6 ships with three pre-defined artifacts:

Models

Datasets

Evaluation Metrics

The Models and Datasets can be visualized in the Metadata Web UI so that end-users can start tracking them out-of-the-box, with no extra work required.

The following demonstrates the artifact tracking enhancements using an XGBoost training example with Fairing and Metadata logging for a Housing Price Prediction model:

Zoom into the metadata logging setup to represent the training process.

Metadata UI: artifact list view with filtering and sorting.

Metadata UI: model details for various versions with different HParams.

Versioning for Data Volumes

Kubeflow v0.6 extends the Kubeflow Pipelines’ DSL to seamlessly support the use of Persistent Volumes and Volume Snapshots as distinct Kubeflow Pipelines resources. The storage volumes are managed by standards-based, vendor-neutral Kubernetes primitives, namely PersistentVolumeClaim and VolumeSnapshot API objects. These primitives simplify the delivery of persistent data volumes to Kubeflow Pipelines users and eliminate the burden of manually manipulating low-level K8s objects as part of a Pipeline workflow. Pipelines can now exchange multi-GB data via standard file I/O on mounted Persistent Volumes, without having to upload & download data to & from external object stores.

In v0.6, Kubeflow Pipelines supports data versioning via integration with external storage and data management systems. For example, the primitives are easily added to critical Pipeline steps and provide for immutable versioned snapshots. In addition, the extensions enable simplified data sharing between Kubeflow components, i.e. attaching Pipelines results into Notebooks for further experimentation. They also lay the foundation a feature candidate for our next release: snapshotting a step’s whole execution context, including data (volumes) and metadata (K8s objects).

Building and deploying ML pipelines (faster) is a core value of Kubeflow. With the v0.6 functionality, we reduce the steps required to create and run Kubeflow Pipelines with versioning for data volumes. As superior pipelines might require scores of runs, an automated version tracking solution enables data scientists to build better models faster. This also supports pipeline reproducibility, critical for ML projects that have compliance, auditing and bias analysis requirements.

With v0.6, we’ve reduced the number of steps needed to build and run a pipeline in an on-prem environment by ~50%. Data scientists have less need to wait for an ML engineer to create persistent volumes and seed data into their pipelines (via YAML files and low-level kubernetes commands). For example, in the popular Chicago Taxi Cab demo, a data scientist developing models on-prem can now build and deploy the complete pipeline from a Python notebook and the Pipelines UI:

Pipelines Improvements

Kubeflow v0.6 also includes a number of new features and bug fixes for Pipelines, including support for pre-emptible VMs, which can enable large reductions in usage costs.

We have also introduced a number of usability and UI improvements, such as streamlined run creation, improved visualization of pipeline metadata, support for default experiments, and 10x performance improvements when handling UI queries. v0.6 also introduced a new Kubernetes controller for managing Tensorboard instances, which are tightly integrated with the UI.

The Pipelines authoring SDK also has been updated with a number of new features and the documentation has been comprehensively revamped. Notably, the SDK now allows free-form Python functions to be packaged as pipeline components.

Finally, community-contributed pipelines continue to grow, along with pipelines working against Google Cloud, IBM Cloud and Watson, and AWS Sagemaker instances. A full list of new features can be found in the changelog.

Multi-user Authentication & Authorization with SSO

Kubeflow v0.6 provides a flexible architecture for multi-user isolation and Single Sign-on (SSO). It leverages Istio and K8s namespaces, which incorporate the new “Profiles” K8s Custom Resource. These building blocks enable dynamic per-user creation of namespaces, and each user can run isolated by default. In addition to isolation, Kubeflow’s new Istio functionality enables integration with authentication services, i.e. LDAP and/or Active Directory along with RBAC-based authorization services. If needed, this configuration can be installed and operated in air gapped environments, which do not have internet connectivity. For a detailed description of the Istio integration with independent OIDC providers on-prem, please take a look at this post:

Kubeflow 0.6 operational updates and improvements

In addition, Kubeflow v0.6 includes valuable operational updates and improvements, including the replacement of ksonnet with kustomize. The Kubeflow and kustomize technical teams have similar philosophies, and are already collaborating on advanced use cases. Please find a quick summary on “Why Kustomize?” below:

Easier to read, YAML

Integrated with K8s

Kubectl level commands, no extra tools

Supports the common customizations needs

images, env, secrets, config maps

Kustomize Overlays provide for easier extensions

Support complex parameterization or customization without PR

Doesn’t create another API

In addition, we are also happy to announce two Kubernetes / Kubeflow operators that have graduated from incubating to graduating: TFJob and PyTorchJob. These operators have matured their code, documentation, v1 APIs and test plans.

Kubeflow doc sprint

The Kubeflow community held our first doc sprint on July 10–12. For three days, in time zones around the world, we worked together to write documentation and build samples that help people understand and use Kubeflow. This allowed us to merge 27 pull requests and counting, closing 28 docs issues. You can find more details in our blog post.

What’s next

With this release under our belts, the community is starting to plan for the v0.7 release, which will focus on enhancing data scientist usability. Some work areas include:

Simplifying Tensorboard creation and management

Enhancing Metadata functionality with cluster-level logging and parameter-setting values from Kubeflow subsystems i.e. Pipelines, Katib, TFJob, etc.

Building a GUI-based volume manager to simplify the access and sharing of data between Kubeflow subsystems, and between users.

Defining operational improvements such as simplified upgradability for Kubeflow and its APIs.

Offering additional architectural options for multi-user authentication and authorization, as detailed in this Multi-User Critical User Journey (CUJ).

You can follow our release progress in the 0.7 Kanban board. The Kubeflow roadmap is driving our development towards Kubeflow 1.0, and we welcome your input!

Community-driven development

We put a lot of work in improving Kubeflow’s stability, fit and finish over 150+ closed issues and 250+ merged PRs. For this release, the community gathered extensive end-user input from Kubecon Barcelona, Kubeflow User Surveys, Kubeflow Community Meetings and several reviews of the Customer User Journeys (CUJs) we use to define core experiences to build for each release.

Finally, thanks to all who contributed to v0.6! Kubeflow is home to 150+ contributors from 25+ organizations working together to build a Kubernetes-native, portable and scalable ML stack, and we need even more help. Here’s how to get involved:

Thanks to Jeff Fogarty (US Bank), Laura Schornack (City Scholars/Chase), Constantinos Venetsanopoulos (Arrikto), Animesh Singh (IBM), Johnu George (Cisco), Zhenghui Wang, Pavel Dournov, Sarah Maddox, Abishek Gupta, Anand Iyer, Ajay Gopinathan and Thea Lamkin (all of Google) for contributing to this post.","['versioning', 'metadata', 'ui', 'kubeflow', 'user', 'v06', 'artifact', 'data', 'pipelines', 'tracking', 'support', 'multiuser', 'pipeline', 'volumes']","The Kubeflow Community is excited to announce the release of Kubeflow v0.6, which introduces new primitives to enable artifact tracking, and data versioning in Istio-based multi-user environments.
Kubeflow v0.6 extends the Kubeflow Pipelines’ Domain Specific Language (DSL) by adding new primitives that enable data and execution context versioning in Kubeflow Pipelines.
Versioning for Data VolumesKubeflow v0.6 extends the Kubeflow Pipelines’ DSL to seamlessly support the use of Persistent Volumes and Volume Snapshots as distinct Kubeflow Pipelines resources.
In v0.6, Kubeflow Pipelines supports data versioning via integration with external storage and data management systems.
With the v0.6 functionality, we reduce the steps required to create and run Kubeflow Pipelines with versioning for data volumes.",en,['Josh Bottum'],2019-07-31 20:03:39.900000+00:00,"{'Kubeflow Pipelines', 'Kubeflow', 'Kubernetes', 'Machine Learning', 'TensorFlow'}","{'https://miro.medium.com/max/1908/0*bc1ab7O19p9_I75R', 'https://miro.medium.com/max/2520/0*vxkNtmDEv4Nb-CFo', 'https://miro.medium.com/max/60/0*pdleILFsEtJ0oyPD?q=20', 'https://miro.medium.com/max/2520/0*pdleILFsEtJ0oyPD', 'https://miro.medium.com/fit/c/160/160/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/fit/c/80/80/1*XTnyfuovNWBDIfCDuHbFpg.jpeg', 'https://miro.medium.com/max/2520/0*1gyVZm4iOdx6caPL', 'https://miro.medium.com/max/60/0*1gyVZm4iOdx6caPL?q=20', 'https://miro.medium.com/max/60/0*iZRC2FRjHcoGLKUD?q=20', 'https://miro.medium.com/fit/c/80/80/2*8qYzMnCauVpCpfq0QH-Hrw.jpeg', 'https://miro.medium.com/max/2520/0*iZRC2FRjHcoGLKUD', 'https://miro.medium.com/fit/c/96/96/2*RXJ8MDWG2MsLXAh_Do_73w.jpeg', 'https://miro.medium.com/fit/c/160/160/2*RXJ8MDWG2MsLXAh_Do_73w.jpeg', 'https://miro.medium.com/fit/c/80/80/2*RXJ8MDWG2MsLXAh_Do_73w.jpeg', 'https://miro.medium.com/max/60/0*bc1ab7O19p9_I75R?q=20', 'https://miro.medium.com/max/72/1*N6-Y5wQnibxqff24-GilSg.png', 'https://miro.medium.com/max/56/0*vxkNtmDEv4Nb-CFo?q=20', 'https://miro.medium.com/max/2520/0*WEGlWDgNF9ejrtlE', 'https://miro.medium.com/max/60/0*WEGlWDgNF9ejrtlE?q=20', 'https://miro.medium.com/max/1200/0*WEGlWDgNF9ejrtlE'}",2020-03-05 00:26:40.464530,1.2199623584747314
https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da,Named Entity Recognition with NLTK and SpaCy,"Named Entity Recognition with NLTK and SpaCy

NER is used in many fields in Natural Language Processing (NLP)

Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions, such as:

Which companies were mentioned in the news article?

Were specified products mentioned in complaints or reviews?

Does the tweet contain the name of a person? Does the tweet contain this person’s location?

This article describes how to build named entity recognizer with NLTK and SpaCy, to identify the names of things, such as persons, organizations, or locations in the raw text. Let’s get started!

NLTK

import nltk

from nltk.tokenize import word_tokenize

from nltk.tag import pos_tag

Information Extraction

I took a sentence from The New York Times, “European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.”

ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'

Then we apply word tokenization and part-of-speech tagging to the sentence.

def preprocess(sent):

sent = nltk.word_tokenize(sent)

sent = nltk.pos_tag(sent)

return sent

Let’s see what we get:

sent = preprocess(ex)

sent

Figure 1

We get a list of tuples containing the individual words in the sentence and their associated part-of-speech.

Now we’ll implement noun phrase chunking to identify named entities using a regular expression consisting of rules that indicate how sentences should be chunked.

Our chunk pattern consists of one rule, that a noun phrase, NP, should be formed whenever the chunker finds an optional determiner, DT, followed by any number of adjectives, JJ, and then a noun, NN.

pattern = 'NP: {<DT>?<JJ>*<NN>}'

Chunking

Using this pattern, we create a chunk parser and test it on our sentence.

cp = nltk.RegexpParser(pattern)

cs = cp.parse(sent)

print(cs)

Figure 2

The output can be read as a tree or a hierarchy with S as the first level, denoting sentence. we can also display it graphically.","['persons', 'recognition', 'record', 'nltk', 'entity', 'noun', 'used', 'spacy', 'sentence', 'times', 'named', 'text']","Named Entity Recognition with NLTK and SpaCyNER is used in many fields in Natural Language Processing (NLP)Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
This article describes how to build named entity recognizer with NLTK and SpaCy, to identify the names of things, such as persons, organizations, or locations in the raw text.
Now we’ll implement noun phrase chunking to identify named entities using a regular expression consisting of rules that indicate how sentences should be chunked.
pattern = 'NP: {<DT>?<JJ>*<NN>}'ChunkingUsing this pattern, we create a chunk parser and test it on our sentence.
cp = nltk.RegexpParser(pattern)cs = cp.parse(sent)print(cs)Figure 2The output can be read as a tree or a hierarchy with S as the first level, denoting sentence.",en,['Susan Li'],2018-12-06 02:39:35.867000+00:00,"{'Python', 'Machine Learning', 'Towards Data Science', 'NLP', 'Named Entity Recognition'}","{'https://miro.medium.com/max/1824/1*qQggIPMugLcy-ndJ8X_aAA.png', 'https://miro.medium.com/max/60/1*sUjSb4dEuSk2w5bU8llgbw.png?q=20', 'https://miro.medium.com/max/60/1*bJzRK3Lk8Jri26y7O7xBnA.png?q=20', 'https://miro.medium.com/max/60/1*EuFzO2mKVdJNdFqQ1avShg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/1050/1*R7MQ6H8hFNXegvxB-BgMiw.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*1OyJd6gpq8hbbjzCYyxKlQ.png?q=20', 'https://miro.medium.com/max/60/1*14p0aAqLZX3KX4Gkpouz6g.png?q=20', 'https://miro.medium.com/max/50/1*H2q7RsGUR3iCpgJVjXSw1Q.png?q=20', 'https://miro.medium.com/max/44/1*KFG-dWUIatG6bJxJ4dhktg.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3012/1*bJzRK3Lk8Jri26y7O7xBnA.png', 'https://miro.medium.com/max/60/1*B0lin04fOg4tVpX5_wRuwA.png?q=20', 'https://miro.medium.com/max/1032/1*QKvX3tj62IiemjC8cdBAIA.png', 'https://miro.medium.com/max/2896/1*DUmLS-6lBRuX3ppVUlsZ-A.png', 'https://miro.medium.com/max/1292/1*EuFzO2mKVdJNdFqQ1avShg.png', 'https://miro.medium.com/max/760/1*oWNyssn2LLeNrIus33tkyw.png', 'https://miro.medium.com/max/1376/1*1OyJd6gpq8hbbjzCYyxKlQ.png', 'https://miro.medium.com/max/60/1*_sYTlDj2p_p-pcSRK25h-Q.png?q=20', 'https://miro.medium.com/max/1200/1*14p0aAqLZX3KX4Gkpouz6g.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*uoAnpkcIIKztaLU1ty7EDQ.png?q=20', 'https://miro.medium.com/max/60/1*QKvX3tj62IiemjC8cdBAIA.png?q=20', 'https://miro.medium.com/max/882/1*KFG-dWUIatG6bJxJ4dhktg.png', 'https://miro.medium.com/max/3764/1*y9DvFxsu3pCecI46yDwMxA.png', 'https://miro.medium.com/max/2560/1*14p0aAqLZX3KX4Gkpouz6g.png', 'https://miro.medium.com/max/36/1*oWNyssn2LLeNrIus33tkyw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/60/1*DUmLS-6lBRuX3ppVUlsZ-A.png?q=20', 'https://miro.medium.com/max/60/1*dqYZiUL8ElnIbltlg54lMQ.png?q=20', 'https://miro.medium.com/max/1584/1*_sYTlDj2p_p-pcSRK25h-Q.png', 'https://miro.medium.com/max/52/1*R7MQ6H8hFNXegvxB-BgMiw.png?q=20', 'https://miro.medium.com/max/3016/1*B0lin04fOg4tVpX5_wRuwA.png', 'https://miro.medium.com/max/1044/1*3Gmd7LaEtAsuPmRSJZyxZg.png', 'https://miro.medium.com/max/60/1*qQggIPMugLcy-ndJ8X_aAA.png?q=20', 'https://miro.medium.com/max/2550/1*dqYZiUL8ElnIbltlg54lMQ.png', 'https://miro.medium.com/max/1048/1*sUjSb4dEuSk2w5bU8llgbw.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/1198/1*H2q7RsGUR3iCpgJVjXSw1Q.png', 'https://miro.medium.com/max/60/1*y9DvFxsu3pCecI46yDwMxA.png?q=20', 'https://miro.medium.com/max/48/1*3Gmd7LaEtAsuPmRSJZyxZg.png?q=20', 'https://miro.medium.com/max/60/1*JNHlyK5-jQA6JBKj3nDYcA.png?q=20', 'https://miro.medium.com/max/842/1*uoAnpkcIIKztaLU1ty7EDQ.png', 'https://miro.medium.com/max/2902/1*JNHlyK5-jQA6JBKj3nDYcA.png'}",2020-03-05 00:26:42.501225,2.035693407058716
https://towardsdatascience.com/custom-entity-extraction-e966e00f6f47,Parsing Structured Documents with Custom Entity Extraction,"Let’s talk about parsing structured documents with entity extraction!

There are lots of great tutorials on the web that explain how to classify chunks of text with Machine Learning. But what if, rather than just categorize text, you want to categorize individual words, like this:

You can’t make me apologize for loving Comic Sans.

This is called entity extraction (or named-entity recognition) and it comes in handy a lot. You could use this technique, for example, to pick out all of the people and places mentioned in news articles and use them as article tags (newsrooms sometimes do this).

Entity Extraction (EE) is also useful for parsing structured documents like forms, W4s, receipts, business cards, and restaurant menus (which is what we’ll be using it for today).

For Google I/O this year, I wanted to build an app that could take a photo of a restaurant menu and automatically parse it — extracting all of the foods, their prices, and more. (You can see me demo it on stage here.)

I wanted to be able to upload a picture of a menu like this:

Step one: upload a photo of a menu

And then use machine learning to magically extract entities from the menu, like the restaurant’s address, phone number, all of the food headings (“Salads,” “Mains”), all of the foods, their prices, and their descriptions (i.e. “on a pretzel bun”).

The idea was that if you’re a restaurant that wants to get listed on an app like Seamless or Grubhub, you could input your menu without having to manually type the whole thing out.

Step two: identify all the foods, plus the food headings (“Salads,” “Mains”) and their descriptions (“On a pretzel bun”).

So how does it work?

First, I used the Google Cloud Vision API’s text detection feature to convert my picture of a menu into raw text.

Once I had my menu in text format, I used entity extraction to parse it. I did this with two techniques:

I extracted the restaurant’s address, phone number, and all of the listed prices by using the Google Cloud Natural Language API. To identify food items, headings, and food descriptions, I had to build a custom machine learning model (more on that later).

The Natural Language API is able to automatically detect common entities. For example, if I send it the string:

Dale would like to buy a $3.50 croissant from the coffee shop at 1600 Ampitheatre Pkwy, Mountain View, CA 94043, whose phone number is (650) 253–0000.

The NL API recognizes entities like people, consumer goods, addresses, price, phone numbers, and more.

Next, I made my app a little sleeker by adding a restaurant photo, star ratings, and a map. None of this info was directly printed on the menu, but the restaurant’s phone number was. Using this phone number, I could query the Google Places API to pull photos, star ratings, etc.

The restaurant’s star rating, photo, and GPS location come from the Places API.

Building a Custom Entity Extraction Model

Okay, but now for the hard (and most critical) bit: how do we extract all of the foods on the menu? And how do we tell the difference between what a food item is (“Veggie Burger”), what a food heading is (“Entrees”), and what a food description is (“on a pretzel bun”)?

These entities are totally domain-specific. In our case, we want to recognize that “entree” is a food heading. But if we were instead parsing newspapers, we might want to recognize the difference between headlines and article text. Or if we were analyzing resumes, we’d want to identify that “Sr. Software Engineer” is a job title, and “Python” is a programming language.

While the Natural Language API I mentioned before recognizes phone numbers and addresses and more, it’s not trained to recognize these domain-specific entities.

For that, we’ll have to build a custom entity extraction model. There are lots of ways of doing this, but I’ll show you the one I think is easiest (with minimal code), using Google AutoML Natural Language. We’ll use it to train a custom entity extraction model.

It works like this:

Upload a labeled dataset of menus Train a model Use your model to identify custom entities via a REST API

Labeling Data

But where to find a dataset of labeled menus?

Conveniently, I found this nice dataset of scans of menus hosted (and labeled!) by the New York Public Library. Sweet!

To get started, I used the Vision API again to convert all of the menus in that dataset to text.

In order to train a custom entity extraction model, I had to transform my data and its labels into jsonl format (the format that AutoML expects). It looks like this:

jsonl file format

I’m going to be honest and warn you that actually labeling these menus was a pain. I used some hackish Python script to pair the dataset’s labels with the OCR-extracted menu text, but they often didn’t line up. I had to go into the AutoML UI and hand-label a bunch of these menus, like this:

Hand-labeling menus using the AutoML editor.

In retrospect, I probably should have used a data labeling service so I didn’t have to waste time labeling myself (or interns, maybe?).

I also scraped Wikipedia’s cuisine pages and generated some fake menus to help augment my dataset.

Training a Model

With Google AutoML, the hardest part of building a model is building a labeled dataset. Actually training a model with that data is pretty straightforward — just hop into the “Train” tab and click “Start Training.” It takes about 4 hours to build a custom model.

Behind the scenes, Google trains a neural network to extract your custom entities.

Making Predictions

When our entity extraction model is done training, we can test it out in the AutoML UI:

The model tagged most of the entities, but missed “Potato Chips” and “Fish and Chips.”

As you can see, it doesn’t work perfectly.

But what do you want?! That’s machine learning. It’s not perfect.

That said, not bad considering I had a pretty small dataset (less than 150 menus). I also didn’t take into account things like the size of the text or its location on the page (headers are usually bigger than dish names, and dishes are bigger than their descriptions).

But still, if I were a restaurant and I could use this tool to import my menu instead of manually typing out every single food by hand, I’d save lots of time (though I might have to do a bit of editing). Plus, the more hypothetical restaurants use my app, the more hypothetical data points I’ll get to improve my model.

Meanwhile, lunch at the Google Cafe calls. Ciao.

P.S. If you train your own entity extraction model, tell me about it in the comments or @dalequark.

Happy Modeling!","['menus', 'parsing', 'custom', 'food', 'entity', 'phone', 'documents', 'model', 'structured', 'extraction', 'google', 'text', 'menu']","Let’s talk about parsing structured documents with entity extraction!
Entity Extraction (EE) is also useful for parsing structured documents like forms, W4s, receipts, business cards, and restaurant menus (which is what we’ll be using it for today).
For that, we’ll have to build a custom entity extraction model.
We’ll use it to train a custom entity extraction model.
If you train your own entity extraction model, tell me about it in the comments or @dalequark.",en,['Dale Markowitz'],2019-05-20 13:59:35.611000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'NLP'}","{'https://miro.medium.com/fit/c/96/96/2*yYwnDQzXeySENmwBXbGR5w.jpeg', 'https://miro.medium.com/max/576/1*IvuXVs5oaXcCjeQBjjrAIQ.png', 'https://miro.medium.com/max/2840/1*uU12jBCu7c6BMLAbdQ0lPw.png', 'https://miro.medium.com/max/60/1*lV3T1BFiovJFVGXSoKXP8A.png?q=20', 'https://miro.medium.com/max/46/1*m_JHj66IaF_IFYHj99VxEg.png?q=20', 'https://miro.medium.com/max/60/1*Djx3WkASNhBBfmjSjem2cg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/3412/1*lV3T1BFiovJFVGXSoKXP8A.png', 'https://miro.medium.com/max/2160/1*Ft2gFJl7YXh-1Nrzscdk2g.jpeg', 'https://miro.medium.com/max/480/1*m_JHj66IaF_IFYHj99VxEg.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*grNL3a3mujCUIk1qnMAfSA.png?q=20', 'https://miro.medium.com/max/60/1*GEhYh3uYZ2dEYQT_gLyZrw.png?q=20', 'https://miro.medium.com/max/1080/1*Ft2gFJl7YXh-1Nrzscdk2g.jpeg', 'https://miro.medium.com/max/2812/1*UWUmnw8LwihxrS5AXmzRqQ.png', 'https://miro.medium.com/max/5880/1*dBXA5hQd_kWB9uA88pHkEA.png', 'https://miro.medium.com/fit/c/160/160/2*yYwnDQzXeySENmwBXbGR5w.jpeg', 'https://miro.medium.com/max/4464/1*Djx3WkASNhBBfmjSjem2cg.png', 'https://miro.medium.com/max/60/1*dBXA5hQd_kWB9uA88pHkEA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*Ft2gFJl7YXh-1Nrzscdk2g.jpeg?q=20', 'https://miro.medium.com/max/3144/1*grNL3a3mujCUIk1qnMAfSA.png', 'https://miro.medium.com/max/42/1*IvuXVs5oaXcCjeQBjjrAIQ.png?q=20', 'https://miro.medium.com/max/60/1*UWUmnw8LwihxrS5AXmzRqQ.png?q=20', 'https://miro.medium.com/max/60/1*uU12jBCu7c6BMLAbdQ0lPw.png?q=20', 'https://miro.medium.com/max/3312/1*GEhYh3uYZ2dEYQT_gLyZrw.png'}",2020-03-05 00:26:48.889592,6.388367652893066
https://medium.com/@srobtweets/classifying-congressional-bills-with-machine-learning-d6d769d818fd,Classifying congressional bills with machine learning,"I’m always looking for new datasets for ML projects, so I was particularly excited to discover this public domain dataset of ~400k congressional bills. The dataset has 20+ data points for each bill. Here’s an example a subset of this data for one bill:

Title : A bill to provide for the expansion of the James Campbell National Wildlife Refuge Honolulu County Hawaii

: A bill to provide for the expansion of the James Campbell National Wildlife Refuge Honolulu County Hawaii ID : 109-S-1165

: 109-S-1165 URL : https://www.congress.gov/bill/109th-congress/senate-bill/1165

: https://www.congress.gov/bill/109th-congress/senate-bill/1165 Topic : Public Lands

: Public Lands Date Introduced : 6 June 2005

: 6 June 2005 Date Passed: 25 May 2006

25 May 2006 Congressperson who introduced it : Daniel Inouye

: Daniel Inouye Passed: Yes

The bills from this dataset were all manually assigned a topic by domain experts. The resolutions in the dataset are unlabeled which makes it a great fit for AutoML Natural Language — we can use the labeled bills to train the model and see how it performs on unlabeled resolutions. We’ll use the title of the bill as the input to our model, and the label will be the topic. The original dataset includes a higher level topic ( Major field in the dataset) and a more specific ( Minor ) topic for each bill. To keep things simple, we’ll use just the Major topic field to categorize bills.

Note that there are an infinite number of possible input and label combinations you could use to build ML models from this data, we’re just using two in this example, Title and Major topic. Here are some sample inputs and predictions for our model:

Title: To permit the televising of Supreme Court proceedings. Category: Technology ----- Title: A bill to provide a program of tax adjustment for small business and for persons engaged in small business. Category: Domestic commerce

The first step in building a model with AutoML NL is uploading a CSV of training data. I wrote a script to extract only bills with a topic assigned and strip some characters from the text. The resulting CSV looks like this:

If you’d like to play with the dataset I used to train the model, I’ve made the CSV file publicly available here.

Training an AutoML NL model

Now that we’ve got a CSV with our text inputs and their associated labels, we can upload this directly to the AutoML UI to create our dataset:

Once it uploads, we can look at all of our training data in the UI:

To train our model, all we need to do is press the train button:

How cool is that?! We don’t need to write any of the underlying model code — AutoML will handle that automagically for us.

Evaluating our model

To evaluate our model we’ll look at the confusion matrix in the Evaluate tab of the UI:

It may look confusing (it is called a confusion matrix after all), but turns out it’s not so hard to understand: what we ideally want to see here is a strong diagonal from the top left. This tells us the percentage of text items from our test set that the model was able to classify correctly. Side note: AutoML automagically splits the data we upload into training, test, and validation sets.

The confusion matrix shows the accuracy for 10 of our 20 topics, but we can also look individually at a topic to see specific examples that our model classified correctly and incorrectly. Looking here, we might want to improve the training data for Domestic Commerce bills, since our model only classified 78% of those correctly, and confused about 10% of them as Macroeconomics .

Generating predictions on unlabeled bills

Next it’s time for the best part — generating predictions on unlabeled bills. We can try this out right in the AutoML NL UI. Let’s try the following unlabeled bill:

A concurrent resolution making the necessary arrangements for the inauguration of the President-elect and Vice President-elect of the United States.

The model says there’s a 95.6% chance this bill is related to Government Operations . I’ve run two more examples through our model to see how it performs on new data:

Bill: Honoring women who have served, and who are currently serving, as members of the Armed Forces and recognizing the recently expanded service opportunities available to female members of the Armed Forces. Predicted label: Civil Rights 88.5%, Defense 7.5% ----- Bill: Setting forth the congressional budget for the United States Government for fiscal year 2010 and including the appropriate budgetary levels for fiscal years 2009 and 2011 through 2014. Predicted label: Macroeconomics 99.5%

We don’t have the ground truth label to compare our model’s predictions to since these are unlabeled, but the results generated look pretty accurate. If we want to build an app that auto-classifies new bills, we could do that with a simple AutoML API request to our trained model. Here’s an example using curl:

Be on the lookout for a post from one of my teammates that covers using this API to classify new bills.

What’s next?

For the American readers here: since Election Day is coming up in the US and this post covered a political dataset, I’ll use this opportunity as a shameless plug to encourage you to vote :)

Ok, back to my regularly scheduled programming — check out these resources to learn more about what I covered here:

And if writing model code is your thing, check out this post I did on building a text classification model with Keras to predict the price of wine given its description, or this one on using TF Hub to build a text classification model. Stay tuned for more blog posts on this dataset. I’d also love to hear if you do anything interesting with it! You can leave a comment below or find me on Twitter at @SRobTweets.","['machine', 'classifying', 'bill', 'text', 'automl', 'dataset', 'bills', 'learning', 'congressional', 'model', 'data', 'look', 'topic', 'unlabeled']","I’m always looking for new datasets for ML projects, so I was particularly excited to discover this public domain dataset of ~400k congressional bills.
We’ll use the title of the bill as the input to our model, and the label will be the topic.
The original dataset includes a higher level topic ( Major field in the dataset) and a more specific ( Minor ) topic for each bill.
I wrote a script to extract only bills with a topic assigned and strip some characters from the text.
Generating predictions on unlabeled billsNext it’s time for the best part — generating predictions on unlabeled bills.",en,['Sara Robinson'],2018-11-19 14:48:43.514000+00:00,"{'Politics', 'Machine Learning', 'Congress', 'NLP'}","{'https://miro.medium.com/max/1200/0*mJkpg9s13QZtetZu', 'https://miro.medium.com/max/2334/0*NYWiW-3E1NFdRBQR', 'https://miro.medium.com/max/2308/0*PUqhrYrjxF0GE306', 'https://miro.medium.com/fit/c/80/80/1*RRUfAf9hJnccGrgDFXmtAg.png', 'https://miro.medium.com/max/3200/0*V1PaW9BDa33nmhPy', 'https://miro.medium.com/fit/c/80/80/2*HkGXEUFQCJsPyu9qr8F-qQ.jpeg', 'https://miro.medium.com/fit/c/96/96/1*RRUfAf9hJnccGrgDFXmtAg.png', 'https://miro.medium.com/max/3200/0*fjglVk3tiW1HnAJa', 'https://miro.medium.com/max/60/0*V1PaW9BDa33nmhPy?q=20', 'https://miro.medium.com/max/60/0*fjglVk3tiW1HnAJa?q=20', 'https://miro.medium.com/fit/c/160/160/1*RRUfAf9hJnccGrgDFXmtAg.png', 'https://miro.medium.com/max/3200/0*mJkpg9s13QZtetZu', 'https://miro.medium.com/fit/c/80/80/1*-bQlkmzl4xSztVCAjNJvvQ.jpeg', 'https://miro.medium.com/max/44/0*NYWiW-3E1NFdRBQR?q=20', 'https://miro.medium.com/max/60/0*mJkpg9s13QZtetZu?q=20', 'https://miro.medium.com/max/60/0*PUqhrYrjxF0GE306?q=20'}",2020-03-05 00:26:50.768379,1.8777852058410645
https://medium.com/district-data-labs/named-entity-recognition-and-classification-for-entity-extraction-6f23342aa7c5,Named Entity Recognition and Classification for Entity Extraction,"Named Entity Recognition and Classification for Entity Extraction

Combining NERCs to Improve Entity Extraction

By Linwood Creekmore III

The overwhelming amount of unstructured text data available today from traditional media sources as well as newer ones, like social media, provides a rich source of information if the data can be structured. Named Entity Extraction forms a core subtask to build knowledge from semi-structured and unstructured text sources. Some of the first researchers working to extract information from unstructured texts recognized the importance of “units of information” like names (such as person, organization, and location names) and numeric expressions (such as time, date, money, and percent expressions). They coined the term “Named Entity” in 1996 to represent these.

Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them. These knowledge bases are key contributors to intelligent computer behavior. Not surprisingly, Named Entity Extraction operates at the core of several popular technologies such as smart assistants (Siri, Google Now), machine reading, and deep interpretation of natural language.

This post explores how to perform Named Entity Extraction, formally known as “Named Entity Recognition and Classification (NERC). In addition, the article surveys open-source NERC tools that work with Python and compares the results obtained using them against hand-labeled data.

The specific steps include:

Preparing semi-structured natural language data for ingestion using regular expressions; creating a custom corpus in the Natural Language Toolkit

Using a suite of open source NERC tools to extract entities and store them in JSON format

Comparing the performance of the NERC tools

Implementing a simplistic ensemble classifier

The information extraction concepts and tools in this article constitute a first step in the overall process of structuring unstructured data. They can be used to perform more complex natural language processing to derive unique insights from large collections of unstructured data.

Environment Set-Up

In order to follow along with the work in this article, we recommend using Anaconda, which is an easy-to-install, free, enterprise-ready Python distribution for data analytics, processing, and scientific computing. With a few lines of code, you can have all the dependencies used in this post with, the exception of one function (email extractor).

Install Anaconda Download the requirements.yml (remember where you saved it on your computer) Follow the Use Environment from File instructions on Anaconda’s website.

If you use an alternative method to set up a virtual environment, make sure you have all the files installed from the yml file. The one dependency not in the yml file is the email extractor. Cut and paste the function from this Gist, save it to a .py file, and make sure it is in your sys.path or environment path.

Data Source

The proceedings from the Knowledge Discovery and Data Mining (KDD) conferences in New York City (2014) and Sydney, Australia (2015) serve as our source of unstructured text and contain over 200 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science, and their applications. The full conference proceedings can be purchased for $60 at the Association for Computing Machinery’s Digital Library (includes ACM membership).

This post will work on a data set that is equivalent to the combined conference proceedings, but only use abstracts and extracts from the text, rather than the full proceedings, a data set that can be found on the ACM website. We will explore reading PDF data and discuss follow-on analytics if the full proceedings are available to you.

Initial Data Exploration

Visual inspection reveals that the target filenames begin with a “p” and end with “pdf.” As a first step, we determine the number of files and the naming conventions by using a loop to iterate over the files in the directory and printing out the filenames. Each filename also gets saved to a list, and the length of the list tells us the total number of files in the dataset.

import os ## Set important paths

BASE = os.path.join(os.path.dirname(__file__), "".."")

DOCS = os.path.join(BASE, ""data"", ""docs"") def get_documents(path=DOCS):

""""""

Returns a filtered list of paths to PDF files representing our corpus.

""""""

for name in os.listdir(path):

if name.startswith('p') and name.endswith('.pdf'):

yield os.path.join(path, name) # Print the total number of documents

print(len(list(get_documents())))

A total of 253 files exist in the directory. Opening one of these reveals that our data is in PDF format and that it is semi-structured (follows journal article format with separate sections for “abstract” and “title”). While PDFs provide an easily readable presentation of data, they are extremely difficult to work with in data analysis. In your work, if you have an option to get to data before conversion to a PDF format, be sure to take that option.

Creating a Custom NLTK Corpus

We used several Python tools to ingest our data, including the following libraries:

Pdfminer — contains a command line tool called “pdf2txt.py” that extracts text contents from a PDF file (you can visit the pdfminer homepage for download instructions).

Subprocess — a standard library module that allows us to invoke the “pdf2txt.py” command line tool within our code.

NLTK — the Natural Language Tool Kit, or NLTK, serves as one of Python’s leading platforms to analyze natural language data.

String — provides variable substitutions and value formatting to strip non-printable characters from the output of the text extracted from our journal article PDFs.

Unicodedata — allows Latin Unicode characters to degrade gracefully into ASCII. This is an important feature because some Unicode characters won’t extract nicely.

Our task begins by iterating over the files in the directory with names that begin with “p” and end with “pdf.” This time, however, we will strip the text from the pdf file, write the .txt file to a newly created directory, and use the fname variable to name the files we write to disk. Keep in mind that this task may take a few minutes depending on the processing power of your computer.

import re

import nltk

import codecs

import string

import subprocess

import unicodedata ## Create a path to extract the corpus.

CORPUS = os.path.join(BASE, ""data"", ""corpus"") def extract_corpus(docs=DOCS, corpus=CORPUS):

""""""

Extracts a text corpus from the PDF documents and writes them to disk.

"""""" # Create corpus directory if it doesn't exist.

if not os.path.exists(corpus):

os.mkdir(corpus) # For each PDF path, use pdf2txt to extract the text file.

for path in get_documents(docs):

# Call the subprocess command (must be on your path)

document = subprocess.check_output(

['pdf2txt.py', path]

) # Encode UTF-u and remove non-printable characters

document = filter(

lambda char: char in string.printable,

unicodedata.normalize('NFKD', document.decode('utf-8'))

) # Write the document out to the corpus directory

fname = os.path.splitext(os.path.basename(path))[0] + "".txt""

outpath = os.path.join(corpus, fname) with codecs.open(outpath, 'w') as f:

f.write(document) # Run the extraction

extract_corpus()

Next, we build a custom NLTK corpus. Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.

# Create an NLTK corpus reader to access text data on disk.

kddcorpus = nltk.corpus.PlaintextCorpusReader(CORPUS, '.*\.txt')

We now have a semi-structured dataset in a format that we can query and analyze. First, let’s see how many words (including stop words) we have in our entire corpus and the vocabulary of the corpus.

words = nltk.FreqDist(kddcorpus.words())

count = sum(words.values())

vocab = len(words) print(""Corpus contains a vocabulary of {} and a word count of {}."".format(

count, vocab

)) Corpus contains a vocabulary of 70,073 and a word count of 2,785,178

The NLTK book has an excellent section on processing raw text and unicode issues. It provides a helpful discussion of some problems you may encounter.

Using Regular Expressions to Extract Specific Sections

To begin our exploration of regular expressions (a.k.a. “regex”), it’s important to point out some good resources for those new to the topic. An excellent resource may be found in Johns Hopkins University’s Coursera video titled Getting and Cleaning Data.

As a simple example, let’s extract titles from the first 10 documents.

def titles(fileid=None,corpus=kddcorpus):

""""""

Use a regular expression to extract the titles from the corpus.

""""""

pattern = re.compile(r'^(.*)[\s]+[\s]?(.*)?') if fileid is not None:

match = pattern.search(kddcorpus.raw(fileid))

yield "" "".join(map(lambda s: s.strip(), match.groups()))

else:

for fileid in corpus.fileids():

# Search for a pattern match

match = pattern.search(corpus.raw(fileid)) if match:

# If we find one, yield the space joined groups.

yield "" "".join(map(lambda s: s.strip(), match.groups())) for idx, title in enumerate(titles()):

print title

if idx >= 10:

break

The result is as follows:

Online Controlled Experiments: Lessons from Running A/B/n Tests for 12 Years

Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages

Why It Happened: Identifying and Modeling the Reasons of the Happening of Social Events

Matrix Completion with Queries Natali Ruchansky

Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference

Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts

TimeCrunch: Interpretable Dynamic Graph Summarization Neil Shah

Inside Jokes: Identifying Humorous Cartoon Captions Dafna Shahaf

Community Detection based on Distance Dynamics Junming Shao

Discovery of Meaningful Rules in Time Series Mohammad Shokoohi-Yekta Yanping Chen Bilson Campana Bing Hu

This code extracts the titles, but some author names get caught up in the extraction as well.

For simplicity, let’s focus on wrangling the data to use the NERC tools on two sections of the paper: the “top” section and the “references” section. The “top” section includes the names of authors and schools. This section represents all of the text above the article’s abstract. The “references” section appears at the end of the article. The regex tools of choice to extract sections are the positive lookbehind and positive lookahead expressions. Using these, we will build two functions designed to extract the “top” and “references” sections of each document.

In addition to extracting the relevant sections of the documents, our two functions will obtain a character count for each section, extract emails, count the number of references and store that value, calculate a word per reference count, and store all the above data as a nested dictionary with filenames as the key.

def sectpull(fileids=None, section=None, corpus=kddcorpus):

""""""

Uses a regular expression to pull sections from a file: - ""top"": everything until the references section

- ""ref"": the references and anything that follows. Yields the text as top, ref respectively.

""""""

# Select either a single fileid or a list of fileids fileids = fileids or corpus.fileids()

if section == None:

section = None

elif section == 'references':

section = re.compile('(?<=' + 'REFERENCES' + ')(.+)')

elif section == 'body':

section = re.compile(""(.+?)(?=""+'REFERENCES'+ "")"")

elif section == 'top':

section = [""ABSTRACT"",""Abstract"",""Bio"",""Panel Summary""]

for sect in section:

try:

section = re.compile(""(.+?)(?=""+sect+ "")"")

break

except:

pass # Iterate through all text for each file id.

for fileid in fileids:

text = corpus.raw(fileid) # Extract the text and search for the section target

if section == None:

target = re.sub('[\s]', "" "", text)

else:

text = re.sub('[\s]', "" "", text)

target = section.search(text) if target:

yield fileid, target.group(0), target.group(1)

def refstats(fileids=None, section=None, corpus=kddcorpus):

""""""

Code to pull only the references section, store a character count, number

of references, as well as a ""words per reference"" count. Pass either a specific document id, a list of ids, or None for all ids.

""""""

# Create reference number to match pattern

refnum = re.compile(r'\[[0-9]{1,3}\]', re.I) for fileid, top, refs in sectpull(fileids, section, corpus): # Yield the statistics about the references

n_refs = len(set((refnum.findall(refs)))) words = sum(1 for word in nltk.word_tokenize(refs))

wp_ref = float(words) / float(n_refs) if n_refs else 0 # Yield the data from the generator

yield (fileid, len(refs), n_refs, wp_ref)

The above code also makes use of the nltk.word_tokenize tool to create the ""word per reference"" statistic (takes time to run).

I want to take an opportunity here to say few words about the data. When working with natural language, one should always be prepared to deal with irregularities in the data set. This corpus is no exception. It comes from a top-notch data mining organization, but human error and a lack of standardization makes its way into the picture. For example, in one paper the header section is entitled “Categories and Subject Descriptors,” while in another the title is “Categories & Subject Descriptors.” While that may seem like a small difference, these types of differences cause significant problems. There are also some documents that will be missing sections altogether, i.e. keynote speaker documents do not contain a “references” section. When encountering similar issues in your work, you must decide whether to account for these differences or ignore them. I worked to include as much of the 253-document corpus as possible.

Next, let’s test the “references” extraction function and look at the output by obtaining the first 10 entries of the dictionary created by the function. This dictionary holds all the extracted data and various calculations. The tabulate module is a great tool to visualize descriptive outputs in table format.

from tabulate import tabulate

from operator import itemgetter

import random # Create table sorted by number of references

table = sorted(list(refstats(random.sample(kddcorpus.fileids(),15),section='body'))) # Print the table with headers

headers = ('File', 'Characters', 'References', 'Words per Reference') print(tabulate(table, headers=headers))

The output is as follows:

File Characters References Words per Reference

--------- ------------ ------------ ---------------------

p1005.txt 50190 22 488

p1205.txt 47071 20 423.95

p1583.txt 46346 38 297.289

p2167.txt 45739 18 480.833

p537.txt 54152 24 445.875

p59.txt 46215 24 374.708

p715.txt 30376 18 346

p725.txt 49690 18 555.5

p835.txt 50786 28 389.286

p865.txt 47636 16 604.875

p935.txt 58307 21 554.429

... [snip] ...

As you can see, this is a good start to performing bibliographic analysis with Python.

Open Source NERC Tools

Now that we have a method to obtain the corpus from the “top” and “references” sections of each article in the dataset, we are ready to perform the named entity extractions. In this post, we examine three popular, open source NERC tools. The tools are NLTK, Stanford NER, and Polyglot. A brief description of each follows.

NLTK has a chunk package that uses NLTK’s recommended named entity chunker to chunk the given list of tagged tokens. A string is tokenized and tagged with parts of speech (POS) tags. The NLTK chunker then identifies non-overlapping groups and assigns them to an entity class. You can read more about NLTK's chunking capabilities in the NLTK book.

package that uses NLTK’s recommended named entity chunker to chunk the given list of tagged tokens. A string is tokenized and tagged with parts of speech (POS) tags. The NLTK chunker then identifies non-overlapping groups and assigns them to an entity class. You can read more about NLTK's chunking capabilities in the NLTK book. Stanford’s Named Entity Recognizer, often called Stanford NER, is a Java implementation of linear chain Conditional Random Field (CRF) sequence models functioning as a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text that are the names of things, such as person and company names, or gene and protein names. NLTK contains an interface to Stanford NER written by Nitin Madnani. Details for using the Stanford NER tool are on the NLTK page and the required jar files can be downloaded here.

Polyglot is a natural language pipeline that supports massive multilingual (i.e. language) applications. It supports tokenization in 165 languages, language detection in 196 languages, named entity recognition in 40 languages, part of speech tagging in 16 languages, sentiment analysis in 136 languages, word embeddings in 137 languages, morphological analysis in 135 languages, and transliteration in 69 languages. It is a powerhouse tool for natural language processing. We will use the named entity recognition feature for English language in this exercise. Polyglot is available via pypi.

We can now test how well these open source NERC tools extract entities from the “top” and “reference” sections of our corpus. For two documents, I hand labeled authors, organizations, and locations from the “top” section of the article and the list of all authors from the “references” section. I also created a combined list of the authors, joining the lists from the “top” and “references” sections. Hand labeling is a time consuming and tedious process. For just the two documents, this involved 295 cut-and-pastes of names or organizations.

An easy test for the accuracy of a NERC tool is to compare the entities extracted by the tools to the hand-labeled extractions. Before beginning, we take advantage of the NLTK functionality to obtain the “top” and “references” sections of the two documents used for the hand labeling:

# We need the top and references sections from p19.txt and p29.txt

annotated = sectpull(['p19.txt', 'p29.txt'])

For each NERC tool, I created functions to extract entities and return classes of objects in different lists.

from collections import defaultdict from nltk import ne_chunk

from polyglot.text import Text

from nltk.tag import StanfordNERTagger

def polyglot_entities(fileids=None, section = None, corpus=kddcorpus):

""""""

Extract entities from each file using polyglot

""""""

results = defaultdict(lambda: defaultdict(list))

fileids = fileids or corpus.fileids() for fileid in fileids:

if section is not None:

text = Text((list(sectpull([fileid],section=section))[0][1]))

else:

text = Text(corpus.raw(fileid)) for entity in text.entities:

etext = "" "".join(entity) if entity.tag == 'I-PER':

key = 'persons'

elif entity.tag == 'I-ORG':

key = 'organizations'

elif entity.tag == 'I-locations':

key = 'locations'

else:

key = 'other' results[fileid][key].append(etext) return results def stanford_entities(model, jar, fileids=None, corpus=kddcorpus, section = None):

""""""

Extract entities using the Stanford NER tagger.

Must pass in the path to the tagging model and jar as downloaded from the

Stanford Core NLP website.

""""""

results = defaultdict(lambda: defaultdict(list))

fileids = fileids or corpus.fileids()

tagger = StanfordNERTagger(model, jar)

section = section for fileid in fileids:

if section is not None:

text = nltk.word_tokenize(list(sectpull([fileid],section=section))[0][1])

else:

text = corpus.words(fileid) chunk = [] for token, tag in tagger.tag(text):

if tag == 'O':

if chunk:

# Flush the current chunk

etext = "" "".join([c[0] for c in chunk])

etag = chunk[0][1]

chunk = [] if etag == 'PERSON':

key = 'persons'

elif etag == 'ORGANIZATION':

key = 'organizations'

elif etag == 'LOCATION':

key = 'locations'

else:

key = 'other' results[fileid][key].append(etext) else:

# Build chunk from tags

chunk.append((token, tag)) return results

def nltk_entities(fileids=None, section = None,corpus=kddcorpus):

""""""

Extract entities using the NLTK named entity chunker.

""""""

results = defaultdict(lambda: defaultdict(list))

fileids = fileids or corpus.fileids() for fileid in fileids:

if section is not None:

text = nltk.pos_tag(nltk.word_tokenize(list(sectpull([fileid],section=section))[0][1]))

else:

text = nltk.pos_tag(corpus.words(fileid)) for entity in nltk.ne_chunk(text):

if isinstance(entity, nltk.tree.Tree):

etext = "" "".join([word for word, tag in entity.leaves()])

label = entity.label()

else:

continue if label == 'PERSON':

key = 'persons'

elif label == 'ORGANIZATION':

key = 'organizations'

elif label == 'LOCATION':

key = 'locations'

elif label == 'GPE':

key = 'other'

else:

key = None if key:

results[fileid][key].append(etext) return results

In this next block of code, we will apply the NLTK standard chunker, Stanford Named Entity Recognizer, and Polyglot extractor to our corpus. We pass our data, the “top” and “references” section of the two documents of interest, into the functions created with each NERC tool and build a nested dictionary of the extracted entities — author names, locations, and organization names. This code may take a bit of time to run (30 secs to a minute).

# Only extract our annotated files.

fids = ['p19.txt', 'p29.txt'] # NLTK Entities

nltkents = nltk_entities(fids, section='top') # Polyglot Entities

polyents = polyglot_entities(fids, section='top') # Stanford Model Loading

root = os.path.expanduser('~/models/stanford-ner-2014-01-04/') model = os.path.join(root, 'classifiers/english.muc.7class.distsim.crf.ser.gz') jar = os.path.join(root, 'stanford-ner-2014-01-04.jar') # Stanford Entities

stanents = stanford_entities(model, jar, fids, section='top')

We will focus specifically on the “persons” entity extractions from the “top” section of the documents to estimate performance. However, a similar exercise is possible with the extractions of “organizations” entity extractions or “locations” entity extractions too, as well as from the “references” section.

To get a better look at how each NERC tool performed on the named person entities, we will use the Pandas dataframe. Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. The dataframe provides a visual comparison of the extractions from each NERC tool and the hand-labeled extractions. Just a few lines of code accomplish the task:

import pandas as pd

import json p19Authors = json.load(open('./data/p19Truth.json')) df1 = pd.Series(polyents['p19.txt']['persons'], index=None,

dtype=None, name='Polyglot NERC Authors', copy=False, fastpath=False) df2=pd.Series([re.sub('\*',"""",l) for l in stanents['p19.txt']['persons']],

index=None, dtype=None, name='Stanford NERC Authors', copy=False, fastpath=False) df3=pd.Series([re.sub('\*',"""",l) for l in nltkents['p19.txt']['persons']],

index=None, dtype=None, name='NLTKStandard NERC Authors', copy=False, fastpath=False) df4 = pd.Series(p19Authors['authors'], index=None,

dtype=None, name='Hand-labeled True Authors', copy=False, fastpath=False) met = pd.concat([df4,df3,df2,df1], axis=1).fillna('')

met

The above dataframe illustrates the mixed results from the NERC tools. NLTK Standard NERC appears to have extracted 3 false positives while the Stanford NER missed 2 true positives and the Polyglot NERC extracted all but one true positive (partially extracted; returned first name only).

Let’s calculate some key performance metrics:

True Negatives (TN): case was negative and predicted negative True Positives (TP): case was positive and predicted positive False Negatives (FN): case was positive but predicted negative False Positives (FP): case was negative but predicted positive

Here is a quick figure summarizing overall model performance.

The following function calculates the metrics for the three NERC tools:

# Calculations and logic from http://www.kdnuggets.com/faq/precision-recall.html def metrics(truth,run):

truth = truth

run = run

TP = float(len(set(run) & set(truth))) if float(len(run)) >= float(TP):

FP = len(run) - TP

else:

FP = TP - len(run)

TN = 0

if len(truth) >= len(run):

FN = len(truth) - len(run)

else:

FN = 0 accuracy = (float(TP)+float(TN))/float(len(truth))

recall = (float(TP))/float(len(truth))

precision = float(TP)/(float(FP)+float(TP))

print ""The accuracy is %r"" % accuracy

print ""The recall is %r"" % recall

print ""The precision is %r"" % precision d = {'Predicted Negative': [TN,FN], 'Predicted Positive': [FP,TP]}

metricsdf = pd.DataFrame(d, index=['Negative Cases','Positive Cases']) return metricsdf

Now let’s pass our values into the function to calculate the performance metrics:

print

print

str1 = ""NLTK Standard NERC Tool Metrics"" print str1.center(40, ' ')

print

print

metrics(p19Authors['authors'], [re.sub('\*',"""",l) for l in nltkents['p19.txt']['persons']]) NLTK Standard NERC Tool Metrics The accuracy is 1.0

The recall is 1.0

The precision is 0.6666666666666666

print

print

str2 = ""Stanford NERC Tool Metrics"" print str2.center(40, ' ')

print

print

metrics(p19Authors['authors'], [re.sub('\*',"""",l) for l in stanents['p19.txt']['persons']]) Stanford NER Metrics The accuracy is 0.6666666666666666

The recall is 0.6666666666666666

The precision is 1.0

print

print

str3 = ""Polyglot NERC Tool Metrics"" print str3.center(40, ' ')

print

print

metrics(p19Authors['authors'],polyents['p19.txt']['persons']) Polyglot NERC Tool Metrics The accuracy is 0.8333333333333334

The recall is 0.8333333333333334

The precision is 0.8333333333333334

The basic metrics above reveal some quick takeaways about each tool based on the specific extraction task. The NLTK Standard Chunker has perfect accuracy and recall but lacks in precision. It successfully extracted all the authors for the document, but also extracted 3 false entities. NLTK’s chunker would serve well in an entity extraction pipeline where the data scientist is concerned with identifying all possible entities

The Stanford NER tool is very precise (specificity vs sensitivity). The entities it extracts were 100% accurate, but it failed to identify half of the true entities. The Stanford NER tool would be best used when a data scientist wanted to extract only those entities that have a high likelihood of being named entities, suggesting an unconscious acceptance of leaving behind some information.

The Polyglot Named Entity Recognizer identified five named entities exactly, but only partially identified the sixth (first name returned only). The data scientist looking for a balance between sensitivity and specificity would likely use Polyglot, as it will balance extracting the 100% accurate entities and those which may not necessarily be a named entity.

A Simple Ensemble Classifier

In our discussion above, we notice the varying levels of performance by the different NERC tools. Using the idea that combining the outputs from various classifiers in an ensemble method can improve the reliability of classifications, we can improve the performance of our named entity extractor tools by creating an ensemble classifier. Each NERC tool had at least 3 named persons that were true positives, but no two NERC tools had the same false positive or false negative. Our ensemble classifier voting rule is very simple: Return all named entities that exist in at least two of the true positive named entity result sets from our NERC tools.

We implement this rule using the set module. We first do an intersection operation of the NERC results vs the hand labeled entities to get our ""true positive"" set.

Here is our code to accomplish the task:

# Create intersection of true authors from NLTK standard output

a =set(sorted(nltkents['p19.txt']['persons'])) & set(p19Authors['authors']) # Create intersection of true authors from Stanford NER output

b =set(sorted(stanents['p19.txt']['persons'])) & set(p19Authors['authors']) # Create intersection of true authors from Polyglot output

c = set(sorted(polyents['p19.txt']['persons'])) & set(p19Authors['authors']) # Create union of all true positives from each NERC output

(a.union(b)).union(c) {'Kevin Murphy',

'Safa Alai',

'Tim Althoff',

'Van Dang',

'Wei Zhang',

'Xin Luna Dong'}

To get a visual comparison of the extractions for each tool and the ensemble set side by side, we return to our dataframe from earlier. In this case, we use the concat operation in pandas to append the new ensemble set to the dataframe.

dfensemble = pd.Series(list((a.union(b)).union(c)), index=None, dtype=None, name='Ensemble Method Authors',

copy=False, fastpath=False)

met = pd.concat([df4,dfensemble,df3,df2,df1], axis=1).fillna('')

met

First, a quick visual to see how performance improved.

And we get a look at the performance metrics to see if we push our scores up in all categories:

print

print

str = ""Ensemble NERC Metrics"" print str.center(40, ' ')

print

print

metrics(p19Authors['authors'],list((a.union(b)).union(c)))

Ensemble NERC Metrics The accuracy is 1.0

The recall is 1.0

The precision is 1.0

Exactly as expected, we see improved performance across all performance metric scores and, in the end, get a perfect extraction of all named persons from this document.

Before we go any further, the idea of moving from “okay” to “perfect” is unrealistic. Moreover, this is a very small sample and only intended to show the application of an ensemble method. Applying this method to other sections of the journal articles will not lead to a perfect extraction, but it will indeed improve the performance of the extraction considerably.

Getting Your Data in Open File Format

A good rule for any data analytics project is to store the results or output in an open file format. I selected JavaScript Object Notation (JSON), which is an open standard format that uses human-readable text to transmit data objects consisting of attribute–value pairs.

Let’s take our list of persons from the ensemble results, store it as a Python dictionary, and then convert it to JSON. Alternatively, we could use the dumps function from the json module to return dictionaries, and ensure we get the open file format at every step. This way, other data scientists or users could pick and choose what portions of code to use in their projects.

# Add ensemble results for author to the nested python dictionary; use our functions

p19={'docName':'p19.txt','ensembleAuthors':list((a.union(b)).union(c)),

'body':list(sectpull(['p19.txt'], section='body'))[0][1],'title':list(titles(['p19.txt']))[0]} # covert nested dictionary to json for open data storage

# json can be stored in mongodb or any other disk store

output = json.dumps(p19, ensure_ascii=False,indent=3) # print out the authors section we just created in our json

print json.dumps(json.loads(output)['ensembleAuthors'],indent=3) # uncomment to see full json output

#print json.dumps((json.loads(output)),indent=3) [

""Wei Zhang"",

""Tim Althoff"",

""Xin Luna Dong"",

""Van Dang"",

""Kevin Murphy"",

""Safa Alai""

]

Conclusion

In this post, we’ve covered the entire data science pipeline in a natural language processing job that compared the performance of three different NERC tools. A core task in this pipeline involved ingesting plaintext into an NLTK corpus so that we could easily retrieve and manipulate the corpus. Then we used the results from the various NERC tools to create a simplistic ensemble classifier that improved the overall performance.

The techniques in this post can be applied to other domains, larger datasets or any other corpus. Everything I used in this post (with the exception of the Regular expression resource from Coursera) was not taught in a classroom or structured learning environment. It all came from online resources, posts from others, and books (that includes learning how to code in Python). If you have the motivation, you can do it.

Further Reading and Other Resources

Throughout the article, there are hyperlinks to resources and reading materials for reference, but here is a central list:","['corpus', 'recognition', 'tools', 'nltk', 'entity', 'references', 'entities', 'data', 'nerc', 'named', 'classification', 'extraction', 'tool']","Named Entity Extraction forms a core subtask to build knowledge from semi-structured and unstructured text sources.
This post explores how to perform Named Entity Extraction, formally known as “Named Entity Recognition and Classification (NERC).
Stanford’s Named Entity Recognizer, often called Stanford NER, is a Java implementation of linear chain Conditional Random Field (CRF) sequence models functioning as a Named Entity Recognizer.
We will use the named entity recognition feature for English language in this exercise.
The Polyglot Named Entity Recognizer identified five named entities exactly, but only partially identified the sixth (first name returned only).",en,['District Data Labs'],2017-12-27 18:43:41.066000+00:00,"{'interface', 'Data Science', 'Python', 'NLTK interface to the Stanford NER', 'Machine Learning', 'NLP', 'Named Entity Recognition'}","{'https://miro.medium.com/max/282/1*8Ox9-8Km5U6N4sHIh7Al7A.png', 'https://miro.medium.com/max/60/0*s1Z-GIwUtbvc3ViG.png?q=20', 'https://miro.medium.com/max/60/0*vFAdaf453JKJKDxV.png?q=20', 'https://miro.medium.com/max/60/0*EtI_SpKKOgCrL85V.png?q=20', 'https://miro.medium.com/max/60/0*UOwABsw9FgT1C5Q-.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/790/0*OVm-WoDw1ibmdi-2.png', 'https://miro.medium.com/max/1580/0*OVm-WoDw1ibmdi-2.png', 'https://miro.medium.com/max/790/0*PwBDk-wfIt_zpqzv.png', 'https://miro.medium.com/fit/c/160/160/1*tym3oKQBxBY29YxbHbXYTA.png', 'https://miro.medium.com/max/1580/0*d1lkfT-_Pd2AfsWC.png', 'https://miro.medium.com/max/284/1*8Ox9-8Km5U6N4sHIh7Al7A.png', 'https://miro.medium.com/max/790/0*ncrRKzX0VHpQSCOI.png', 'https://miro.medium.com/fit/c/96/96/1*tym3oKQBxBY29YxbHbXYTA.png', 'https://miro.medium.com/max/790/0*UOwABsw9FgT1C5Q-.png', 'https://miro.medium.com/max/60/0*PwBDk-wfIt_zpqzv.png?q=20', 'https://miro.medium.com/max/1580/0*vFAdaf453JKJKDxV.png', 'https://miro.medium.com/max/60/0*OVm-WoDw1ibmdi-2.png?q=20', 'https://miro.medium.com/max/60/0*d1lkfT-_Pd2AfsWC.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*tj9G7lTrU9_N7QB9Swi5ww.jpeg', 'https://miro.medium.com/max/60/0*ncrRKzX0VHpQSCOI.png?q=20', 'https://miro.medium.com/max/1580/0*s1Z-GIwUtbvc3ViG.png', 'https://miro.medium.com/fit/c/80/80/1*gD19hBZRqmiCqmENPfxVjg@2x.jpeg', 'https://miro.medium.com/max/790/0*EtI_SpKKOgCrL85V.png'}",2020-03-05 00:26:52.363522,1.5941414833068848
https://towardsdatascience.com/using-bash-for-data-pipelines-cf05af6ded6f,Using Bash for Data Pipelines,"Part 2: Parsing the files for the required information

Here we are going to create a much more complicated script, that will be more in depth. I will go through each line, explaining what is does, and then put it all together as a script at the end.

I named this second script: process_data.sh where once again you can name it whatever you would like as long as it ends with the .sh extension.

You will use the same process as above to edit and save your bash script. You can follow along with the article, or scroll down and copy the entire script in using nano.

Line 1: The shebang

In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (#!) at the beginning of a script.

#!/bin/bash

We are going to pass the name of the file in as a command line argument and save out a text file with our results. Due to this we will use a command called echo which prints out the value following it. To access the arguments, we will enumerate them with the first being $1 and second being $2 and so on…

We want it to save the name of the file so we will use the command:

echo ""The name of the file is:"" $1

Now we will use the following command to count the number of rows in the csv:

lines=$(wc -l < $1) echo ""The file has"" $lines ""lines""

We create the variable lines which counts the number of lines in our file that we passed in as the command line argument! For some more information on the wc command go here

Then we print out the number of lines the file has!

Next up, we are going to get the column names from our csv file! To do this we will use the following command in our bash script.

colnames=$(head -n 1 < $1)

This creates a variable that has just the first line from our csv in it! Putting all of this together (and a little more that I added so that the date auto populates into the text file) we get the following script:

#!/bin/bash echo ""Data Processed by Elliott Saslow"" DATE=`date +%Y-%m-%d` echo ""Date is: ""$DATE echo """" echo ""The name of the file is:"" $1 echo """" lines=$(wc -l < $1) echo """" echo ""The file has"" $lines ""lines"" echo """" colnames=$(head -n 1 < $1) echo """" echo ""Column names are: "" echo """" echo $colnames

Now to run the script, we save it like we did above with nano, and run it in the command line with the following command:

bash process_data.sh speed.csv > text.txt

This command does the following:

Calls the script

Passes the file that we are looking at ( speed.csv )

) Passes the output to a text file called text.txt

If you run this, and have done everything else correctly, you will have a text file in your folder containing the beginning of quality control checks that you can use for your data pipeline!

Let me know in the comments where you get stuck and how I can help!

Cheers","['save', 'file', 'line', 'script', 'number', 'lines', 'data', 'command', 'following', 'pipelines', 'echo', 'text', 'using', 'bash']","You will use the same process as above to edit and save your bash script.
You can follow along with the article, or scroll down and copy the entire script in using nano.
#!/bin/bashWe are going to pass the name of the file in as a command line argument and save out a text file with our results.
For some more information on the wc command go hereThen we print out the number of lines the file has!
To do this we will use the following command in our bash script.",en,['Elliott Saslow'],2018-12-03 18:01:46.540000+00:00,"{'Computing', 'Data Science', 'Data Pipeline', 'Terminal', 'Command Line'}","{'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/3000/1*L_DEuvFFT6c64SuPdB5ksw.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*L_DEuvFFT6c64SuPdB5ksw.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/160/160/1*dLCgcignlYa9NehXUdTkBw.jpeg', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/1*dLCgcignlYa9NehXUdTkBw.jpeg', 'https://miro.medium.com/max/1200/1*L_DEuvFFT6c64SuPdB5ksw.png'}",2020-03-05 00:26:59.926625,7.0791099071502686
https://towardsdatascience.com/five-command-line-tools-for-data-science-29f04e5b9c16,Five Command Line Tools for Data Science,"One of the most frustrating aspects of data science can be the constant switching between different tools whilst working. You can be editing some code in a Jupyter Notebook, having to install a new tool on the command line and maybe editing a function in an IDE all whilst working on the same task. Sometimes it is nice to find ways of doing more things in the same piece of software.

In the following post, I am going to list some of the best tools I have found for doing data science on the command line. It turns out there are many tasks that can be completed via simple terminal commands than I first thought and I wanted to share some of those here.

This is a useful tool for obtaining data from any server via a variety of protocols including HTTP.

I’ll give a couple of example use cases for obtaining publically available data sets. The UCI Machine Learning Repository is an excellent resource for obtaining datasets for machine learning projects. I am going to use a simple curl command to download a data set taken from the blood transfusion centre in Hsin-Chu City, Taiwan. If we simply run curl [url] which in our example will be curl https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data this will print the data to the terminal.

Adding some additional arguments will download and save the data using a specified filename. The file will now be available in your current working directory.

curl -o data_dl.csv https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data

Another common method of obtaining data for data science projects is via an API. This tool also supports both GET and POST requests for interacting with an API. Running the following command will obtain a single record from the OpenWeatherMap API and save as a JSON file named weather.json . For a more comprehensive tutorial on cURL see this excellent article by Zaiste.

curl -o weather.json -X GET \ 'https://api.openweathermap.org/data/2.5/weather?lat=37.3565982&lon=-121.9689848&units=imperial&appid=fd4698c940c6d1da602a70ac34f0b147' \ -H 'Postman-Token: dcf3c17f-ef3f-4711-85e1-c2d928e1ea1a' \ -H 'cache-control: no-cache'

csvkit is a set of command line tools for working with CSV files. The tasks that it can execute can be divided into three areas: input, processing and output. Let’s look at a quick real-world example of how you can use this.

Firstly let’s install the tool using pip install.

pip install csvkit

For the purposes of this example, I am going to be using the same CSV file I created from the UCI Machine Learning Repository via a curl command above.

First, let’s use csvclean to make sure that our CSV file is in the correct format. This function will automatically fix common CSV errors and remove any bad rows. A useful aspect of this function is that it automatically outputs a new cleaned version of the CSV file so that the raw data is preserved. The new file always has the following naming convention [filename]_out.csv . If you would prefer for the original file to be overwritten you can add the optional -n argument.

csvclean data_dl.csv

In the example file I have, there are no errors but this can be a really useful way to reduce errors further down the line when working with CSV files.

Now let’s say we want to quickly inspect the file. We can use csvcut and csvgrep to do this.

Firstly let’s print out the column names.

csvcut -n data_dl_out.csv | cut -c6- Recency (months) Frequency (times) Monetary (c.c. blood) Time (months) whether he/she donated blood in March 2007

Let’s now determine how many classes there are in the target column whether he/she donated blood in March 2007 .

csvcut -c ""whether he/she donated blood in March 2007"" data_dl_out.csv | sed 1d | sort | uniq 0 1

The csvgrep function allows you to filter CSV files based on regular expression matching.

Let’s use this function to extract only the rows that match class 1.

csvgrep -c ""whether he/she donated blood in March 2007"" -m 1 data_dl_out.csv

You can also use csvkit to perform simple data analysis using the csvstat function.

Simply running csvstat data_dl_out.csv prints descriptive statistics for the entire file to the command line. You can also just request the result of only one statistic with an optional command.

csvstat --mean data_dl_out.csv 1. a: 373.5 2. Recency (months): 9.507 3. Frequency (times): 5.515 4. Monetary (c.c. blood): 1,378.676 5. Time (months): 34.282 6. whether he/she donated blood in March 2007: None

IPython

IPython gives access to enhanced interactive python from the shell. In essence, it means you can do most of the things that you can do in a Jupyter Notebook from the command line.

You can follow these steps to install it if you do not already have it available in your terminal.

To initiate IPython simply type ipython at the command line. You are now in the interactive shell. Here you can import python installed libraries and I find this tool most useful for doing some quick data analysis on the command line.

Let’s perform some basic tasks on the data set we have already been using. First I will import pandas, read in the file and inspect the first few rows of data.

import pandas as pd data = pd.read_csv('data_dl_out.csv') data.head()

The file column names are quite long so next, I am going to use pandas to rename them, and then export the resulting dataframe to a new CSV file for later use.

data = data.rename(columns={'Recency (months)': 'recency',

'Frequency (times)': 'frequency',

'Monetary (c.c. blood)': 'volumne',

'Time (months)': 'time',

'whether he/she donated blood in March 2007': 'target'}) data.to_csv('data_clean.csv')

As a final exercise let’s inspect the correlation between the features and the target variable using the pandas corr() function.

corr_matrix = data.corr()

corr_matrix['target'].sort_values(ascending=False)

To exit IPython simply type exit .

At times you may also want to obtain a data set via a SQL query on a database. The tool csvsql, which is also part of the csvkit tool, supports querying, writing and creating tables directly on a database. It also supports SQL statements for querying a CSV file. Let’s run an example query on the cleaned dataset.

csvsql --query ""select frequency, count(*) as rows from data_clean where target = 1 group by frequency order by 2 desc"" data_clean.csv

Yes, you can perform machine learning at the command line! There are a few tools for this but SciKit-Learn Laboratory is probably one of the most accessible. Let’s build a model using our blood donations data set.

SciKit-Learn laboratory relies on the correct files being placed in consistently named directories. So to begin with we will make a directory named train and copy, move and rename the data file to features.csv .

mkdir train

cp data_clean.csv train/features.csv

Next, we need to create a config file named predict-donations.cfg and place it in our data directory.

[General]

experiment_name = Blood_Donations

task = cross_validate [Input]

train_directory = train

featuresets = [[""features.csv""]]

learners = [""RandomForestClassifier"", ""DecisionTreeClassifier"", ""SVC"", ""MultinomialNB""]

label_col = target [Tuning]

grid_search = false

objective = accuracy [Output]

log = output

results = output

predictions = output

Then we simply run this command run_experiment -l predict-donations.cfg .

This automatically runs the experiment and creates an output folder containing the results.

We can run a SQL query to summarise the results in the Blood_Donations_summary.tsv file.

cd output < Blood_Donations_summary.tsv csvsql --query ""SELECT learner_name, accuracy FROM stdin ""\

> ""WHERE fold = 'average' ORDER BY accuracy DESC"" | csvlook

There are many other command line tools that can be useful for data science but I wanted to highlight here those that I had found useful in my work. For a really comprehensive view of data science at the command line, I found the book Data Science at the Command Line which is freely available online to be extremely useful.","['file', 'csv', 'blood', 'lets', 'tools', 'useful', 'data', 'command', 'line', 'tool', 'using', 'science']","In the following post, I am going to list some of the best tools I have found for doing data science on the command line.
curl -o data_dl.csv https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.dataAnother common method of obtaining data for data science projects is via an API.
curl -o weather.json -X GET \ 'https://api.openweathermap.org/data/2.5/weather?lat=37.3565982&lon=-121.9689848&units=imperial&appid=fd4698c940c6d1da602a70ac34f0b147' \ -H 'Postman-Token: dcf3c17f-ef3f-4711-85e1-c2d928e1ea1a' \ -H 'cache-control: no-cache'csvkit is a set of command line tools for working with CSV files.
In essence, it means you can do most of the things that you can do in a Jupyter Notebook from the command line.
For a really comprehensive view of data science at the command line, I found the book Data Science at the Command Line which is freely available online to be extremely useful.",en,['Rebecca Vickery'],2019-06-21 13:12:35.359000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Programming'}","{'https://miro.medium.com/fit/c/96/96/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/max/60/1*CFyHZsLYclR_QPUIpy2fRw.jpeg?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/382/1*2x0Wphg8u0OHW-RR91iRJQ.png', 'https://miro.medium.com/fit/c/160/160/1*rhvwW5suGypWKG_iJqFWcA.jpeg', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1200/1*CFyHZsLYclR_QPUIpy2fRw.jpeg', 'https://miro.medium.com/max/10128/1*CFyHZsLYclR_QPUIpy2fRw.jpeg', 'https://miro.medium.com/max/60/1*2x0Wphg8u0OHW-RR91iRJQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png'}",2020-03-05 00:27:03.193163,3.2655813694000244
https://towardsdatascience.com/recommendation-systems-in-the-real-world-51e3948772f3,Recommendation Systems in the Real world,"Recommender Pipeline

A typical recommender system pipeline consists of the following five phases:

A typical recommender system pipeline

Let’s say we are building a movie recommender system. The system has no prior knowledge of the users or the movies but only the interactions that users have with the movies through rating given by them. Here is a dataframe which consists of movie ID, user ID and the ratings of the movie.

Movie rating Dataframe

Since we only have the ratings with us and nothing else, we shall be using collaborative filtering for our Recommender system.

1. Pre-Processing

Utility matrix conversion

We need to first transform the movie rating dataframe into an user-item matrix, also called a utility matrix.

Every cell of the matrix is populated by the ratings that the user has given for the movie. This matrix is typically represented as a scipy sparse matrix since many of the cells are empty due to the absence of any rating for that particular movie. Collaborative filtering doesn’t work well if the data is sparse so we need to calculate the sparsity of the matrix.

If the sparsity value comes out to be around 0.5 or more, then collaborative filtering might not be the best solution. Another important point to note here is that the empty cells actually represent new users and new movies. Therefore, if there is a high proportion of new users then again we might think of using some other recommender methods like content-based filtering or hybrid filtering.

Normalization

There will always be users who are overly positive(always leave a 4 or 5 rating) or overly negative(rate everything as 1 or 2). Therefore we need to normalise the ratings to account for the user and item bias. This can be done by taking the Mean Normalisation.

2. Model Training

After the data has been pre-processed we need to start the model building process. Matrix Factorisation is a commonly used technique in collaborative filtering although there are other methods also like Neighbourhood methods. Here are the steps involved:

Factorize the user-item matrix to get 2 latent factor matrices — user-factor matrix and item-factor matrix.

The user ratings are features of the movies that are generated by humans. These features are directly observable things that we assume are important. However, there are also a certain set of features which are not directly observable but are also important in rating predictions. These set of hidden features are called Latent features.

The Latent Features can be thought of as features that underlie the interactions between users and items. Essentially, we do not explicitly know what each latent feature represents but it can be assumed that one feature might represent that a user likes a comedy movie and another latent feature could represent that user likes animation movie and so on.

Predict missing ratings from the inner product of these two latent matrices.

Latent factors here are represented by K. This reconstructed matrix populates the empty cells in the original user-item matrix and so the unknown ratings are now known.

But how do we implement the Matrix Factorisation shown above? Well, it turns out that there are a number of ways of doing that by using one of the methods below:

Alternating Least Squares(ALS)

Stochastic Gradient Descent(SGD)

Singular Value Decomposition(SVD)

3. Hyperparameter Optimisation

Before tuning the parameters we need to pick up an evaluation metric. A popular evaluation metric for recommenders is Precision at K which looks at the top k recommendations and calculates what proportion of those recommendations were actually relevant to a user.

Therefore, our goal is to find the parameters that give the best precision at K or any other evaluation metric that one wants to optimize. Once the parameters are found, we can re-train our model to get our predicted ratings and we can use these results to generate our recommendations.

4. Post Processing

We can then sort all of the predicted ratings and get the top N recommendations for the user. We would also want to exclude or filter out items that a user has already interacted with before. In the case of movies, there is no point in recommending a movie that a user has previously watched or disliked earlier.

5. Evaluation

We have already covered this before but let’s talk in a bit more detail here. The best way to evaluate any recommender system is to test it out in the wild. Techniques like A/B testing is the best since one can get actual feedback from real users. However, if that’s not possible, then we have to resort to some offline evaluation.

In traditional machine learning, we split our original dataset to create a training set and a validation set. This, however, doesn’t work for recommender models since the model won’t work if we train all of our data on a separate user population and validate it on another. So for recommenders, we actually mask some of the known ratings in the matrix randomly. We then predict these masked ratings through machine learning and then compare the predicted rating with the actual rating.

Earlier we talked about Precision as an evaluation metric. Here are some of the others that can be used.","['latent', 'user', 'recommendation', 'users', 'features', 'world', 'movie', 'ratings', 'rating', 'real', 'systems', 'system', 'recommender', 'matrix']","Recommender PipelineA typical recommender system pipeline consists of the following five phases:A typical recommender system pipelineLet’s say we are building a movie recommender system.
Movie rating DataframeSince we only have the ratings with us and nothing else, we shall be using collaborative filtering for our Recommender system.
Pre-ProcessingUtility matrix conversionWe need to first transform the movie rating dataframe into an user-item matrix, also called a utility matrix.
These set of hidden features are called Latent features.
The best way to evaluate any recommender system is to test it out in the wild.",en,['Parul Pandey'],2019-05-25 04:48:36.849000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Recommender Systems'}","{'https://miro.medium.com/max/1352/1*MeO9aR25JByg_K0AmXcF9A.png', 'https://miro.medium.com/max/60/1*3kSprT4pi3oeZjlkopYOIQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/1396/1*_aUEQxvHqb5mY2TOBhULoA.png', 'https://miro.medium.com/max/60/1*bFmQ2DzGokaBUssGkFR3gg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*zZsy_ahEMMBHryeiQ4ZDiA.png?q=20', 'https://miro.medium.com/max/1560/1*HCELzfFP_VxVbrOGbgMLMQ.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/60/1*HCELzfFP_VxVbrOGbgMLMQ.png?q=20', 'https://miro.medium.com/max/1218/1*zZsy_ahEMMBHryeiQ4ZDiA.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2086/1*Na5M52RRnAjYRyPLN29TgA.png', 'https://miro.medium.com/max/1394/1*bFmQ2DzGokaBUssGkFR3gg.png', 'https://miro.medium.com/max/60/1*MeO9aR25JByg_K0AmXcF9A.png?q=20', 'https://miro.medium.com/max/524/1*3kSprT4pi3oeZjlkopYOIQ.png', 'https://miro.medium.com/max/60/1*Na5M52RRnAjYRyPLN29TgA.png?q=20', 'https://miro.medium.com/max/6000/1*EEeiDk1zxT19PYAnktnzgw.jpeg', 'https://miro.medium.com/max/60/1*EEeiDk1zxT19PYAnktnzgw.jpeg?q=20', 'https://miro.medium.com/max/60/1*4hxzLTEYgdlhF6DqQk5Fng.png?q=20', 'https://miro.medium.com/max/60/1*_aUEQxvHqb5mY2TOBhULoA.png?q=20', 'https://miro.medium.com/max/1326/1*6xDYvxKsI1Nh2krcw7eMYg.png', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/60/1*Dkwu3kA-N13H8D8l4ONNTQ.png?q=20', 'https://miro.medium.com/max/1354/1*4hxzLTEYgdlhF6DqQk5Fng.png', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/698/1*Bh3VV5prvEyW6F25LL-v0Q.png', 'https://miro.medium.com/max/1200/1*EEeiDk1zxT19PYAnktnzgw.jpeg', 'https://miro.medium.com/max/60/1*6xDYvxKsI1Nh2krcw7eMYg.png?q=20', 'https://miro.medium.com/max/1386/1*Dkwu3kA-N13H8D8l4ONNTQ.png', 'https://miro.medium.com/max/60/1*rko110OgQP-FvIMf2H0gvg.png?q=20', 'https://miro.medium.com/max/60/1*Bh3VV5prvEyW6F25LL-v0Q.png?q=20', 'https://miro.medium.com/max/1328/1*rko110OgQP-FvIMf2H0gvg.png'}",2020-03-05 00:27:10.852238,7.658074378967285
https://towardsdatascience.com/the-remarkable-world-of-recommender-systems-bff4b9cbe6a7,The Remarkable world of Recommender Systems,"Recommender Systems

Recommendation Engines try to make a product or service recommendation to people. In a way, Recommenders try to narrow down choices for people by presenting them with suggestions that they are most likely to buy or use. Recommendation systems are almost everywhere from Amazon to Netflix; from Facebook to Linkedin. In fact, a large chunk of Amazon’s revenue is generated from recommendations alone. Companies like Youtube and Netflix depend on their recommendation engines to help users discover new content. Some examples of recommendations in our everyday lives are:

Amazon

Amazon uses data from its millions of customers to identify which items are usually bought together and makes recommendations based on that. The recommendations in Amazon.com are provided on the basis of explicitly provided ratings, buying behaviour, and browsing history.

I intended to buy ‘Show Dog’ but ended up buying ‘The Compound effect’ too!

Linkedin

Linkedin utilises data from your past experience, current job titles and endorsements to suggest probable jobs to you.

Netflix

When we rate a movie or set up our preferences on Netflix, it uses this data and similar data from hundreds of other subscribers to recommend movies and shows. These ratings and actions are then used by Netflix to make recommendations.

Facebook

Recommender systems, such as Facebook, do not directly recommend products but they recommend connections.

Apart from this, Spotify, Youtube, IMDB, Trip Advisor, Google News, and many other platforms continuously give out recommendations and suggestions to suit our needs.","['recommendation', 'youtube', 'uses', 'try', 'world', 'remarkable', 'suggestions', 'data', 'systems', 'recommender', 'netflix', 'recommendations', 'recommend']","Recommender SystemsRecommendation Engines try to make a product or service recommendation to people.
Recommendation systems are almost everywhere from Amazon to Netflix; from Facebook to Linkedin.
Companies like Youtube and Netflix depend on their recommendation engines to help users discover new content.
Some examples of recommendations in our everyday lives are:AmazonAmazon uses data from its millions of customers to identify which items are usually bought together and makes recommendations based on that.
FacebookRecommender systems, such as Facebook, do not directly recommend products but they recommend connections.",en,['Parul Pandey'],2019-07-19 04:55:51.412000+00:00,"{'Data Science', 'Artificial Intelligence', 'Machine Learning', 'Towards Data Science', 'Recommender Systems'}","{'https://miro.medium.com/max/934/1*pABODAsgn-FYopXwjS-PPw.png', 'https://miro.medium.com/max/60/1*drbslVSlF6M5WL1NsBdRQQ.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/max/1330/0*oU4EPrrgiVW4CGH8', 'https://miro.medium.com/max/780/1*O-LoIu6zKMsAamgqYwR-Yg.png', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/8440/1*zbDABl5-RaISCsXvsbPkbQ.jpeg', 'https://miro.medium.com/max/60/1*j1g-Ha4pGvpdzm2gh39x9Q.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*Ug0Cz6zLClCm_VV8WELMNQ.png?q=20', 'https://miro.medium.com/max/1558/1*drbslVSlF6M5WL1NsBdRQQ.png', 'https://miro.medium.com/max/1200/1*zbDABl5-RaISCsXvsbPkbQ.jpeg', 'https://miro.medium.com/max/716/1*rn1bn4MMTYlamfamiA7IPw.png', 'https://miro.medium.com/max/58/1*pABODAsgn-FYopXwjS-PPw.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/696/1*-Nj7PxJWhSCaEFo5vzdiLQ.png', 'https://miro.medium.com/max/686/1*qxHSjUipRWF6qP082zisdQ.png', 'https://miro.medium.com/max/60/1*pzIT4ad3xbWzLIHew97GHA.png?q=20', 'https://miro.medium.com/max/554/1*mND7miwG0wy4B94WXC6WLw.png', 'https://miro.medium.com/max/48/1*mND7miwG0wy4B94WXC6WLw.png?q=20', 'https://miro.medium.com/max/48/1*-Nj7PxJWhSCaEFo5vzdiLQ.png?q=20', 'https://miro.medium.com/max/1576/1*j1g-Ha4pGvpdzm2gh39x9Q.png', 'https://miro.medium.com/max/60/1*PnF2IvO6reoZ81c_T9OaVQ.png?q=20', 'https://miro.medium.com/max/60/0*oU4EPrrgiVW4CGH8?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/60/1*qxHSjUipRWF6qP082zisdQ.png?q=20', 'https://miro.medium.com/max/1490/1*pzIT4ad3xbWzLIHew97GHA.png', 'https://miro.medium.com/max/60/1*zbDABl5-RaISCsXvsbPkbQ.jpeg?q=20', 'https://miro.medium.com/max/50/1*rn1bn4MMTYlamfamiA7IPw.png?q=20', 'https://miro.medium.com/max/1232/1*PnF2IvO6reoZ81c_T9OaVQ.png', 'https://miro.medium.com/max/1670/1*27HylcAcNaG41CMaPZi8SQ.png', 'https://miro.medium.com/fit/c/96/96/1*-ooorT2_5GQSfQoVFxJHXw.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1600/1*Ug0Cz6zLClCm_VV8WELMNQ.png', 'https://miro.medium.com/max/60/1*O-LoIu6zKMsAamgqYwR-Yg.png?q=20', 'https://miro.medium.com/max/60/1*27HylcAcNaG41CMaPZi8SQ.png?q=20'}",2020-03-05 00:27:13.205247,2.3520092964172363
https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9,Building a Recommendation System Using Neural Network Embeddings,"Supervised Learning Task

To learn meaningful embeddings, our neural network must be trained to accomplish an objective. Working from the guiding assumption of the project — that similar books link to similar Wikipedia pages — we can formulate the problem as follows: given a (book title, wikilink) pair, determine if the wikilink is present in the book’s article.

We won’t actually need to give the network the book article. Instead, we’ll feed in hundreds of thousands of training examples consisting of book title, wikilink, and label. We give the network some true examples — actually present in the dataset — and some false examples, and eventually it learns embeddings to distinguish when a wikilink is on a book’s page.

Expressing the supervised learning task is the most important part of this project. Embeddings are learned for a specific task and are relevant only to that problem. If our task was to determine which books were written by Jane Austen, then the embeddings would reflect that goal, placing books written by Austen closer together in embedding space. We hope that by training to tell if a book has a certain wikilink on its page, the network learns embeddings that places similar books — in terms of content — closer to one another.

Once we’ve outlined the learning task, we need to implement it in code. To get started, because the neural network can only accept integer inputs, we create a mapping from each unique book to an integer:

# Mapping of books to index and index to books

book_index = {book[0]: idx for idx, book in enumerate(books)}



book_index['Anna Karenina'] 22494

We also do the same thing with the links. After this, to create a training set, we make a list of all (book, wikilink) pairs in the data. This requires iterating through each book and recording an example for each wikilink on its page:

pairs = []



# Iterate through each book

for book in books:



title = book[0]

book_links = book[2] # Iterate through wikilinks in book article

for link in book_links: # Add index of book and index of link to pairs

pairs.extend((book_index[title],

link_index[link]))

This gives us a total of 772798 true examples that we can sample from to train the model. To generate the false examples — done later — we’ll simply pick a link index and book index at random, make sure it’s not in the pairs , and then use it as a negative observation.

Note about Training / Testing Sets

While using a separate validation and testing set is a must for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate embeddings. The prediction task is just the means by which we train our network for those embeddings. At the end of training, we are not going to be testing our model on new data, so we don’t need to evaluate the performance or use a validation set to prevent overfitting. To get the best embeddings, we’ll use all examples for training.","['network', 'task', 'wikilink', 'books', 'index', 'book', 'examples', 'recommendation', 'link', 'neural', 'embeddings', 'system', 'training', 'building', 'using']","Supervised Learning TaskTo learn meaningful embeddings, our neural network must be trained to accomplish an objective.
We won’t actually need to give the network the book article.
Instead, we’ll feed in hundreds of thousands of training examples consisting of book title, wikilink, and label.
After this, to create a training set, we make a list of all (book, wikilink) pairs in the data.
The prediction task is just the means by which we train our network for those embeddings.",en,['Will Koehrsen'],2018-10-06 03:50:36.059000+00:00,"{'Deep Learning', 'Data Science', 'Machine Learning', 'Towards Data Science', 'Education'}","{'https://miro.medium.com/max/60/1*A0D3WPIfYZk4-Mxy-IH6lQ.png?q=20', 'https://miro.medium.com/max/1796/1*g3GnDEKtQ0kGJUGtXoA4rg.png', 'https://miro.medium.com/max/3760/1*YHEInlkRiN0xJ3SaDNAtlw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/max/1802/1*A0D3WPIfYZk4-Mxy-IH6lQ.png', 'https://miro.medium.com/max/60/1*9Gq6-KxBafIu8yGGokuwXA.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/max/2098/1*9Gq6-KxBafIu8yGGokuwXA.png', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*BMJpJIPdzE9kIjQi4ZAW6w.png?q=20', 'https://miro.medium.com/max/1866/1*R1FbOGXW8h6DGUlu_t_iow.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/1600/1*thJA-2tRfHaW3kpoRNarUw.gif', 'https://miro.medium.com/max/60/1*ohzYzc7o1sGkihmaiDZzqQ.png?q=20', 'https://miro.medium.com/max/1816/1*nj8vqBB7P9d0nMEzORkpdA.png', 'https://miro.medium.com/max/60/1*YHEInlkRiN0xJ3SaDNAtlw.jpeg?q=20', 'https://miro.medium.com/max/60/1*UOB8aLykAROuW7cTzKiyyA.png?q=20', 'https://miro.medium.com/max/60/1*g3GnDEKtQ0kGJUGtXoA4rg.png?q=20', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1934/1*UOB8aLykAROuW7cTzKiyyA.png', 'https://miro.medium.com/max/2062/1*VA3TJ_N5pOGkIsNu_jiFWg.png', 'https://miro.medium.com/max/60/1*VA3TJ_N5pOGkIsNu_jiFWg.png?q=20', 'https://miro.medium.com/max/60/1*nj8vqBB7P9d0nMEzORkpdA.png?q=20', 'https://miro.medium.com/max/2016/1*BMJpJIPdzE9kIjQi4ZAW6w.png', 'https://miro.medium.com/max/1396/1*ohzYzc7o1sGkihmaiDZzqQ.png', 'https://miro.medium.com/freeze/max/60/1*thJA-2tRfHaW3kpoRNarUw.gif?q=20', 'https://miro.medium.com/fit/c/160/160/1*SckxdIFfjlR-cWXkL5ya-g.jpeg', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/60/1*R1FbOGXW8h6DGUlu_t_iow.png?q=20', 'https://miro.medium.com/max/1200/1*YHEInlkRiN0xJ3SaDNAtlw.jpeg'}",2020-03-05 00:27:19.804870,6.599623441696167
https://towardsdatascience.com/developing-a-prescriptive-recommender-system-through-matrix-factorization-8b0c69cce611,Developing a prescriptive recommender system through Matrix Factorization,"By — Ravindra Shukla ( AI/ML practitioner)

Abstract — Matrix Factorization is a very powerful algorithm with many valuable use cases in multiple industries. Some of the well-known application of the Matrix Factorization is — Netflix 1-million-dollar prize for movie recommendation and Amazon online application (much-refined version) for recommending books to various readers. Although it is a very popular method, it has some limitations. Our focus in the current paper is to understand latent factors and how to make it more prescriptive. The algorithm works well when we have products in the set already rated by a few users OR every user has rated few products. However, if we are including new products in the set which have not been rated by any user or new users in the set whose preferences for the products are not known — it will be difficult to create a recommendation for unrated products and the new users. There are challenges in co-relating the underlying features of the products and users’ preferences based on key features. The paper below addresses the challenge of interpreting latent factors related to the product and the user features. Once we have clarity on the products and the user features — we can move towards the prescriptive journey. It will help in designing future products with more popular features OR finding a market for a particular product.

Background — I generally quote Wikipedia for the understanding of the basic terms and background.

Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.

https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)

There is a constant need to correlate two entities around us. Think of scenarios — Patients vs Diseases, Users vs Products, Men vs Women, Users vs Movies preference, and Job applicants vs Open positions. We can expand the concept to many other scenarios in our daily lives and find relevance. When we are trying to find a correlation between two entities, matrix factorization comes very handily.

Use Cases of Matrix Factorization -

· Marketing strategy

· Cross-sell and Up-sell

· Calculate product propensity and customer propensity

· Recommendation Engine/Personalization

· Launching new products

· Understanding customer360

· Filtering and recommendation at job application sites

· Filtering and recommendation in matrimonial or dating sites

The above scenarios are just a few samples, the concept is very generic and has a wide scope for usage.

Innovative use — In-fact in one of our previous projects, we used Matrix factorization to fill missing values in the dataset. The exercise was to calculate health score (a concept very similar to credit score — where higher health score is indicative of better health). Let us say we are missing A1C reading of one patient (P1), we can correlate — which other patients have similar features (age, sex, BMI, demography, blood pressure, cholesterol, lifestyle, etc), to patient P1 and estimate A1C reading of P1 accordingly.

Basics in Machine Learning — Usage of Matrices and vectors

Before we go deep into Matrix factorization and how to interpret latent factors, let us cover some basics in machine learning.

Machine Learning is based on mathematics which involves iterative optimization based on gradient descent (https://en.wikipedia.org/wiki/Gradient_descent) to minimize the error in prediction. Data that is stored in the database cannot be passed as input to the algorithms, it needs to be represented in terms of a matrix. Matrix is a combination of different vectors, which is useful in defining mathematical operations.

A User may have the following features — User-id, User-Name, Address, Age, Sex, Income, Education, preferences for the product’s features, User-type (students, workers, IT workers, Salesperson)

Similarly, we can define the basic features of a patient based on his/her biometric readings.

Products like Laptop will have features such as — touch screen, RAM, CPU, Brand, Color, Cost (range), etc. Products like a Book will have features such as — Author, Subject, period-written, Genre, poetry vs prose, Fiction vs Non-fiction, Long vs short, hardcover vs softcover, paper copy vs e-book, technical books, etc

However, we can’t use the above data as it is in Machine Learning algorithms. Data need to be arranged in the form of vectors and matrices so that we can pass them as a matrix in mathematical operations.

Representing an instance of the entity in mathematical term — Let us see how to represent a patient in N dimension with key features -

A patient can have the following key features (sample) –

· Patient-id — it is a unique identifier useful in the database. However, it is not relevant for machine learning and algorithms to work based on key features. The ID is just an identifier within the database system.

· Name — similar to the Patient-id, no significance in ML

· Other Demographic info (race, ethnicity, marital status, income, education, and employment)

· Age — between 0 to 120 years

· Sex — Male or Female

· Location (State, Country)

· Bio-metric readings (Blood pressure — Systolic (range — 0–200, Diastolic — 0–200), Blood Sugar 0–400, A1C (0–10), Cholesterol (0–400), Smoking — Yes or No, Chronic condition — Yes or No, Comorbidity — Yes or No)

It can have many more features, but the above detail is good enough for our discussion purpose.

Patient record in Epic System database -

An instance of the patient (a record) will be stored in a regular database like below -

A patient record in regular database

To represent a patient in N dimensions as a vector (P) so that we pass it as input in the ML algorithm –

patient record with numeric values only

We need to convert all text data into numbers and make it meaningful so that it can be used in the calculation. There is a special way to expand categorical variables (for example — gender, state, comorbidity, etc). The process is called Onehotencoder. There are many other activities in data pre-processing, but we are not covering those details here (like normalize all the values, clean the missing data, etc before invoking ML algorithms).

The above representation of the patient instance is called the P vector. We combine all instances of patient vector to create the Patient matrix P.

What is the meaning of vector multiplication –

De-factorizing a known matrix (Rating matrix) in two unknown matrices (user features and product features) is key exercise in Matrix Factorization. Let us understand what the significance of multiplication of two matrices is (user matrix and product matrix)

A = a1.i + a2.j + a3.k — Vector A in three dimensions (let us say — this is user feature vector)

B = b1.i + b2.j + b3.k — Vector B in three dimensions (let us say this is product feature vector)

A x B = a1.b1 + a2.b2 + a3.b3 — Multiplication of A and B vectors

Let us say i represents the safety/security, j represents family vehicle and k represents the performance coefficient of the product and user vector. The value of (AxB) will be higher for the user who prefers a higher-performing family vehicle with good security features. For a user who prefers sports car with high maintenance costs will have a lower value of AxB for the same vehicle. We can clearly see a higher value of AxB will indicate better affinity of user A for product B. Let us take an example to understand — what is the significance of two vectors multiplication.

Neeru likes classic action movies, directed by Akira Kurosawa with a good screenplay, good editing, and background music. She does not like horror, comedy or suspense or movies with a poor storyline.

User vector — Neeru — P1

Product vector Q1

Product vector — Q2

If we multiply two vectors –

P1 (Neeru instance of a user) x Q1 (Seven samurai — instance of a product) = (.8x1 + .1x.2 + .3x.5 + .9x.8 + 1x1 + .4x0 + .4x.8 + .2x.2 + .7x1) = 3.75

P1 (Neeru instance of a user) x Q2 (Blair Witch — instance of a product) = (.8x.2 + .1x.8 + .3x.3 + .9x.4 + 1x0 + .4x.2 + .4x.6 + .2x.6 + .7x0) = 1.13

This clearly indicates Neeru prefers watching Seven Samurai over Blair Witch.

Interpreting the latent factors -

Matrix Factorization is based on the following equation -

R = P x QT

· R is a matrix of rated products by various users

· P is the User matrix

· Q is Product matrix

· QT is the transpose of the matrix Q

We do not have the details of Products and Users features. We just have users’ preferences (ratings) for the various products (derived from historical data). The basic approach is — start with assuming certain coefficients for Products and Users matrix, derive the predicted value of the ratings, find the difference between predicted vs actual value as an error, sum the errors, calculate the gradient to change the coefficient value and recalculate the predicted value and error again. Keep iterating till you converge to a certain minimum error. All this can be done with 50 lines of code in Python.

Same above equations with dimensions can be written as -

R [M, N] = P[M, K] x QT[K, N]

R[i,j] — Value of rated product j (Qj) from user i (Pi)

Q [i][1, 2, 3, ….k] = Product Qi vector with k key features

P[j][1, 2, 3, ….k] = User Pi vector defined by k key features

We are assuming to have K latent features in the above equation.

What are the Latent Factors?

In Matrix factorization, our effort is to relate two entities (ex. Products and Users) through key features. Every Product will have some key features and every user liking to the product will depend upon his/her preferences for those key features.

Many of these correlations can be established through Matrix factorization. When I first read about matrix factorization and how to use gradient descent to achieve the prediction, I was excited. So much can be achieved with 50–60 lines of basic coding in python. The excitement was short-lived when I started thinking — how do I make it more prescriptive. Mathematics beauty is lost unless we find a practical way to interpret and expand it.

We do a basic assumption that two entities are related through K latent factors. K is just a number. We do not know what do these K factors represent. That’s why they are also called latent. This creates critical hurdles in making Matrix factorization more prescriptive.

I tried different combinations to derive some meaning of latent factors. One of them was — keep increasing the value of K and see if, after a certain number, the increasing value of K may not have an impact on user and product coefficients. User and product coefficients converge and increasing K further will not matter. It did not work out.

In-fact when I increased the value of K and tried multiple integrations — I started getting different combinations of matrix P and Q coefficients.

Deciphering Matrix decomposition -

Let us analyze the details with an example.

R Matrix — rating for 4 different products by 8 users.

Rating Matrix — R

Let us assume — we have four latent factors. In other words, products have 4 main features and users will have preferences for the products based on these key features.

R [8x4] = P[8x4] X Q[4x4]

P (Users matrix) — expanded for four latent features

Users Matrix — P

QT (Transpose of Product Matrix — Q) — expanded for four latent factors

Product Matrix Transpose — QT

This is a purely Mathematical exercise where we have known values of matrix — R (not all ratings are known). We start by initializing the User matrix and the Product matrix with random values.

User U4 rating for Product P2 (u4p2) is the multiplication of two vectors — User (u4) and Product (p2).

u4p2 = u41.p21 + u42.p22 + u43.p23 + u44.p24

If we notice the above equation — Preference on any product by a particular user is directly proportional to sum of coefficient of various dimensions of two vectors.

Let us look at the Matrix factorization equation again –

R = P x Q

Here both P and Q are unknown. Only a few coefficients of R (users rated products) values are known.

There are multiple solution pairs (P, Q) for R when both are P and Q are variables.

u4p2 = u41.p21 + u42.p22 + u43.p23 + u44.p24

Original Matrix R —

Original Rating Matrix — R

High-lighted cells are the ones that were not rated by users. They become test cells.

Non-lighted cells are where we already know the product ratings.

I tried multiple iterations to solve the values of P and Q matrix and got different results Q every time.

Iteration-1:

Predicted rating Matrix — X after iteration-1

Users Matrix — P after iteration 1

Product Matrix Q after iteration 1

X — Predicted values for User-product rating (R is the original matrix)

P — User matrix after de-factorization

Q — Product matrix after de-factorization

Iteration-2: Output from iteration-2

Matrices after iteration 2

Iteration-3: Output from iteration-3

Matrices after iteration 3

Iteration-4: Output from iteration-4

Matrices after iteration 4

In simple term — we have 8x4 + 4x4 = 48 variables but less than 32 equations. So it’s obvious — we will have multiple pairs for P and Q matrix.

This is the struggle just to get a consistent value of P and Q, besides interpreting what do k1, k2, k3, k4 signify? Unless we have consistency in getting converging values of P and Q, our prediction for unrated products will keep fluctuating.

Issues with the current model –

De-factoring helps us in understanding — how can we predict the preference for other users who have not yet rated/or used certain products. Based on predicted preferences — we can decide the marketing strategy for better sales (cross-sell and up-sell).

1. Number of latent factors are unknown

2. Every iteration will give different matrix for the user and product matrix — this means there is no consistency in the prediction

3. Latent factors are not known — this creates an issue in making model prescriptive. It means — suppose we are launching a new product, we will not be sure — which features we should concentrate more on to get better sales.

Solution -

When we have only one equation R = P x Q, one way to solve this is to fix the variable Q (product features) so that we have to deal with only one variable (P — user features).

Empirically thinking — that is not a bad option. If we are selling certain products, it is better we know what the product’s features are and how many of the key features we have.

Let us understand how fixing the value of K and the matrix Q will help in getting consistent results for P and converging prediction for value R.

When we analyzed above equation –

Product rating (u4p2) = u41.p21 + u42.p22 + u43.p23 + u44.p24

User feature vector — u4 = u41.i + u42.j + u43.k + u44.t

Product feature vector — p2 = p21.i + p22.j + p23.k + p24.t

where i, j, k, t — are four different dimensions of the vector. Each dimension corresponds to a unique feature.

When we fix the value of Q to –

Product Matrix Q (fixed one)

We start getting consistency in the value of P and predicted values of rating (X) and user matrix — P -

Iteration-1:

Rating Matrix X and User Matrix P after iteration 1

The user matrix (P) starts converging. Please see the values for iteration 2, 3, and 4.

Iteration-2:

Matrices after iteration 2

Iteration-3:

Matrices after iteration 3

Iteration-4:

Matrices after iteration 4

Based on the above data — we can determine the User feature vector, which represents User preferences.

Now the key question remains — how to fix the product matrix?

Option-1:

The number of features — K — has to be estimated based on product features understanding.

Coefficient values for the product matrix — can range from 0 to 1 (0 indicates feature does not have any impact on the product, 1 indicates — feature has the highest impact on the product).

Once we know what do features (k1, k2, ….Kn) means, we can assign coefficients ranging from 0 to 1 based on their contribution to the product’s quality.

Both business SME and technical teams need to work together to determine initial key features and determining the weight of coefficients.

Option-2:

The number of features — K — has to be estimated based on product features understanding (similar to option-1).

Now de-factorize matrix R and determine matrix P and Q in a couple of iterations. Take a best guess based on two iterations of Q values and understanding of the product features.

Fix the value of Q and solve for matrix P. Coefficients of matrix P will converge once matrix Q is fixed.

In-fact fixing the value of K and product coefficients are good measures. In that way, we can calculate the equivalent coefficient of the user matrix.

Let us say — we decided to categorize products based on 5 key features and determine — what is the significance of each coefficient in overall product preference — This will help in fixing the relevant features in the user matrix where which coefficient corresponds to which feature will also be known.

Summary of the key steps –

1. Assume K — based on product features understanding

2. Start with random values of P and Q coefficients

3. Iterate and determine the first set of P and Q values

4. Review Q coefficients — try to align Q with product features (Product/Items features should be known)

5. Fix value of Q and number of key features (K — latent factors)

6. Adjust the Q matrix (product features)

7. Use the adjusted value of Q to determine the value of P

Key things to keep in mind –

I have ignored the user’s bias in product rating (just to make calculation simpler).

Product features correspond to the individual dimension. Dimensions are perpendicular to one another. There should be no co-linearity in the feature vector.

While we are determining the Product features let us say k1, k2, k3, k4… we can interpret initial meaning attached to different latent factors based on relative values of coefficients (higher dominating features will have a higher coefficient).

User features will correspond to the same set of product features. Let us say if Product k1 represents the Product security feature, User k1 will indicate user preference for security.

Conclusion –

Recently I was watching Google CEO (Sundar Pichai) speech at Google 2018 I/O where he talked about predicting cardiovascular events based on eye scanned image.

Impact of many past events (medical history, environment, habits, medications, etc) will be influencing many other events (health condition — Cardiovascular event) and eye internals.

A set of events has occurred in the past. Now they are currently influencing the chances of cardiac arrest as well as internal of eyes.

By measuring the changes/conditions of eye scan — we can correlate what is the chance of cardiac arrest.

Understanding common factors which impact heart and Eyes conditions

Similarly, we can find many other usages — Disease correlation vs water quality in different locations.

Above relation, we can also map in terms of neural network layers in deep learning.

Entity 1 (E) — Different types of scanned Eye images

Entity 2 (H) — Different types of heart conditions

Latent factors — Different sets of activities, previous medical history, demography, etc are the feature vectors. They act as input in deep learning models. We can collect various data patient records on — Eye scan and heart conditions. Using Matrix factorization — we can determine the correlation between two entities.

Representation in neural network term

I recently came to know — Prostate cancer is very prevalent in Eastern UP. We just need to collect data of different locations vs disease prevalent in that area, understand what are the possible latent factors and go deeper into prescriptive medicine. This can be very useful in the context of population health management (ACO).

Similarly, we can study the impact of water quality, air quality, industry presence, pollution level, soil quality on various diseases in a different geography.

Audience — Implementing a data science project needs collaboration from both the business team as well as the technical team (data engineering and data science team). The data science group develops the algorithm to model the data and evaluate the performance while the business group helps in understanding the product features and collecting the relevant data for the modeling.

It will be critical to utilize business SME experience in understanding the relevance of the product’s features, putting them into the right sequence, assigning the correct weight while initializing users and products matrix. Hence both the teams need to work closely to create robust/prescriptive systems.

References –

https://www.youtube.com/watch?v=MSpF84kevyU

http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/

https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)

https://www.youtube.com/watch?v=XkY2DOUCWMU&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=5&t=0s

http://infolab.stanford.edu/~ullman/mmds/ch9.pdf","['factorization', 'k', 'developing', 'user', 'q', 'users', 'product', 'features', 'products', 'p', 'prescriptive', 'system', 'recommender', 'matrix', 'value']","We combine all instances of patient vector to create the Patient matrix P.What is the meaning of vector multiplication –De-factorizing a known matrix (Rating matrix) in two unknown matrices (user features and product features) is key exercise in Matrix Factorization.
We start by initializing the User matrix and the Product matrix with random values.
Every iteration will give different matrix for the user and product matrix — this means there is no consistency in the prediction3.
When we fix the value of Q to –Product Matrix Q (fixed one)We start getting consistency in the value of P and predicted values of rating (X) and user matrix — P -Iteration-1:Rating Matrix X and User Matrix P after iteration 1The user matrix (P) starts converging.
Fix the value of Q and solve for matrix P. Coefficients of matrix P will converge once matrix Q is fixed.",en,['Ravindra Shukla'],2019-12-11 18:16:20.173000+00:00,"{'AI', 'Matrix Factorization', 'Machine Learning', 'Recommender Systems', 'Recommender Engine'}","{'https://miro.medium.com/max/30/1*8sYH9hOqKh3pn6E5xAYZqw.png?q=20', 'https://miro.medium.com/max/1492/1*2nR9MlkNo8XqVxjbYqSJLA.png', 'https://miro.medium.com/max/2742/1*-XgSqd5e409IEJDAsioQ0g.png', 'https://miro.medium.com/max/1558/1*bfS1uMi1B48cCaQ4vjEHkQ.png', 'https://miro.medium.com/max/60/1*-1KSg6htzIVkhik-mYlI3g.png?q=20', 'https://miro.medium.com/max/60/1*w4MytlTvazK3WrrWX_igdQ.png?q=20', 'https://miro.medium.com/max/60/1*7g5dq4nK0a-IouTz0TAmhQ.png?q=20', 'https://miro.medium.com/max/60/1*y7yLNaBpL-W8XhbHPDKCWA.png?q=20', 'https://miro.medium.com/max/60/1*4xJa9iNtWE3UgMg1Ev3_Cg.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/fit/c/96/96/0*meZNwT4b7EZKRk_W.jpg', 'https://miro.medium.com/max/552/1*8sYH9hOqKh3pn6E5xAYZqw.png', 'https://miro.medium.com/max/60/1*Ho_JUrKAJRrEls4j9PTWOA.png?q=20', 'https://miro.medium.com/max/1194/1*JtcUlkMYWnRoPW4Cz-RuWQ.png', 'https://miro.medium.com/max/2110/1*0mmGZ9le8HGDgehb8HvVjg.png', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2740/1*27kTpKul12g0qTVwrUJdCg.png', 'https://miro.medium.com/max/60/1*bfS1uMi1B48cCaQ4vjEHkQ.png?q=20', 'https://miro.medium.com/max/2128/1*OLJ1FOa_osH_MDP9ZCAthg.png', 'https://miro.medium.com/max/2782/1*Ho_JUrKAJRrEls4j9PTWOA.png', 'https://miro.medium.com/max/2240/1*TkJ7xdehJd0BYfsrwLDjEQ.png', 'https://miro.medium.com/max/60/1*2nR9MlkNo8XqVxjbYqSJLA.png?q=20', 'https://miro.medium.com/max/1162/1*4xJa9iNtWE3UgMg1Ev3_Cg.png', 'https://miro.medium.com/max/1982/1*y7yLNaBpL-W8XhbHPDKCWA.png', 'https://miro.medium.com/max/1100/1*eH0TG4ZcRHRf6NJYc4sNFg.png', 'https://miro.medium.com/max/60/1*YyAECsezObEJmDKfRy9Wyg.png?q=20', 'https://miro.medium.com/max/60/1*vo6yLWfmPVDZrWrsR167Ww.png?q=20', 'https://miro.medium.com/max/60/1*eH0TG4ZcRHRf6NJYc4sNFg.png?q=20', 'https://miro.medium.com/max/60/1*YjchT3yJChgkQMCDDa6jcw.png?q=20', 'https://miro.medium.com/max/1652/1*SRlHpmU9T6SDQVKNZCOU_w.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1402/1*5UubjksgDRrfjJyJcDRmuA.png', 'https://miro.medium.com/max/60/1*facSnbOLTLj7Qsp02tcZGQ.png?q=20', 'https://miro.medium.com/max/1306/1*YjchT3yJChgkQMCDDa6jcw.png', 'https://miro.medium.com/max/2202/1*vo6yLWfmPVDZrWrsR167Ww.png', 'https://miro.medium.com/max/276/1*8sYH9hOqKh3pn6E5xAYZqw.png', 'https://miro.medium.com/max/60/1*27kTpKul12g0qTVwrUJdCg.png?q=20', 'https://miro.medium.com/max/2004/1*facSnbOLTLj7Qsp02tcZGQ.png', 'https://miro.medium.com/max/1946/1*YyAECsezObEJmDKfRy9Wyg.png', 'https://miro.medium.com/fit/c/160/160/0*meZNwT4b7EZKRk_W.jpg', 'https://miro.medium.com/max/60/1*-XgSqd5e409IEJDAsioQ0g.png?q=20', 'https://miro.medium.com/max/60/1*SRlHpmU9T6SDQVKNZCOU_w.png?q=20', 'https://miro.medium.com/max/60/1*OLJ1FOa_osH_MDP9ZCAthg.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/1916/1*7g5dq4nK0a-IouTz0TAmhQ.png', 'https://miro.medium.com/max/60/1*JtcUlkMYWnRoPW4Cz-RuWQ.png?q=20', 'https://miro.medium.com/max/1572/1*w4MytlTvazK3WrrWX_igdQ.png', 'https://miro.medium.com/max/60/1*0mmGZ9le8HGDgehb8HvVjg.png?q=20', 'https://miro.medium.com/max/60/1*TkJ7xdehJd0BYfsrwLDjEQ.png?q=20', 'https://miro.medium.com/max/1298/1*-1KSg6htzIVkhik-mYlI3g.png', 'https://miro.medium.com/max/60/1*5UubjksgDRrfjJyJcDRmuA.png?q=20'}",2020-03-05 00:27:22.327501,2.521590232849121
https://towardsdatascience.com/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b,"Building and Testing Recommender Systems With Surprise, Step-By-Step","Building and Testing Recommender Systems With Surprise, Step-By-Step

Learn how to build your own recommendation engine with the help of Python and Surprise Library, Collaborative Filtering

Recommender systems are one of the most common used and easily understandable applications of data science. Lots of work has been done on this topic, the interest and demand in this area remains very high because of the rapid growth of the internet and the information overload problem. It has become necessary for online businesses to help users to deal with information overload and provide personalized recommendations, content and services to them.

Two of the most popular ways to approach recommender systems are collaborative filtering and content-based recommendations. In this post, we will focus on the collaborative filtering approach, that is: the user is recommended items that people with similar tastes and preferences liked in the past. In another word, this method predicts unknown ratings by using the similarities between users.

We’ll be working with the Book-Crossing, a book ratings data set to develop recommendation system algorithms, with the Surprise library, which was built by Nicolas Hug. Let’s get started!

The Data

The Book-Crossing data comprises three tables, we will use two of them: The users table and the book ratings table.

user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")

user.columns = ['userID', 'Location', 'Age']

rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")

rating.columns = ['userID', 'ISBN', 'bookRating']

df = pd.merge(user, rating, on='userID', how='inner')

df.drop(['Location', 'Age'], axis=1, inplace=True)

df.head()

Figure 1

EDA

Ratings Distribution

ratings_distribution.py

Figure 2

We can see that over 62% of all ratings in the data are 0, and very few ratings are 1 or 2, or 3, low rating books mean they are generally really bad.

Ratings Distribution By Book

ratings_distribution_by_book.py

Figure 3

df.groupby('ISBN')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]

Figure 4

Most of the books in the data received less than 5 ratings, and very few books have many ratings, although the most rated book has received 2,502 ratings.

Ratings Distribution By User

ratings_distribution_by_user.py

Figure 5

df.groupby('userID')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]

Figure 6

Most of the users in the data gave less than 5 ratings, and not many users gave many ratings, although the most productive user have given 13,602 ratings.

I’m sure you have noticed that the above two plots share the same distribution. The number of ratings per book and the number of ratings per user decay exponentially.

To reduce the dimensionality of the data set, and avoid running into “memory error”, we will filter out rarely rated books and rarely rating users.

filter_dataframe.py

Figure 7

Surprise

To load a data set from the above pandas data frame, we will use the load_from_df() method, we will also need a Reader object, and the rating_scale parameter must be specified. The data frame must have three columns, corresponding to the user ids, the item ids, and the ratings in this order. Each row thus corresponds to a given rating.

reader = Reader(rating_scale=(0, 9))

data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)

With the Surprise library, we will benchmark the following algorithms:

Basic algorithms

NormalPredictor

NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal. This is one of the most basic algorithms that do not do much work.

BaselineOnly

BaselineOnly algorithm predicts the baseline estimate for given user and item.

k-NN algorithms

KNNBasic

KNNBasic is a basic collaborative filtering algorithm.

KNNWithMeans

KNNWithMeans is basic collaborative filtering algorithm, taking into account the mean ratings of each user.

KNNWithZScore

KNNWithZScore is a basic collaborative filtering algorithm, taking into account the z-score normalization of each user.

KNNBaseline

KNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating.

Matrix Factorization-based algorithms

SVD

SVD algorithm is equivalent to Probabilistic Matrix Factorization

SVDpp

The SVDpp algorithm is an extension of SVD that takes into account implicit ratings.

NMF

NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. It is very similar with SVD.

Slope One

SlopeOne is a straightforward implementation of the SlopeOne algorithm.

Co-clustering

Coclustering is a collaborative filtering algorithm based on co-clustering.

We use “rmse” as our accuracy metric for the predictions.

benchmark.py

Figure 8

Train and Predict

BaselineOnly algorithm gave us the best rmse, therefore, we will train and predict with BaselineOnly and use Alternating Least Squares (ALS).

print('Using ALS')

bsl_options = {'method': 'als',

'n_epochs': 5,

'reg_u': 12,

'reg_i': 5

}

algo = BaselineOnly(bsl_options=bsl_options)

cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)

Figure 9

We use the train_test_split() to sample a trainset and a testset with given sizes, and use the accuracy metric of rmse. We’ll then use the fit() method which will train the algorithm on the trainset, and the test() method which will return the predictions made from the testset.

trainset, testset = train_test_split(data, test_size=0.25)

algo = BaselineOnly(bsl_options=bsl_options)

predictions = algo.fit(trainset).test(testset)

accuracy.rmse(predictions)

Figure 10

To inspect our predictions in details, we are going to build a pandas data frame with all the predictions. The following code were largely taken from this notebook.

predictions_details.py

Best Predictions:

Figure 11

The above are the best predictions, and they are not lucky guesses. Because Ui is anywhere between 25 to 146, they are not really small, meaning that significant number of users have rated the target book.

Worst predictions:

Figure 12

The worst predictions look pretty surprise. Let’s look in more details of the last one ISBN “055358264X”. The book was rated by 47 users, user “26544” rated 10, our BaselineOnly algorithm predicts this user would rate 0.

import matplotlib.pyplot as plt

%matplotlib notebook df_new.loc[df_new['ISBN'] == '055358264X']['bookRating'].hist()

plt.xlabel('rating')

plt.ylabel('Number of ratings')

plt.title('Number of ratings book ISBN 055358264X has received')

plt.show();

Figure 13

It turns out, most of the ratings this book received was 0, in another word, most of the users in the data rated this book 0, only very few users rated 10. Same with the other predictions in “worst predictions” list. It seems that for each prediction, the users are some kind of outsiders.

That was it! I hope you enjoyed the recommendation (or rather, a rating prediction) journey with Surprise. Jupyter notebook can be found on Github. Happy Holidays!

Reference: Surprise’ documentation","['algorithm', 'filtering', 'stepbystep', 'book', 'user', 'users', 'testing', 'surprise', 'ratings', 'data', 'systems', 'collaborative', 'recommender', 'rated', 'building']","Building and Testing Recommender Systems With Surprise, Step-By-StepLearn how to build your own recommendation engine with the help of Python and Surprise Library, Collaborative FilteringRecommender systems are one of the most common used and easily understandable applications of data science.
Two of the most popular ways to approach recommender systems are collaborative filtering and content-based recommendations.
We’ll be working with the Book-Crossing, a book ratings data set to develop recommendation system algorithms, with the Surprise library, which was built by Nicolas Hug.
KNNBaselineKNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating.
NMFNMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization.",en,['Susan Li'],2019-09-26 18:20:34.412000+00:00,"{'Collaborative Filtering', 'Data Science', 'Python', 'Recommendation System', 'Machine Learning'}","{'https://miro.medium.com/max/1876/1*D2UJoF1BiVHm8tBL5LWiHA.png', 'https://miro.medium.com/max/864/1*ROMqDkiIVYDl9rLtn7VB8A.png', 'https://miro.medium.com/max/60/1*QQzsp06yKOEXYj05tIVc7A.png?q=20', 'https://miro.medium.com/max/770/1*Ia6GPaH9HcxhFNuLCHC9XQ.png', 'https://miro.medium.com/max/60/1*dOM8OeGZq6FkquXQq-l7HA.jpeg?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*FXsZ2qM4fUA_82-p4XGaiw.png?q=20', 'https://miro.medium.com/max/1610/1*QQzsp06yKOEXYj05tIVc7A.png', 'https://miro.medium.com/max/1522/1*AY7A-Cy8wJ4N6ZA3a4ym7g.png', 'https://miro.medium.com/max/60/1*1IRSk5qEjDp_S6kf6jmb9w.png?q=20', 'https://miro.medium.com/max/60/1*_dzaXRoZ78z5So-RymrDTQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/792/1*S_EVTbhOHD-p9MoM2Mh-bg.png', 'https://miro.medium.com/max/926/1*s7AwIVfuWC8LYpVIxQPwKA.png', 'https://miro.medium.com/max/1830/1*omNqnQA37yEZOUmrHurJeQ.png', 'https://miro.medium.com/max/60/1*AY7A-Cy8wJ4N6ZA3a4ym7g.png?q=20', 'https://miro.medium.com/max/60/1*C2MvdF2DtWO2KDwdjYEWHA.png?q=20', 'https://miro.medium.com/max/2734/1*OiPHSMaQJKBhx3hxHxwDLQ.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/2746/1*1IRSk5qEjDp_S6kf6jmb9w.png', 'https://miro.medium.com/max/60/1*D2UJoF1BiVHm8tBL5LWiHA.png?q=20', 'https://miro.medium.com/max/1200/1*dOM8OeGZq6FkquXQq-l7HA.jpeg', 'https://miro.medium.com/fit/c/160/160/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/56/1*ROMqDkiIVYDl9rLtn7VB8A.png?q=20', 'https://miro.medium.com/max/2560/1*dOM8OeGZq6FkquXQq-l7HA.jpeg', 'https://miro.medium.com/max/1344/1*FXsZ2qM4fUA_82-p4XGaiw.png', 'https://miro.medium.com/max/60/1*s7AwIVfuWC8LYpVIxQPwKA.png?q=20', 'https://miro.medium.com/max/50/1*Ia6GPaH9HcxhFNuLCHC9XQ.png?q=20', 'https://miro.medium.com/max/60/1*omNqnQA37yEZOUmrHurJeQ.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/fit/c/96/96/1*ffGGDMNA116sjMHXgQNodg.jpeg', 'https://miro.medium.com/max/2232/1*_dzaXRoZ78z5So-RymrDTQ.png', 'https://miro.medium.com/max/2770/1*C2MvdF2DtWO2KDwdjYEWHA.png', 'https://miro.medium.com/max/60/1*S_EVTbhOHD-p9MoM2Mh-bg.png?q=20', 'https://miro.medium.com/max/60/1*OiPHSMaQJKBhx3hxHxwDLQ.png?q=20'}",2020-03-05 00:27:29.095017,6.766484022140503
https://towardsdatascience.com/how-to-build-a-simple-song-recommender-296fcbc8c85,How to build a simple song recommender system,"This blog post is inspired by Siraj Raval’s Deep Learning Foundation Nanodegree at Udacity. Then repo of this exercise can be found here.

Building a recommendation system is a common task that is faced by Amazon, Netflix, Spotify and Google. The underlying goal of the recommendation system is to personalize content and identify relevant data for our audiences. These contents can be articles, movies, games, etc

There are 3 types of recommendation system: content-based, collaborative and popularity.

In this exercise, we will learn how to build a music recommendation system using real data. Our Million Songs Dataset contains of two files: triplet_file and metadata_file. The triplet_file contains user_id, song_id and listen time. The metadat_file contains song_id, title, release_by and artist_name. Million Songs Dataset is a mixture of song from various website with the rating that users gave after listening to the song. A few examples are Last.fm, thisismyjam, musixmatch, etc

Our first job is to integrate our dataset, which is very important every time we want to build a data processing pipeline.To integrate both triplet_file and metadata_file, we are going to use a popular Python library called pandas

We first define the two files we are going to work with:

triplets_file = 'https://static.turi.com/datasets/millionsong/10000.txt' songs_metadata_file = 'https://static.turi.com/datasets/millionsong/song_data.csv'

We then read the table of triplet_file using pandas and define the 3 columns as user_id, song_id and listen_count ( df here means dataframe)

song_df_1 = pandas.read_table(triplets_file,header=None)

song_df_1.columns = ['user_id', 'song_id', 'listen_count']

We also read the metadat_file and going to combine the metadata_file with triplets_file. Whenever you combine 2 or more datasets, there will be duplicate columns. Here we drop the duplicates between 2 datasets using song_id



song_df_2 = pandas.read_csv(songs_metadata_file)



song_df = pandas.merge(song_df_1, song_df_2.drop_duplicates(['song_id']), on=""song_id"", how=""left"")

Using command song_df.head() allows us to visualize the combined data set:

Here we have the song index, user_id, song_id, listen_count, title, release and artist_name. Running len(song_df) returns the the total length of this dataset indexed by song are 2,000,000.

The second step of this exercise is data transformation, where we’re going to select a subset of this data (the first 10,000 songs). We then merge the song and artist_name into one column, aggregated by number of time a particular song is listened too in general by all users. The first line in the code below group the song_df by number of listen_count ascending. The second line calculate the group_sum by summing the listen_count of each song. The third line add a new column called percentage , and calculate this percentage by dividing the listen_count by the sum of listen_count of all songs and then multiply by 100. The last line list the song in the ascending order of popularity for a given song

song_grouped = song_df.groupby(['song']).agg({'listen_count': 'count'}).reset_index()

grouped_sum = song_grouped['listen_count'].sum()

song_grouped['percentage'] = song_grouped['listen_count'].div(grouped_sum)*100

song_grouped.sort_values(['listen_count', 'song'], ascending = [0,1])

Below are the example of our dataset after transformation step:

Doing data transformation allows us to further simplify our dataset and make it easy and simple to understand.

Next step, we’re going to follow a naive approach when building a recommendation system. We’re going to count the number of unique users and songs in our subset of data

users = song_df['user_id'].unique()

len(users) ## return 365 unique users songs = song_df['song'].unique()

len(songs) ## return 5151 unique songs

We then create a song recommender by splitting our dataset into training and testing data. We use the train_test_split function of scikit-learn library. It’s important to note that whenever we build a machine learning system, before we train our model, we always want to split our data into training and testing dataset

train_data, test_data = train_test_split(song_df, test_size = 0.20, random_state=0)

We arbitrarily pick 20% as our testing size. We then used a popularity based recommender class as a blackbox to train our model. We create an instance of popularity based recommender class and feed it with our training data. The code below achieves the following goal: based on the popularity of each song, create a recommender that accept a user_id as input and out a list of recommended song of that user

pm = Recommenders.popularity_recommender_py()

pm.create(train_data, 'user_id', 'song') #user the popularity model to make some prediction

user_id = users[5]

pm.recommend(user_id)

The code for the Recommender Systems model is below. This system is a naive approach and not personalized. It first get a unique count of user_id (ie the number of time that song was listened to in general by all user) for each song and tag it as a recommendation score. The recommend function then accept a user_id and output the top ten recommended song for any given user. Keeping in my that since this is the naive approach, the recommendation is not personalized and will be the same for all users.

#Class for Popularity based Recommender System modelclass popularity_recommender_py():

def __init__(self):

self.train_data = None

self.user_id = None

self.item_id = None

self.popularity_recommendations = None #Create the popularity based recommender system model

def create(self, train_data, user_id, item_id):

self.train_data = train_data

self.user_id = user_id

self.item_id = item_id



#Get a count of user_ids for each unique song as recommendation score

train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: 'count'}).reset_index()

train_data_grouped.rename(columns = {'user_id': 'score'},inplace=True) #Sort the songs based upon recommendation score

train_data_sort = train_data_grouped.sort_values(['score', self.item_id], ascending = [0,1]) #Generate a recommendation rank based upon score

train_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first') #Get the top 10 recommendations

self.popularity_recommendations = train_data_sort.head(10) #Use the popularity based recommender system model to

#make recommendations def recommend(self, user_id): user_recommendations = self.popularity_recommendations #Add user_id column for which the recommendations are being generated user_recommendations['user_id'] = user_id #Bring user_id column to the front

cols = user_recommendations.columns.tolist()

cols = cols[-1:] + cols[:-1]

user_recommendations = user_recommendations[cols]

return user_recommendations

The second part of this exercise is to create a ML personalized song recommender system by leveraging the item similarity based collaborative filtering model. Recall that recommender system is divided into 2 types: content based and collaborative based. Content based system predicts what a user like based on what that user like in the past. Collaborative based system predict what a particular user like based on what other similar users like. Most companies like Netflix and Hulu use the hybrid approach, which provide recommendation based on the combination of what content a user like in the past as well as what other similar user like.

According to Agnes Jóhannsdóttir (Twitter: @agnesjohanns) at Cambridge Coding Academy, Memory-based collaborative filtering can be divided into two main approaches: user-item filtering and item-item filtering.

Item-item filtering approach involves defining a co-occurrence matrix based on a song a user likes. We are seeking to answer a question, for each song, what a number of time a user, who have listened to that song, will also listen to another set of other songs. To further simplify this, based on what you like in the past, what other similar song that you will like based on what other similar user have liked. Let’s apply this to our code. First we create an instance item similarity based recommender class and feed it with our training data.

is_model = Recommenders.item_similarity_recommender_py()

is_model.create(train_data, 'user_id', 'song')

Notice that inside the recommender system’s source code, the generate_top_recommendations function calculated a weighted average of the scores in cooccurence matrix for all user song. This cooccurence matrix will tend to be sparse matrix because it’s not possible to predict if a user like a particular song, whether or not he/she will like a million other song. The possibility is so vast. Using our model, we will be able to predict the list of song that a user will like

#Print the songs for the user in training data

user_id = users[5]

user_items = is_model.get_user_items(user_id)

#

print(""------------------------------------------------------------------------------------"")

print(""Training data songs for the user userid: %s:"" % user_id)

print(""------------------------------------------------------------------------------------"")



for user_item in user_items:

print(user_item)



print(""----------------------------------------------------------------------"")

print(""Recommendation process going on:"")

print(""----------------------------------------------------------------------"")



#Recommend songs for the user using personalized model

is_model.recommend(user_id)

output:

------------------------------------------------------------------------------------

Training data songs for the user userid: 4bd88bfb25263a75bbdd467e74018f4ae570e5df:

------------------------------------------------------------------------------------

Just Lose It - Eminem

Without Me - Eminem

16 Candles - The Crests

Speechless - Lady GaGa

Push It - Salt-N-Pepa

Ghosts 'n' Stuff (Original Instrumental Mix) - Deadmau5

Say My Name - Destiny's Child

My Dad's Gone Crazy - Eminem / Hailie Jade

The Real Slim Shady - Eminem

Somebody To Love - Justin Bieber

Forgive Me - Leona Lewis

Missing You - John Waite

Ya Nada Queda - Kudai

----------------------------------------------------------------------

Recommendation process going on:

----------------------------------------------------------------------

No. of unique songs for the user: 13

no. of unique songs in the training set: 4483

Non zero values in cooccurence_matrix :2097

We can also use our item similarity based collaborative filtering model to find similar songs to any songs in our dataset:

is_model.get_similar_items(['U Smile - Justin Bieber'])

this output

no. of unique songs in the training set: 4483

Non zero values in cooccurence_matrix :271

It’s worth to note that this method is not Deep Learning but purely based on linear algebra.

To recap, in this exercise we discussed 2 models. The first model is popularity based recommender, meaning it is not personalized toward any user and will output the same list of recommended songs. The second model is personalized recommender leveraging the item similarity based collaborative filtering model (ie the cooccurence matrix) to find a personalized list of song that a user might like based on what other similar user have liked.

Next we will discuss how to measure the performance of these two models using a precision recall curve to quantitatively compare the popularity based model and personalized collaborative filtering model.

To quantitatively measure the performance of recommender system, we use three different metrics: Precision , Recall and F-1 Score

According to Marcel Caraciolo, Precision is “the proportion of top results that are relevant, considering some definition of relevant for your problem domain”. In our case, the definition of relevant for our problem domain is the length that a song is listened to, a number of user have all liked the song. Recall would “measure the proportion of all relevant results included in the top results”.In our case, it means precision seeks to measure the relevancy of songs in relation to the top ten results of recommended song, whereas recall seeks to measure the relevancy of songs in relation to all the songs

Observing the precision recall curve of both our popularity based model and personalized item similarity model, item similarity model perform better (ie having higher number of recall and precision) up to certain point in precision-recall curve.

The last type of recommender system is Matrix Factorization based Recommender System. This type of recommender system uses what is called a Singular Value Decomposition (SVD) factorized matrix of the original similarity matrix to build recommender system.

To compute SVD and recommendations, we use the following code:

#constants defining the dimensions of our User Rating Matrix (URM) MAX_PID = 4

MAX_UID = 5 #Compute SVD of the user ratings matrix def computeSVD(urm, K):

U, s, Vt = sparsesvd(urm, K)

dim = (len(s), len(s))

S = np.zeros(dim, dtype=np.float32)

for i in range(0, len(s)):

S[i,i] = mt.sqrt(s[i])

U = csc_matrix(np.transpose(U), dtype=np.float32)

S = csc_matrix(S, dtype=np.float32)

Vt = csc_matrix(Vt, dtype=np.float32)

return U, S, Vt

In this code, U represents user vector, S represents the item vector.Vt represent the joint of these two vectors as collection of points (ie vector) in 2 dimensional spaces. We’re going to use these vectors to measure the distance from one user’s preferences to another user’s preferences.

In another word, we are vectorizing matrices in order to compute the distance between matrices. To further clarify this, we’re going to talk through an example. Assume we have a user song matrix below:

Song0 Song1 Song2 Song3

User0 3 1 2 3

User1 4 3 4 3

User2 3 2 1 5

User3 1 6 5 2

User4 0 0 5 0

Once we perform SVD, the output is going to be vectors and measuring distance between vectors gives us recommendation

#Compute estimated rating for the test user

def computeEstimatedRatings(urm, U, S, Vt, uTest, K, test):

rightTerm = S*Vt estimatedRatings = np.zeros(shape=(MAX_UID, MAX_PID), dtype=np.float16)

for userTest in uTest:

prod = U[userTest, :]*rightTerm

#we convert the vector to dense format in order to get the #indices

#of the movies with the best estimated ratings

estimatedRatings[userTest, :] = prod.todense()

recom = (-estimatedRatings[userTest, :]).argsort()[:250]

return recom

#Used in SVD calculation (number of latent factors)

K=2 #Initialize a sample user rating matrix

urm = np.array([[3, 1, 2, 3],[4, 3, 4, 3],[3, 2, 1, 5], [1, 6, 5, 2], [5, 0,0 , 0]])

urm = csc_matrix(urm, dtype=np.float32) #Compute SVD of the input user ratings matrix

U, S, Vt = computeSVD(urm, K) #Test user set as user_id 4 with ratings [0, 0, 5, 0]

uTest = [4]

print(""User id for whom recommendations are needed: %d"" % uTest[0]) #Get estimated rating for test user

print(""Predictied ratings:"")

uTest_recommended_items = computeEstimatedRatings(urm, U, S, Vt, uTest, K, True)

print(uTest_recommended_items)

will output:

User id for whom recommendations are needed: 4

Predictied ratings:

[0 3 2 1]

Next, we discuss the real world example of how Hulu applying Deep Learning to Collaborative Filtering to build its industry leading recommendation system. At Hulu, features like Personalized Masthead, Watchlist and Top Picks are all powered by collaborative filtering.

The method Hulu used is CF-NADE. Let’s take an example. Suppose we have 4 movies: “Transformers”, “SpongeBob”, “Teenage Mutant Ninja Turtles” and “Interstellar”, with scores 4,2,3 and 5. In CF-NADE, the joint probability of vector (4,2,3,5) is factorized as a product of conditionals by chain rule, which are the following:

1/ The probability that the user gives “Transformers” 4-star conditioned on nothing; 2/ The probability that the user gives “SpongeBob” 2-star conditioned on giving “Transformers” 4-star; 3/ The probability that the user gives “Teenage Mutant Ninja Turtles” a 3-star conditioned on giving 4-star and 2-star to “Transformers” and “SpongeBob”, respectively; 4/ The probability that the user gives “Interstellar” a 5-star conditioned on giving 4-star, 2-star and 3-star to “Transformers”, “SpongeBob” and “Teenage Mutant Ninja Turtles”, respectively;

To summarize, this is the chain of probability based on what previously have occurred. Each conditional is modeled by its own neural network and the parameter for all of these neural networks are shared amongst all models.

Source:

1/Siraj Raval’s Deep Learning Foundation Nanodegree (https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101)

2/https://www.youtube.com/watch?v=18adykNGhHU

3/https://github.com/llSourcell/recommender_live

4/Applying Deep Learning to Collaborative Filtering: How Hulu builds its industry leading(http://tech.hulu.com/blog/2016/08/01/cfnade.html)

5/Implementing your own recommender systems in Python(http://online-dev.cambridgecoding.com/notebooks/eWReNYcAfB/implementing-your-own-recommender-systems-in-python-2)","['songs', 'user_id', 'user', 'recommendation', 'simple', 'based', 'model', 'build', 'data', 'system', 'recommender', 'song']","Building a recommendation system is a common task that is faced by Amazon, Netflix, Spotify and Google.
These contents can be articles, movies, games, etcThere are 3 types of recommendation system: content-based, collaborative and popularity.
In this exercise, we will learn how to build a music recommendation system using real data.
The last type of recommender system is Matrix Factorization based Recommender System.
This type of recommender system uses what is called a Singular Value Decomposition (SVD) factorized matrix of the original similarity matrix to build recommender system.",en,['Eric Le'],2017-04-26 17:22:37.460000+00:00,"{'Deep Learning', 'Data Science', 'Machine Learning', 'Towards Data Science', 'Linear Algebra'}","{'https://miro.medium.com/max/60/1*xOy6s_J9a7CGZIrv_cWr5A.png?q=20', 'https://miro.medium.com/max/3060/1*bTjP-ls6s8NOY8mR0HXoJg.png', 'https://miro.medium.com/max/60/1*IeXtERBoZtbkxQJscZNX2w.png?q=20', 'https://miro.medium.com/fit/c/80/80/1*BRLQ-7VjBxPLuBW6vG3rgw.jpeg', 'https://miro.medium.com/fit/c/160/160/1*hVxgUA6kP-PgL5TJjuyePg.png', 'https://miro.medium.com/max/60/1*7F-Icfi0o2EL4-eRMXH14g.png?q=20', 'https://miro.medium.com/fit/c/80/80/0*DKoctRl5mJ-xoQIK.', 'https://miro.medium.com/max/2272/1*gVEWkr_Fj_hw9CxvYLhGxw.png', 'https://miro.medium.com/max/2084/1*XtYH4816dIhqDXtWzGdFVA.png', 'https://miro.medium.com/max/1560/1*xOy6s_J9a7CGZIrv_cWr5A.png', 'https://miro.medium.com/max/200/1*mG6i4Bh_LgixUYXJgQpYsg@2x.png', 'https://miro.medium.com/max/1200/1*Fd8wNWDC5Wmh2R58PzZSNw.jpeg', 'https://miro.medium.com/fit/c/96/96/1*9eaLImkxVvCEYuFotVsxsA.jpeg', 'https://miro.medium.com/max/60/1*bTjP-ls6s8NOY8mR0HXoJg.png?q=20', 'https://miro.medium.com/max/2188/1*IeXtERBoZtbkxQJscZNX2w.png', 'https://miro.medium.com/max/60/1*gVEWkr_Fj_hw9CxvYLhGxw.png?q=20', 'https://miro.medium.com/fit/c/160/160/1*9eaLImkxVvCEYuFotVsxsA.jpeg', 'https://miro.medium.com/max/60/1*XtYH4816dIhqDXtWzGdFVA.png?q=20', 'https://miro.medium.com/fit/c/80/80/2*QpFzamkOYHha2ouIq7-5YQ.png', 'https://miro.medium.com/max/2520/1*Fd8wNWDC5Wmh2R58PzZSNw.jpeg', 'https://miro.medium.com/max/3356/1*7F-Icfi0o2EL4-eRMXH14g.png', 'https://miro.medium.com/max/60/1*Fd8wNWDC5Wmh2R58PzZSNw.jpeg?q=20'}",2020-03-05 00:27:36.010377,6.914368629455566
